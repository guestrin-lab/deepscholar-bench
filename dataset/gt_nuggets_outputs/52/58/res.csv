qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2504.20458v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
Conversational recommendation systems (CRSs) use multi-turn interaction to
capture user preferences and provide personalized recommendations. A
fundamental challenge in CRSs lies in effectively understanding user
preferences from conversations. User preferences can be multifaceted and
complex, posing significant challenges for accurate recommendations even with
access to abundant external knowledge. While interaction with users can clarify
their true preferences, frequent user involvement can lead to a degraded user
experience.
  To address this problem, we propose a generative reward model based simulated
user, named GRSU, for automatic interaction with CRSs. The simulated user
provides feedback to the items recommended by CRSs, enabling them to better
capture intricate user preferences through multi-turn interaction. Inspired by
generative reward models, we design two types of feedback actions for the
simulated user: i.e., generative item scoring, which offers coarse-grained
feedback, and attribute-based item critique, which provides fine-grained
feedback. To ensure seamless integration, these feedback actions are unified
into an instruction-based format, allowing the development of a unified
simulated user via instruction tuning on synthesized data. With this simulated
user, automatic multi-turn interaction with CRSs can be effectively conducted.
Furthermore, to strike a balance between effectiveness and efficiency, we draw
inspiration from the paradigm of reward-guided search in complex reasoning
tasks and employ beam search for the interaction process. On top of this, we
propose an efficient candidate ranking method to improve the recommendation
results derived from interaction. Extensive experiments on public datasets
demonstrate the effectiveness, efficiency, and transferability of our approach.","[{'text': 'Conversational recommender systems (CRSs) use multi-turn interactions', 'importance': 'vital', 'assignment': 'support'}, {'text': 'CRSs elicit user preferences via free-form natural language conversations', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Challenge: understanding multifaceted, complex user preferences in CRSs', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Frequent user involvement can degrade user experience in CRSs', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Simulated users can automate CRS interaction and preference elicitation', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Instruction tuning enables unified simulated user development', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Beam search balances effectiveness and efficiency in CRS interaction', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'GRSU: generative reward model-based simulated user for CRSs', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'CRSs optimize interaction policy with pre-defined actions and templates', 'importance': 'okay', 'assignment': 'support'}, {'text': 'External knowledge sources: knowledge graphs, LLMs, conversational corpora', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Generative reward models unify generation and reward modeling', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Best-of-N strategy: reward models rank and select candidate solutions', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Generative reward models represent reward as token probabilities', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Critiques in generation improve reward modeling', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Candidate ranking methods improve recommendation results', 'importance': 'okay', 'assignment': 'not_support'}]","[{'text': 'Conversational recommender systems (CRSs) use multi-turn interactions', 'importance': 'vital', 'assignment': 'support'}, {'text': 'CRSs elicit user preferences via free-form natural language conversations', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Simulated users can automate CRS interaction and preference elicitation', 'importance': 'vital', 'assignment': 'support'}, {'text': 'CRSs optimize interaction policy with pre-defined actions and templates', 'importance': 'okay', 'assignment': 'support'}, {'text': 'External knowledge sources: knowledge graphs, LLMs, conversational corpora', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Generative reward models unify generation and reward modeling', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Best-of-N strategy: reward models rank and select candidate solutions', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Generative reward models represent reward as token probabilities', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Critiques in generation improve reward modeling', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Conversational recommender systems (CRSs) use multi-turn interactions', 'importance': 'vital', 'assignment': 'support'}, {'text': 'CRSs elicit user preferences via free-form natural language conversations', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Challenge: understanding multifaceted, complex user preferences in CRSs', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Simulated users can automate CRS interaction and preference elicitation', 'importance': 'vital', 'assignment': 'support'}, {'text': 'GRSU: generative reward model-based simulated user for CRSs', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'CRSs optimize interaction policy with pre-defined actions and templates', 'importance': 'okay', 'assignment': 'support'}, {'text': 'External knowledge sources: knowledge graphs, LLMs, conversational corpora', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Generative reward models unify generation and reward modeling', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Best-of-N strategy: reward models rank and select candidate solutions', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Generative reward models represent reward as token probabilities', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Critiques in generation improve reward modeling', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 0.375, 'strict_all_score': 0.6, 'vital_score': 0.5, 'all_score': 0.6666666666666666}"
