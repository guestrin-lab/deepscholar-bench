qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2505.07166v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
Dense retrievers utilize pre-trained backbone language models (e.g., BERT,
LLaMA) that are fine-tuned via contrastive learning to perform the task of
encoding text into sense representations that can be then compared via a
shallow similarity operation, e.g. inner product. Recent research has
questioned the role of fine-tuning vs. that of pre-training within dense
retrievers, specifically arguing that retrieval knowledge is primarily gained
during pre-training, meaning knowledge not acquired during pre-training cannot
be sub-sequentially acquired via fine-tuning. We revisit this idea here as the
claim was only studied in the context of a BERT-based encoder using DPR as
representative dense retriever. We extend the previous analysis by testing
other representation approaches (comparing the use of CLS tokens with that of
mean pooling), backbone architectures (encoder-only BERT vs. decoder-only
LLaMA), and additional datasets (MSMARCO in addition to Natural Questions). Our
study confirms that in DPR tuning, pre-trained knowledge underpins retrieval
performance, with fine-tuning primarily adjusting neuron activation rather than
reorganizing knowledge. However, this pattern does not hold universally, such
as in mean-pooled (Contriever) and decoder-based (LLaMA) models. We ensure full
reproducibility and make our implementation publicly available at
https://github.com/ielab/DenseRetriever-Knowledge-Acquisition.","[{'text': 'Dense retrievers encode text into dense vectors for retrieval', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Pre-trained language models (BERT, LLaMA) serve as retriever backbones', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Fine-tuning via contrastive learning adapts retrievers to retrieval tasks', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Retrieval knowledge is mainly acquired during pre-training, not fine-tuning', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Fine-tuning in DPR adjusts neuron activation, not knowledge organization', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Contriever and decoder-based models show different fine-tuning effects', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Encoder-based retrievers use dot product for similarity', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Mean pooling captures distributed information in long passages', 'importance': 'vital', 'assignment': 'support'}, {'text': 'DPR uses encoder-based BERT with [CLS] token for representation', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Mean pooling (Contriever) averages token embeddings for document representation', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Decoder-based retrievers (e.g., LLaMA, RePLAMA) use autoregressive decoding', 'importance': 'okay', 'assignment': 'partial_support'}, {'text': 'MS MARCO and Natural Questions are standard dense retrieval benchmarks', 'importance': 'okay', 'assignment': 'support'}, {'text': 'SimCSE and Sentence-BERT enable zero-shot dense retrieval', 'importance': 'okay', 'assignment': 'support'}, {'text': 'PromptReps uses prompt-based learning for dense retrieval', 'importance': 'okay', 'assignment': 'support'}, {'text': 'RePLAMA reframes retrieval as sequence generation', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Dense retrievers encode text into dense vectors for retrieval', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Encoder-based retrievers use dot product for similarity', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Mean pooling captures distributed information in long passages', 'importance': 'vital', 'assignment': 'support'}, {'text': 'DPR uses encoder-based BERT with [CLS] token for representation', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Mean pooling (Contriever) averages token embeddings for document representation', 'importance': 'okay', 'assignment': 'support'}, {'text': 'MS MARCO and Natural Questions are standard dense retrieval benchmarks', 'importance': 'okay', 'assignment': 'support'}, {'text': 'SimCSE and Sentence-BERT enable zero-shot dense retrieval', 'importance': 'okay', 'assignment': 'support'}, {'text': 'PromptReps uses prompt-based learning for dense retrieval', 'importance': 'okay', 'assignment': 'support'}, {'text': 'RePLAMA reframes retrieval as sequence generation', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Dense retrievers encode text into dense vectors for retrieval', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Pre-trained language models (BERT, LLaMA) serve as retriever backbones', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Fine-tuning via contrastive learning adapts retrievers to retrieval tasks', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Contriever and decoder-based models show different fine-tuning effects', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Encoder-based retrievers use dot product for similarity', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Mean pooling captures distributed information in long passages', 'importance': 'vital', 'assignment': 'support'}, {'text': 'DPR uses encoder-based BERT with [CLS] token for representation', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Mean pooling (Contriever) averages token embeddings for document representation', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Decoder-based retrievers (e.g., LLaMA, RePLAMA) use autoregressive decoding', 'importance': 'okay', 'assignment': 'partial_support'}, {'text': 'MS MARCO and Natural Questions are standard dense retrieval benchmarks', 'importance': 'okay', 'assignment': 'support'}, {'text': 'SimCSE and Sentence-BERT enable zero-shot dense retrieval', 'importance': 'okay', 'assignment': 'support'}, {'text': 'PromptReps uses prompt-based learning for dense retrieval', 'importance': 'okay', 'assignment': 'support'}, {'text': 'RePLAMA reframes retrieval as sequence generation', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 0.375, 'strict_all_score': 0.6, 'vital_score': 0.5625, 'all_score': 0.7333333333333333}"
