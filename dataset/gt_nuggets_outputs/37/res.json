{
  "qid": "2505.22194v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nVision Transformers (ViTs) leverage the transformer architecture to\neffectively capture global context, demonstrating strong performance in\ncomputer vision tasks. A major challenge in ViT hardware acceleration is that\nthe model family contains complex arithmetic operations that are sensitive to\nmodel accuracy, such as the Softmax and LayerNorm operations, which cannot be\nmapped onto efficient hardware with low precision. Existing methods only\nexploit parallelism in the matrix multiplication operations of the model on\nhardware and keep these complex operations on the CPU. This results in\nsuboptimal performance due to the communication overhead between the CPU and\naccelerator. Can new data formats solve this problem?\n  In this work, we present the first ViT accelerator that maps all operations\nof the ViT models onto FPGAs. We exploit a new arithmetic format named\nMicroscaling Integer (MXInt) for datapath designs and evaluate how different\ndesign choices can be made to trade off accuracy, hardware performance, and\nhardware utilization. Our contributions are twofold. First, we quantize ViTs\nusing the MXInt format, achieving both high area efficiency and accuracy.\nSecond, we propose MXInt-specific hardware optimization that map these complex\narithmetic operations into custom hardware. Within 1\\% accuracy loss, our\nmethod achieves at least 93$\\times$ speedup compared to Float16 and at least\n1.9$\\times$ speedup compared to related work.",
  "nuggets": [
    {
      "text": "MXInt quantization enables efficient ViT hardware acceleration",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Prior ViT accelerators use fixed-point quantization, not MXInt",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Existing ViT accelerators offload complex ops to CPU, causing overhead",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Most prior work accelerates only matrix multiplications on FPGA",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "MXInt-specific datapath optimization enables full ViT model acceleration",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "MXInt achieves high area efficiency and accuracy for ViTs",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "MXInt-based accelerator maps all ViT operations onto FPGA",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Darvish et al. introduced MXInt quantization for DNNs, not hardware",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "MXFP extends MXInt to non-integer shared components",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Block floating-point quantization groups values for shared digits",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Layer-wise, channel-wise, vector-wise quantization explored for CNNs",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "FACT and FlightLLM use mixed-precision quantization for transformers",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "GOBO and EdgeBERT use software-hardware co-design for transformer acceleration",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "MXInt accelerator achieves 93x speedup over Float16, 1.9x over prior work",
      "importance": "okay",
      "assignment": "not_support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "Prior ViT accelerators use fixed-point quantization, not MXInt",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "MXInt-specific datapath optimization enables full ViT model acceleration",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "MXInt-based accelerator maps all ViT operations onto FPGA",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Darvish et al. introduced MXInt quantization for DNNs, not hardware",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "MXFP extends MXInt to non-integer shared components",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Block floating-point quantization groups values for shared digits",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Layer-wise, channel-wise, vector-wise quantization explored for CNNs",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "FACT and FlightLLM use mixed-precision quantization for transformers",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "GOBO and EdgeBERT use software-hardware co-design for transformer acceleration",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "MXInt quantization enables efficient ViT hardware acceleration",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Prior ViT accelerators use fixed-point quantization, not MXInt",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Most prior work accelerates only matrix multiplications on FPGA",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "MXInt-specific datapath optimization enables full ViT model acceleration",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "MXInt-based accelerator maps all ViT operations onto FPGA",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Darvish et al. introduced MXInt quantization for DNNs, not hardware",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "MXFP extends MXInt to non-integer shared components",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Block floating-point quantization groups values for shared digits",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Layer-wise, channel-wise, vector-wise quantization explored for CNNs",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "FACT and FlightLLM use mixed-precision quantization for transformers",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "GOBO and EdgeBERT use software-hardware co-design for transformer acceleration",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.42857142857142855,
    "strict_all_score": 0.6428571428571429,
    "vital_score": 0.5714285714285714,
    "all_score": 0.7142857142857143
  }
}