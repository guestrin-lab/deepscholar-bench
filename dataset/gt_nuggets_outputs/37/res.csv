qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2505.22194v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
Vision Transformers (ViTs) leverage the transformer architecture to
effectively capture global context, demonstrating strong performance in
computer vision tasks. A major challenge in ViT hardware acceleration is that
the model family contains complex arithmetic operations that are sensitive to
model accuracy, such as the Softmax and LayerNorm operations, which cannot be
mapped onto efficient hardware with low precision. Existing methods only
exploit parallelism in the matrix multiplication operations of the model on
hardware and keep these complex operations on the CPU. This results in
suboptimal performance due to the communication overhead between the CPU and
accelerator. Can new data formats solve this problem?
  In this work, we present the first ViT accelerator that maps all operations
of the ViT models onto FPGAs. We exploit a new arithmetic format named
Microscaling Integer (MXInt) for datapath designs and evaluate how different
design choices can be made to trade off accuracy, hardware performance, and
hardware utilization. Our contributions are twofold. First, we quantize ViTs
using the MXInt format, achieving both high area efficiency and accuracy.
Second, we propose MXInt-specific hardware optimization that map these complex
arithmetic operations into custom hardware. Within 1\% accuracy loss, our
method achieves at least 93$\times$ speedup compared to Float16 and at least
1.9$\times$ speedup compared to related work.","[{'text': 'MXInt quantization enables efficient ViT hardware acceleration', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Prior ViT accelerators use fixed-point quantization, not MXInt', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Existing ViT accelerators offload complex ops to CPU, causing overhead', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Most prior work accelerates only matrix multiplications on FPGA', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'MXInt-specific datapath optimization enables full ViT model acceleration', 'importance': 'vital', 'assignment': 'support'}, {'text': 'MXInt achieves high area efficiency and accuracy for ViTs', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'MXInt-based accelerator maps all ViT operations onto FPGA', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Darvish et al. introduced MXInt quantization for DNNs, not hardware', 'importance': 'okay', 'assignment': 'support'}, {'text': 'MXFP extends MXInt to non-integer shared components', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Block floating-point quantization groups values for shared digits', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Layer-wise, channel-wise, vector-wise quantization explored for CNNs', 'importance': 'okay', 'assignment': 'support'}, {'text': 'FACT and FlightLLM use mixed-precision quantization for transformers', 'importance': 'okay', 'assignment': 'support'}, {'text': 'GOBO and EdgeBERT use software-hardware co-design for transformer acceleration', 'importance': 'okay', 'assignment': 'support'}, {'text': 'MXInt accelerator achieves 93x speedup over Float16, 1.9x over prior work', 'importance': 'okay', 'assignment': 'not_support'}]","[{'text': 'Prior ViT accelerators use fixed-point quantization, not MXInt', 'importance': 'vital', 'assignment': 'support'}, {'text': 'MXInt-specific datapath optimization enables full ViT model acceleration', 'importance': 'vital', 'assignment': 'support'}, {'text': 'MXInt-based accelerator maps all ViT operations onto FPGA', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Darvish et al. introduced MXInt quantization for DNNs, not hardware', 'importance': 'okay', 'assignment': 'support'}, {'text': 'MXFP extends MXInt to non-integer shared components', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Block floating-point quantization groups values for shared digits', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Layer-wise, channel-wise, vector-wise quantization explored for CNNs', 'importance': 'okay', 'assignment': 'support'}, {'text': 'FACT and FlightLLM use mixed-precision quantization for transformers', 'importance': 'okay', 'assignment': 'support'}, {'text': 'GOBO and EdgeBERT use software-hardware co-design for transformer acceleration', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'MXInt quantization enables efficient ViT hardware acceleration', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Prior ViT accelerators use fixed-point quantization, not MXInt', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Most prior work accelerates only matrix multiplications on FPGA', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'MXInt-specific datapath optimization enables full ViT model acceleration', 'importance': 'vital', 'assignment': 'support'}, {'text': 'MXInt-based accelerator maps all ViT operations onto FPGA', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Darvish et al. introduced MXInt quantization for DNNs, not hardware', 'importance': 'okay', 'assignment': 'support'}, {'text': 'MXFP extends MXInt to non-integer shared components', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Block floating-point quantization groups values for shared digits', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Layer-wise, channel-wise, vector-wise quantization explored for CNNs', 'importance': 'okay', 'assignment': 'support'}, {'text': 'FACT and FlightLLM use mixed-precision quantization for transformers', 'importance': 'okay', 'assignment': 'support'}, {'text': 'GOBO and EdgeBERT use software-hardware co-design for transformer acceleration', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 0.42857142857142855, 'strict_all_score': 0.6428571428571429, 'vital_score': 0.5714285714285714, 'all_score': 0.7142857142857143}"
