qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2504.17448v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
Active learning (AL) reduces human annotation costs for machine learning
systems by strategically selecting the most informative unlabeled data for
annotation, but performing it individually may still be insufficient due to
restricted data diversity and annotation budget. Federated Active Learning
(FAL) addresses this by facilitating collaborative data selection and model
training, while preserving the confidentiality of raw data samples. Yet,
existing FAL methods fail to account for the heterogeneity of data distribution
across clients and the associated fluctuations in global and local model
parameters, adversely affecting model accuracy. To overcome these challenges,
we propose CHASe (Client Heterogeneity-Aware Data Selection), specifically
designed for FAL. CHASe focuses on identifying those unlabeled samples with
high epistemic variations (EVs), which notably oscillate around the decision
boundaries during training. To achieve both effectiveness and efficiency,
\model{} encompasses techniques for 1) tracking EVs by analyzing inference
inconsistencies across training epochs, 2) calibrating decision boundaries of
inaccurate models with a new alignment loss, and 3) enhancing data selection
efficiency via a data freeze and awaken mechanism with subset sampling.
Experiments show that CHASe surpasses various established baselines in terms of
effectiveness and efficiency, validated across diverse datasets, model
complexities, and heterogeneous federation settings.","[{'text': 'Active learning (AL) reduces annotation costs by selecting informative data', 'importance': 'vital', 'assignment': 'support'}, {'text': 'AL methods perform well in federated learning (FL) with IID data', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Federated Active Learning (FAL) integrates AL into FL for collaborative selection', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Existing FAL methods often ignore client data heterogeneity', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Selections based on instant models can be unstable during training', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Epistemic variation (EV) measures model uncertainty over time', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'CHASe tracks EVs by analyzing inference inconsistencies across epochs', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'CHASe calibrates decision boundaries with a new alignment loss', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'CHASe improves data selection efficiency with freeze and awaken mechanism', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'CHASe outperforms baselines across datasets and federation settings', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'AL methods: uncertainty-based, distribution-based, loss-based', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Uncertainty-based AL uses prediction uncertainty, margin, or entropy', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Bayesian networks, MC-Dropout, Ensembles estimate model uncertainty', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Distribution-based AL selects diverse samples via clustering or core-set', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Core-set AL finds small, representative subsets for training', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Loss-based AL selects hard samples using loss functions', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LL4AL estimates uncertainty and diversity for sample selection', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Recent AL works align data distributions with adversarial classifiers', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Federated learning (FL) enables collaborative model training without sharing raw data', 'importance': 'okay', 'assignment': 'partial_support'}, {'text': 'Non-IID data in FL degrades model performance', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Some FAL works select samples using global or local models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Recent FAL studies consider intra- and inter-class diversity', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Active Federated Learning (AFL) selects clients, not data samples', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Some FL data selection works assume fully labeled data, not AL', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Active learning (AL) reduces annotation costs by selecting informative data', 'importance': 'vital', 'assignment': 'support'}, {'text': 'AL methods perform well in federated learning (FL) with IID data', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Existing FAL methods often ignore client data heterogeneity', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Selections based on instant models can be unstable during training', 'importance': 'vital', 'assignment': 'support'}, {'text': 'AL methods: uncertainty-based, distribution-based, loss-based', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Uncertainty-based AL uses prediction uncertainty, margin, or entropy', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Bayesian networks, MC-Dropout, Ensembles estimate model uncertainty', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Distribution-based AL selects diverse samples via clustering or core-set', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Core-set AL finds small, representative subsets for training', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Loss-based AL selects hard samples using loss functions', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LL4AL estimates uncertainty and diversity for sample selection', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Recent AL works align data distributions with adversarial classifiers', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Non-IID data in FL degrades model performance', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Some FAL works select samples using global or local models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Recent FAL studies consider intra- and inter-class diversity', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Active Federated Learning (AFL) selects clients, not data samples', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Some FL data selection works assume fully labeled data, not AL', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Active learning (AL) reduces annotation costs by selecting informative data', 'importance': 'vital', 'assignment': 'support'}, {'text': 'AL methods perform well in federated learning (FL) with IID data', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Federated Active Learning (FAL) integrates AL into FL for collaborative selection', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Existing FAL methods often ignore client data heterogeneity', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Selections based on instant models can be unstable during training', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Epistemic variation (EV) measures model uncertainty over time', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'AL methods: uncertainty-based, distribution-based, loss-based', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Uncertainty-based AL uses prediction uncertainty, margin, or entropy', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Bayesian networks, MC-Dropout, Ensembles estimate model uncertainty', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Distribution-based AL selects diverse samples via clustering or core-set', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Core-set AL finds small, representative subsets for training', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Loss-based AL selects hard samples using loss functions', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LL4AL estimates uncertainty and diversity for sample selection', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Recent AL works align data distributions with adversarial classifiers', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Federated learning (FL) enables collaborative model training without sharing raw data', 'importance': 'okay', 'assignment': 'partial_support'}, {'text': 'Non-IID data in FL degrades model performance', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Some FAL works select samples using global or local models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Recent FAL studies consider intra- and inter-class diversity', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Active Federated Learning (AFL) selects clients, not data samples', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Some FL data selection works assume fully labeled data, not AL', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 0.4, 'strict_all_score': 0.7083333333333334, 'vital_score': 0.5, 'all_score': 0.7708333333333334}"
