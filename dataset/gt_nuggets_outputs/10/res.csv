qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2505.22757v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
Multi-token prediction (MTP) is a recently proposed pre-training objective
for language models. Rather than predicting only the next token (NTP), MTP
predicts the next $k$ tokens at each prediction step, using multiple prediction
heads. MTP has shown promise in improving downstream performance, inference
speed, and training efficiency, particularly for large models. However, prior
work has shown that smaller language models (SLMs) struggle with the MTP
objective. To address this, we propose a curriculum learning strategy for MTP
training, exploring two variants: a forward curriculum, which gradually
increases the complexity of the pre-training objective from NTP to MTP, and a
reverse curriculum, which does the opposite. Our experiments show that the
forward curriculum enables SLMs to better leverage the MTP objective during
pre-training, improving downstream NTP performance and generative output
quality, while retaining the benefits of self-speculative decoding. The reverse
curriculum achieves stronger NTP performance and output quality, but fails to
provide any self-speculative decoding benefits.","[{'text': 'Curriculum learning benefits pre-training of encoder-only and decoder-only language models', 'importance': 'vital', 'assignment': 'support'}, {'text': 'ProphetNet first large-scale transformer to predict multiple n-grams per step', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Next-token prediction models can encode information for multiple future tokens', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Architectural tweaks enable efficient multi-token prediction with transformer heads', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Multi-token prediction improves downstream performance, inference speed, and training efficiency', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Smaller language models struggle with multi-token prediction objectives', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'Curriculum learning improves training in various machine learning domains', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Two-stage curriculum based on text quality improves training outcomes', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Curriculum learning popular in data-constrained pre-training setups', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Objective-based curriculum outperforms non-curriculum baselines in data-constrained setups', 'importance': 'okay', 'assignment': 'support'}, {'text': 'ProphetNet uses n-stream self-attention with higher computational overhead', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Speculative decoding enables faster inference in language models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Self-speculative decoding methods include early-exit, layer skipping, and architectural changes', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Medusa enables efficient self-speculative decoding for NTP-pretrained LLMs', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Curriculum learning benefits pre-training of encoder-only and decoder-only language models', 'importance': 'vital', 'assignment': 'support'}, {'text': 'ProphetNet first large-scale transformer to predict multiple n-grams per step', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Next-token prediction models can encode information for multiple future tokens', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Architectural tweaks enable efficient multi-token prediction with transformer heads', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Curriculum learning improves training in various machine learning domains', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Two-stage curriculum based on text quality improves training outcomes', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Curriculum learning popular in data-constrained pre-training setups', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Objective-based curriculum outperforms non-curriculum baselines in data-constrained setups', 'importance': 'okay', 'assignment': 'support'}, {'text': 'ProphetNet uses n-stream self-attention with higher computational overhead', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Speculative decoding enables faster inference in language models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Self-speculative decoding methods include early-exit, layer skipping, and architectural changes', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Medusa enables efficient self-speculative decoding for NTP-pretrained LLMs', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Curriculum learning benefits pre-training of encoder-only and decoder-only language models', 'importance': 'vital', 'assignment': 'support'}, {'text': 'ProphetNet first large-scale transformer to predict multiple n-grams per step', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Next-token prediction models can encode information for multiple future tokens', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Architectural tweaks enable efficient multi-token prediction with transformer heads', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Curriculum learning improves training in various machine learning domains', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Two-stage curriculum based on text quality improves training outcomes', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Curriculum learning popular in data-constrained pre-training setups', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Objective-based curriculum outperforms non-curriculum baselines in data-constrained setups', 'importance': 'okay', 'assignment': 'support'}, {'text': 'ProphetNet uses n-stream self-attention with higher computational overhead', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Speculative decoding enables faster inference in language models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Self-speculative decoding methods include early-exit, layer skipping, and architectural changes', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Medusa enables efficient self-speculative decoding for NTP-pretrained LLMs', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 0.6666666666666666, 'strict_all_score': 0.8571428571428571, 'vital_score': 0.6666666666666666, 'all_score': 0.8571428571428571}"
