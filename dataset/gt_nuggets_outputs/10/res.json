{
  "qid": "2505.22757v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nMulti-token prediction (MTP) is a recently proposed pre-training objective\nfor language models. Rather than predicting only the next token (NTP), MTP\npredicts the next $k$ tokens at each prediction step, using multiple prediction\nheads. MTP has shown promise in improving downstream performance, inference\nspeed, and training efficiency, particularly for large models. However, prior\nwork has shown that smaller language models (SLMs) struggle with the MTP\nobjective. To address this, we propose a curriculum learning strategy for MTP\ntraining, exploring two variants: a forward curriculum, which gradually\nincreases the complexity of the pre-training objective from NTP to MTP, and a\nreverse curriculum, which does the opposite. Our experiments show that the\nforward curriculum enables SLMs to better leverage the MTP objective during\npre-training, improving downstream NTP performance and generative output\nquality, while retaining the benefits of self-speculative decoding. The reverse\ncurriculum achieves stronger NTP performance and output quality, but fails to\nprovide any self-speculative decoding benefits.",
  "nuggets": [
    {
      "text": "Curriculum learning benefits pre-training of encoder-only and decoder-only language models",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "ProphetNet first large-scale transformer to predict multiple n-grams per step",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Next-token prediction models can encode information for multiple future tokens",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Architectural tweaks enable efficient multi-token prediction with transformer heads",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Multi-token prediction improves downstream performance, inference speed, and training efficiency",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Smaller language models struggle with multi-token prediction objectives",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Curriculum learning improves training in various machine learning domains",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Two-stage curriculum based on text quality improves training outcomes",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Curriculum learning popular in data-constrained pre-training setups",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Objective-based curriculum outperforms non-curriculum baselines in data-constrained setups",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "ProphetNet uses n-stream self-attention with higher computational overhead",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Speculative decoding enables faster inference in language models",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Self-speculative decoding methods include early-exit, layer skipping, and architectural changes",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Medusa enables efficient self-speculative decoding for NTP-pretrained LLMs",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "Curriculum learning benefits pre-training of encoder-only and decoder-only language models",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "ProphetNet first large-scale transformer to predict multiple n-grams per step",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Next-token prediction models can encode information for multiple future tokens",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Architectural tweaks enable efficient multi-token prediction with transformer heads",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Curriculum learning improves training in various machine learning domains",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Two-stage curriculum based on text quality improves training outcomes",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Curriculum learning popular in data-constrained pre-training setups",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Objective-based curriculum outperforms non-curriculum baselines in data-constrained setups",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "ProphetNet uses n-stream self-attention with higher computational overhead",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Speculative decoding enables faster inference in language models",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Self-speculative decoding methods include early-exit, layer skipping, and architectural changes",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Medusa enables efficient self-speculative decoding for NTP-pretrained LLMs",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "Curriculum learning benefits pre-training of encoder-only and decoder-only language models",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "ProphetNet first large-scale transformer to predict multiple n-grams per step",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Next-token prediction models can encode information for multiple future tokens",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Architectural tweaks enable efficient multi-token prediction with transformer heads",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Curriculum learning improves training in various machine learning domains",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Two-stage curriculum based on text quality improves training outcomes",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Curriculum learning popular in data-constrained pre-training setups",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Objective-based curriculum outperforms non-curriculum baselines in data-constrained setups",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "ProphetNet uses n-stream self-attention with higher computational overhead",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Speculative decoding enables faster inference in language models",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Self-speculative decoding methods include early-exit, layer skipping, and architectural changes",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Medusa enables efficient self-speculative decoding for NTP-pretrained LLMs",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.6666666666666666,
    "strict_all_score": 0.8571428571428571,
    "vital_score": 0.6666666666666666,
    "all_score": 0.8571428571428571
  }
}