{
  "qid": "2505.03484v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nRecent deep sequential recommendation models often struggle to effectively\nmodel key characteristics of user behaviors, particularly in handling sequence\nlength variations and capturing diverse interaction patterns. We propose\nSTAR-Rec, a novel architecture that synergistically combines preference-aware\nattention and state-space modeling through a sequence-level mixture-of-experts\nframework. STAR-Rec addresses these challenges by: (1) employing\npreference-aware attention to capture both inherently similar item\nrelationships and diverse preferences, (2) utilizing state-space modeling to\nefficiently process variable-length sequences with linear complexity, and (3)\nincorporating a mixture-of-experts component that adaptively routes different\nbehavioral patterns to specialized experts, handling both focused\ncategory-specific browsing and diverse category exploration patterns. We\ntheoretically demonstrate how the state space model and attention mechanisms\ncan be naturally unified in recommendation scenarios, where SSM captures\ntemporal dynamics through state compression while attention models both similar\nand diverse item relationships. Extensive experiments on four real-world\ndatasets demonstrate that STAR-Rec consistently outperforms state-of-the-art\nsequential recommendation methods, particularly in scenarios involving diverse\nuser behaviors and varying sequence lengths.",
  "nuggets": [
    {
      "text": "Transformers and RNNs are foundational for sequential recommendation",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "TransRec and matrix factorization struggled with multiple behaviors, long sequences",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SASRec uses multi-head attention for sequence modeling",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "BERT4Rec employs bidirectional transformers for contextual information",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Transformer models face quadratic complexity with long sequences",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "State-space models (SSMs) excel at capturing temporal dynamics",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SSMs struggle to balance long/short-term modeling and pattern diversity",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "STAR-Rec unifies preference-aware attention and state-space modeling",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "STAR-Rec uses mixture-of-experts for diverse behavioral patterns",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "STAR-Rec efficiently handles variable-length sequences and diverse item relationships",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "LinRec introduces linear complexity attention for efficiency",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "GRU4Rec offers linear complexity but limited effectiveness",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Mamba4Rec improves efficiency with selective state space modeling",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "ECHO-Mamba4Rec combines bidirectional Mamba and frequency-domain filtering",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "RecMamba handles lifelong recommendation scenarios",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Mamba4KT adapts Mamba for knowledge tracing",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "SIGMA uses bi-directional structure and selective gating for context modeling",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "Transformers and RNNs are foundational for sequential recommendation",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "TransRec and matrix factorization struggled with multiple behaviors, long sequences",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SASRec uses multi-head attention for sequence modeling",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "BERT4Rec employs bidirectional transformers for contextual information",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Transformer models face quadratic complexity with long sequences",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "State-space models (SSMs) excel at capturing temporal dynamics",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SSMs struggle to balance long/short-term modeling and pattern diversity",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "STAR-Rec unifies preference-aware attention and state-space modeling",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "STAR-Rec uses mixture-of-experts for diverse behavioral patterns",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "STAR-Rec efficiently handles variable-length sequences and diverse item relationships",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "LinRec introduces linear complexity attention for efficiency",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "GRU4Rec offers linear complexity but limited effectiveness",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Mamba4Rec improves efficiency with selective state space modeling",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "ECHO-Mamba4Rec combines bidirectional Mamba and frequency-domain filtering",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "RecMamba handles lifelong recommendation scenarios",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Mamba4KT adapts Mamba for knowledge tracing",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "SIGMA uses bi-directional structure and selective gating for context modeling",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "Transformers and RNNs are foundational for sequential recommendation",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "TransRec and matrix factorization struggled with multiple behaviors, long sequences",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SASRec uses multi-head attention for sequence modeling",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "BERT4Rec employs bidirectional transformers for contextual information",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Transformer models face quadratic complexity with long sequences",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "State-space models (SSMs) excel at capturing temporal dynamics",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SSMs struggle to balance long/short-term modeling and pattern diversity",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "STAR-Rec unifies preference-aware attention and state-space modeling",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "STAR-Rec uses mixture-of-experts for diverse behavioral patterns",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "STAR-Rec efficiently handles variable-length sequences and diverse item relationships",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "LinRec introduces linear complexity attention for efficiency",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "GRU4Rec offers linear complexity but limited effectiveness",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Mamba4Rec improves efficiency with selective state space modeling",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "ECHO-Mamba4Rec combines bidirectional Mamba and frequency-domain filtering",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "RecMamba handles lifelong recommendation scenarios",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Mamba4KT adapts Mamba for knowledge tracing",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "SIGMA uses bi-directional structure and selective gating for context modeling",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 1.0,
    "strict_all_score": 1.0,
    "vital_score": 1.0,
    "all_score": 1.0
  }
}