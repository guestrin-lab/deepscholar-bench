qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2505.03484v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
Recent deep sequential recommendation models often struggle to effectively
model key characteristics of user behaviors, particularly in handling sequence
length variations and capturing diverse interaction patterns. We propose
STAR-Rec, a novel architecture that synergistically combines preference-aware
attention and state-space modeling through a sequence-level mixture-of-experts
framework. STAR-Rec addresses these challenges by: (1) employing
preference-aware attention to capture both inherently similar item
relationships and diverse preferences, (2) utilizing state-space modeling to
efficiently process variable-length sequences with linear complexity, and (3)
incorporating a mixture-of-experts component that adaptively routes different
behavioral patterns to specialized experts, handling both focused
category-specific browsing and diverse category exploration patterns. We
theoretically demonstrate how the state space model and attention mechanisms
can be naturally unified in recommendation scenarios, where SSM captures
temporal dynamics through state compression while attention models both similar
and diverse item relationships. Extensive experiments on four real-world
datasets demonstrate that STAR-Rec consistently outperforms state-of-the-art
sequential recommendation methods, particularly in scenarios involving diverse
user behaviors and varying sequence lengths.","[{'text': 'Transformers and RNNs are foundational for sequential recommendation', 'importance': 'vital', 'assignment': 'support'}, {'text': 'TransRec and matrix factorization struggled with multiple behaviors, long sequences', 'importance': 'vital', 'assignment': 'support'}, {'text': 'SASRec uses multi-head attention for sequence modeling', 'importance': 'vital', 'assignment': 'support'}, {'text': 'BERT4Rec employs bidirectional transformers for contextual information', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Transformer models face quadratic complexity with long sequences', 'importance': 'vital', 'assignment': 'support'}, {'text': 'State-space models (SSMs) excel at capturing temporal dynamics', 'importance': 'vital', 'assignment': 'support'}, {'text': 'SSMs struggle to balance long/short-term modeling and pattern diversity', 'importance': 'vital', 'assignment': 'support'}, {'text': 'STAR-Rec unifies preference-aware attention and state-space modeling', 'importance': 'vital', 'assignment': 'support'}, {'text': 'STAR-Rec uses mixture-of-experts for diverse behavioral patterns', 'importance': 'vital', 'assignment': 'support'}, {'text': 'STAR-Rec efficiently handles variable-length sequences and diverse item relationships', 'importance': 'vital', 'assignment': 'support'}, {'text': 'LinRec introduces linear complexity attention for efficiency', 'importance': 'okay', 'assignment': 'support'}, {'text': 'GRU4Rec offers linear complexity but limited effectiveness', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Mamba4Rec improves efficiency with selective state space modeling', 'importance': 'okay', 'assignment': 'support'}, {'text': 'ECHO-Mamba4Rec combines bidirectional Mamba and frequency-domain filtering', 'importance': 'okay', 'assignment': 'support'}, {'text': 'RecMamba handles lifelong recommendation scenarios', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Mamba4KT adapts Mamba for knowledge tracing', 'importance': 'okay', 'assignment': 'support'}, {'text': 'SIGMA uses bi-directional structure and selective gating for context modeling', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Transformers and RNNs are foundational for sequential recommendation', 'importance': 'vital', 'assignment': 'support'}, {'text': 'TransRec and matrix factorization struggled with multiple behaviors, long sequences', 'importance': 'vital', 'assignment': 'support'}, {'text': 'SASRec uses multi-head attention for sequence modeling', 'importance': 'vital', 'assignment': 'support'}, {'text': 'BERT4Rec employs bidirectional transformers for contextual information', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Transformer models face quadratic complexity with long sequences', 'importance': 'vital', 'assignment': 'support'}, {'text': 'State-space models (SSMs) excel at capturing temporal dynamics', 'importance': 'vital', 'assignment': 'support'}, {'text': 'SSMs struggle to balance long/short-term modeling and pattern diversity', 'importance': 'vital', 'assignment': 'support'}, {'text': 'STAR-Rec unifies preference-aware attention and state-space modeling', 'importance': 'vital', 'assignment': 'support'}, {'text': 'STAR-Rec uses mixture-of-experts for diverse behavioral patterns', 'importance': 'vital', 'assignment': 'support'}, {'text': 'STAR-Rec efficiently handles variable-length sequences and diverse item relationships', 'importance': 'vital', 'assignment': 'support'}, {'text': 'LinRec introduces linear complexity attention for efficiency', 'importance': 'okay', 'assignment': 'support'}, {'text': 'GRU4Rec offers linear complexity but limited effectiveness', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Mamba4Rec improves efficiency with selective state space modeling', 'importance': 'okay', 'assignment': 'support'}, {'text': 'ECHO-Mamba4Rec combines bidirectional Mamba and frequency-domain filtering', 'importance': 'okay', 'assignment': 'support'}, {'text': 'RecMamba handles lifelong recommendation scenarios', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Mamba4KT adapts Mamba for knowledge tracing', 'importance': 'okay', 'assignment': 'support'}, {'text': 'SIGMA uses bi-directional structure and selective gating for context modeling', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Transformers and RNNs are foundational for sequential recommendation', 'importance': 'vital', 'assignment': 'support'}, {'text': 'TransRec and matrix factorization struggled with multiple behaviors, long sequences', 'importance': 'vital', 'assignment': 'support'}, {'text': 'SASRec uses multi-head attention for sequence modeling', 'importance': 'vital', 'assignment': 'support'}, {'text': 'BERT4Rec employs bidirectional transformers for contextual information', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Transformer models face quadratic complexity with long sequences', 'importance': 'vital', 'assignment': 'support'}, {'text': 'State-space models (SSMs) excel at capturing temporal dynamics', 'importance': 'vital', 'assignment': 'support'}, {'text': 'SSMs struggle to balance long/short-term modeling and pattern diversity', 'importance': 'vital', 'assignment': 'support'}, {'text': 'STAR-Rec unifies preference-aware attention and state-space modeling', 'importance': 'vital', 'assignment': 'support'}, {'text': 'STAR-Rec uses mixture-of-experts for diverse behavioral patterns', 'importance': 'vital', 'assignment': 'support'}, {'text': 'STAR-Rec efficiently handles variable-length sequences and diverse item relationships', 'importance': 'vital', 'assignment': 'support'}, {'text': 'LinRec introduces linear complexity attention for efficiency', 'importance': 'okay', 'assignment': 'support'}, {'text': 'GRU4Rec offers linear complexity but limited effectiveness', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Mamba4Rec improves efficiency with selective state space modeling', 'importance': 'okay', 'assignment': 'support'}, {'text': 'ECHO-Mamba4Rec combines bidirectional Mamba and frequency-domain filtering', 'importance': 'okay', 'assignment': 'support'}, {'text': 'RecMamba handles lifelong recommendation scenarios', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Mamba4KT adapts Mamba for knowledge tracing', 'importance': 'okay', 'assignment': 'support'}, {'text': 'SIGMA uses bi-directional structure and selective gating for context modeling', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 1.0, 'strict_all_score': 1.0, 'vital_score': 1.0, 'all_score': 1.0}"
