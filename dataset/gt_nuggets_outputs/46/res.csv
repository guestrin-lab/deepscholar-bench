qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2505.21811v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
Sequential recommendation is a popular paradigm in modern recommender
systems. In particular, one challenging problem in this space is cross-domain
sequential recommendation (CDSR), which aims to predict future behaviors given
user interactions across multiple domains. Existing CDSR frameworks are mostly
built on the self-attention transformer and seek to improve by explicitly
injecting additional domain-specific components (e.g. domain-aware module
blocks). While these additional components help, we argue they overlook the
core self-attention module already present in the transformer, a naturally
powerful tool to learn correlations among behaviors. In this work, we aim to
improve the CDSR performance for simple models from a novel perspective of
enhancing the self-attention. Specifically, we introduce a Pareto-optimal
self-attention and formulate the cross-domain learning as a multi-objective
problem, where we optimize the recommendation task while dynamically minimizing
the cross-domain attention scores. Our approach automates knowledge transfer in
CDSR (dubbed as AutoCDSR) -- it not only mitigates negative transfer but also
encourages complementary knowledge exchange among auxiliary domains. Based on
the idea, we further introduce AutoCDSR+, a more performant variant with slight
additional cost. Our proposal is easy to implement and works as a plug-and-play
module that can be incorporated into existing transformer-based recommenders.
Besides flexibility, it is practical to deploy because it brings little extra
computational overheads without heavy hyper-parameter tuning. AutoCDSR on
average improves Recall@10 for SASRec and Bert4Rec by 9.8% and 16.0% and
NDCG@10 by 12.0% and 16.7%, respectively. Code is available at
https://github.com/snap-research/AutoCDSR.","[{'text': 'Sequential recommendation predicts user behaviors from historical interactions', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Transformers excel at modeling long sequential data in recommendation', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Cross-domain recommendation leverages multiple domains for better performance', 'importance': 'vital', 'assignment': 'support'}, {'text': 'CDSR injects domain-specific components like supervision signals, reweighting, module blocks', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Most CDSR methods overlook the core self-attention in transformers', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Self-attention captures fine-grained correlations among heterogeneous behaviors', 'importance': 'vital', 'assignment': 'support'}, {'text': 'AutoCDSR enhances self-attention for Pareto-optimal cross-domain learning', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'AutoCDSR automates knowledge transfer, mitigates negative transfer, encourages complementarity', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'AutoCDSR improves Recall@10 and NDCG@10 for SASRec and BERT4Rec', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'SASRec and BERT4Rec are transformer-based sequential recommenders', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Early cross-domain methods used matrix factorization, ignoring sequence', 'importance': 'okay', 'assignment': 'support'}, {'text': 'π-net uses domain-aware gating for cross-domain knowledge transfer', 'importance': 'okay', 'assignment': 'support'}, {'text': 'C2DSR and MIFN use graphs/knowledge graphs for CDSR', 'importance': 'okay', 'assignment': 'support'}, {'text': 'MAN and SyNCRec use domain-aware blocks and supervision for CDSR', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Recent works explore large language models for sequential recommendation', 'importance': 'okay', 'assignment': 'support'}, {'text': 'AutoCDSR+ is a more performant, low-cost variant', 'importance': 'okay', 'assignment': 'not_support'}, {'text': 'AutoCDSR is plug-and-play, easy to implement, minimal overhead', 'importance': 'okay', 'assignment': 'not_support'}]","[{'text': 'Sequential recommendation predicts user behaviors from historical interactions', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Transformers excel at modeling long sequential data in recommendation', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Cross-domain recommendation leverages multiple domains for better performance', 'importance': 'vital', 'assignment': 'support'}, {'text': 'CDSR injects domain-specific components like supervision signals, reweighting, module blocks', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Most CDSR methods overlook the core self-attention in transformers', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Self-attention captures fine-grained correlations among heterogeneous behaviors', 'importance': 'vital', 'assignment': 'support'}, {'text': 'SASRec and BERT4Rec are transformer-based sequential recommenders', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Early cross-domain methods used matrix factorization, ignoring sequence', 'importance': 'okay', 'assignment': 'support'}, {'text': 'π-net uses domain-aware gating for cross-domain knowledge transfer', 'importance': 'okay', 'assignment': 'support'}, {'text': 'C2DSR and MIFN use graphs/knowledge graphs for CDSR', 'importance': 'okay', 'assignment': 'support'}, {'text': 'MAN and SyNCRec use domain-aware blocks and supervision for CDSR', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Recent works explore large language models for sequential recommendation', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Sequential recommendation predicts user behaviors from historical interactions', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Transformers excel at modeling long sequential data in recommendation', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Cross-domain recommendation leverages multiple domains for better performance', 'importance': 'vital', 'assignment': 'support'}, {'text': 'CDSR injects domain-specific components like supervision signals, reweighting, module blocks', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Most CDSR methods overlook the core self-attention in transformers', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Self-attention captures fine-grained correlations among heterogeneous behaviors', 'importance': 'vital', 'assignment': 'support'}, {'text': 'SASRec and BERT4Rec are transformer-based sequential recommenders', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Early cross-domain methods used matrix factorization, ignoring sequence', 'importance': 'okay', 'assignment': 'support'}, {'text': 'π-net uses domain-aware gating for cross-domain knowledge transfer', 'importance': 'okay', 'assignment': 'support'}, {'text': 'C2DSR and MIFN use graphs/knowledge graphs for CDSR', 'importance': 'okay', 'assignment': 'support'}, {'text': 'MAN and SyNCRec use domain-aware blocks and supervision for CDSR', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Recent works explore large language models for sequential recommendation', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 0.6666666666666666, 'strict_all_score': 0.7058823529411765, 'vital_score': 0.6666666666666666, 'all_score': 0.7058823529411765}"
