{
  "qid": "2505.21811v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nSequential recommendation is a popular paradigm in modern recommender\nsystems. In particular, one challenging problem in this space is cross-domain\nsequential recommendation (CDSR), which aims to predict future behaviors given\nuser interactions across multiple domains. Existing CDSR frameworks are mostly\nbuilt on the self-attention transformer and seek to improve by explicitly\ninjecting additional domain-specific components (e.g. domain-aware module\nblocks). While these additional components help, we argue they overlook the\ncore self-attention module already present in the transformer, a naturally\npowerful tool to learn correlations among behaviors. In this work, we aim to\nimprove the CDSR performance for simple models from a novel perspective of\nenhancing the self-attention. Specifically, we introduce a Pareto-optimal\nself-attention and formulate the cross-domain learning as a multi-objective\nproblem, where we optimize the recommendation task while dynamically minimizing\nthe cross-domain attention scores. Our approach automates knowledge transfer in\nCDSR (dubbed as AutoCDSR) -- it not only mitigates negative transfer but also\nencourages complementary knowledge exchange among auxiliary domains. Based on\nthe idea, we further introduce AutoCDSR+, a more performant variant with slight\nadditional cost. Our proposal is easy to implement and works as a plug-and-play\nmodule that can be incorporated into existing transformer-based recommenders.\nBesides flexibility, it is practical to deploy because it brings little extra\ncomputational overheads without heavy hyper-parameter tuning. AutoCDSR on\naverage improves Recall@10 for SASRec and Bert4Rec by 9.8% and 16.0% and\nNDCG@10 by 12.0% and 16.7%, respectively. Code is available at\nhttps://github.com/snap-research/AutoCDSR.",
  "nuggets": [
    {
      "text": "Sequential recommendation predicts user behaviors from historical interactions",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Transformers excel at modeling long sequential data in recommendation",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Cross-domain recommendation leverages multiple domains for better performance",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "CDSR injects domain-specific components like supervision signals, reweighting, module blocks",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Most CDSR methods overlook the core self-attention in transformers",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Self-attention captures fine-grained correlations among heterogeneous behaviors",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "AutoCDSR enhances self-attention for Pareto-optimal cross-domain learning",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "AutoCDSR automates knowledge transfer, mitigates negative transfer, encourages complementarity",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "AutoCDSR improves Recall@10 and NDCG@10 for SASRec and BERT4Rec",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "SASRec and BERT4Rec are transformer-based sequential recommenders",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Early cross-domain methods used matrix factorization, ignoring sequence",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "π-net uses domain-aware gating for cross-domain knowledge transfer",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "C2DSR and MIFN use graphs/knowledge graphs for CDSR",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "MAN and SyNCRec use domain-aware blocks and supervision for CDSR",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Recent works explore large language models for sequential recommendation",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "AutoCDSR+ is a more performant, low-cost variant",
      "importance": "okay",
      "assignment": "not_support"
    },
    {
      "text": "AutoCDSR is plug-and-play, easy to implement, minimal overhead",
      "importance": "okay",
      "assignment": "not_support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "Sequential recommendation predicts user behaviors from historical interactions",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Transformers excel at modeling long sequential data in recommendation",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Cross-domain recommendation leverages multiple domains for better performance",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "CDSR injects domain-specific components like supervision signals, reweighting, module blocks",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Most CDSR methods overlook the core self-attention in transformers",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Self-attention captures fine-grained correlations among heterogeneous behaviors",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SASRec and BERT4Rec are transformer-based sequential recommenders",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Early cross-domain methods used matrix factorization, ignoring sequence",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "π-net uses domain-aware gating for cross-domain knowledge transfer",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "C2DSR and MIFN use graphs/knowledge graphs for CDSR",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "MAN and SyNCRec use domain-aware blocks and supervision for CDSR",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Recent works explore large language models for sequential recommendation",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "Sequential recommendation predicts user behaviors from historical interactions",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Transformers excel at modeling long sequential data in recommendation",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Cross-domain recommendation leverages multiple domains for better performance",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "CDSR injects domain-specific components like supervision signals, reweighting, module blocks",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Most CDSR methods overlook the core self-attention in transformers",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Self-attention captures fine-grained correlations among heterogeneous behaviors",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "SASRec and BERT4Rec are transformer-based sequential recommenders",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Early cross-domain methods used matrix factorization, ignoring sequence",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "π-net uses domain-aware gating for cross-domain knowledge transfer",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "C2DSR and MIFN use graphs/knowledge graphs for CDSR",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "MAN and SyNCRec use domain-aware blocks and supervision for CDSR",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Recent works explore large language models for sequential recommendation",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.6666666666666666,
    "strict_all_score": 0.7058823529411765,
    "vital_score": 0.6666666666666666,
    "all_score": 0.7058823529411765
  }
}