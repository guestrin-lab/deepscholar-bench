{
  "qid": "2505.19307v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nNeural retrieval models excel in Web search, but their training requires\nsubstantial amounts of labeled query-document pairs, which are costly to\nobtain. With the widespread availability of Web document collections like\nClueWeb22, synthetic queries generated by large language models offer a\nscalable alternative. Still, synthetic training queries often vary in quality,\nwhich leads to suboptimal downstream retrieval performance. Existing methods\ntypically filter out noisy query-document pairs based on signals from an\nexternal re-ranker. In contrast, we propose a framework that leverages Direct\nPreference Optimization (DPO) to integrate ranking signals into the query\ngeneration process, aiming to directly optimize the model towards generating\nhigh-quality queries that maximize downstream retrieval effectiveness.\nExperiments show higher ranker-assessed relevance between query-document pairs\nafter DPO, leading to stronger downstream performance on the MS~MARCO benchmark\nwhen compared to baseline models trained with synthetic data.",
  "nuggets": [
    {
      "text": "Synthetic queries from documents are widely used for model training",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Filtering hallucinated queries enhances downstream retrieval performance",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "LLMs enable few-shot synthetic query generation (e.g., InPars, Promptagator)",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "InPars filters queries by generation probability; InPars-v2 uses supervised ranker",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Reinforcement learning (e.g., TPPO) optimizes query generation with token-level rewards",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Direct Preference Optimization (DPO) integrates ranking signals into query generation",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "DPO directly optimizes for high-quality, retrieval-effective queries",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Transformer-based bi-encoders are standard for dense neural retrieval",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Contrastive objectives and hard negative mining improve retrieval training",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Retrieval-aligned pre-training on in-domain corpora boosts effectiveness",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Doc2Query and DocT5Query expand document representations with synthetic queries",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Gecko iteratively refines synthetic queries via retrieval, re-ranking, relabeling",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "TPPO targets interactive query suggestion, not offline document-based generation",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "Synthetic queries from documents are widely used for model training",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Filtering hallucinated queries enhances downstream retrieval performance",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "LLMs enable few-shot synthetic query generation (e.g., InPars, Promptagator)",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "InPars filters queries by generation probability; InPars-v2 uses supervised ranker",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Reinforcement learning (e.g., TPPO) optimizes query generation with token-level rewards",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Transformer-based bi-encoders are standard for dense neural retrieval",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Contrastive objectives and hard negative mining improve retrieval training",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Retrieval-aligned pre-training on in-domain corpora boosts effectiveness",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Doc2Query and DocT5Query expand document representations with synthetic queries",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Gecko iteratively refines synthetic queries via retrieval, re-ranking, relabeling",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "TPPO targets interactive query suggestion, not offline document-based generation",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "Synthetic queries from documents are widely used for model training",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Filtering hallucinated queries enhances downstream retrieval performance",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "LLMs enable few-shot synthetic query generation (e.g., InPars, Promptagator)",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "InPars filters queries by generation probability; InPars-v2 uses supervised ranker",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Reinforcement learning (e.g., TPPO) optimizes query generation with token-level rewards",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Transformer-based bi-encoders are standard for dense neural retrieval",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Contrastive objectives and hard negative mining improve retrieval training",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Retrieval-aligned pre-training on in-domain corpora boosts effectiveness",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Doc2Query and DocT5Query expand document representations with synthetic queries",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Gecko iteratively refines synthetic queries via retrieval, re-ranking, relabeling",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "TPPO targets interactive query suggestion, not offline document-based generation",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.7142857142857143,
    "strict_all_score": 0.8461538461538461,
    "vital_score": 0.7142857142857143,
    "all_score": 0.8461538461538461
  }
}