qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2506.02634v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
Serving large language models (LLMs) is important for cloud providers, and
caching intermediate results (KV\$) after processing each request substantially
improves serving throughput and latency. However, there is limited
understanding of how LLM serving benefits from KV\$ caching, where system
design decisions like cache eviction policies are highly workload-dependent. In
this paper, we present the first systematic characterization of the KV\$
workload patterns from one of the leading LLM service providers. We draw
observations that were not covered by previous studies focusing on synthetic
workloads, including: KV\$ reuses are skewed across requests, where reuses
between single-turn requests are equally important as multi-turn requests; the
reuse time and probability are diverse considering all requests, but for a
specific request category, the pattern tends to be predictable; and the overall
cache size required for an ideal cache hit ratio is moderate. Based on the
characterization, we further propose a workload-aware cache eviction policy
that improves the serving performance under real-world traces, especially with
limited cache capacity.","[{'text': 'KV cache reuse accelerates LLM serving', 'importance': 'vital', 'assignment': 'support'}, {'text': 'General-purpose cache eviction policies: LRU, SLRU, LIRS, ARC, etc.', 'importance': 'vital', 'assignment': 'support'}, {'text': 'LLM serving workload characterization is underexplored', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Prior studies focus on synthetic LLM workloads', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Workload-aware cache eviction policies improve LLM serving performance', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Production LLM systems use prefix-match KV cache reuse', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Non-prefix KV cache reuse studied but not widely deployed', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Runtime KV cache compression and deletion reduce cache size', 'importance': 'okay', 'assignment': 'support'}, {'text': 'StreamingLLM, H2O, InfiniGen optimize KV cache memory usage', 'importance': 'okay', 'assignment': 'support'}, {'text': 'KVQuant enables 3-bit KV cache quantization with minimal accuracy loss', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Domain-specific cache policies studied for web and serverless workloads', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LLM serving optimization is an active research area', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'KV cache reuse accelerates LLM serving', 'importance': 'vital', 'assignment': 'support'}, {'text': 'General-purpose cache eviction policies: LRU, SLRU, LIRS, ARC, etc.', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Prior studies focus on synthetic LLM workloads', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Workload-aware cache eviction policies improve LLM serving performance', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Production LLM systems use prefix-match KV cache reuse', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Non-prefix KV cache reuse studied but not widely deployed', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Runtime KV cache compression and deletion reduce cache size', 'importance': 'okay', 'assignment': 'support'}, {'text': 'StreamingLLM, H2O, InfiniGen optimize KV cache memory usage', 'importance': 'okay', 'assignment': 'support'}, {'text': 'KVQuant enables 3-bit KV cache quantization with minimal accuracy loss', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Domain-specific cache policies studied for web and serverless workloads', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LLM serving optimization is an active research area', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'KV cache reuse accelerates LLM serving', 'importance': 'vital', 'assignment': 'support'}, {'text': 'General-purpose cache eviction policies: LRU, SLRU, LIRS, ARC, etc.', 'importance': 'vital', 'assignment': 'support'}, {'text': 'LLM serving workload characterization is underexplored', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Prior studies focus on synthetic LLM workloads', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Workload-aware cache eviction policies improve LLM serving performance', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Production LLM systems use prefix-match KV cache reuse', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Non-prefix KV cache reuse studied but not widely deployed', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Runtime KV cache compression and deletion reduce cache size', 'importance': 'okay', 'assignment': 'support'}, {'text': 'StreamingLLM, H2O, InfiniGen optimize KV cache memory usage', 'importance': 'okay', 'assignment': 'support'}, {'text': 'KVQuant enables 3-bit KV cache quantization with minimal accuracy loss', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Domain-specific cache policies studied for web and serverless workloads', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LLM serving optimization is an active research area', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 0.8, 'strict_all_score': 0.9166666666666666, 'vital_score': 0.9, 'all_score': 0.9583333333333334}"
