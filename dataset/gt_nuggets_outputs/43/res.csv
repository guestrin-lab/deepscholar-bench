qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2504.09307v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
Training LLMs in distributed environments presents significant challenges due
to the complexity of model execution, deployment systems, and the vast space of
configurable strategies. Although various optimization techniques exist,
achieving high efficiency in practice remains difficult. Accurate performance
models that effectively characterize and predict a model's behavior are
essential for guiding optimization efforts and system-level studies. We propose
Lumos, a trace-driven performance modeling and estimation toolkit for
large-scale LLM training, designed to accurately capture and predict the
execution behaviors of modern LLMs. We evaluate Lumos on a production ML
cluster with up to 512 NVIDIA H100 GPUs using various GPT-3 variants,
demonstrating that it can replay execution time with an average error of just
3.3%, along with other runtime details, across different models and
configurations. Additionally, we validate its ability to estimate performance
for new setups from existing traces, facilitating efficient exploration of
model and deployment configurations.","[{'text': 'Transformer-based LLMs require 3D parallelism: data, tensor, pipeline', 'importance': 'vital', 'assignment': 'support'}, {'text': '3D parallelism enables efficient large-scale LLM training', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Configuring parallelism strategies is complex and error-prone', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Analytical models (AmPeD, Calculon) estimate LLM training performance', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Analytical models often lack generality and fine-grained accuracy', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Trace-based models (ASTRA-sim, Daydream, dPRO) simulate distributed training', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Existing trace-based models struggle with LLM execution complexity', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Lumos is the first to accurately model LLMs using detailed traces', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Profiling tools like NVProf, CUPTI, Nsight expose hardware performance counters', 'importance': 'okay', 'assignment': 'support'}, {'text': 'PyTorch Kineto integrates hardware-level traces for operator-level statistics', 'importance': 'okay', 'assignment': 'support'}, {'text': 'GSPMD, Alpa, Galvatron automate parallelism configuration and optimization', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Sequence parallelism reduces memory and communication for long sequences', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Trace-based models predict runtime using profiled execution traces', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Transformer-based LLMs require 3D parallelism: data, tensor, pipeline', 'importance': 'vital', 'assignment': 'support'}, {'text': '3D parallelism enables efficient large-scale LLM training', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Configuring parallelism strategies is complex and error-prone', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Analytical models (AmPeD, Calculon) estimate LLM training performance', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Analytical models often lack generality and fine-grained accuracy', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Trace-based models (ASTRA-sim, Daydream, dPRO) simulate distributed training', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Existing trace-based models struggle with LLM execution complexity', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Lumos is the first to accurately model LLMs using detailed traces', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Profiling tools like NVProf, CUPTI, Nsight expose hardware performance counters', 'importance': 'okay', 'assignment': 'support'}, {'text': 'PyTorch Kineto integrates hardware-level traces for operator-level statistics', 'importance': 'okay', 'assignment': 'support'}, {'text': 'GSPMD, Alpa, Galvatron automate parallelism configuration and optimization', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Sequence parallelism reduces memory and communication for long sequences', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Trace-based models predict runtime using profiled execution traces', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Transformer-based LLMs require 3D parallelism: data, tensor, pipeline', 'importance': 'vital', 'assignment': 'support'}, {'text': '3D parallelism enables efficient large-scale LLM training', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Configuring parallelism strategies is complex and error-prone', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Analytical models (AmPeD, Calculon) estimate LLM training performance', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Analytical models often lack generality and fine-grained accuracy', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Trace-based models (ASTRA-sim, Daydream, dPRO) simulate distributed training', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Existing trace-based models struggle with LLM execution complexity', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Lumos is the first to accurately model LLMs using detailed traces', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Profiling tools like NVProf, CUPTI, Nsight expose hardware performance counters', 'importance': 'okay', 'assignment': 'support'}, {'text': 'PyTorch Kineto integrates hardware-level traces for operator-level statistics', 'importance': 'okay', 'assignment': 'support'}, {'text': 'GSPMD, Alpa, Galvatron automate parallelism configuration and optimization', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Sequence parallelism reduces memory and communication for long sequences', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Trace-based models predict runtime using profiled execution traces', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 1.0, 'strict_all_score': 1.0, 'vital_score': 1.0, 'all_score': 1.0}"
