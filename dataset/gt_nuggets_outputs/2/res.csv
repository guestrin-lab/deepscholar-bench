qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2506.00958v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
Nonverbal communication is integral to human interaction, with gestures,
facial expressions, and body language conveying critical aspects of intent and
emotion. However, existing large language models (LLMs) fail to effectively
incorporate these nonverbal elements, limiting their capacity to create fully
immersive conversational experiences. We introduce MARS, a multimodal language
model designed to understand and generate nonverbal cues alongside text,
bridging this gap in conversational AI. Our key innovation is VENUS, a
large-scale dataset comprising annotated videos with time-aligned text, facial
expressions, and body language. Leveraging VENUS, we train MARS with a
next-token prediction objective, combining text with vector-quantized nonverbal
representations to achieve multimodal understanding and generation within a
unified framework. Based on various analyses of the VENUS datasets, we validate
its substantial scale and high effectiveness. Our quantitative and qualitative
results demonstrate that MARS successfully generates text and nonverbal
languages, corresponding to conversational input.","[{'text': 'MARS: multimodal language model for text and nonverbal cue generation', 'importance': 'vital', 'assignment': 'not_support'}, {'text': 'VENUS: large-scale dataset with annotated videos, text, facial, body cues', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Existing LLMs lack effective nonverbal communication modeling', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Prior datasets lack detailed 3D facial and body language annotations', 'importance': 'vital', 'assignment': 'support'}, {'text': 'MARS generates aligned text, facial expressions, and body language', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'LLaVA, Qwen-VL, MiniGPT-4: vision-augmented LLMs for visual understanding', 'importance': 'okay', 'assignment': 'support'}, {'text': 'VideoChat, Video-LLaMA: LLMs extended to video comprehension', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Unified-IO-2, GPT-4-O: multimodal LLMs with auditory and visual reasoning', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Dialogue datasets: IEMOCAP, CMU-MOSEI, MELD for multimodal sentiment analysis', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Champagne (YTD-18M), MultiDialog: video-based dialogue generation datasets', 'importance': 'okay', 'assignment': 'support'}, {'text': 'EmotionCLIP, Shafique et al.: nonverbal cue recognition in conversation', 'importance': 'okay', 'assignment': 'support'}, {'text': 'FurChat, Lee et al.: nonverbal signals for robotic facial expressions', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Recent 3D human reconstruction advances enable scalable pseudo-ground truth', 'importance': 'okay', 'assignment': 'support'}, {'text': 'MotionLLM, HumanToMaTo: text-to-human motion generation models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Ng et al.: listener facial motion generation from text', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Prior datasets lack detailed 3D facial and body language annotations', 'importance': 'vital', 'assignment': 'support'}, {'text': 'LLaVA, Qwen-VL, MiniGPT-4: vision-augmented LLMs for visual understanding', 'importance': 'okay', 'assignment': 'support'}, {'text': 'VideoChat, Video-LLaMA: LLMs extended to video comprehension', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Unified-IO-2, GPT-4-O: multimodal LLMs with auditory and visual reasoning', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Dialogue datasets: IEMOCAP, CMU-MOSEI, MELD for multimodal sentiment analysis', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Champagne (YTD-18M), MultiDialog: video-based dialogue generation datasets', 'importance': 'okay', 'assignment': 'support'}, {'text': 'EmotionCLIP, Shafique et al.: nonverbal cue recognition in conversation', 'importance': 'okay', 'assignment': 'support'}, {'text': 'FurChat, Lee et al.: nonverbal signals for robotic facial expressions', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Recent 3D human reconstruction advances enable scalable pseudo-ground truth', 'importance': 'okay', 'assignment': 'support'}, {'text': 'MotionLLM, HumanToMaTo: text-to-human motion generation models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Ng et al.: listener facial motion generation from text', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'VENUS: large-scale dataset with annotated videos, text, facial, body cues', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Existing LLMs lack effective nonverbal communication modeling', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Prior datasets lack detailed 3D facial and body language annotations', 'importance': 'vital', 'assignment': 'support'}, {'text': 'MARS generates aligned text, facial expressions, and body language', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'LLaVA, Qwen-VL, MiniGPT-4: vision-augmented LLMs for visual understanding', 'importance': 'okay', 'assignment': 'support'}, {'text': 'VideoChat, Video-LLaMA: LLMs extended to video comprehension', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Unified-IO-2, GPT-4-O: multimodal LLMs with auditory and visual reasoning', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Dialogue datasets: IEMOCAP, CMU-MOSEI, MELD for multimodal sentiment analysis', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Champagne (YTD-18M), MultiDialog: video-based dialogue generation datasets', 'importance': 'okay', 'assignment': 'support'}, {'text': 'EmotionCLIP, Shafique et al.: nonverbal cue recognition in conversation', 'importance': 'okay', 'assignment': 'support'}, {'text': 'FurChat, Lee et al.: nonverbal signals for robotic facial expressions', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Recent 3D human reconstruction advances enable scalable pseudo-ground truth', 'importance': 'okay', 'assignment': 'support'}, {'text': 'MotionLLM, HumanToMaTo: text-to-human motion generation models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Ng et al.: listener facial motion generation from text', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 0.2, 'strict_all_score': 0.7333333333333333, 'vital_score': 0.5, 'all_score': 0.8333333333333334}"
