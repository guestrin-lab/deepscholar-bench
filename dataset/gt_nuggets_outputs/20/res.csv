qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2505.23180v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
Deep-unrolling and plug-and-play (PnP) approaches have become the de-facto
standard solvers for single-pixel imaging (SPI) inverse problem. PnP
approaches, a class of iterative algorithms where regularization is implicitly
performed by an off-the-shelf deep denoiser, are flexible for varying
compression ratios (CRs) but are limited in reconstruction accuracy and speed.
Conversely, unrolling approaches, a class of multi-stage neural networks where
a truncated iterative optimization process is transformed into an end-to-end
trainable network, typically achieve better accuracy with faster inference but
require fine-tuning or even retraining when CR changes. In this paper, we
address the challenge of integrating the strengths of both classes of solvers.
To this end, we design an efficient deep image restorer (DIR) for the unrolling
of HQS (half quadratic splitting) and ADMM (alternating direction method of
multipliers). More importantly, a general proximal trajectory (PT) loss
function is proposed to train HQS/ADMM-unrolling networks such that learned DIR
approximates the proximal operator of an ideal explicit restoration
regularizer. Extensive experiments demonstrate that, the resulting proximal
unrolling networks can not only flexibly handle varying CRs with a single model
like PnP algorithms, but also outperform previous CR-specific unrolling
networks in both reconstruction accuracy and speed. Source codes and models are
available at https://github.com/pwangcs/ProxUnroll.","[{'text': 'Mainstream SPI solvers: iterative optimization, PnP, single-stage, unrolling networks', 'importance': 'vital', 'assignment': 'support'}, {'text': 'PnP algorithms use deep denoisers for implicit regularization', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'PnP is flexible for varying compression ratios but limited in accuracy and speed', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Unrolling networks transform optimization into end-to-end trainable multi-stage networks', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Unrolling achieves better accuracy and speed but needs retraining for CR changes', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'PnP and unrolling are de-facto standard tools for SPI reconstruction', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Proximal learning trains neural networks as proximal operators of explicit regularizers', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Deep denoisers rarely meet theoretical conditions for convexity and nonexpansiveness', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Gradient denoisers train as explicit gradient steps on deep parameterized functionals', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Proximal learning without constraints remains an open challenge', 'importance': 'vital', 'assignment': 'support'}, {'text': 'SPI reconstruction is a classical inverse problem in compressive imaging', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Iterative algorithms use hand-crafted regularizers and proximal algorithms (ISTA, AMP, HQS, ADMM)', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Single-stage networks underperform due to limited imaging model utilization', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Regularization by denoising links denoisers to gradient steps on convex functions', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Contractive gradient or ICNN constraints limit denoiser expressivity', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Mainstream SPI solvers: iterative optimization, PnP, single-stage, unrolling networks', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Unrolling networks transform optimization into end-to-end trainable multi-stage networks', 'importance': 'vital', 'assignment': 'support'}, {'text': 'PnP and unrolling are de-facto standard tools for SPI reconstruction', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Proximal learning trains neural networks as proximal operators of explicit regularizers', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Deep denoisers rarely meet theoretical conditions for convexity and nonexpansiveness', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Gradient denoisers train as explicit gradient steps on deep parameterized functionals', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Proximal learning without constraints remains an open challenge', 'importance': 'vital', 'assignment': 'support'}, {'text': 'SPI reconstruction is a classical inverse problem in compressive imaging', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Iterative algorithms use hand-crafted regularizers and proximal algorithms (ISTA, AMP, HQS, ADMM)', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Single-stage networks underperform due to limited imaging model utilization', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Regularization by denoising links denoisers to gradient steps on convex functions', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Contractive gradient or ICNN constraints limit denoiser expressivity', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Mainstream SPI solvers: iterative optimization, PnP, single-stage, unrolling networks', 'importance': 'vital', 'assignment': 'support'}, {'text': 'PnP algorithms use deep denoisers for implicit regularization', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'PnP is flexible for varying compression ratios but limited in accuracy and speed', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Unrolling networks transform optimization into end-to-end trainable multi-stage networks', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Unrolling achieves better accuracy and speed but needs retraining for CR changes', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'PnP and unrolling are de-facto standard tools for SPI reconstruction', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Proximal learning trains neural networks as proximal operators of explicit regularizers', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Deep denoisers rarely meet theoretical conditions for convexity and nonexpansiveness', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Gradient denoisers train as explicit gradient steps on deep parameterized functionals', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Proximal learning without constraints remains an open challenge', 'importance': 'vital', 'assignment': 'support'}, {'text': 'SPI reconstruction is a classical inverse problem in compressive imaging', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Iterative algorithms use hand-crafted regularizers and proximal algorithms (ISTA, AMP, HQS, ADMM)', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Single-stage networks underperform due to limited imaging model utilization', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Regularization by denoising links denoisers to gradient steps on convex functions', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Contractive gradient or ICNN constraints limit denoiser expressivity', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 0.7, 'strict_all_score': 0.8, 'vital_score': 0.85, 'all_score': 0.9}"
