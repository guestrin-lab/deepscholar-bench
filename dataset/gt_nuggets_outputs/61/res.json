{
  "qid": "2504.12900v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nPersonalized outfit generation aims to construct a set of compatible and\npersonalized fashion items as an outfit. Recently, generative AI models have\nreceived widespread attention, as they can generate fashion items for users to\ncomplete an incomplete outfit or create a complete outfit. However, they have\nlimitations in terms of lacking diversity and relying on the supervised\nlearning paradigm. Recognizing this gap, we propose a novel framework\nFashionDPO, which fine-tunes the fashion outfit generation model using direct\npreference optimization. This framework aims to provide a general fine-tuning\napproach to fashion generative models, refining a pre-trained fashion outfit\ngeneration model using automatically generated feedback, without the need to\ndesign a task-specific reward function. To make sure that the feedback is\ncomprehensive and objective, we design a multi-expert feedback generation\nmodule which covers three evaluation perspectives, \\ie quality, compatibility\nand personalization. Experiments on two established datasets, \\ie iFashion and\nPolyvore-U, demonstrate the effectiveness of our framework in enhancing the\nmodel's ability to align with users' personalized preferences while adhering to\nfashion compatibility principles. Our code and model checkpoints are available\nat https://github.com/Yzcreator/FashionDPO.",
  "nuggets": [
    {
      "text": "Outfit recommendation focuses on compatibility and personalization",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Personalized outfit generation combines compatibility and user preferences",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Generative models improve quality and diversity of fashion item generation",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Direct Preference Optimization (DPO) fine-tunes models using preference feedback",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "DPO reduces reliance on explicit rewards in model training",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Early works retrieved compatible outfits, lacking personalization",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Bundle recommendation generalizes outfit recommendation using advanced models",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Retrieval-based methods limited by dataset variety and product details",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "PFITB task generates personalized outfits from user interaction history",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Fashion image generation uses deep learning for clothing design and try-on",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "GANs and diffusion models enhance virtual try-on image quality",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Personalized image generation aligns with reference styles or elements",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Diffusion-DPO and D3PO apply DPO to text-to-image diffusion models",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "SPO assesses preferences at each diffusion step for image generation",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Preference-based learning enables richer clothing textures and details",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "Outfit recommendation focuses on compatibility and personalization",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Personalized outfit generation combines compatibility and user preferences",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Generative models improve quality and diversity of fashion item generation",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Direct Preference Optimization (DPO) fine-tunes models using preference feedback",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "DPO reduces reliance on explicit rewards in model training",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Early works retrieved compatible outfits, lacking personalization",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Bundle recommendation generalizes outfit recommendation using advanced models",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Retrieval-based methods limited by dataset variety and product details",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "PFITB task generates personalized outfits from user interaction history",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Fashion image generation uses deep learning for clothing design and try-on",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "GANs and diffusion models enhance virtual try-on image quality",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Personalized image generation aligns with reference styles or elements",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Diffusion-DPO and D3PO apply DPO to text-to-image diffusion models",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "SPO assesses preferences at each diffusion step for image generation",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Preference-based learning enables richer clothing textures and details",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "Outfit recommendation focuses on compatibility and personalization",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Personalized outfit generation combines compatibility and user preferences",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Generative models improve quality and diversity of fashion item generation",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Direct Preference Optimization (DPO) fine-tunes models using preference feedback",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "DPO reduces reliance on explicit rewards in model training",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Early works retrieved compatible outfits, lacking personalization",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Bundle recommendation generalizes outfit recommendation using advanced models",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Retrieval-based methods limited by dataset variety and product details",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "PFITB task generates personalized outfits from user interaction history",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Fashion image generation uses deep learning for clothing design and try-on",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "GANs and diffusion models enhance virtual try-on image quality",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Personalized image generation aligns with reference styles or elements",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Diffusion-DPO and D3PO apply DPO to text-to-image diffusion models",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "SPO assesses preferences at each diffusion step for image generation",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Preference-based learning enables richer clothing textures and details",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 1.0,
    "strict_all_score": 1.0,
    "vital_score": 1.0,
    "all_score": 1.0
  }
}