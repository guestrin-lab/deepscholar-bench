qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2504.12900v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
Personalized outfit generation aims to construct a set of compatible and
personalized fashion items as an outfit. Recently, generative AI models have
received widespread attention, as they can generate fashion items for users to
complete an incomplete outfit or create a complete outfit. However, they have
limitations in terms of lacking diversity and relying on the supervised
learning paradigm. Recognizing this gap, we propose a novel framework
FashionDPO, which fine-tunes the fashion outfit generation model using direct
preference optimization. This framework aims to provide a general fine-tuning
approach to fashion generative models, refining a pre-trained fashion outfit
generation model using automatically generated feedback, without the need to
design a task-specific reward function. To make sure that the feedback is
comprehensive and objective, we design a multi-expert feedback generation
module which covers three evaluation perspectives, \ie quality, compatibility
and personalization. Experiments on two established datasets, \ie iFashion and
Polyvore-U, demonstrate the effectiveness of our framework in enhancing the
model's ability to align with users' personalized preferences while adhering to
fashion compatibility principles. Our code and model checkpoints are available
at https://github.com/Yzcreator/FashionDPO.","[{'text': 'Outfit recommendation focuses on compatibility and personalization', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Personalized outfit generation combines compatibility and user preferences', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Generative models improve quality and diversity of fashion item generation', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Direct Preference Optimization (DPO) fine-tunes models using preference feedback', 'importance': 'vital', 'assignment': 'support'}, {'text': 'DPO reduces reliance on explicit rewards in model training', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Early works retrieved compatible outfits, lacking personalization', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Bundle recommendation generalizes outfit recommendation using advanced models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Retrieval-based methods limited by dataset variety and product details', 'importance': 'okay', 'assignment': 'support'}, {'text': 'PFITB task generates personalized outfits from user interaction history', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Fashion image generation uses deep learning for clothing design and try-on', 'importance': 'okay', 'assignment': 'support'}, {'text': 'GANs and diffusion models enhance virtual try-on image quality', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Personalized image generation aligns with reference styles or elements', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Diffusion-DPO and D3PO apply DPO to text-to-image diffusion models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'SPO assesses preferences at each diffusion step for image generation', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Preference-based learning enables richer clothing textures and details', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Outfit recommendation focuses on compatibility and personalization', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Personalized outfit generation combines compatibility and user preferences', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Generative models improve quality and diversity of fashion item generation', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Direct Preference Optimization (DPO) fine-tunes models using preference feedback', 'importance': 'vital', 'assignment': 'support'}, {'text': 'DPO reduces reliance on explicit rewards in model training', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Early works retrieved compatible outfits, lacking personalization', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Bundle recommendation generalizes outfit recommendation using advanced models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Retrieval-based methods limited by dataset variety and product details', 'importance': 'okay', 'assignment': 'support'}, {'text': 'PFITB task generates personalized outfits from user interaction history', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Fashion image generation uses deep learning for clothing design and try-on', 'importance': 'okay', 'assignment': 'support'}, {'text': 'GANs and diffusion models enhance virtual try-on image quality', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Personalized image generation aligns with reference styles or elements', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Diffusion-DPO and D3PO apply DPO to text-to-image diffusion models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'SPO assesses preferences at each diffusion step for image generation', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Preference-based learning enables richer clothing textures and details', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Outfit recommendation focuses on compatibility and personalization', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Personalized outfit generation combines compatibility and user preferences', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Generative models improve quality and diversity of fashion item generation', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Direct Preference Optimization (DPO) fine-tunes models using preference feedback', 'importance': 'vital', 'assignment': 'support'}, {'text': 'DPO reduces reliance on explicit rewards in model training', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Early works retrieved compatible outfits, lacking personalization', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Bundle recommendation generalizes outfit recommendation using advanced models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Retrieval-based methods limited by dataset variety and product details', 'importance': 'okay', 'assignment': 'support'}, {'text': 'PFITB task generates personalized outfits from user interaction history', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Fashion image generation uses deep learning for clothing design and try-on', 'importance': 'okay', 'assignment': 'support'}, {'text': 'GANs and diffusion models enhance virtual try-on image quality', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Personalized image generation aligns with reference styles or elements', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Diffusion-DPO and D3PO apply DPO to text-to-image diffusion models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'SPO assesses preferences at each diffusion step for image generation', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Preference-based learning enables richer clothing textures and details', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 1.0, 'strict_all_score': 1.0, 'vital_score': 1.0, 'all_score': 1.0}"
