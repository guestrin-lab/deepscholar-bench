qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2506.02847v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
Deploying large language models (LLMs) on edge devices is crucial for
delivering fast responses and ensuring data privacy. However, the limited
storage, weight, and power of edge devices make it difficult to deploy
LLM-powered applications. These devices must balance latency requirements with
energy consumption and model accuracy. In this paper, we first quantify the
challenges of deploying LLMs on off-the-shelf edge devices and then we present
CLONE, an in-depth algorithm-hardware co-design at both the model- and
system-level that intelligently integrates real-time, energy optimization while
maintaining robust generality. In order to maximize the synergistic benefits of
these algorithms in always-on and intermediate edge computing settings, we
specialize in a 28nm scalable hardware accelerator system. We implement and
extensively evaluate CLONE on two off-the-shelf edge platforms. Experiments
show that CLONE effectively accelerates the inference process up to 11.92x, and
saves energy up to 7.36x, while maintaining high-generation.","[{'text': 'LLMs are memory-, compute-, and energy-intensive, often requiring cloud inference', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Edge LLM inference enhances data privacy and real-time service delivery', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Edge devices face storage, weight, and power constraints for LLM deployment', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Model-level optimizations: quantization, pruning, distillation, architecture search', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Most prior work neglects system-level trade-offs like storage and weight efficiency', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Existing model customization methods struggle with task-agnostic, general LLMs', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Edge–cloud collaboration enables selective offloading for complex LLM tasks', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Edge-first execution preserves privacy and real-time responsiveness', 'importance': 'vital', 'assignment': 'support'}, {'text': 'LLM compilers and software stacks enable co-processor and near-sensor integration', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Software stack integration can add computational and communication overhead', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LLM decoder layers and stochastic outputs offer hardware optimization opportunities', 'importance': 'okay', 'assignment': 'support'}, {'text': 'DVFS dynamically adjusts processor voltage/frequency for system-level optimization', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Most DVFS strategies target discriminative models, not generative LLMs', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Selective cloud offloading amortizes bandwidth and compute costs', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'LLMs are memory-, compute-, and energy-intensive, often requiring cloud inference', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Edge LLM inference enhances data privacy and real-time service delivery', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Model-level optimizations: quantization, pruning, distillation, architecture search', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Most prior work neglects system-level trade-offs like storage and weight efficiency', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Existing model customization methods struggle with task-agnostic, general LLMs', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Edge–cloud collaboration enables selective offloading for complex LLM tasks', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Edge-first execution preserves privacy and real-time responsiveness', 'importance': 'vital', 'assignment': 'support'}, {'text': 'LLM compilers and software stacks enable co-processor and near-sensor integration', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Software stack integration can add computational and communication overhead', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LLM decoder layers and stochastic outputs offer hardware optimization opportunities', 'importance': 'okay', 'assignment': 'support'}, {'text': 'DVFS dynamically adjusts processor voltage/frequency for system-level optimization', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Most DVFS strategies target discriminative models, not generative LLMs', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Selective cloud offloading amortizes bandwidth and compute costs', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'LLMs are memory-, compute-, and energy-intensive, often requiring cloud inference', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Edge LLM inference enhances data privacy and real-time service delivery', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Edge devices face storage, weight, and power constraints for LLM deployment', 'importance': 'vital', 'assignment': 'partial_support'}, {'text': 'Model-level optimizations: quantization, pruning, distillation, architecture search', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Most prior work neglects system-level trade-offs like storage and weight efficiency', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Existing model customization methods struggle with task-agnostic, general LLMs', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Edge–cloud collaboration enables selective offloading for complex LLM tasks', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Edge-first execution preserves privacy and real-time responsiveness', 'importance': 'vital', 'assignment': 'support'}, {'text': 'LLM compilers and software stacks enable co-processor and near-sensor integration', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Software stack integration can add computational and communication overhead', 'importance': 'okay', 'assignment': 'support'}, {'text': 'LLM decoder layers and stochastic outputs offer hardware optimization opportunities', 'importance': 'okay', 'assignment': 'support'}, {'text': 'DVFS dynamically adjusts processor voltage/frequency for system-level optimization', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Most DVFS strategies target discriminative models, not generative LLMs', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Selective cloud offloading amortizes bandwidth and compute costs', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 0.875, 'strict_all_score': 0.9285714285714286, 'vital_score': 0.9375, 'all_score': 0.9642857142857143}"
