{
  "qid": "2506.02847v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nDeploying large language models (LLMs) on edge devices is crucial for\ndelivering fast responses and ensuring data privacy. However, the limited\nstorage, weight, and power of edge devices make it difficult to deploy\nLLM-powered applications. These devices must balance latency requirements with\nenergy consumption and model accuracy. In this paper, we first quantify the\nchallenges of deploying LLMs on off-the-shelf edge devices and then we present\nCLONE, an in-depth algorithm-hardware co-design at both the model- and\nsystem-level that intelligently integrates real-time, energy optimization while\nmaintaining robust generality. In order to maximize the synergistic benefits of\nthese algorithms in always-on and intermediate edge computing settings, we\nspecialize in a 28nm scalable hardware accelerator system. We implement and\nextensively evaluate CLONE on two off-the-shelf edge platforms. Experiments\nshow that CLONE effectively accelerates the inference process up to 11.92x, and\nsaves energy up to 7.36x, while maintaining high-generation.",
  "nuggets": [
    {
      "text": "LLMs are memory-, compute-, and energy-intensive, often requiring cloud inference",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Edge LLM inference enhances data privacy and real-time service delivery",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Edge devices face storage, weight, and power constraints for LLM deployment",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Model-level optimizations: quantization, pruning, distillation, architecture search",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Most prior work neglects system-level trade-offs like storage and weight efficiency",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Existing model customization methods struggle with task-agnostic, general LLMs",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Edge–cloud collaboration enables selective offloading for complex LLM tasks",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Edge-first execution preserves privacy and real-time responsiveness",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "LLM compilers and software stacks enable co-processor and near-sensor integration",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Software stack integration can add computational and communication overhead",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "LLM decoder layers and stochastic outputs offer hardware optimization opportunities",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "DVFS dynamically adjusts processor voltage/frequency for system-level optimization",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Most DVFS strategies target discriminative models, not generative LLMs",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Selective cloud offloading amortizes bandwidth and compute costs",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "LLMs are memory-, compute-, and energy-intensive, often requiring cloud inference",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Edge LLM inference enhances data privacy and real-time service delivery",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Model-level optimizations: quantization, pruning, distillation, architecture search",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Most prior work neglects system-level trade-offs like storage and weight efficiency",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Existing model customization methods struggle with task-agnostic, general LLMs",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Edge–cloud collaboration enables selective offloading for complex LLM tasks",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Edge-first execution preserves privacy and real-time responsiveness",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "LLM compilers and software stacks enable co-processor and near-sensor integration",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Software stack integration can add computational and communication overhead",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "LLM decoder layers and stochastic outputs offer hardware optimization opportunities",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "DVFS dynamically adjusts processor voltage/frequency for system-level optimization",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Most DVFS strategies target discriminative models, not generative LLMs",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Selective cloud offloading amortizes bandwidth and compute costs",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "LLMs are memory-, compute-, and energy-intensive, often requiring cloud inference",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Edge LLM inference enhances data privacy and real-time service delivery",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Edge devices face storage, weight, and power constraints for LLM deployment",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Model-level optimizations: quantization, pruning, distillation, architecture search",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Most prior work neglects system-level trade-offs like storage and weight efficiency",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Existing model customization methods struggle with task-agnostic, general LLMs",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Edge–cloud collaboration enables selective offloading for complex LLM tasks",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Edge-first execution preserves privacy and real-time responsiveness",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "LLM compilers and software stacks enable co-processor and near-sensor integration",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Software stack integration can add computational and communication overhead",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "LLM decoder layers and stochastic outputs offer hardware optimization opportunities",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "DVFS dynamically adjusts processor voltage/frequency for system-level optimization",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Most DVFS strategies target discriminative models, not generative LLMs",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Selective cloud offloading amortizes bandwidth and compute costs",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.875,
    "strict_all_score": 0.9285714285714286,
    "vital_score": 0.9375,
    "all_score": 0.9642857142857143
  }
}