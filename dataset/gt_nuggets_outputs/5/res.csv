qid,query,nuggets,supported_nuggets,partially_supported_nuggets,nuggets_metrics
2505.24754v1,"Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:
In this work, we investigate an important task named instruction-following
text embedding, which generates dynamic text embeddings that adapt to user
instructions, highlighting specific attributes of text. Despite recent
advancements, existing approaches suffer from significant computational
overhead, as they require re-encoding the entire corpus for each new
instruction. To address this challenge, we propose GSTransform, a novel
instruction-following text embedding framework based on Guided Space
Transformation. Our key observation is that instruction-relevant information is
inherently encoded in generic embeddings but remains underutilized. Instead of
repeatedly encoding the corpus for each instruction, GSTransform is a
lightweight transformation mechanism that adapts pre-computed embeddings in
real time to align with user instructions, guided by a small amount of text
data with instruction-focused label annotation. We conduct extensive
experiments on three instruction-awareness downstream tasks across nine
real-world datasets, demonstrating that GSTransform improves
instruction-following text embedding quality over state-of-the-art methods
while achieving dramatic speedups of 6~300x in real-time processing on
large-scale datasets. The source code is available at
https://github.com/YingchaojieFeng/GSTransform.","[{'text': 'Instruction-following text embedding adapts embeddings to user instructions', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Existing methods require re-encoding corpus for each instruction', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Re-encoding leads to high computational overhead and latency', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Generic text embeddings use self-supervised learning and Transformer models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'InstructOR concatenates instructions with text for instruction-based embeddings', 'importance': 'okay', 'assignment': 'support'}, {'text': 'InBedder treats instructions as questions for fine-grained embeddings', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Instruction-following embeddings evaluated on Triplet Alignment, STS, Clustering', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Instruction-aware retrieval leverages user intent for improved search', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Large language models enhance state-of-the-art text embeddings', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Contrastive objectives commonly used in training embedding models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Prompt-based retrieval methods offer alternative instruction-aware approaches', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Instruction-following text embedding adapts embeddings to user instructions', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Existing methods require re-encoding corpus for each instruction', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Re-encoding leads to high computational overhead and latency', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Generic text embeddings use self-supervised learning and Transformer models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'InstructOR concatenates instructions with text for instruction-based embeddings', 'importance': 'okay', 'assignment': 'support'}, {'text': 'InBedder treats instructions as questions for fine-grained embeddings', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Instruction-following embeddings evaluated on Triplet Alignment, STS, Clustering', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Instruction-aware retrieval leverages user intent for improved search', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Large language models enhance state-of-the-art text embeddings', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Contrastive objectives commonly used in training embedding models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Prompt-based retrieval methods offer alternative instruction-aware approaches', 'importance': 'okay', 'assignment': 'support'}]","[{'text': 'Instruction-following text embedding adapts embeddings to user instructions', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Existing methods require re-encoding corpus for each instruction', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Re-encoding leads to high computational overhead and latency', 'importance': 'vital', 'assignment': 'support'}, {'text': 'Generic text embeddings use self-supervised learning and Transformer models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'InstructOR concatenates instructions with text for instruction-based embeddings', 'importance': 'okay', 'assignment': 'support'}, {'text': 'InBedder treats instructions as questions for fine-grained embeddings', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Instruction-following embeddings evaluated on Triplet Alignment, STS, Clustering', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Instruction-aware retrieval leverages user intent for improved search', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Large language models enhance state-of-the-art text embeddings', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Contrastive objectives commonly used in training embedding models', 'importance': 'okay', 'assignment': 'support'}, {'text': 'Prompt-based retrieval methods offer alternative instruction-aware approaches', 'importance': 'okay', 'assignment': 'support'}]","{'strict_vital_score': 1.0, 'strict_all_score': 1.0, 'vital_score': 1.0, 'all_score': 1.0}"
