{
  "qid": "2505.22167v1",
  "query": "Write a Related Works section for an academic paper given the paper's abstract. Here is the paper abstract:\nDiffusion transformers (DiT) have demonstrated exceptional performance in\nvideo generation. However, their large number of parameters and high\ncomputational complexity limit their deployment on edge devices. Quantization\ncan reduce storage requirements and accelerate inference by lowering the\nbit-width of model parameters. Yet, existing quantization methods for image\ngeneration models do not generalize well to video generation tasks. We identify\ntwo primary challenges: the loss of information during quantization and the\nmisalignment between optimization objectives and the unique requirements of\nvideo generation. To address these challenges, we present Q-VDiT, a\nquantization framework specifically designed for video DiT models. From the\nquantization perspective, we propose the Token-aware Quantization Estimator\n(TQE), which compensates for quantization errors in both the token and feature\ndimensions. From the optimization perspective, we introduce Temporal\nMaintenance Distillation (TMD), which preserves the spatiotemporal correlations\nbetween frames and enables the optimization of each frame with respect to the\noverall video context. Our W3A6 Q-VDiT achieves a scene consistency of 23.40,\nsetting a new benchmark and outperforming current state-of-the-art quantization\nmethods by 1.9$\\times$. Code will be available at\nhttps://github.com/cantbebetter2/Q-VDiT.",
  "nuggets": [
    {
      "text": "Diffusion transformers (DiT) excel in video generation tasks",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "DiT models have high parameter counts and computational complexity",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Quantization reduces model size and accelerates inference",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Existing quantization methods target image generation, not video",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Video generation quantization faces information loss and optimization misalignment",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Q-VDiT is a quantization framework for video DiT models",
      "importance": "vital",
      "assignment": "not_support"
    },
    {
      "text": "Diffusion models generate high-quality images via denoising Gaussian noise",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Q-DM, BinaryDM, BiDM, TerDiT use quantization-aware training for diffusion models",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Q-Diffusion, PTQ4DM, PTQ-D, TFMQ-DM, QuEST, EfficientDM, MixDQ focus on Unet-based diffusion quantization",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Q-DiT, PTQ4DiT, SVDQuant, ViDiT-Q address diffusion transformer quantization",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Current quantization methods require extensive training or focus on images",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Token-aware Quantization Estimator (TQE) compensates for token and feature quantization errors",
      "importance": "okay",
      "assignment": "not_support"
    },
    {
      "text": "Temporal Maintenance Distillation (TMD) preserves spatiotemporal frame correlations",
      "importance": "okay",
      "assignment": "not_support"
    },
    {
      "text": "Q-VDiT achieves state-of-the-art scene consistency and outperforms prior methods",
      "importance": "okay",
      "assignment": "not_support"
    }
  ],
  "supported_nuggets": [
    {
      "text": "Existing quantization methods target image generation, not video",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Diffusion models generate high-quality images via denoising Gaussian noise",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Q-DM, BinaryDM, BiDM, TerDiT use quantization-aware training for diffusion models",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Q-Diffusion, PTQ4DM, PTQ-D, TFMQ-DM, QuEST, EfficientDM, MixDQ focus on Unet-based diffusion quantization",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Q-DiT, PTQ4DiT, SVDQuant, ViDiT-Q address diffusion transformer quantization",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Current quantization methods require extensive training or focus on images",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "partially_supported_nuggets": [
    {
      "text": "Existing quantization methods target image generation, not video",
      "importance": "vital",
      "assignment": "support"
    },
    {
      "text": "Video generation quantization faces information loss and optimization misalignment",
      "importance": "vital",
      "assignment": "partial_support"
    },
    {
      "text": "Diffusion models generate high-quality images via denoising Gaussian noise",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Q-DM, BinaryDM, BiDM, TerDiT use quantization-aware training for diffusion models",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Q-Diffusion, PTQ4DM, PTQ-D, TFMQ-DM, QuEST, EfficientDM, MixDQ focus on Unet-based diffusion quantization",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Q-DiT, PTQ4DiT, SVDQuant, ViDiT-Q address diffusion transformer quantization",
      "importance": "okay",
      "assignment": "support"
    },
    {
      "text": "Current quantization methods require extensive training or focus on images",
      "importance": "okay",
      "assignment": "support"
    }
  ],
  "nuggets_metrics": {
    "strict_vital_score": 0.16666666666666666,
    "strict_all_score": 0.42857142857142855,
    "vital_score": 0.25,
    "all_score": 0.4642857142857143
  }
}