{
    "type": "final",
    "id": "1e1d190d-c97f-46e9-8259-23b04fffb54c",
    "title": "Related Works",
    "query": "Your task is to write a Related Works section for an academic paper given the paper's abstract. Your response should provide the Related Works section and references. Only include references from arXiv that are published before 2025-06-03T08:51:38+00:00. Mention them in a separate, numbered reference list at the end and use the reference numbers to provide in-line citations in the Related Works section for all claims referring to a source (e.g., description of source [3]. Further details [6][7][8][9][10].) Each in-line citation must consist of a single reference number within a pair of brackets. Do not use any other citation format. Do not exceed 600 words for the related works section. Here is the paper abstract:\nServing large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity..",
    "description": "The efficient serving of large language models (LLMs) in production environments has driven significant research into key-value (KV) cache management, as the KV cache is a primary bottleneck for both memory usage and inference latency during long-context or high-throughput workloads. This section reviews the landscape of KV cache management, focusing on cache eviction policies, compression techniques, workload characterization, and system-level optimizations.",
    "introduction": "## Related Works\n\nThe efficient serving of large language models (LLMs) in production environments has driven significant research into key-value (KV) cache management, as the KV cache is a primary bottleneck for both memory usage and inference latency during long-context or high-throughput workloads. This section reviews the landscape of KV cache management, focusing on cache eviction policies, compression techniques, workload characterization, and system-level optimizations.\n\n### KV Cache Management and Compression\n\nKV cache management strategies are broadly categorized into token-level, model-level, and system-level optimizations. Token-level approaches include selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level strategies focus on architectural changes and attention mechanisms to enhance KV reuse. System-level methods address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\].\n\nA prominent line of work addresses KV cache compression to mitigate memory overhead. Quantization-based methods, such as KIVI, propose tuning-free, hardware-friendly 2-bit quantization schemes that exploit the distinct distributional properties of keys and values, enabling significant memory reduction with minimal quality loss \\[[Zirui Liu' 2024-02-05](http://arxiv.org/abs/2402.02750v2)\\]. MILLION introduces product quantization with outlier handling, achieving 4-bit KV cache compression and over 2x end-to-end performance gains at long context lengths \\[[Zongwu Wang' 2025-03-12](http://arxiv.org/abs/2504.03661v2)\\]. GEAR combines ultra-low precision quantization, low-rank approximation, and sparse correction to achieve near-lossless compression and up to 2.38x throughput improvement \\[[Hao Kang' 2024-03-08](http://arxiv.org/abs/2403.05527v4)\\]. Mixed-precision approaches, such as MiKV, retain important KV pairs at high precision while compressing less critical pairs, balancing compression ratio and generation quality \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. For video LLMs, VidKV demonstrates that aggressive quantization (down to 1.5 bits) can be achieved with minimal performance loss by leveraging mixed-precision and per-channel strategies \\[[Keda Tao' 2025-03-20](http://arxiv.org/abs/2503.16257v1)\\].\n\nPruning and eviction-based methods focus on removing less important tokens from the cache. KVzip introduces a query-agnostic eviction method that quantifies KV pair importance via context reconstruction, reducing cache size by 3-4x with negligible performance loss \\[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\\]. CAKE frames cache eviction as a \"cake-slicing problem,\" allocating cache resources adaptively across layers based on attention dynamics, and achieves over 10x speedup in decoding latency at extreme compression ratios \\[[Ziran Qin' 2025-03-16](http://arxiv.org/abs/2503.12491v1)\\]. CAOTE proposes an eviction criterion based on the contribution of cached tokens to attention outputs, integrating value vector information for more accurate token selection \\[[Raghavv Goel' 2025-04-18](http://arxiv.org/abs/2504.14051v3)\\]. RoCo leverages temporal attention scores and robustness measures to improve eviction",
    "overview": "\n\n### KV Cache Management and Compression\n\nKV cache management strategies are broadly categorized into token-level, model-level, and system-level optimizations. Token-level approaches include selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level strategies focus on architectural changes and attention mechanisms to enhance KV reuse. System-level methods address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\].\n\nA prominent line of work addresses KV cache compression to mitigate memory overhead. Quantization-based methods, such as KIVI, propose tuning-free, hardware-friendly 2-bit quantization schemes that exploit the distinct distributional properties of keys and values, enabling significant memory reduction with minimal quality loss \\[[Zirui Liu' 2024-02-05](http://arxiv.org/abs/2402.02750v2)\\]. MILLION introduces product quantization with outlier handling, achieving 4-bit KV cache compression and over 2x end-to-end performance gains at long context lengths \\[[Zongwu Wang' 2025-03-12](http://arxiv.org/abs/2504.03661v2)\\]. GEAR combines ultra-low precision quantization, low-rank approximation, and sparse correction to achieve near-lossless compression and up to 2.38x throughput improvement \\[[Hao Kang' 2024-03-08](http://arxiv.org/abs/2403.05527v4)\\]. Mixed-precision approaches, such as MiKV, retain important KV pairs at high precision while compressing less critical pairs, balancing compression ratio and generation quality \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. For video LLMs, VidKV demonstrates that aggressive quantization (down to 1.5 bits) can be achieved with minimal performance loss by leveraging mixed-precision and per-channel strategies \\[[Keda Tao' 2025-03-20](http://arxiv.org/abs/2503.16257v1)\\].\n\nPruning and eviction-based methods focus on removing less important tokens from the cache. KVzip introduces a query-agnostic eviction method that quantifies KV pair importance via context reconstruction, reducing cache size by 3-4x with negligible performance loss \\[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\\]. CAKE frames cache eviction as a \"cake-slicing problem,\" allocating cache resources adaptively across layers based on attention dynamics, and achieves over 10x speedup in decoding latency at extreme compression ratios \\[[Ziran Qin' 2025-03-16](http://arxiv.org/abs/2503.12491v1)\\]. CAOTE proposes an eviction criterion based on the contribution of cached tokens to attention outputs, integrating value vector information for more accurate token selection \\[[Raghavv Goel' 2025-04-18](http://arxiv.org/abs/2504.14051v3)\\]. RoCo leverages temporal attention scores and robustness measures to improve eviction",
    "researchGroups": [],
    "paperGroups": [],
    "sources": {
        "arxiv": 30
    },
    "full_report": "## Related Works\n\nThe efficient serving of large language models (LLMs) in production environments has driven significant research into key-value (KV) cache management, as the KV cache is a primary bottleneck for both memory usage and inference latency during long-context or high-throughput workloads. This section reviews the landscape of KV cache management, focusing on cache eviction policies, compression techniques, workload characterization, and system-level optimizations.\n\n### KV Cache Management and Compression\n\nKV cache management strategies are broadly categorized into token-level, model-level, and system-level optimizations. Token-level approaches include selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level strategies focus on architectural changes and attention mechanisms to enhance KV reuse. System-level methods address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\].\n\nA prominent line of work addresses KV cache compression to mitigate memory overhead. Quantization-based methods, such as KIVI, propose tuning-free, hardware-friendly 2-bit quantization schemes that exploit the distinct distributional properties of keys and values, enabling significant memory reduction with minimal quality loss \\[[Zirui Liu' 2024-02-05](http://arxiv.org/abs/2402.02750v2)\\]. MILLION introduces product quantization with outlier handling, achieving 4-bit KV cache compression and over 2x end-to-end performance gains at long context lengths \\[[Zongwu Wang' 2025-03-12](http://arxiv.org/abs/2504.03661v2)\\]. GEAR combines ultra-low precision quantization, low-rank approximation, and sparse correction to achieve near-lossless compression and up to 2.38x throughput improvement \\[[Hao Kang' 2024-03-08](http://arxiv.org/abs/2403.05527v4)\\]. Mixed-precision approaches, such as MiKV, retain important KV pairs at high precision while compressing less critical pairs, balancing compression ratio and generation quality \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. For video LLMs, VidKV demonstrates that aggressive quantization (down to 1.5 bits) can be achieved with minimal performance loss by leveraging mixed-precision and per-channel strategies \\[[Keda Tao' 2025-03-20](http://arxiv.org/abs/2503.16257v1)\\].\n\nPruning and eviction-based methods focus on removing less important tokens from the cache. KVzip introduces a query-agnostic eviction method that quantifies KV pair importance via context reconstruction, reducing cache size by 3-4x with negligible performance loss \\[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\\]. CAKE frames cache eviction as a \"cake-slicing problem,\" allocating cache resources adaptively across layers based on attention dynamics, and achieves over 10x speedup in decoding latency at extreme compression ratios \\[[Ziran Qin' 2025-03-16](http://arxiv.org/abs/2503.12491v1)\\]. CAOTE proposes an eviction criterion based on the contribution of cached tokens to attention outputs, integrating value vector information for more accurate token selection \\[[Raghavv Goel' 2025-04-18](http://arxiv.org/abs/2504.14051v3)\\]. RoCo leverages temporal attention scores and robustness measures to improve eviction"
}