1e1d190d-c97f-46e9-8259-23b04fffb54c, generate_queries, 0.6087052822113037, {'generate_queries': 2}
1e1d190d-c97f-46e9-8259-23b04fffb54c, lotus_search_async, 1.216679573059082, {'multiquery_search': 83}
1e1d190d-c97f-46e9-8259-23b04fffb54c, multiquery_arxiv_search, 1.8305549621582031, {'step': 'Generated arxiv queries and got arxiv search results', 'extra_info': {'source': ['arxiv'], 'num_queries': 2, 'num_docs': 83}, 'message': 'Generated 2 queries and got 83 results'}
1e1d190d-c97f-46e9-8259-23b04fffb54c, generate_background, 16.056593418121338, {'step': 'Generated summary of web search results', 'extra_info': {'num_papers_to_summarize': 83}, 'message': 'Generated summary of 83 results'}
1e1d190d-c97f-46e9-8259-23b04fffb54c, generate_queries, 0.6641144752502441, {'generate_queries': 2}
1e1d190d-c97f-46e9-8259-23b04fffb54c, lotus_search_async, 1.1057355403900146, {'multiquery_search': 87}
1e1d190d-c97f-46e9-8259-23b04fffb54c, multiquery_arxiv_search, 1.7753822803497314, {'step': 'Generated arxiv queries and got arxiv search results', 'extra_info': {'source': ['arxiv'], 'num_queries': 2, 'num_docs': 87}, 'message': 'Generated 2 queries and got 87 results'}
1e1d190d-c97f-46e9-8259-23b04fffb54c, generate_background, 25.64255666732788, {'step': 'Generated summary of web search results', 'extra_info': {'num_papers_to_summarize': 87}, 'message': 'Generated summary of 87 results'}
1e1d190d-c97f-46e9-8259-23b04fffb54c, sem_filter, 52.57128858566284, {'papers_after_sem_filter': 67}
1e1d190d-c97f-46e9-8259-23b04fffb54c, sem_topk, 389.2932679653168, {'papers_after_sem_topk': 30}
1e1d190d-c97f-46e9-8259-23b04fffb54c, filter_docs, 441.8902087211609, {'step': 'Filtered arxiv search results', 'extra_info': {'num_filtered_papers': 30}, 'message': 'Filtered search results to 30 documents'}
1e1d190d-c97f-46e9-8259-23b04fffb54c, generate_insights, 10.192206144332886, {'step': 'Generated insights from papers', 'extra_info': {'num_papers_for_insight': 30}, 'message': 'Derived insights from 30 results'}
1e1d190d-c97f-46e9-8259-23b04fffb54c, gen_background, 11.409759283065796, {'step': 'Generated background section', 'extra_info': {}, 'message': 'Generated background section'}
1e1d190d-c97f-46e9-8259-23b04fffb54c, no_taxonomy_pipeline, 509.3147728443146, {'step': 'No taxonomy Pipeline Executed', 'extra_info': {'id': '1e1d190d-c97f-46e9-8259-23b04fffb54c', 'topic': "Your task is to write a Related Works section for an academic paper given the paper's abstract. Your response should provide the Related Works section and references. Only include references from arXiv that are published before 2025-06-03T08:51:38+00:00. Mention them in a separate, numbered reference list at the end and use the reference numbers to provide in-line citations in the Related Works section for all claims referring to a source (e.g., description of source [3]. Further details [6][7][8][9][10].) Each in-line citation must consist of a single reference number within a pair of brackets. Do not use any other citation format. Do not exceed 600 words for the related works section. Here is the paper abstract:\nServing large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity..\nNote that today's date is 07/22/25.", 'date': '07/22/25', 'intro_section': '## Related Works\n\nThe efficient serving of large l', 'num_papers': 30, 'num_categories': 0, 'query': "Your task is to write a Related Works section for an academic paper given the paper's abstract. Your response should provide the Related Works section and references. Only include references from arXiv that are published before 2025-06-03T08:51:38+00:00. Mention them in a separate, numbered reference list at the end and use the reference numbers to provide in-line citations in the Related Works section for all claims referring to a source (e.g., description of source [3]. Further details [6][7][8][9][10].) Each in-line citation must consist of a single reference number within a pair of brackets. Do not use any other citation format. Do not exceed 600 words for the related works section. Here is the paper abstract:\nServing large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.."}, 'message': "Generated report from 30 papers for Your task is to write a Related Works section for an academic paper given the paper's abstract. Your response should provide the Related Works section and references. Only include references from arXiv that are published before 2025-06-03T08:51:38+00:00. Mention them in a separate, numbered reference list at the end and use the reference numbers to provide in-line citations in the Related Works section for all claims referring to a source (e.g., description of source [3]. Further details [6][7][8][9][10].) Each in-line citation must consist of a single reference number within a pair of brackets. Do not use any other citation format. Do not exceed 600 words for the related works section. Here is the paper abstract:\nServing large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.."}
