## Related Works

The rapid growth in the deployment of large language models (LLMs) has led to significant research on optimizing inference efficiency, with key-value (KV) cache management emerging as a central focus. KV caching, which stores intermediate results from attention computations, is widely recognized for its ability to accelerate LLM serving by reducing redundant computation and improving throughput and latency \[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\]\[[Yucheng Li' 2024-12-13](http://arxiv.org/abs/2412.10319v2)\]. However, the increasing context lengths and diverse workload patterns in real-world deployments have introduced new challenges in cache management, particularly regarding cache eviction policies and their impact on system performance.

### KV Cache Compression and Eviction Strategies

A substantial body of work has explored KV cache compression and eviction to address the memory and computational bottlenecks associated with long-context LLM inference. Early approaches primarily focused on token-level strategies, such as pruning unimportant tokens based on attention scores or quantizing KV pairs to lower precision \[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\]\[[Hao Kang' 2024-03-08](http://arxiv.org/abs/2403.05527v4)\]\[[Jiebin Zhang' 2024-12-17](http://arxiv.org/abs/2412.12706v2)\]\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\]\[[Zongwu Wang' 2025-03-12](http://arxiv.org/abs/2504.03661v2)\]\[[Zirui Liu' 2024-02-05](http://arxiv.org/abs/2402.02750v2)\]\[[Yanqi Zhang' 2024-12-04](http://arxiv.org/abs/2412.03131v2)\]\[[Isaac Rehg' 2024-09-30](http://arxiv.org/abs/2410.00161v2)\]\[[Keda Tao' 2025-03-20](http://arxiv.org/abs/2503.16257v1)\]. For instance, GEAR integrates quantization, low-rank approximation, and sparse correction to achieve near-lossless compression, significantly improving throughput and reducing memory usage \[[Hao Kang' 2024-03-08](http://arxiv.org/abs/2403.05527v4)\]. Similarly, MILLION employs product quantization to address outlier sensitivity in KV values, enabling 4-bit quantization with minimal accuracy loss \[[Zongwu Wang' 2025-03-12](http://arxiv.org/abs/2504.03661v2)\]. KIVI further refines quantization by applying per-channel and per-token strategies for keys and values, respectively, achieving substantial memory savings without compromising quality \[[Zirui Liu' 2024-02-05](http://arxiv.org/abs/2402.02750v2)\].

Beyond quantization, eviction-based methods have been developed to selectively remove less important KV pairs from the cache. KVzip introduces a query-agnostic eviction mechanism that quantifies KV pair importance via context reconstruction, enabling effective reuse and substantial cache size reduction with negligible performance loss \[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\]. CAKE frames eviction as a resource allocation problem across layers, adaptively distributing cache capacity based on layer-specific attention dynamics and token importance, resulting in significant memory savings and speedup \[[Ziran Qin' 2025-03-16](http://arxiv.org/abs/2503.12491v1)\]. Other works, such as RoCo, leverage temporal attention scores and robustness measures to improve eviction decisions, demonstrating superior performance over prior policies \[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\].

Recent research has also highlighted the limitations of uniform compression or eviction rates across attention heads and layers. UNComp addresses this by employing uncertainty-aware, adaptive compression based on matrix entropy, grouping layers and heads according to their uncertainty and selectively compressing both hidden states and KV cache \[[Jing Xiong' 2024-10-04](http://arxiv.org/abs/2410.03090v1)\]. KV-Compress similarly applies variable compression rates per attention head within a paged attention framework, achieving high compression ratios with minimal performance degradation \[[Isaac Rehg' 2024-09-30](http://arxiv.org/abs/2410.00161v2)\]. These adaptive approaches are particularly effective in heterogeneous workloads, where the importance of tokens and attention patterns can vary significantly across model components.

### Semantic and Structural Approaches

To further enhance cache efficiency, several works have proposed leveraging semantic or structural properties of the input. ClusterKV clusters tokens in semantic space, enabling recallable compression at the granularity of semantic clusters rather than fixed textual positions, which preserves accuracy and output quality even under aggressive compression \[[Guangda Liu' 2024-12-04](http://arxiv.org/abs/2412.03213v2)\]. PyramidInfer exploits the observation that the number of crucial KV pairs decreases across layers, retaining only the most influential context in a layer-wise manner to reduce memory usage without sacrificing performance \[[Dongjie Yang' 2024-05-21](http://arxiv.org/abs/2405.12532v2)\]. KVMerger identifies and merges similar KV states within a sequence, capitalizing on token-level similarity to adaptively compress the cache for long-context tasks \[[Zheng Wang' 2024-07-11](http://arxiv.org/abs/2407.08454v2)\].

### Mixed-Precision and Hybrid Methods

Mixed-precision strategies have gained traction as a means to balance compression and performance. MiKV proposes retaining evicted KV pairs in low precision while preserving important pairs at higher precision, mitigating the risks of context loss, hallucinations, and safety breaches associated with aggressive eviction \[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\]. Quantized pruning, which stores more tokens at lower precision, has been shown to enhance long-context performance and retrieval tasks, offering a stable and effective trade-off across various pruning and quantization methods \[[Jiebin Zhang' 2024-12-17](http://arxiv.org/abs/2412.12706v2)\]. LeanKV advances this line of work by differentiating the impact of keys and values, token importance, and dynamic sparsity patterns across attention heads, achieving high compression ratios with near-lossless accuracy \[[Yanqi Zhang' 2024-12-04](http://arxiv.org/abs/2412.03131v2)\].

### System-Level and Workload-Aware Optimizations

System-level optimizations have also been explored to address the challenges of dynamic and bursty workloads in real-world LLM serving. MorphServe introduces runtime mechanisms for quantized layer swapping and pressure-aware KV cache resizing, dynamically adapting to workload fluctuations and reducing service-level objective (SLO) violations without compromising generation quality \[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\]. PrefillOnly targets prefill-only workloads, where only a single output token is generated, by storing KV cache for only the last computed layer, drastically reducing memory footprint and enabling efficient scheduling \[[Kuntai Du' 2025-05-12](http://arxiv.org/abs/2505.07203v1)\]. Cache-Craft focuses on retrieval-augmented generation (RAG) scenarios, managing and reusing chunk-caches to minimize redundant computation and improve throughput \[[Shubham Agarwal' 2025-02-05](http://arxiv.org/abs/2502.15734v1)\].

The importance of workload-aware cache management is further underscored by studies that analyze real-world usage patterns. For example, KVzip and KV-Compress demonstrate that query-agnostic and variable-rate eviction policies can outperform traditional query-aware methods, especially in multi-query and long-context scenarios \[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\]\[[Isaac Rehg' 2024-09-30](http://arxiv.org/abs/2410.00161v2)\]. Analytical models have also been proposed to evaluate the economic trade-offs of KV cache reuse in cloud environments, showing that reuse can yield both delay and cost savings across diverse workloads \[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\].

### Benchmarks and Taxonomies

Comprehensive surveys and benchmarks have been developed to systematically evaluate KV cache management techniques. A recent survey categorizes strategies into token-level, model-level, and system-level optimizations, providing taxonomies and comparative analyses to guide future research \[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\]. SCBench offers a KV cache-centric benchmark that evaluates long-context methods across cache generation, compression, retrieval, and loading, revealing that dynamic sparsity and adaptive strategies yield more expressive and efficient caches than static patterns \[[Yucheng Li' 2024-12-13](http://arxiv.org/abs/2412.10319v2)\].

### Specialized and Privacy-Preserving Methods

Specialized methods have been proposed for unique deployment scenarios. MPCache adapts KV cache eviction for secure multi-party computation (MPC), combining static and dynamic selection algorithms with MPC-friendly optimizations to reduce latency and communication overhead in privacy-preserving inference \[[Wenxuan Zeng' 2025-01-12](http://arxiv.org/abs/2501.06807v1)\]. VidKV extends quantization techniques to video LLMs, introducing mixed-precision strategies tailored to the unique characteristics of visual tokens and demonstrating effective compression with minimal performance loss \[[Keda Tao' 2025-03-20](http://arxiv.org/abs/2503.16257v1)\].

### Limitations and Open Challenges

Despite significant progress, several challenges remain. Many existing methods rely on synthetic workloads or single-request benchmarks, which may not capture the diversity and unpredictability of real-world usage patterns \[[Yucheng Li' 2024-12-13](http://arxiv.org/abs/2412.10319v2)\]. Uniform compression or eviction strategies can harm performance in heterogeneous workloads, underscoring the need for adaptive, workload-aware approaches \[[Jing Xiong' 2024-10-04](http://arxiv.org/abs/2410.03090v1)\]\[[Isaac Rehg' 2024-09-30](http://arxiv.org/abs/2410.00161v2)\]. Additionally, the trade-offs between cache size, hit ratio, and system cost require further exploration, particularly in cloud environments with varying resource constraints \[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\].

### Summary Table: Key Approaches in KV Cache Management

| Approach Type         | Example Methods         | Key Features/Contributions                | Reference |
|-----------------------|------------------------|-------------------------------------------|-----------|
| Quantization          | GEAR, MILLION, KIVI    | Low-bit quantization, outlier handling    | \[[Hao Kang' 2024-03-08](http://arxiv.org/abs/2403.05527v4)\]\[[Zongwu Wang' 2025-03-12](http://arxiv.org/abs/2504.03661v2)\]\[[Zirui Liu' 2024-02-05](http://arxiv.org/abs/2402.02750v2)\] |
| Eviction/Pruning      | KVzip, CAKE, RoCo      | Importance-based, adaptive, temporal      | \[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\]\[[Ziran Qin' 2025-03-16](http://arxiv.org/abs/2503.12491v1)\]\[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\] |
| Mixed-Precision       | MiKV, Quantized Pruning| Retain important tokens at high precision | \[[Jiebin Zhang' 2024-12-17](http://arxiv.org/abs/2412.12706v2)\]\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\]  |
| Semantic/Structural   | ClusterKV, PyramidInfer| Semantic clustering, layer-wise retention | \[[Guangda Liu' 2024-12-04](http://arxiv.org/abs/2412.03213v2)\]\[[Dongjie Yang' 2024-05-21](http://arxiv.org/abs/2405.12532v2)\]   |
| Adaptive Compression  | UNComp, KV-Compress    | Uncertainty-aware, variable-rate eviction | \[[Jing Xiong' 2024-10-04](http://arxiv.org/abs/2410.03090v1)\]\[[Isaac Rehg' 2024-09-30](http://arxiv.org/abs/2410.00161v2)\]  |
| System-Level          | MorphServe, PrefillOnly| Runtime adaptation, workload awareness    | \[[Kuntai Du' 2025-05-12](http://arxiv.org/abs/2505.07203v1)\]\[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\]  |
| Privacy-Preserving    | MPCache                | MPC-friendly eviction and selection       | \[[Wenxuan Zeng' 2025-01-12](http://arxiv.org/abs/2501.06807v1)\]      |
| RAG Optimization      | Cache-Craft            | Chunk-cache reuse in retrieval scenarios  | \[[Shubham Agarwal' 2025-02-05](http://arxiv.org/abs/2502.15734v1)\]      |
| Benchmarking/Survey   | SCBench, Survey        | Taxonomies, comprehensive evaluation      | \[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\]\[[Yucheng Li' 2024-12-13](http://arxiv.org/abs/2412.10319v2)\]   |

### Conclusion

The literature on KV cache management for LLM serving is extensive and rapidly evolving, with a wide array of strategies addressing compression, eviction, quantization, and system-level adaptation. While significant advances have been made in reducing memory footprint and improving inference efficiency, the diversity of real-world workloads and the need for adaptive, workload-aware policies remain open challenges. Recent works increasingly emphasize the importance of characterizing actual usage patterns and developing flexible, context-sensitive cache management techniques to optimize LLM serving in production environments.