{
    "latency": 49.58656907081604,
    "output": "Related Works\n\nServing large language models (LLMs) efficiently has been a focal point of research, particularly as the demand for scalable, low-latency inference grows. A crucial component of LLM serving is the use of key-value (KV) caching to store intermediate results, which enables improved throughput and reduced latency. Recent studies have systematically examined both the mechanics and policies of KV caching to better align cache utilization with real-world workload demands.\n\nEarly approaches considered static cache eviction or uniform allocation strategies, which generally provided either simplicity or a limited improvement in storage efficiency. More recent work introduces adaptive and learning-based methods to optimize cache management. For instance, SAGE-KV leverages self-attention dynamics to identify and drop the least important KV pairs at both the token and head level, leading to substantial memory savings with minimal impact on accuracy [7]. In a similar direction, CAKE proposes a cascading and adaptive eviction policy that dynamically adjusts cache allocation across layers based on attention patterns, yielding significant speed gains and efficient cache utilization [6].\n\nImportance-aware approaches have proven particularly effective. The MiKV system compresses KV caches using mixed precision, retaining evicted pairs in lower precision to balance memory usage with generation quality [2]. BaKlaVa further advances this concept by budgeted allocation of memory to different KV-caches based on their contextual importance, resulting in high compression ratios and competitive accuracy [4]. Ada-KV applies adaptive budget allocation at a head-wise granularity, demonstrating that such fine-grained control can outperform uniform allocation while remaining compatible with other strategies [3].\n\nPractical deployment of LLMs also motivates novel architectural solutions. SpeCache, for example, speculatively offloads less critical portions of the KV cache to CPU memory while prefetching key pairs deemed likely to be reused, thereby reducing reliance on expensive VRAM and enabling efficient long-sequence inference [5]. Similarly, Attention-Gate equips LLMs with an in-context gating mechanism to determine in real-time which KV states to retain or discard, improving cache hit rates and maintaining overall model efficiency [1].\n\nA common thread across these studies is the observation that KV cache accesses and reuses are highly skewed, and workload characteristics significantly impact the optimality of cache policies. Despite synthetic benchmarks helping to establish baselines, multiple works\u2014including those using real-world traces\u2014have highlighted the need for workload-aware, dynamic eviction mechanisms to address practical serving scenarios. These studies underscore both the diversity and predictability of reuse patterns across different request categories, supporting the case for fine-grained and contextually adaptive cache management.\n\nReferences:\n1. In-context KV-Cache Eviction for LLMs via Attention-Gate (Oct 2024), http://arxiv.org/abs/2410.12876v3\n2. No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization (Feb 2024), http://arxiv.org/abs/2402.18096v1\n3. Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference (July 2024), http://arxiv.org/abs/2407.11550v4\n4. BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference (Feb 2025), http://arxiv.org/abs/2502.13176v2\n5. SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs (Mar 2025), http://arxiv.org/abs/2503.16163v1\n6. CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences (Mar 2025), http://arxiv.org/abs/2503.12491v1\n7. LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for Efficient Long-Context Inference (Mar 2025), http://arxiv.org/abs/2503.08879v1",
    "ctxs": [
        {
            "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
            "text": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
            "url": "http://arxiv.org/abs/2410.12876v3",
            "id": "http://arxiv.org/abs/2410.12876v3",
            "date": "2024-10-15 05:01:19+00:00"
        },
        {
            "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
            "text": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
            "url": "http://arxiv.org/abs/2503.16163v1",
            "id": "http://arxiv.org/abs/2503.16163v1",
            "date": "2025-03-20 14:01:56+00:00"
        },
        {
            "title": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization",
            "text": "Key-Value (KV) Caching has become an essential technique for accelerating the\ninference speed and throughput of generative Large Language Models~(LLMs).\nHowever, the memory footprint of the KV cache poses a critical bottleneck in\nLLM deployment as the cache size grows with batch size and sequence length,\noften surpassing even the size of the model itself. Although recent methods\nwere proposed to select and evict unimportant KV pairs from the cache to reduce\nmemory consumption, the potential ramifications of eviction on the generative\nprocess are yet to be thoroughly examined. In this paper, we examine the\ndetrimental impact of cache eviction and observe that unforeseen risks arise as\nthe information contained in the KV pairs is exhaustively discarded, resulting\nin safety breaches, hallucinations, and context loss. Surprisingly, we find\nthat preserving even a small amount of information contained in the evicted KV\npairs via reduced precision quantization substantially recovers the incurred\ndegradation. On the other hand, we observe that the important KV pairs must be\nkept at a relatively higher precision to safeguard the generation quality.\nMotivated by these observations, we propose \\textit{Mixed-precision KV\ncache}~(MiKV), a reliable cache compression method that simultaneously\npreserves the context details by retaining the evicted KV pairs in\nlow-precision and ensure generation quality by keeping the important KV pairs\nin high-precision. Experiments on diverse benchmarks and LLM backbones show\nthat our proposed method offers a state-of-the-art trade-off between\ncompression ratio and performance, compared to other baselines.",
            "url": "http://arxiv.org/abs/2402.18096v1",
            "id": "http://arxiv.org/abs/2402.18096v1",
            "date": "2024-02-28 06:34:54+00:00"
        },
        {
            "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
            "text": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
            "url": "http://arxiv.org/abs/2502.13176v2",
            "id": "http://arxiv.org/abs/2502.13176v2",
            "date": "2025-02-18 04:08:29+00:00"
        },
        {
            "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query",
            "text": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements.",
            "url": "http://arxiv.org/abs/2505.20334v1",
            "id": "http://arxiv.org/abs/2505.20334v1",
            "date": "2025-05-24 10:34:38+00:00"
        },
        {
            "title": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences",
            "text": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv.",
            "url": "http://arxiv.org/abs/2503.12491v1",
            "id": "http://arxiv.org/abs/2503.12491v1",
            "date": "2025-03-16 12:49:44+00:00"
        },
        {
            "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference",
            "text": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods.",
            "url": "http://arxiv.org/abs/2407.11550v4",
            "id": "http://arxiv.org/abs/2407.11550v4",
            "date": "2024-07-16 09:53:32+00:00"
        },
        {
            "title": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for Efficient Long-Context Inference",
            "text": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest.",
            "url": "http://arxiv.org/abs/2503.08879v1",
            "id": "http://arxiv.org/abs/2503.08879v1",
            "date": "2025-03-11 20:45:02+00:00"
        }
    ],
    "query": "Your task is to write a Related Works section for an academic paper given the paper's abstract. Your response should provide the Related Works section and references. Only include references from arXiv that are published before {03 Jun 2025}. Mention them in a separate, numbered reference list at the end and use the reference numbers to provide in-line citations in the Related Works section for all claims referring to a source (e.g., description of source [3]. Further details [6][7][8][9][10].) Each in-line citation must consist of a single reference number within a pair of brackets. Do not use any other citation format. Do not exceed 600 words for the related works section. Here is the paper abstract:\\n{Serving large language models (LLMs) is important for cloud providers, and\\ncaching intermediate results (KV\\\\$) after processing each request substantially\\nimproves serving throughput and latency. However, there is limited\\nunderstanding of how LLM serving benefits from KV\\\\$ caching, where system\\ndesign decisions like cache eviction policies are highly workload-dependent. In\\nthis paper, we present the first systematic characterization of the KV\\\\$\\nworkload patterns from one of the leading LLM service providers. We draw\\nobservations that were not covered by previous studies focusing on synthetic\\nworkloads, including: KV\\\\$ reuses are skewed across requests, where reuses\\nbetween single-turn requests are equally important as multi-turn requests; the\\nreuse time and probability are diverse considering all requests, but for a\\nspecific request category, the pattern tends to be predictable; and the overall\\ncache size required for an ideal cache hit ratio is moderate. Based on the\\ncharacterization, we further propose a workload-aware cache eviction policy\\nthat improves the serving performance under real-world traces, especially with\\nlimited cache capacity.}.",
    "usage": {
        "total_tokens": 9176,
        "cache_hits": 0,
        "total_cost_usd_dollar": 0.026637999999999995
    }
}