{
    "arxiv_id": "2506.02634v1",
    "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
    "authors": "Jiahao Wang, Jinbo Han, Xingda Wei, Sijie Shen, Dingyan Zhang, Chenguang Fang, Rong Chen, Wenyuan Yu, Haibo Chen",
    "abstract": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
    "categories": "cs.DC, cs.AI",
    "published_date": "2025-06-03T08:51:38+00:00",
    "updated_date": "2025-06-03T08:51:38+00:00",
    "abs_url": "http://arxiv.org/abs/2506.02634v1",
    "query": "\nYour task is to write a Related Works section for an academic paper given the paper's abstract. Your response should provide the Related Works section and references. Only include references that are published before 2025-06-03T08:51:38+00:00. Mention them in a separate reference list at the end and cite them properly in the Related Works section. Here is the paper abstract:\nServing large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity..\n",
    "id": 1,
    "raw_query": "\nYour task is to write a Related Works section for an academic paper given the paper's abstract. Your response should provide the Related Works section and references. Only include references that are published before 2025-06-03T08:51:38+00:00. Mention them in a separate reference list at the end and cite them properly in the Related Works section. Here is the paper abstract:\nServing large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity..\n",
    "ctxs": [
        {
            "id": "4735765",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \\textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \\textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \\textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \\textit{throttLL'eM} achieves up to 43.8\\% lower energy consumption and an energy efficiency improvement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's Triton server.",
            "retrieval next text": "Structural and Electrical Studies of Polycrystalline Pr\ud835\udc65Y1\u2212\ud835\udc65FeO3 Materials Prepared Through Sol-Gel Route Objectives: Investigations on structural, morphological and electrical studies of Pr substituted YFeO3 samples synthesized through sol-gel route are presented in this paper. Methods: PrxY1-xFeO3 (x=0.0, 0.2, 0.4, 0.6, 0.8 and 1.0) samples are synthesized by sol-gel auto combustion method. Structural characterization of the samples is done by using X-ray diffraction (XRD), Field emission scanning electron microscope (FESEM) and Energy dispersive X-ray (EDX) techniques. Electrical properties are studied by J-E and Ln J-Ln E measurements. Findings: Rietveld refined XRD structural analysis shows sharp intense peaks indicating the crystalline nature of the samples and all the samples possess distorted orthorhombic structure with space group Pbnm. FESEM analysis shows non-uniform nature of grain size with increasing porosity. EDX data confirms elemental composition, weight and atomic percentage of the prepared samples which further confirms their purity. The leakage current density increases with increase in Pr doping. The slopes of Ln J-Ln E graphs of all the samples are in the range of 1.11 to 1.51 confirming that Ohmic conduction is predominant conduction mechanism. Novelty: Investigations on the effect of Pr substitution on electrical properties of single phase YFeO3\u00ad samples reveal that the substitution of Pr in YFeO3 decreases the ferroelectric nature of the samples. Keywords: Sol-gel method, XRD, FESEM, EDX, Electrical Properties",
            "retrieval score": "1.4963772",
            "text": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \\textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \\textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \\textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \\textit{throttLL'eM} achieves up to 43.8\\% lower energy consumption and an energy efficiency improvement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's Triton server.",
            "title": ""
        },
        {
            "id": "5719275",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "the quantity of LLM-annotated data on which the first student model is trained, focusing on the setup with retraining (Table 6).We consider the two more challenging tasks, FEVER and Openbook.We find that QBC performs best overall, and the performance of MS is more sensitive to the initial budget.This observation suggests that determining the decision criteria for transitioning from a frontloading regime to MS poses a relevant question, which we leave for future exploration.and (ii) previous literature in active learning can transfer well to this setup.This is, to our knowledge, the first work that leverages online knowledge distillation, which we believe could play a key role in caching LLMs and saving unnecessary calls to expensive models. In this work, we focused on a stationary (i.i.d.) stream of requests.In practice, the distribution of requests is likely to change over time (Cacciarelli and Kulahci, 2023).As suggested by the online AL literature (Bifet and Gavald\u00e0, 2007), this should further increase the gap between the AL-based approaches and static strategies, e.g., front-loading.In those cases, we would expect improvements in both online and final accuracy.We leave this investigation for future work. Figure 1 : Figure 1: Neural caching (one iteration): A student generates a response to a user request.The policy algorithm determines whether to rely on the student's response or to call an LLM.LLM responses are stored and used to re-train the student as more data becomes available. Figure 3 : Figure 3: Accuracy curve with respect to budgets, in the neural caching problem with student retraining.Error lines indicate variance.We have averaged",
            "retrieval next text": "results across the four datasets. Table 1 : The accuracy of the LLM is similar to training the simple model with 5000 gold labels. Table 2 : Online accuracy (AUC) for neural caching with no student retraining. Table 3 : and per-dataset in Figure4.We observe that both MS and QBC considerably outperform the other methods.The embedding-based strategy Accuracy curve with respect to budgets, in the neural caching problem with student retraining.Error lines indicate variance.Online accuracy (AUC) for neural caching with student retraining. Table 4 : Final accuracy (AUC) of the last student model for neural caching with student retraining. Table 5 : Table5shows the absolute improvements in the online and final accu-Absolute improvements for the online and final accuracy using an oracle that allows us to discard instances with wrong labels from the LLM, averaged across datasets.The improvements are with respect to values from Table3 and 4.",
            "retrieval score": "1.4442198",
            "text": "the quantity of LLM-annotated data on which the first student model is trained, focusing on the setup with retraining (Table 6).We consider the two more challenging tasks, FEVER and Openbook.We find that QBC performs best overall, and the performance of MS is more sensitive to the initial budget.This observation suggests that determining the decision criteria for transitioning from a frontloading regime to MS poses a relevant question, which we leave for future exploration.and (ii) previous literature in active learning can transfer well to this setup.This is, to our knowledge, the first work that leverages online knowledge distillation, which we believe could play a key role in caching LLMs and saving unnecessary calls to expensive models. In this work, we focused on a stationary (i.i.d.) stream of requests.In practice, the distribution of requests is likely to change over time (Cacciarelli and Kulahci, 2023).As suggested by the online AL literature (Bifet and Gavald\u00e0, 2007), this should further increase the gap between the AL-based approaches and static strategies, e.g., front-loading.In those cases, we would expect improvements in both online and final accuracy.We leave this investigation for future work. Figure 1 : Figure 1: Neural caching (one iteration): A student generates a response to a user request.The policy algorithm determines whether to rely on the student's response or to call an LLM.LLM responses are stored and used to re-train the student as more data becomes available. Figure 3 : Figure 3: Accuracy curve with respect to budgets, in the neural caching problem with student retraining.Error lines indicate variance.We have averaged",
            "title": ""
        },
        {
            "id": "5277774",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "We have included three more similar recent works (years 2020 and 2021) in the \"Related work\" section. These works show how the latency is today an important problem in NVM architectures due to the mix of the very irregular data access patterns and the high inherent latency of NVM modules. Different hardware prefetching techniques appear in current literature showing the insterest of this topic in the research community. Related work: improve the contrast with current works to highlight novelty of our work We have included current researcho work covering the same topic. We compare the characteristics and limitations of the diferent works against our proposal showing how we deal with some of those limitations by allowing a more complex and complete prefetch solution. The \"Related work\" section should be described in the introduction This section is described in the \"Introduction\". Reviewer #3 Comments Impact of HMM in the production final price. The impact of our prefetch technique in the final production price is an important issue. Sometimes as researchers we are too focused in the design and technical topics and we forget that contributions to the state-of-the-art should also be applied in order to be useful to society. We appreciate the reviewer #3 comments and we have included, in subsection \"Prefetcher Implementation\", an study of the cost of an HMM implementation on an actual processor.",
            "retrieval next text": "Expression, Localization, and Effect of High Salt Intake on Electroneutral Na+/HCO3\u2013 Cotransporter NBCn2 in Rat Small Intestine: Implication in Intestinal NaCl Absorption The electroneutral Na+/HCO3\u2013 cotransporter NBCn2 (SLC4A10) of solute carrier family 4 (SLC4) plays important physiological and pathological roles in the body. Our previous study showed that NBCn2 is expressed on the protein level in the small intestine of rat. Here, by reverse-transcription polymerase chain reaction (PCR), we identified a novel full-length NBCn2 variant, i.e., NBCn2-K, from rat small intestine. By pHi measurement with Xenopus oocytes, the activity of NBCn2-K is not significantly different from NBCn2-G. By western blotting, NBCn2 and the Na+/H+ exchanger NHE3 (SLC9A3) are predominantly expressed in the jejunum of rat small intestine. By immunofluorescence, NBCn2 and NHE3 are localized at the apical domain of the jejunum. NaCl overload decreases the expression of NBCn2 by 56% and that of NHE3 by 40% in the small intestine. We propose that NBCn2 is involved in the transepithelial NaCl absorption in the small intestine, and that the down-regulation of NBCn2 by NaCl represents an adaptive response to high salt intake in rat. INTRODUCTION Na + /HCO 3 \u2212 cotransporter NBCn2, the product of SLC4A10 gene, plays significant physiological and pathological roles in the body. In human, genetic abnormality in locus 2q24 spanning SLC4A10 is associated with complex epilepsy, mental retardation, autism spectra, cognitive disabilities, and hearing impairment (Sebat et al., 2007;Gurnett et al., 2008;Krepischi et al., 2010;Belengeanu et al., 2014;Nilsson et al., 2017;Zhao et al., 2018). In mouse, genetic disruption of Slc4a10 reduces neuronal excitability,",
            "retrieval score": "1.4986959",
            "text": "We have included three more similar recent works (years 2020 and 2021) in the \"Related work\" section. These works show how the latency is today an important problem in NVM architectures due to the mix of the very irregular data access patterns and the high inherent latency of NVM modules. Different hardware prefetching techniques appear in current literature showing the insterest of this topic in the research community. Related work: improve the contrast with current works to highlight novelty of our work We have included current researcho work covering the same topic. We compare the characteristics and limitations of the diferent works against our proposal showing how we deal with some of those limitations by allowing a more complex and complete prefetch solution. The \"Related work\" section should be described in the introduction This section is described in the \"Introduction\". Reviewer #3 Comments Impact of HMM in the production final price. The impact of our prefetch technique in the final production price is an important issue. Sometimes as researchers we are too focused in the design and technical topics and we forget that contributions to the state-of-the-art should also be applied in order to be useful to society. We appreciate the reviewer #3 comments and we have included, in subsection \"Prefetcher Implementation\", an study of the cost of an HMM implementation on an actual processor.",
            "title": ""
        },
        {
            "id": "4512133",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "One Queue Is All You Need: Resolving Head-of-Line Blocking in Large Language Model Serving $ $Large language models (LLMs) have become an increasingly important workload for cloud providers catering to both enterprise and consumer applications. LLM inference requests from these applications have end-to-end latency SLOs that must be adhered to in production settings. However, existing LLM serving systems focus on optimization objectives such as request serving throughput or request execution latency rather than the end-to-end latency SLOs. Achieving end-to-end SLOs for latency-sensitive requests is challenging due to head-of-line (HOL) blocking in the request queue, which results from bursty arrival rates and insufficient resources. To address the above challenge, we propose QLM, a multi-model queue management framework for LLM serving. QLM uses stochastic programming to orchestrate the actions of multiple LLM Serving Operations (LSOs) to reduce HOL blocking and maximize SLO attainment. Specifically, QLM uses the following LSOs: model swapping, request eviction, GPU-CPU state swapping, load balancing, and warm model start. Evaluation on heterogeneous GPU devices and models with real-world LLM serving dataset shows that QLM improves SLO attainment by 40-90% and throughput by 20-400% while maintaining or improving device utilization compared to other state-of-the-art LLM serving systems.",
            "retrieval next text": "Effect of childhood nutrition counselling on intelligence in adolescence: a 15-year follow-up of a cluster-randomised trial Abstract Objective The present study aimed to assess the effects of an early childhood nutrition counselling intervention on intelligence (as measured by the intelligence quotient (IQ)) at age 15\u201316 years. Design A single-blind, cluster-randomised trial. Setting In 1998, in Southern Brazil, mothers of children aged 18 months or younger were enrolled in a nutrition counselling intervention (n 424). Counselling included encouragement and promotion of exclusive breast-feeding until 6 months of age and continued breast-feeding supplemented by protein-, lipid- and carbohydrate-rich foods after age 6 months up to age 2 years. The control group received routine feeding advice. In 2013, the fourth round of follow-up of these individuals, at the age of 15\u201316 years, was undertaken. IQ was assessed using the short form of the Wechsler Adult Intelligence Scale (WAIS-III). Mental disorders (evaluated using the Development and Well-Being Assessment (DAWBA)) and self-reported school failure, smoking and alcohol use were also investigated. Adjusted analyses were conducted using a multilevel model in accordance with the sampling process. Subjects Adolescents, mean (sd) age of 15\u00b74 (0\u00b75) years (n 339). Results Mean (sd) total IQ score was lower in the intervention group than the control group (93\u00b74 (11\u00b74) and 95\u00b78 (11\u00b72), respectively) but the association did not persist after adjustment. The prevalence of any mental disorders was similar between intervention and control groups (23\u00b71 and 23\u00b75 %, respectively). There were no differences between groups regarding school failure, smoking and alcohol use. Conclusions Nutrition counselling intervention",
            "retrieval score": "1.4781084",
            "text": "One Queue Is All You Need: Resolving Head-of-Line Blocking in Large Language Model Serving $ $Large language models (LLMs) have become an increasingly important workload for cloud providers catering to both enterprise and consumer applications. LLM inference requests from these applications have end-to-end latency SLOs that must be adhered to in production settings. However, existing LLM serving systems focus on optimization objectives such as request serving throughput or request execution latency rather than the end-to-end latency SLOs. Achieving end-to-end SLOs for latency-sensitive requests is challenging due to head-of-line (HOL) blocking in the request queue, which results from bursty arrival rates and insufficient resources. To address the above challenge, we propose QLM, a multi-model queue management framework for LLM serving. QLM uses stochastic programming to orchestrate the actions of multiple LLM Serving Operations (LSOs) to reduce HOL blocking and maximize SLO attainment. Specifically, QLM uses the following LSOs: model swapping, request eviction, GPU-CPU state swapping, load balancing, and warm model start. Evaluation on heterogeneous GPU devices and models with real-world LLM serving dataset shows that QLM improves SLO attainment by 40-90% and throughput by 20-400% while maintaining or improving device utilization compared to other state-of-the-art LLM serving systems.",
            "title": ""
        },
        {
            "id": "13606629",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "workload performance with different file systems is unknown.Thirdly, different types of SSDs have significantly different performance properties such as peak throughput, latency, and read/write interference behavior [23,35].For example, flash-based SSDs have unpredictable performance and read/write interference, but non-flash-based ultra-low latency (UUL) SSDs such as Intel Optane SSDs have stable performance and no read/write interference [44,47]. In conclusion, the lack of understanding of how Kyber's configurations affect the achieved performance with different workloads, file systems, and types of SSDs makes it unclear how to optimize Kyber in practice.Specifically, we investigate the following research questions (RQs) around how Kyber's configurations affect the workloads' performance with different workloads, file systems and types of SSDs: (RQ1) How does Kyber affect the performance of workloads when workloads run concurrently and interfere with each other?We investigate how Kyber affects the performance of different workloads by studying the relation between target latency and the workloads' achieved performance.(RQ2) How to configure Kyber's parameters for diverse types NVMe SSDs and diverse file systems to meet workloads' requirements? The key motivation is to find out if and how our findings on Kyber's configurations performance effects can be generalized to different file systems and types of SSDs. We also provide guidelines on how to configure Kyber to meet the workloads' requirements in practice with diverse software and hardware environments. To address these questions, we conduct a first-of-its-kind systematic study of Linux' Kyber I/O scheduler with various kinds of workloads, file systems, and types of SSDs to establish guidelines on how to configure Kyber in practice.Our key contributions",
            "retrieval next text": "in this work include: \u2022 We extensively study how Kyber with different configurations affects workload performance using different combinations of latency-sensitive and throughput-bound workloads on 2 types of SSDs, resulting in 11 observations.To the best of our knowledge, we are the first to investigate the effect of Kyber's configurations on workloads.Flash-based SSDs are composed of a controller that is connected to an array of flash chips.Each flash chip is organized in a hierarchy of dies, planes, blocks, and pages.SSDs have high internal parallelism as both dies and planes can operate in parallel.The NVMe protocol [10] exposes this parallelism to workloads with a multi-queue interface that allows SSDs to execute multiple I/O requests in parallel.Nevertheless, to fully utilize this parallelism, workloads need to issue multiple concurrent I/O requests to the SSD.A challenge here is that a plane can not execute different types of commands (read or write) in parallel.If a read is issued to a die where a write is already being executed, the read is blocked until the write finishes, leading to a 10-40\u00d7 longer read latency.This performance degradation is called read/write interference [17,45].Moreover, the physical constraints of flash chips do not allow inplace updates or intra-block random writes.Pages in a block can only be written sequentially, and written pages need to be erased before they can be rewritten.Erasures happen at the unit of blocks, not at the unit of pages.To imitate the block interface provided by hard disks, the Flash Translation Layer (FTL) in SSD controllers maps logical addresses provided in the block interface to",
            "retrieval score": "1.450895",
            "text": "workload performance with different file systems is unknown.Thirdly, different types of SSDs have significantly different performance properties such as peak throughput, latency, and read/write interference behavior,35.For example, flash-based SSDs have unpredictable performance and read/write interference, but non-flash-based ultra-low latency (UUL) SSDs such as Intel Optane SSDs have stable performance and no read/write interference,47. In conclusion, the lack of understanding of how Kyber's configurations affect the achieved performance with different workloads, file systems, and types of SSDs makes it unclear how to optimize Kyber in practice.Specifically, we investigate the following research questions (RQs) around how Kyber's configurations affect the workloads' performance with different workloads, file systems and types of SSDs: (RQ1) How does Kyber affect the performance of workloads when workloads run concurrently and interfere with each other?We investigate how Kyber affects the performance of different workloads by studying the relation between target latency and the workloads' achieved performance.(RQ2) How to configure Kyber's parameters for diverse types NVMe SSDs and diverse file systems to meet workloads' requirements? The key motivation is to find out if and how our findings on Kyber's configurations performance effects can be generalized to different file systems and types of SSDs. We also provide guidelines on how to configure Kyber to meet the workloads' requirements in practice with diverse software and hardware environments. To address these questions, we conduct a first-of-its-kind systematic study of Linux' Kyber I/O scheduler with various kinds of workloads, file systems, and types of SSDs to establish guidelines on how to configure Kyber in practice.Our key contributions",
            "title": ""
        },
        {
            "id": "7981849",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "either cause premature eviction of a useful cache block, leading to an additional cache miss or cause delay in eviction of a dead block, leading to wastage of cache capacity. This calls for cache management mechanisms and policies that can tolerate variability in the reuse behavior of cache blocks to maximize cache e ciency. Our Proposals Aim of this thesis is: To design robust cache management mechanisms and policies for LLC that minimize cache misses in the face of variability in the reuse behavior of cache blocks, while keeping the cost and complexity of the hardware implementation low. To that end, we propose two cache management techniques, one domain-agnostic and one domain-specialized, that introduce robust mechanisms and policies to address variability in reuse prediction. The rest of the chapter provides a brief overview of both proposals. Domain-Agnostic Cache Management In this part of the thesis, we consider a conventional approach to cache management, namely domain-agnostic cache management, in which the LLC is managed completely in hardware. Such approach is quite attractive in practice as the cache management remains fully transparent to the application software. There has been a rich history of works that proposed various domain-agnostic techniques to improve cache e ciency [8,18,37,39,40,54,59,63,67,69,71,73,76,78,80,81,82,85,86,87,88,89,97,103,110]. The state-of-the-art techniques employ prediction mechanisms that seek to correctly identify as many dead blocks as possible and evict them immediately after their last use to reduce cache thrashing. These predictors all rely on some metric of temporal reuse to make their decisions regarding the end of a given block's useful life. Previous",
            "retrieval next text": "works have suggested hit count [81], last-touch PC [73], and number of references to the block's set since the last reference [59], among others, as metrics for determining whether the block is dead at a given point in time. However, we observe that existing metrics limit the accurate identi cation of dead blocks in the face of variability. For example, when the number of hits to a cache block is inconsistent across generations, a technique relying on this metric (i.e., hit count) would either prematurely classify the cache block dead or may not classify the cache block dead altogether until its eviction, both of which lead to cache ine ciency. This calls for robust metrics and policies that can tolerate inconsistencies. To that end, we propose Live Distance, a new metric of temporal reuse based on stack distance; stack distance of a given access to a cache block is de ned as the number of unique cache blocks accessed since the previous access to the cache block [112]. For a given generation of a cache block (from allocation to eviction), live distance is de ned as the largest observed stack distance in the generation. Live distance provides a conservative estimate of a cache block's useful lifetime. We introduce Leeway, a new domain-agnostic cache management technique that uses live distance as a metric for dead block predictions. Leeway uses code-data correlation to associate live distance for a group of blocks with a PC that brings the block into the cache. While live distance as a metric provides",
            "retrieval score": "1.4820393",
            "text": "either cause premature eviction of a useful cache block, leading to an additional cache miss or cause delay in eviction of a dead block, leading to wastage of cache capacity. This calls for cache management mechanisms and policies that can tolerate variability in the reuse behavior of cache blocks to maximize cache e ciency. Our Proposals Aim of this thesis is: To design robust cache management mechanisms and policies for LLC that minimize cache misses in the face of variability in the reuse behavior of cache blocks, while keeping the cost and complexity of the hardware implementation low. To that end, we propose two cache management techniques, one domain-agnostic and one domain-specialized, that introduce robust mechanisms and policies to address variability in reuse prediction. The rest of the chapter provides a brief overview of both proposals. Domain-Agnostic Cache Management In this part of the thesis, we consider a conventional approach to cache management, namely domain-agnostic cache management, in which the LLC is managed completely in hardware. Such approach is quite attractive in practice as the cache management remains fully transparent to the application software. There has been a rich history of works that proposed various domain-agnostic techniques to improve cache e ciency,18,37,39,40,54,59,63,67,69,71,73,76,78,80,81,82,85,86,87,88,89,97,103,110. The state-of-the-art techniques employ prediction mechanisms that seek to correctly identify as many dead blocks as possible and evict them immediately after their last use to reduce cache thrashing. These predictors all rely on some metric of temporal reuse to make their decisions regarding the end of a given block's useful life. Previous",
            "title": ""
        },
        {
            "id": "11337438",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "in predicting future resource consumption and detect slow tenants. Additionally, different resource models that either combine resource types in a non-linear function or use the dominant resource fairness approach may improve fairness further. Finally, it appears beneficial to extend Cassandra's gossip protocol to integrate a more robust leader selection algorithm for local weight adjustment. Workload-Aware Resource Reservation for Multi-Tenant NoSQL Stores Resource reservation is a common approach to avoiding performance interference among tenants. The basic idea is to dedicate a portion of a resource to a tenant for its use. Chapter 4 uses throughput regulation to provide fair access among tenants. Such an approach can be viewed as a special case of resource reservation -throughput represents the underlying actual resource consumptions and is treated as a \"resource\" for each tenant. As workloads usually have multiple resources involved e.g. memory for caching, CPU for serialization or deserialization, disk for reading or writing data, a tenant needs to acquire a reservation on each resource. But reservations are not all alike: a workload that has a hotspot access pattern may require more cache than does a workload with a random access pattern. An equal reservation of cache and disk usage for both workloads will not yield the best result. So reservations have to be based on workload characteristics, also called workload-aware reservation. A workload-aware reservation becomes more complicated if a workload bears dynamics i.e. a workload changes its access pattern during the access, which requires the system to be able to adjust accordingly. In addition, the distributed nature of",
            "retrieval next text": "NoSQL stores makes the workload-aware reservation more difficult. For a typical NoSQL store, a request is sent 58 to one node which may contact several other nodes to fetch the data. It is complicated to have a coordination among different resources and nodes. Previous research on preventing performance interference does so by simplifying the scenario, either by considering a single resource [24,59,84,105] (e.g. CPU, cache), or representing multiple resources consumption as a single \"virtual resource\" consumption [95,113]. Similarly, work in Chapter 4 uses throughput to approximate the underlying resource consumption of each tenant and regulates the throughput to provide fair access. Ignoring various resource demands that workloads have could lead to low resource utilization as the system imposes unnecessary constraints to tenants and even failure of preventing interference. Therefore, we propose Argus (the 100-eyed watchman in Greek mythology), a workloadaware resource reservation framework that targets multiple resource reservations and aims to prevent performance interference, in terms of fair throughput violation, in NoSQL stores. Specifically, Argus focuses on cache and disk reservations. It enforces the cache reservation by splitting the cache space among tenants. It approximates the disk usage by the throughput of a distributed file system and uses a request scheduler to enforce throughput reservation. Argus models the workload-aware reservation as a constrained optimization and uses the stochastic hill climbing algorithm to find the proper reservation according to various workloads' resource demands. We applied the idea of Argus to HBase [1], a stateof-art NoSQL data store. In summary, this chapter makes the following contributions: \u2022 Quantitative",
            "retrieval score": "1.4501907",
            "text": "in predicting future resource consumption and detect slow tenants. Additionally, different resource models that either combine resource types in a non-linear function or use the dominant resource fairness approach may improve fairness further. Finally, it appears beneficial to extend Cassandra's gossip protocol to integrate a more robust leader selection algorithm for local weight adjustment. Workload-Aware Resource Reservation for Multi-Tenant NoSQL Stores Resource reservation is a common approach to avoiding performance interference among tenants. The basic idea is to dedicate a portion of a resource to a tenant for its use. Chapter 4 uses throughput regulation to provide fair access among tenants. Such an approach can be viewed as a special case of resource reservation -throughput represents the underlying actual resource consumptions and is treated as a \"resource\" for each tenant. As workloads usually have multiple resources involved e.g. memory for caching, CPU for serialization or deserialization, disk for reading or writing data, a tenant needs to acquire a reservation on each resource. But reservations are not all alike: a workload that has a hotspot access pattern may require more cache than does a workload with a random access pattern. An equal reservation of cache and disk usage for both workloads will not yield the best result. So reservations have to be based on workload characteristics, also called workload-aware reservation. A workload-aware reservation becomes more complicated if a workload bears dynamics i.e. a workload changes its access pattern during the access, which requires the system to be able to adjust accordingly. In addition, the distributed nature of",
            "title": ""
        },
        {
            "id": "11456247",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient. Relevant code can be found at https://github.com/hdong920/LESS.",
            "retrieval next text": "High rate of tuberculin conversion in Indochinese refugees. From January 1979 through December 1980, a total of 664 Indochinese refugees were screened for tuberculosis in Monroe County, New York; 307 (46%) had positive 5-tuberculin unit PPD Mantoux tests. When 217 refugees with initially negative tests were retested at 60 days, 94 (43%) had converted from a negative to a positive test. Although none of the converters had evidence of active tuberculosis disease, 90 (96%) were given isoniazid preventive therapy. This conversion phenomenon has continued over time, discounting an effect of local, national, or international policy changes. The conversions do not seem to be a function of antigen, tester, or reader; criteria for testing, retesting, or reading; home country, camp country, or length of time in camp; or history of prior BCG vaccination, tuberculosis, or immunization exposure, current illness, or testing intervals since arrival in the United States and subsequent testing. Further studies to evaluate boosting or anergy are in progress to assess this potential public health problem.",
            "retrieval score": "1.5416313",
            "text": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient. Relevant code can be found at https://github.com/hdong920/LESS.",
            "title": ""
        },
        {
            "id": "8833196",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "sequence lengths can vary significantly across different LLM inference requests. Finally, we focused on sequence lengths of up to 3K, and P:D ratio in the range of 1-200. We believe that these are representative of many real-world deployments. However, there has also been an increased interest in supporting very long sequences (e.g., 10s-100s of thousands [18]). Such large sequence lengths may pose new challenges as the cost of attention grows quadratically with the number of tokens. We are actively investigating these challenges. Related Work In this section, we provide a brief summary of related work along two dimensions: systems optimizations and model innovations. Systems Optimizations Memory management: In auto-regressive decoding, the number of tokens that need to be generated for a given request is not known apriori. Therefore, conventional systems pre-allocate memory for the KV cache based on a conservative estimation of the maximum number of tokens. Recently, vLLM showed that this approach is inefficient and proposed a framework -motivated by the virtual memory abstraction -that enables incremental memory allocation for KV caches [20]. This helps improve the batch size, especially when the number of tokens varies significantly across different requests. FlexGen [42] focuses on improving the throughput of offline LLM inference in resource-constrained scenarios e.g., running a large model on a single GPU. Toward this goal, FlexGen employs a judicious combination of memory offloading, quantization, and scheduling. Optimizing (self-)attention: In [40], the authors propose an algorithm to reduce the memory requirement of self-attention from O(n 2 ) to O(1), with respect to the sequence length.",
            "retrieval next text": "FlashAttention [29] proposed a tiling-based algorithm that speeds up attention computation by minimizing the number of bytes read/written between different levels of GPU memory. Follow-up work [28] on FlashAttention further improved it along parallelism and work partitioning [28]. In our experiments, we found the xformers memory efficient attention implementation [21] to be the most efficient. Kernel-level optimizations: FasterTransformer [6] proposed optimized layers for the transformer's encoder and decoder blocks. These are based on low-level GPU optimizations such as kernel fusion. We expect that such low-level optimizations would equally benefit SARATHI as well. Scheduling optimizations: Orca proposed an iteration-level scheduling framework that avoids wasting compute due to token padding that was used earlier to batch together requests with different sequence lengths [48]. Further, Orca reduces latency by returning the response as soon as a request's endof-sequence token gets generated. FastServe proposed a preemptive scheduling framework to minimize the job completion times [46]. Some other scheduling frameworks include Triton [13] and Clipper [27] that separate the serving layer from the execution engine of the model. Our current work focuses on optimizing the execution layer and can be used with different scheduling policies proposed by such systems. The optimizations proposed by several of the prior works can complement our optimizations e.g., more optimized attention implementations will enable scaling SARATHI to longer sequence lengths and dynamic memory allocation will help in supporting larger batch sizes and so on. Model Innovations A significant body of work around model innovations has attempted to address the shortcomings of transformer-based language models or to",
            "retrieval score": "1.4969716",
            "text": "sequence lengths can vary significantly across different LLM inference requests. Finally, we focused on sequence lengths of up to 3K, and P:D ratio in the range of 1-200. We believe that these are representative of many real-world deployments. However, there has also been an increased interest in supporting very long sequences (e.g., 10s-100s of thousands). Such large sequence lengths may pose new challenges as the cost of attention grows quadratically with the number of tokens. We are actively investigating these challenges. Related Work In this section, we provide a brief summary of related work along two dimensions: systems optimizations and model innovations. Systems Optimizations Memory management: In auto-regressive decoding, the number of tokens that need to be generated for a given request is not known apriori. Therefore, conventional systems pre-allocate memory for the KV cache based on a conservative estimation of the maximum number of tokens. Recently, vLLM showed that this approach is inefficient and proposed a framework -motivated by the virtual memory abstraction -that enables incremental memory allocation for KV caches. This helps improve the batch size, especially when the number of tokens varies significantly across different requests. FlexGen focuses on improving the throughput of offline LLM inference in resource-constrained scenarios e.g., running a large model on a single GPU. Toward this goal, FlexGen employs a judicious combination of memory offloading, quantization, and scheduling. Optimizing (self-)attention: In, the authors propose an algorithm to reduce the memory requirement of self-attention from O(n 2 ) to O(1), with respect to the sequence length.",
            "title": ""
        },
        {
            "id": "8136498",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "with many different choices of which requests to reissue and when. (3) The complex and different workload properties of various interactive services, such as servicetime distributions, arrival patterns, request correlations, and system settings make it difficult to derive general strategies for reducing tail latency. (4) Analytical work using queueing theory is challenging even when making strong assumptions about response-time distributions (e.g. drawn from exponential family), and conclusions draw from such simple models are hard to generalize to more complex systems. Methodology and Key Results. The goal of our work is to find a reissue policy that minimizes a workload's kth percentile tail latency by issuing a fixed percentage (or budget) of redundant requests. We explore the space and devise reissue policies in a principled manner -directed by theoretical analysis to identify the key insights of effective reissue policies, and driven by empirical data from actual systems for wide applicability. We introduce a new family of reissue policies, Single-Time / Random (SingleR), that reissue requests after a delay d with probability q. The use of randomness in SingleR provides an important degree of freedom that allows to bound the reissue budget while also ensuring that reissue requests have sufficient time to respond, exploiting the benefits of both immediate and delayed reissue of prior work. Using a simplified analytical model, we formally prove that Sin-gleR is the optimal trade-off between the immediate and delayed reissue strategies. More precisely, we define the Multiple-Time / Random (MultipleR) policies which reissue requests multiple times with different delays and reissue probabilities. We",
            "retrieval next text": "prove that, surprisingly, the optimal policies in MultipleR and SingleR are equivalent. It is a powerful result, restraining the complexity of reissue policies to one time reissue only while guaranteeing the effectiveness of SingleR. Next, we present how to apply SingleR for interactive services through a data-driven approach to efficiently find the appropriate parameters, reissue time and probability, given sampled response times of the workloads. Our approach takes into account correlations between primary and reissue request response times. It is computationally efficient, finding optimal values of the parameters in close to linear time, with respect to the data size. Moreover, we show how to devise reissue policies for systems which are sensitive to added load by adaptively refining a reissue policy in response to feedback from the system. This method remains oblivious to many system design details, relies on iterative adaptation to discover a system's response-time distributions and its response to added load. This data-driven approach is performed in a principled manner: every refined policy is the solution to a well defined optimization problem based on updated response-time distributions, applicable to a wide range of workloads with varying properties. Empirical evaluation. We illustrate the properties of SingleR using both simulation and system experiments. Through careful simulation, we illustrate two key points: 1) the use of randomization in SingleR is especially important for workloads with correlated service times and queueing delays, 2) the effectiveness of SingleR is robust to varied workload properties and design choices including: utilization, service-time distribution, target latency percentiles, service-time correlations, and load-balancing/request-prioritization strategies.",
            "retrieval score": "1.4578571",
            "text": "with many different choices of which requests to reissue and when. (3) The complex and different workload properties of various interactive services, such as servicetime distributions, arrival patterns, request correlations, and system settings make it difficult to derive general strategies for reducing tail latency. (4) Analytical work using queueing theory is challenging even when making strong assumptions about response-time distributions (e.g. drawn from exponential family), and conclusions draw from such simple models are hard to generalize to more complex systems. Methodology and Key Results. The goal of our work is to find a reissue policy that minimizes a workload's kth percentile tail latency by issuing a fixed percentage (or budget) of redundant requests. We explore the space and devise reissue policies in a principled manner -directed by theoretical analysis to identify the key insights of effective reissue policies, and driven by empirical data from actual systems for wide applicability. We introduce a new family of reissue policies, Single-Time / Random (SingleR), that reissue requests after a delay d with probability q. The use of randomness in SingleR provides an important degree of freedom that allows to bound the reissue budget while also ensuring that reissue requests have sufficient time to respond, exploiting the benefits of both immediate and delayed reissue of prior work. Using a simplified analytical model, we formally prove that Sin-gleR is the optimal trade-off between the immediate and delayed reissue strategies. More precisely, we define the Multiple-Time / Random (MultipleR) policies which reissue requests multiple times with different delays and reissue probabilities. We",
            "title": ""
        },
        {
            "id": "4192237",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "explore key system tradeoffs. We find that: \u2022 Dynamic cache instantiation has the potential to provide significant cost reductions, sometimes more than halving the costs of (optimized) baselines that uses a permanent cache, with the cache size selected so as to minimize the cost. \u2022 The cost reductions are strongly dependent on the object popularity skew. When there is high skew, dynamic instantiation can work particularly well since a newly instantiated cache is quickly populated with frequently requested items that will capture a substantial fraction of the requests. \u2022 Selective Cache on k th request cache insertion policies can be even more beneficial in this context than with conventional edge caches, and, when there is high popularity skew, there is likely only modest room for improvement in cost/performance through use of more complex cache insertion and replacement policies. Overall, these results show that dynamic cache instantiation using Cache on k th request is a promising approach for content delivery applications. Finally, it is important to note that there does not exist any analysis (from prior work) that captures the performance of LRU caches under time-varying workloads. Our development of easy-to-compute approximation expressions of the performance of LRU caches under time varying workloads is therefore an important contribution. The reason we use RCW for our analysis (rather than LRU) is in part because it enables both an exact analysis and because it provides a nice approximation for LRU caches, while still capturing the cache performance under timevarying workload volumes. In contrast, exact analysis of large LRU caches",
            "retrieval next text": "is intractable. Of course, in practice, we expect many systems to keep implementing LRU cache replacement policies or some variation thereof. Roadmap: Section 2 describes our workload and system assumptions, the caching policies considered, and the metrics of interest. Section 3 presents our analysis of RCW caches for the baseline case without use of dynamic instantiation. Section 4 provides an analysis of the period of transient behavior as an RCW cache fills. Optimization models and performance results for dynamic instantiation are presented in Sections 5 and 6, respectively. Throughout the paper we derive and present results for both exact and O(1)-approximations. Section 7 describes related work, before Section 8 concludes the paper. SYSTEM DESCRIPTION AND METRICS Workload Assumptions: We focus on a single region within the service area of a content delivery application, or a cache location to which a subset of geographically distributed clients are directed [17]. For this cache location, we consider a time period of duration T (e.g., one day), over which the total (aggregated over all objects) content request rate \u03bb(t) varies. We assume that these variations are predictable (e.g., based on prior days), and so for any desired cache instantiation duration D < T , it would be possible to identify in advance the interval of duration D with the highest average request rate over all intervals of duration D within the time period. Short-term temporal locality, non-stationary object popularities, and high rates of new content creation make dynamic cache instantiation potentially more promising, since they reduce the value of old",
            "retrieval score": "1.4457889",
            "text": "explore key system tradeoffs. We find that: \u2022 Dynamic cache instantiation has the potential to provide significant cost reductions, sometimes more than halving the costs of (optimized) baselines that uses a permanent cache, with the cache size selected so as to minimize the cost. \u2022 The cost reductions are strongly dependent on the object popularity skew. When there is high skew, dynamic instantiation can work particularly well since a newly instantiated cache is quickly populated with frequently requested items that will capture a substantial fraction of the requests. \u2022 Selective Cache on k th request cache insertion policies can be even more beneficial in this context than with conventional edge caches, and, when there is high popularity skew, there is likely only modest room for improvement in cost/performance through use of more complex cache insertion and replacement policies. Overall, these results show that dynamic cache instantiation using Cache on k th request is a promising approach for content delivery applications. Finally, it is important to note that there does not exist any analysis (from prior work) that captures the performance of LRU caches under time-varying workloads. Our development of easy-to-compute approximation expressions of the performance of LRU caches under time varying workloads is therefore an important contribution. The reason we use RCW for our analysis (rather than LRU) is in part because it enables both an exact analysis and because it provides a nice approximation for LRU caches, while still capturing the cache performance under timevarying workload volumes. In contrast, exact analysis of large LRU caches",
            "title": ""
        },
        {
            "id": "8641883",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "the traffic profile at upper-level caches becomes more random (Jin & Bestavros, 1999). That explains why Random policy achieved better performances for intermediate nodes in some of the discussed scenarios. As expected, workloads with temporal locality property have a strong correlation with caching policies (Garetto, Leonardi & Martina, 2016), and variations in the temporal locality patterns directly impact the variations of caching policies performances. Regarding the context attributes explored by the replacement schemes, only two of the works presented evaluations including context features in the eviction logic that helped differentiate the node's position: like the node's number of interfaces (Wang & Bensaou, 2012a) and the node degree as a general rank according to the topology (Li, Simon & Gravey, 2012). However, other works are exploring those, and other context attributes that could be helpful. The context attributes with their respective classification and reference works are: Although the node's location is a context that should be considered when selecting a replacement policy, it is not easy to foresee a straight map between policies and node positions. First, because there are many policies and diversity of topologies with different requirements, but mostly because there are other contextual factors that can also impact the performance of the policies. As we continue to show in the next sections, this SLR was able to pinpoint some of these factors. Table 8 summarizes the characteristics of the corresponding scenarios. In the following, we discuss the reported impacts: According to Sun et al. (2014), the replacement scheme's optimal choice depends on the cache",
            "retrieval next text": "size and the placement policy. The authors combined seven placement policies with five replacement policies: LRU, LFU, FIFO, TTL, Size -and cache size variations of 0.0007%, 0.007%, 0.07%, and 0.7% of the unique contents. The content routers have homogeneous cache sizes for all experiments. We observe that the most significant impact on the replacement scheme choice happens when passing from 0.0007% to 0.007% of cache sizes. That is, for all combinations of placement policies, the best choice of replacement scheme changed when the cache size moved from 0.0007% to 0.007%. Meanwhile, for most combinations of placement policies, the experiments running with 0.007%, 0.07%, and 0.7% of cache sizes presented their highest performance values with the same replacement policy. For example, combined with LCE, LRU and TTL achieved the highest performances for 0.007% of cache size, while LFU stands out for the other sizes. Chao et al. (2013) also show evidence that variations on cache size can lead to variations on the policy with the best performance. This work presents a content-based replacement policy named FCDC that manages the content popularity propertyrequest count-to classify and replace contents according to popularity categories. The evaluation shows comparisons of the proposed scheme against LRU and RUF policies. According to the results, FCDC presents a better cache hit rate than LRU and RUF when the cache memory is less than 5%. Yet, the performance rank changed for cache sizes larger than 10%, and LRU performed slightly better than FCDC. The authors attribute this behavior to each policy's property, in which FCDC",
            "retrieval score": "1.4382035",
            "text": "the traffic profile at upper-level caches becomes more random (Jin & Bestavros, 1999). That explains why Random policy achieved better performances for intermediate nodes in some of the discussed scenarios. As expected, workloads with temporal locality property have a strong correlation with caching policies (Garetto, Leonardi & Martina, 2016), and variations in the temporal locality patterns directly impact the variations of caching policies performances. Regarding the context attributes explored by the replacement schemes, only two of the works presented evaluations including context features in the eviction logic that helped differentiate the node's position: like the node's number of interfaces (Wang & Bensaou, 2012a) and the node degree as a general rank according to the topology (Li, Simon & Gravey, 2012). However, other works are exploring those, and other context attributes that could be helpful. The context attributes with their respective classification and reference works are: Although the node's location is a context that should be considered when selecting a replacement policy, it is not easy to foresee a straight map between policies and node positions. First, because there are many policies and diversity of topologies with different requirements, but mostly because there are other contextual factors that can also impact the performance of the policies. As we continue to show in the next sections, this SLR was able to pinpoint some of these factors. Table 8 summarizes the characteristics of the corresponding scenarios. In the following, we discuss the reported impacts: According to Sun et al. (2014), the replacement scheme's optimal choice depends on the cache",
            "title": ""
        },
        {
            "id": "5796995",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "cache. They de ne the expected stack distance of a reference with reuse time t to be the average stack distance of all references with reuse time t. The miss-ratio curve is then constructed by computing the expected stack distance of each reuse time weighted by their frequency, this gives us a stack distance distribution. If one were to interpret the Statstack model's relationship between average stack distance vs. time, then we would arrive at the kinetic equation present in the average eviction model (AET). Stackstack performs in the same time bound as the AET model, O(N ) time and O(1) space due to their sampling techniques. OPEN PROBLEMS We have described the history of the miss-ratio curve construction techniques that rely on reuse time and stack distance measurements. In a broader scope, we have the enormous amount of research done in the cache behavior modeling domain. Fixing our limit on constructing practical miss-ratio curves and their applications, we nd that results in the following areas would excite many researchers in this eld. \u2022 To what extent can we relate logical access time to physical clock time? Currently, many models assume logical access time, that is each reference is considered a unique point in time and there are no inactive periods. However, in real systems it is often the case where the number of references per unit of time vary signi cantly. An investigation into how much error is produced when using physical clock time vs. the overhead caused by measuring logical time would give insights",
            "retrieval next text": "on how to better formulate these models for use on real systems. \u2022 How to extend these models to non-LRU caches? While some models could be used for alternative cache policies, there is a large reliance on the nice LRU stack property, this is especially painful since Intel's switch to RRIP policy has made cache modeling signi cantly more di cult. In latest work by Beckmann et. al. [3], they claim a general model, and that policy speci c models will be the subject of future work. \u2022 Despite the above remarks, the theory remains mature and well understood. How can we leverage these new models in day-to-day systems? The implementation of these models leaves much to be desired, rather high overhead has stopped many of these miss-ratio curve guided techniques from making it to practice. What improvements can we expect with miss-ratio curve constructions being done in O(1) space in an online and dynamic workload environment? CONCLUSION Our survey has covered the two main metrics, stack distance and reuse time, and their associated cache models from a miss-ratio curve construction standpoint. In Denning's 1968 work [8], he de ned page residency as how long a page will stay in main memory. In the cache modeling eld, we have called it the stack distance. More recently, it has been called the average eviction time of a cache. You could say that modeling locality has been reused over and over again.",
            "retrieval score": "1.4591122",
            "text": "cache. They de ne the expected stack distance of a reference with reuse time t to be the average stack distance of all references with reuse time t. The miss-ratio curve is then constructed by computing the expected stack distance of each reuse time weighted by their frequency, this gives us a stack distance distribution. If one were to interpret the Statstack model's relationship between average stack distance vs. time, then we would arrive at the kinetic equation present in the average eviction model (AET). Stackstack performs in the same time bound as the AET model, O(N ) time and O(1) space due to their sampling techniques. OPEN PROBLEMS We have described the history of the miss-ratio curve construction techniques that rely on reuse time and stack distance measurements. In a broader scope, we have the enormous amount of research done in the cache behavior modeling domain. Fixing our limit on constructing practical miss-ratio curves and their applications, we nd that results in the following areas would excite many researchers in this eld. \u2022 To what extent can we relate logical access time to physical clock time? Currently, many models assume logical access time, that is each reference is considered a unique point in time and there are no inactive periods. However, in real systems it is often the case where the number of references per unit of time vary signi cantly. An investigation into how much error is produced when using physical clock time vs. the overhead caused by measuring logical time would give insights",
            "title": ""
        },
        {
            "id": "13461126",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "make good admission decisions, however, most accesses (e.g., to popular objects) do not necessitate exact reuse time prediction; these times vary, but are always very short.Based on these findings, SLAP proposes a segmented reuse time model that predicts a reuse time range for each cache miss rather than an exact value.Specifically, after evaluating multiple options, SLAP selects a Long Short-Term Memory (LSTM) [13] model.Prediction accuracy is monitored during runtime, and retraining is triggered when workloads change.Second, deep learning models (like LSTM) have a high prediction latency.The SLAP LSTM architecture was delicately designed to allow a large portion of computation on recent access features to overlap the miss object fetching.SLAP was able to hide 98% of prediction latency by doing so.Finally, SLAP reuses common prediction computation on concurrent requests to reduce computation overhead, and uses trace sampling to reduce the training time of the LSTM model. We show that SLAP is effective at providing good CDN performance (e.g., hit rate), significantly reducing CDN write traffic (hence lower operational costs), and easily adapting to varying cache sizes.Our experiments with real-world traces show that: SLAP enables consistently better hit rate and significantly smaller write traffic, compared to all state-of-the-art learned and heuristic-based admission policies.For example, applying SLAP with the LRU replacement policy improves the SSDs lifetime by 104% -178% and reduces the cache write traffic by 38% -59%, outperforming all existing approaches.In addition, SLAP improves the hit rate by 3.2% -11.7%.Most importantly, SLAP requires no effort to deploy on cache instances of different size (for the same service) and",
            "retrieval next text": "to adapt to varying sizes, whereas other learned policies require significant re-training overheads. The rest of this paper is structured as follows.In Section 2, we introduce the background and motivation.In Section 3, we discuss the design of SLAP, including the segmented reuse time labeling model, and the admission policy architecture.We then present experimental results in Section 4. In Section 5, we discuss related work.Finally, we conclude in Section 6. BACKGROUND AND MOTIVATION We provide background on CDN and describe the goal of CDN cache admission policies.We explain why previous heuristic-based and learned cache admission policies are inadequate for large-scale production CDN deployments. CDN Cache Admission Policies CDNs are an essential component of internet infrastructure for many web services [33].Before accessing data (e.g., video, web page, etc.) over slow Wide-Area Networks (WANs), end users check a CDN cache for the content they are accessing.In production environments, web services such as video streaming pay CDN providers for caching; CDN providers maintain cluster of cache servers to serve these tenants.Flash SSDs are commonly used in CDN servers [1] due to their large capacity and low cost (compared to DRAM). CDN cache admission policies are critical for achieving a high cache hit rate for various services, as well as reducing write traffic to Flash SSDs, limiting SSD wear, and thus lowering operational costs.There have been dozens of cache admission policies designed to achieve these two goals.First, heuristic-based policies [14] define the rules that will be used to determine data admission.For example, in [22], an object is admitted into cache after",
            "retrieval score": "1.4755446",
            "text": "make good admission decisions, however, most accesses (e.g., to popular objects) do not necessitate exact reuse time prediction; these times vary, but are always very short.Based on these findings, SLAP proposes a segmented reuse time model that predicts a reuse time range for each cache miss rather than an exact value.Specifically, after evaluating multiple options, SLAP selects a Long Short-Term Memory (LSTM) model.Prediction accuracy is monitored during runtime, and retraining is triggered when workloads change.Second, deep learning models (like LSTM) have a high prediction latency.The SLAP LSTM architecture was delicately designed to allow a large portion of computation on recent access features to overlap the miss object fetching.SLAP was able to hide 98% of prediction latency by doing so.Finally, SLAP reuses common prediction computation on concurrent requests to reduce computation overhead, and uses trace sampling to reduce the training time of the LSTM model. We show that SLAP is effective at providing good CDN performance (e.g., hit rate), significantly reducing CDN write traffic (hence lower operational costs), and easily adapting to varying cache sizes.Our experiments with real-world traces show that: SLAP enables consistently better hit rate and significantly smaller write traffic, compared to all state-of-the-art learned and heuristic-based admission policies.For example, applying SLAP with the LRU replacement policy improves the SSDs lifetime by 104% -178% and reduces the cache write traffic by 38% -59%, outperforming all existing approaches.In addition, SLAP improves the hit rate by 3.2% -11.7%.Most importantly, SLAP requires no effort to deploy on cache instances of different size (for the same service) and",
            "title": ""
        },
        {
            "id": "8608258",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "queue up, time out, and retry, creating even more work and exacerbating performance problems. Unfortunately, publicly available information on communi-cation latencies in the cloud is scarce, impeding the decisionmaking process regarding critical timing assumptions.There are a plethora of websites and services that provide some information about average or median latency between cloud regions [5][6][7], but these services often do not provide regionlocal latencies or expose finer statistics on latency distributions to help engineers estimate how often, and by how much the communication latency may deviate from the average or median.As a result, most literature pulls these communication latency timing assumptions out of thin air.For example, the aforementioned Copilots [52] work runs on a dedicated cluster with average inter-node communication latency of 0.25ms, making 1 ms timeout a plausible assumption. In this work, we study the predictability of communication in the cloud to understand and empirically justify the timing assumptions engineers and designers make when working on cloud-native services and applications.To that extent, our contributions are two-fold.First, we present CLT, a simple opensource tool 1 to collect communication latency data across many cloud VMs.Second, we use CLT to study the communication latency patterns between VMs in three large cloud providers: Amazon Web Services (AWS), Google Compute Platform (GCP), and Microsoft Azure. Our tool, Cloud Latency Tester or CLT for short, is a simple echo-like application that can deploy to many VMs in different parts of the cloud, such as different placement groups, availability zones, or regions.The tool runs TCP traffic of configurable payload size and frequency between",
            "retrieval next text": "the VMs and collects the round-trip latency between all VM pairs.CLT also includes tools and scripts to process raw data and extract valuable statistics for desired pairs or sub-clusters of VMs (i.e., creating a latency histogram for all node pairs in the same availability zone). We used CLT to collect communication latency data in three large cloud providers.In particular, we look at the communication RTT between VMs in different deployment configurations, such as VMs in the same availability zone (AZ) or across AZs and regions.This data gives a lot of insights into the predictability of cloud communication in several common situations.For instance, we observe the potential for significant tail latency applications may experience in the cloud.The 99.99 th percentile tail latency for VMs in the same subnet of the same AZ is as much as 36\u00d7 higher than the average latency, while maximum RTT reaches as much as 2900\u00d7 the average.Considering that the 99.99 th percentile is not that rare (roughly every 10,000 th round-trip communication exchange), such high-tail latency can significantly impact applications relying on tight timing and communication latency assumptions.We also notice significant latency variations (as much as 7% change in 10 minutes) throughout the day across all tested clouds, suggesting a substantial impact from other cloud workloads/tenants on latency. 2 Background Latency in Cloud Systems The end-to-end communication latency in networked applications and systems consists of more than just the network latency between the nodes.Server hardware, virtualization stack, operating system, and the application itself may introduce additional overheads and jitter, as shown in",
            "retrieval score": "1.468946",
            "text": "queue up, time out, and retry, creating even more work and exacerbating performance problems. Unfortunately, publicly available information on communi-cation latencies in the cloud is scarce, impeding the decisionmaking process regarding critical timing assumptions.There are a plethora of websites and services that provide some information about average or median latency between cloud regions, but these services often do not provide regionlocal latencies or expose finer statistics on latency distributions to help engineers estimate how often, and by how much the communication latency may deviate from the average or median.As a result, most literature pulls these communication latency timing assumptions out of thin air.For example, the aforementioned Copilots work runs on a dedicated cluster with average inter-node communication latency of 0.25ms, making 1 ms timeout a plausible assumption. In this work, we study the predictability of communication in the cloud to understand and empirically justify the timing assumptions engineers and designers make when working on cloud-native services and applications.To that extent, our contributions are two-fold.First, we present CLT, a simple opensource tool 1 to collect communication latency data across many cloud VMs.Second, we use CLT to study the communication latency patterns between VMs in three large cloud providers: Amazon Web Services (AWS), Google Compute Platform (GCP), and Microsoft Azure. Our tool, Cloud Latency Tester or CLT for short, is a simple echo-like application that can deploy to many VMs in different parts of the cloud, such as different placement groups, availability zones, or regions.The tool runs TCP traffic of configurable payload size and frequency between",
            "title": ""
        },
        {
            "id": "2452779",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "A parallel workload has extreme variability In both high-performance computing (HPC) environments and the public cloud, the duration of time to retrieve or save your results is simultaneously unpredictable and important to your over all resource budget. It is generally accepted (\"Google: Taming the Long Latency Tail - When More Machines Equals Worse Results\", Todd Hoff, highscalability.com 2012), but without a robust explanation, that identical parallel tasks do take different durations to complete -- a phenomena known as variability. This paper advances understanding of this topic. We carefully choose a model from which system-level complexity emerges that can be studied directly. We find that a generalized extreme value (GEV) model for variability naturally emerges. Using the public cloud, we find real-world observations have excellent agreement with our model. Since the GEV distribution is a limit distribution this suggests a universal property of parallel systems gated by the slowest communication element of some sort. Hence, this model is applicable to a variety of processing and IO tasks in parallel environments. These findings have important implications, ranging from characterizing ideal performance for parallel codes to detecting degraded behaviour at extreme scales. INTRODUCTION Where they exist at all, current models for variability of parallel workloads on HPC systems implicitly assume I/O variability follows a normal distribution with the mean and standard deviation the only measure of interest [12,21,32,24,28]. An attempt to fit the tail of task duration to the log-normal distribution has also been made [37] with limited success. [20,16] point out that lowering latency for a given PDSW-DICS",
            "retrieval next text": "'16 WIP, November 14th 2016, Salt Lake City, UT service increases competitiveness of that service. Their work focuses on reducing the tail latency of a parallel task by reducing the latency of the individual tasks that makeup the parallel task. Beyond these studies on parallel workloads, there are an increasing number of phenomena in computer science and beyond that are best modeled by methods of extreme statistics [14,3,15,10,25,11,35,4,2,7,22,30,8,26]. MODEL The modern theory of extreme value distributions can be traced back to the 1920's and two mathematicians: Fisher and Tippett. They considered [13] extreme values of n samples, each of size m drawn from the same underlying population. Provided the population values are independent and identically distributed (i.i.d.), they showed that the distribution of the extreme values (smallest or largest) drawn from sufficiently large sub-samples, which in turn are drawn from a larger sample, tended to one of three possible unique asymptotic forms. For a given underlying distribution e.g. the exponential, the extremal distribution will be one of the three, in this case the Gumbel distribution (the others are Fr\u00e9chet, to which the extremes of power laws are attracted, and the Weibull, also well known in failure rate modeling for example.) The probability density function of the GEV with location \u00b5, scale \u03c3, and shape \u03be is: A detailed description, and physical examples of extreme value theory are presented in [23,11,33]. Next, we choose a common an simple parallel task (a write to a parallel file system) and argue that the i.i.d. assumption needed for GEV behavior",
            "retrieval score": "1.4586585",
            "text": "A parallel workload has extreme variability In both high-performance computing (HPC) environments and the public cloud, the duration of time to retrieve or save your results is simultaneously unpredictable and important to your over all resource budget. It is generally accepted (\"Google: Taming the Long Latency Tail - When More Machines Equals Worse Results\", Todd Hoff, highscalability.com 2012), but without a robust explanation, that identical parallel tasks do take different durations to complete -- a phenomena known as variability. This paper advances understanding of this topic. We carefully choose a model from which system-level complexity emerges that can be studied directly. We find that a generalized extreme value (GEV) model for variability naturally emerges. Using the public cloud, we find real-world observations have excellent agreement with our model. Since the GEV distribution is a limit distribution this suggests a universal property of parallel systems gated by the slowest communication element of some sort. Hence, this model is applicable to a variety of processing and IO tasks in parallel environments. These findings have important implications, ranging from characterizing ideal performance for parallel codes to detecting degraded behaviour at extreme scales. INTRODUCTION Where they exist at all, current models for variability of parallel workloads on HPC systems implicitly assume I/O variability follows a normal distribution with the mean and standard deviation the only measure of interest,21,32,24,28. An attempt to fit the tail of task duration to the log-normal distribution has also been made with limited success.,16 point out that lowering latency for a given PDSW-DICS",
            "title": ""
        },
        {
            "id": "7306666",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "cache has a bounded size, CARL leases may cause cache over-allocation or contention , when the number of active leases exceeds the cache size; and cache under-allocation , when the number of active leases is smaller than the cache size. A lease assignment may have the correct average cache size because an overallocated portion of program execution is balanced out by an underallocated portion. Cache over-allocation will lead to contention misses, while under-allocation will result in fewer hits. CLAM is the na\u00efve lease assignment policy and has no mechanism to mitigate these effects. Scope Hooked Eviction Leases . Leases assigned to references based on global reuse interval histograms may target an average cache size. Because these histograms contain no information about when different RIs (and therefore leases) occur, cache allocation may not be balanced in the event that access patterns change significantly throughout execution. If we assume RIs are uniformly distributed throughout execution, then cache usage variance during execution is low, and so contention misses are rare and lease assignments based on average cache size will perform well on a fixed-size cache. However, reuse behavior is not always uniform. Programs may be composed of multiple outer loops, or else alternate between multiple inner loops, each of which may have different reuse behavior. This problem is solved by encoding time information in RI histograms, in a technique we call Scope-Hooked Eviction Leasing (SHEL) . In SHEL, the programmer annotates a set of program scopes. These scopes indicate program phases with possibly different reuse behavior. For simplicity, we",
            "retrieval next text": "assume each reference belongs to a single scope. 2 Thus, by including a scope field in reference RI histogram entries, lease assignment may be done on a per-scope, rather than global, granularity. This allows for leases that are less profitable globally to effectively bypass more profitable leases if they take up space during under-allocated phases. Hence, scope annotation allows for lease assignments that are more balanced throughout program execution, resulting in fewer contention misses. It is possible for the allocation in one phase to spill over into the next phase. SHEL ignores such effects. As a result, SHEL may over-assign leases in a scope if the cache space available to the scope is reduced by the spill-over effect from the previous phase. In programs with coarse-grain phases, cross-phase effects may be negligible, and scopes may be optimized independently. An example is a computation with two steps, and each step computes matrix multiplication. When executed, the second step runs long enough to nullify the lingering effect of any lease assigned in the first step. Intuitively, the spill-over effects can be ignored for a program if all its phases are sufficiently longer than the longest lease. We state this property precisely, as follows: Proposition 1. Let s min be the minimal number of accesses in a phase, l max the longest lease assigned, and c the cache size. If s min l max c, cross-scope RIs can be ignored. Cross-scope RIs can be assumed to be in scope, and the resulting lease is the same. To see why",
            "retrieval score": "1.443748",
            "text": "cache has a bounded size, CARL leases may cause cache over-allocation or contention , when the number of active leases exceeds the cache size; and cache under-allocation , when the number of active leases is smaller than the cache size. A lease assignment may have the correct average cache size because an overallocated portion of program execution is balanced out by an underallocated portion. Cache over-allocation will lead to contention misses, while under-allocation will result in fewer hits. CLAM is the na\u00efve lease assignment policy and has no mechanism to mitigate these effects. Scope Hooked Eviction Leases . Leases assigned to references based on global reuse interval histograms may target an average cache size. Because these histograms contain no information about when different RIs (and therefore leases) occur, cache allocation may not be balanced in the event that access patterns change significantly throughout execution. If we assume RIs are uniformly distributed throughout execution, then cache usage variance during execution is low, and so contention misses are rare and lease assignments based on average cache size will perform well on a fixed-size cache. However, reuse behavior is not always uniform. Programs may be composed of multiple outer loops, or else alternate between multiple inner loops, each of which may have different reuse behavior. This problem is solved by encoding time information in RI histograms, in a technique we call Scope-Hooked Eviction Leasing (SHEL) . In SHEL, the programmer annotates a set of program scopes. These scopes indicate program phases with possibly different reuse behavior. For simplicity, we",
            "title": ""
        },
        {
            "id": "4830043",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "it will require deploying multiple, smaller caches. Additional help is expected from recommendation systems, whose effect is to shape the demand concentrating it around the most popular content items. Intuitively, having fewer, popular items to serve will improve caching performance. In this context, our paper targets three main questions. Vehicular demand. What is the data demand generated by today's vehicular users? Which apps and services represent the most significant contributions thereto? Caching architectures. Given a target hit ratio, what is the relationship between caching architecture and size of the caches we need to deploy? What is the impact of moving caches from core-level switches to individual base stations, on the total cache size, on the distance data must travel within the core network, and on the load thereof? What changes if a recommendation system is in place? Location-specific content. Content items consumed by future vehicular networks are expected to strongly depend on the location -augmented maps for self-driving vehicles being the most obvious example. What will be the impact of this kind of content on caching? We answer these questions using a set of real-world, large-scale measurement data, coming from users of the WeFi app [3]. Due to its crowd-sourced nature, our dataset includes data for: (i) multiple apps, including video (e.g., YouTube) and maps; (ii) multiple types of users, from pedestrian to vehicular ones; (iii) multiple network technologies, including 3G, LTE, and Wi-Fi; (iv) multiple operators. We describe our dataset, as well as the additional processing we need to perform in order to enhance the",
            "retrieval next text": "information it provides, in Section 2. Then, in Section 3 we explain how we model caching and caching architectures in our vehicular scenario. Section 4 presents numerical results and some relevant insights we obtain from them. Finally, Section 6 concludes the paper and sketches future work directions. Input data We describe the WeFi dataset we have access to in Section 2.1. Then in Section 2.2 we detail the processing steps we need, in order to extract further information that is not directly included therein. Finally, Section 2.3 explains how we complement the available information using other datasets and wellknown information. The WeFi dataset Our data comes from the users of an app called WeFi [3]. WeFi provides its users with information on the safest and fastest Wi-Fi access points available at the user's location. At the same time, it collects information about the user's location, connectivity and activity. WeFi reports over seven million downloads of the app globally, and over three billion daily records. In this work, we use the dataset relative to the city of Los Angeles -a vehicle-dominated environment. Its main features are summarized in Table 1. Each record contains the following information: \u2022 day, hour (a coarse-grained timestamp); \u2022 anonymized user identifier and GPS position; \u2022 network operator, cell ID, cell technology and local area (LAC) the user is connected to (if any); \u2022 Wi-Fi network (SSID) and access point (BSSID) the user is connected to (if any); \u2022 active app and amount of downloaded/uploaded data. If the location of the user or",
            "retrieval score": "1.4720025",
            "text": "it will require deploying multiple, smaller caches. Additional help is expected from recommendation systems, whose effect is to shape the demand concentrating it around the most popular content items. Intuitively, having fewer, popular items to serve will improve caching performance. In this context, our paper targets three main questions. Vehicular demand. What is the data demand generated by today's vehicular users? Which apps and services represent the most significant contributions thereto? Caching architectures. Given a target hit ratio, what is the relationship between caching architecture and size of the caches we need to deploy? What is the impact of moving caches from core-level switches to individual base stations, on the total cache size, on the distance data must travel within the core network, and on the load thereof? What changes if a recommendation system is in place? Location-specific content. Content items consumed by future vehicular networks are expected to strongly depend on the location -augmented maps for self-driving vehicles being the most obvious example. What will be the impact of this kind of content on caching? We answer these questions using a set of real-world, large-scale measurement data, coming from users of the WeFi app. Due to its crowd-sourced nature, our dataset includes data for: (i) multiple apps, including video (e.g., YouTube) and maps; (ii) multiple types of users, from pedestrian to vehicular ones; (iii) multiple network technologies, including 3G, LTE, and Wi-Fi; (iv) multiple operators. We describe our dataset, as well as the additional processing we need to perform in order to enhance the",
            "title": ""
        },
        {
            "id": "4290933",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "single GPU workload; they also propose GPU performance and power models for effective workload consolidation on GPUs. Joulemeter [15] that is initially designed as a tool for power usage measurement of virtual machines aims to consolidate multiple workloads on fewer servers for improving resource utilization and power costs. Analyzing the effect of last level processor cache (LLC) on workload consolidation -another topic of interest we investigate in this work-is also covered by several studies. For example, authors of [16] study the behavior of consolidated workloads particularly on sharing caches across a variety of configurations. In [10] authors also study shared resource monitoring to understand resource usage and ways to improve overall throughput as well as quality of service of a data center. A mathematical model has also been proposed to predict the effect of cache contention on the performance of consolidated workloads [17]. After close examination of all these works, we noticed several shortcomings and decided to cover them in this article; thus, we can highlight our contribution in this work through the following items. The first difference of our work with previously reported studies mainly lies in the way we characterize data-intensive applications with two main parameters: file size and request size; such characterization is inspired by well-known filesystem benchmarking tools, namely Iometer [18], IOzone [19], TestDFSIO [20], and Bonnie++ [21]. Our second contribution is related to the Hadoop distributed file system that has been never properly covered in previous studies -to the best of our knowledge. We believe this is the first work that",
            "retrieval next text": "thoroughly analyses inter-relationship between workload consolidation, throughput degradation, and LLC contention for dataintensive applications employing HDFS. Our third contribution is to propose mathematical models for different aspects of this study based on imperial results from TestDFSIO [20]. III. SINGLE WORKLOAD ON SINGLE SERVER In this section, we measure the throughput of a single workload on a single physical server. Here, we show that throughput is a function of file size (FS) and file operation request size (RS) of the workload. RS is the amount of data that workload reads/writes from/to a file in a single file operation. Our experimental results show that increasing FS beyond LLC size noticeably degrades the throughput of workload. A. Workload Characterization We conduct a series of experiments on two physical servers to capture the effect of FS and RS on throughput. The experiments are based on the intuition that data-intensive workloads can be characterized by FS and RS [18][19][20][21]. As expected, the throughput curves for all servers follow the same pattern by varying FS and RS for both read and write operation. Table I shows the experimental setup for these two servers (M 1 and M 2 ). On all physical servers, system file cache and file buffering are always activated in operating systems; thus, workloads always interact with system file cache rather than system disk. Through enabling this feature write-back cache always delays flushing file data until triggered by cache manager -usually at predefined time intervals. Note that although system file cache is a feature of native filesystem not that",
            "retrieval score": "1.5272238",
            "text": "single GPU workload; they also propose GPU performance and power models for effective workload consolidation on GPUs. Joulemeter that is initially designed as a tool for power usage measurement of virtual machines aims to consolidate multiple workloads on fewer servers for improving resource utilization and power costs. Analyzing the effect of last level processor cache (LLC) on workload consolidation -another topic of interest we investigate in this work-is also covered by several studies. For example, authors of study the behavior of consolidated workloads particularly on sharing caches across a variety of configurations. In authors also study shared resource monitoring to understand resource usage and ways to improve overall throughput as well as quality of service of a data center. A mathematical model has also been proposed to predict the effect of cache contention on the performance of consolidated workloads. After close examination of all these works, we noticed several shortcomings and decided to cover them in this article; thus, we can highlight our contribution in this work through the following items. The first difference of our work with previously reported studies mainly lies in the way we characterize data-intensive applications with two main parameters: file size and request size; such characterization is inspired by well-known filesystem benchmarking tools, namely Iometer, IOzone, TestDFSIO, and Bonnie++. Our second contribution is related to the Hadoop distributed file system that has been never properly covered in previous studies -to the best of our knowledge. We believe this is the first work that",
            "title": ""
        },
        {
            "id": "13173327",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "are two main approaches of caching studies currently considered in the literature. In the first approach (conventional) ( [3], [4]), the focus is on considering the eviction policies at individual caches, such as First-In-First-Out (FIFO), Least-Frequently-Used (LFU), Least-Recently-Used (LRU), Time-To-Live (TTL) etc. where maximizing the hit probability in a local cache is the parameter of primary interest. In [5], a hierarchical network based on TTL caches is analyzed. The general cache network is studied in [6]. In [4], the performance of the different caching policies under independent reference model (IRM) traffic and renewal traffic is analyzed. Another group of the works considers caching in a wireless heterogeneous network. The works in [7]- [9] study caches in small cell networks. Some works ( [10], [11]) consider caching in the context of Device-Device (D2D) communication. The second, more recent approach ( [12], [13]) considers static caches sharing a common link with a server. These systems have two different phases: content placement and coded delivery. In the content placement phase caches are populated (usually under low network activity) with files. In the content delivery phase, requests from all the nodes are received by the server and the delivery is performed using coded multicast. This has been shown to reduce the total file transmission rate from the base station substantially as against the above conventional schemes. In [12], [14], an information theoretic approach is taken where the minimum rate required to fulfill requests from all the users is studied. The work in [13] extends similar results to D2D communication. [15]",
            "retrieval next text": "studies coded caching in a system with two layers of caches. In [16], an online coded scheme is presented. These schemes have been widely studied under uniform popularity traffic and have been further extended to general random request case also ([17], [18]). In the above coded caching works, an important aspect of queuing at the server has been ignored. The queuing delay can be the dominant component of the overall delay experienced by a user in a content delivery network. Work in [19] addresses these issues. These authors propose a few queuing models for the cache aided coded multicasting schemes. A queue with multicasting and network coding is also studied in ( [20], [21]) in a different setting where there is no finite library of files to download from and each arriving packet is different and must be received by each receiver. As in [19] we also consider queuing delays at the server. Our major contributions are as follows: 1) We consider a new type of queue called the multicast queue in which new requests for a file are merged with the ones already in the queue. All pending requests for a file get served with one transmission of a file. This exploits the broadcast nature of the wireless channel more effectively than in [19] and reflects the practical scenario more realistically. An immediate impact of this model is that, unlike in [19], our queue at the server is always stable for any request arrival rate from the users for any finite number of files. Furthermore,",
            "retrieval score": "1.4507322",
            "text": "are two main approaches of caching studies currently considered in the literature. In the first approach (conventional) (,), the focus is on considering the eviction policies at individual caches, such as First-In-First-Out (FIFO), Least-Frequently-Used (LFU), Least-Recently-Used (LRU), Time-To-Live (TTL) etc. where maximizing the hit probability in a local cache is the parameter of primary interest. In, a hierarchical network based on TTL caches is analyzed. The general cache network is studied in. In, the performance of the different caching policies under independent reference model (IRM) traffic and renewal traffic is analyzed. Another group of the works considers caching in a wireless heterogeneous network. The works in- study caches in small cell networks. Some works (,) consider caching in the context of Device-Device (D2D) communication. The second, more recent approach (,) considers static caches sharing a common link with a server. These systems have two different phases: content placement and coded delivery. In the content placement phase caches are populated (usually under low network activity) with files. In the content delivery phase, requests from all the nodes are received by the server and the delivery is performed using coded multicast. This has been shown to reduce the total file transmission rate from the base station substantially as against the above conventional schemes. In,, an information theoretic approach is taken where the minimum rate required to fulfill requests from all the users is studied. The work in extends similar results to D2D communication.",
            "title": ""
        },
        {
            "id": "4835728",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "dominated by tightly-coupled parallel jobs and, more recently, by bag-of-tasks applications. The GWA includes traces collected from grid environments; differently from this work, these traces are dominated by bag-of-tasks applications and by virtual-machine lease-release data. Workload characterization, definition, and modeling: There is much related and relevant work in this area, from which we compare only with the closely related; other characterization work does not focus on comparing traces by domain and does not cover a set of characteristics as diverse as this work, leading to so many findings. Closest to this work, the Google cluster-traces have been analyzed from various points of view, e.g., [36,9,34]. Amvrosiadis et al. [3,4] compare the Google cluster traces with three other cluster traces, of 0.3-3 times the size and 3-60 times the duration, and find key differences; our work adds new views and quantitative data on diversity, through both survey and characterization techniques. Bharathi et al. [7] provide a characterization on workflow structures and the effect of workflow input sizes on said structures. Five scientific workflows are used to explain in detail the compositions of their data and computational dependencies. Using the characterization, a workflow generator generator for parameterized workflows is developed. Juve et al. [27] provide a characterization of six scientific workflows using workflow profiling tools that investigate resource consumption and computational characteristics of tasks. The teams of Feitelson and Iosup have provided many characterization and modeling studies for parallel [16], grid [22], and hostedbusiness [41] workloads; and Feitelson has written a seminal book on workload modeling [14]. In",
            "retrieval next text": "contrast, this work addresses in-depth the topic of workloads of workflows. Conclusion and Ongoing Work Responding to the stringent need for diverse workflow traces, in this work we propose the Workflow Trace Archive (WTA), which is an open-access archive containing workflow traces. We conduct a survey of how the systems community uses workflow traces, by systematically inspecting articles accepted in the last decade in peerreviewed conferences and journals. We find that, from all articles that use traces, less than 40% use realistic traces, and less than 15% use any open-access trace. Additionally, the community focuses primarily on scientific workloads, possibly due to the scarcity of traces from other domains. These findings suggest existing limits to the relevance and reproducibility of workflow-based studies and designs. We design and implement the WTA around five key requirements. At the core of the WTA is an unified trace format that, uniquely, supports both workflowand task-level NFRs. The archive contains a large and diverse set of traces, collected from 10 sources and encompassing over 48 million workflows and 2 billion CPU core hours. Finally, we provide deep insight into the WTA traces, through a statistical characterization revealing that: (1) there are large differences in workflow structures between scientific, industrial, and engineering workflows, (2) our two biggest traces-from Alibaba and Google-have the most stable arrival patterns in terms of tasks per hour, (3) industrial workflows tend to have the highest level of parallelism, (4) the level of parallelism per domain is clearly divided, (5) engineering workloads tend to have the most tasks",
            "retrieval score": "1.484954",
            "text": "dominated by tightly-coupled parallel jobs and, more recently, by bag-of-tasks applications. The GWA includes traces collected from grid environments; differently from this work, these traces are dominated by bag-of-tasks applications and by virtual-machine lease-release data. Workload characterization, definition, and modeling: There is much related and relevant work in this area, from which we compare only with the closely related; other characterization work does not focus on comparing traces by domain and does not cover a set of characteristics as diverse as this work, leading to so many findings. Closest to this work, the Google cluster-traces have been analyzed from various points of view, e.g.,,9,34. Amvrosiadis et al.,4 compare the Google cluster traces with three other cluster traces, of 0.3-3 times the size and 3-60 times the duration, and find key differences; our work adds new views and quantitative data on diversity, through both survey and characterization techniques. Bharathi et al. provide a characterization on workflow structures and the effect of workflow input sizes on said structures. Five scientific workflows are used to explain in detail the compositions of their data and computational dependencies. Using the characterization, a workflow generator generator for parameterized workflows is developed. Juve et al. provide a characterization of six scientific workflows using workflow profiling tools that investigate resource consumption and computational characteristics of tasks. The teams of Feitelson and Iosup have provided many characterization and modeling studies for parallel, grid, and hostedbusiness workloads; and Feitelson has written a seminal book on workload modeling. In",
            "title": ""
        },
        {
            "id": "4856391",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs). However, the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself. Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined. In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss. Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced precision quantization substantially recovers the incurred degradation. On the other hand, we observe that the important KV pairs must be kept at a relatively higher precision to safeguard the generation quality. Motivated by these observations, we propose \\textit{Mixed-precision KV cache}~(MiKV), a reliable cache compression method that simultaneously preserves the context details by retaining the evicted KV pairs in low-precision and ensure generation quality by keeping the important KV pairs in high-precision. Experiments on diverse benchmarks and LLM backbones show that our proposed method offers a state-of-the-art trade-off between compression ratio and performance, compared to other baselines.",
            "retrieval next text": "High-resolution spectrum of xenon ions at 13.4 nm. The spectrum of xenon excited in a low-inductance vacuum spark was photographed at high resolution in the region of 9.5-15.5 nm. The observed transitions were identified as belonging to ions from Xe8+ to Xe13+. In the region of importance for extreme-ultraviolet lithography around 13.4 nm, the strongest lines were identified as 4d8-4d7 5p transitions in Xe10+. The identifications were made by use of energy parameters extrapolated along the isoelectronic sequence.",
            "retrieval score": "1.4533141",
            "text": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs). However, the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself. Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined. In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss. Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced precision quantization substantially recovers the incurred degradation. On the other hand, we observe that the important KV pairs must be kept at a relatively higher precision to safeguard the generation quality. Motivated by these observations, we propose \\textit{Mixed-precision KV cache}~(MiKV), a reliable cache compression method that simultaneously preserves the context details by retaining the evicted KV pairs in low-precision and ensure generation quality by keeping the important KV pairs in high-precision. Experiments on diverse benchmarks and LLM backbones show that our proposed method offers a state-of-the-art trade-off between compression ratio and performance, compared to other baselines.",
            "title": ""
        },
        {
            "id": "9048321",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "It's Time to Revisit LRU vs. FIFO We revisit the question of the effectiveness of the popular LRU cache eviction policy versus the FIFO heuristic which attempts to give an LRU like behavior. Several past works have considered this question and commonly stipulated that while FIFO is much easier to implement, the improved hit ratio of LRU outweighs this. We claim that two main trends call for a reevaluation: new caches such as front-ends to cloud storage have very large scales and this makes managing cache metadata in RAM no longer feasible; and new workloads have emerged that possess different characteristics. We model the overall cost of running LRU and FIFO in a very large scale cache and evaluate this cost using a number of publicly available traces. Our main evaluation workload is a new set of traces that we collected from a large public cloud object storage service and on this new trace FIFO exhibits better overall cost than LRU. We hope that these observations reignite the evaluation of cache eviction policies under new circumstances and that the new traces, that we intend to make public, serve as a testing ground for such work.",
            "retrieval next text": "The Biology of Bone Metastasis. Bone metastasis, or the development of secondary tumors within the bone of cancer patients, is a debilitating and incurable disease. Despite its morbidity, the biology of bone metastasis represents one of the most complex and intriguing of all oncogenic processes. This complexity derives from the intricately organized bone microenvironment in which the various stages of hematopoiesis, osteogenesis, and osteolysis are jointly regulated but spatially restricted. Disseminated tumor cells (DTCs) from various common malignancies such as breast, prostate, lung, and kidney cancers or myeloma are uniquely primed to subvert these endogenous bone stromal elements to grow into pathological osteolytic or osteoblastic lesions. This colonization process can be separated into three key steps: seeding, dormancy, and outgrowth. Targeting the processes of dormancy and initial outgrowth offers the most therapeutic promise. Here, we discuss the concepts of the bone metastasis niche, from controlling tumor-cell survival to growth into clinically detectable disease.",
            "retrieval score": "1.5168312",
            "text": "It's Time to Revisit LRU vs. FIFO We revisit the question of the effectiveness of the popular LRU cache eviction policy versus the FIFO heuristic which attempts to give an LRU like behavior. Several past works have considered this question and commonly stipulated that while FIFO is much easier to implement, the improved hit ratio of LRU outweighs this. We claim that two main trends call for a reevaluation: new caches such as front-ends to cloud storage have very large scales and this makes managing cache metadata in RAM no longer feasible; and new workloads have emerged that possess different characteristics. We model the overall cost of running LRU and FIFO in a very large scale cache and evaluate this cost using a number of publicly available traces. Our main evaluation workload is a new set of traces that we collected from a large public cloud object storage service and on this new trace FIFO exhibits better overall cost than LRU. We hope that these observations reignite the evaluation of cache eviction policies under new circumstances and that the new traces, that we intend to make public, serve as a testing ground for such work.",
            "title": ""
        },
        {
            "id": "6689262",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "in the cache whose next use will occur farthest away in the future. Of course, such a policy cannot be implemented in practice as it would require the OS to know in advance what page(s) will be accessed later. Thus, OSs typically use suboptimal, fixed cache eviction heuristics. For example, assuming that the past is a good predictor of the future, many OSs swap out from the cache pages that are least recently used (LRU), implementing some LRU approximation, for instance, the \"second chance\" clock algorithm (Corbat\u00f3, 1969). Hence, existing cache eviction policies are generally effective with workloads that exhibit temporal/spatial locality. Still, they may fail to capture \"unconventional\" patterns of references, thereby losing the advantage of caching and, ultimately, causing the system to degrade. To overcome this limitation, we propose a policy based on MUlti-STep-Ahead Predictions for CacHe Eviction (MUS-TACHE). This is a new page replacement algorithm whose logic is learned from observed memory access requests rather than fixed as existing methods. More specifically, we formulate the page request prediction problem as a categorical -i.e., discrete-valued -time series forecasting task. Then, our method queries the learned page request forecaster to obtain the next k predicted page memory references to arXiv:2211.02177v1 [cs.OS] 3 Nov 2022 better approximate the optimal OPT strategy. As per how to implement the multi-step-ahead page request forecaster, several techniques can be used. In this work, we only consider deep learning techniques, both standard and specifically designed for handling sequence data in general and time series in particular. Among standard approaches, we use",
            "retrieval next text": "multilayer perceptron (MLP). In addition, we study architectures built upon recurrent neural networks (RNNs), such as \"vanilla\" long short-term memory networks (LSTM (Hochreiter & Schmidhuber, 1997)) and sequence-to-sequence (seq2seq) models like Encoder-Decoder LSTM (Sutskever et al., 2014). To train any of these methods, we first collect a very large dataset that logs the memory traces of the execution of benchmark programs contained in the Princeton Application Repository for Shared-Memory Computers (PARSEC). 1 We evaluate the prediction accuracy of each learned page request forecaster offline using a test set previously held out from the entire dataset above. Thus, we consider the bestperforming forecaster as the building block of our MUS-TACHE policy and integrate it into an open-source cache simulator. 2 Finally, we compare MUSTACHE with existing page cache replacement baselines already implemented in the cache simulator when running the test portions of all the programs in the PARSEC suite. Experiments demonstrate the superiority of our approach, as MUSTACHE improves the cache hit ratio by 1.9% and reduces the number of disk operations (i.e., number of reads/writes) by 18.4% and 10.3% compared to the best heuristic (i.e., exact LRU). To summarize, we provide the following contributions: (i) We collect a very large dataset containing the memory accesses referenced during the execution of PARSEC benchmark programs; (ii) We frame the problem of page request prediction into a multi-step-ahead time series forecasting task; (iii) We train several deep learning models for page request prediction; (iv) We implement our cache replacement strategy (MUS-TACHE) using the best-performing page request predictor learned, and",
            "retrieval score": "1.452381",
            "text": "in the cache whose next use will occur farthest away in the future. Of course, such a policy cannot be implemented in practice as it would require the OS to know in advance what page(s) will be accessed later. Thus, OSs typically use suboptimal, fixed cache eviction heuristics. For example, assuming that the past is a good predictor of the future, many OSs swap out from the cache pages that are least recently used (LRU), implementing some LRU approximation, for instance, the \"second chance\" clock algorithm (Corbat\u00f3, 1969). Hence, existing cache eviction policies are generally effective with workloads that exhibit temporal/spatial locality. Still, they may fail to capture \"unconventional\" patterns of references, thereby losing the advantage of caching and, ultimately, causing the system to degrade. To overcome this limitation, we propose a policy based on MUlti-STep-Ahead Predictions for CacHe Eviction (MUS-TACHE). This is a new page replacement algorithm whose logic is learned from observed memory access requests rather than fixed as existing methods. More specifically, we formulate the page request prediction problem as a categorical -i.e., discrete-valued -time series forecasting task. Then, our method queries the learned page request forecaster to obtain the next k predicted page memory references to arXiv:2211.02177v1 [cs.OS 3 Nov 2022 better approximate the optimal OPT strategy. As per how to implement the multi-step-ahead page request forecaster, several techniques can be used. In this work, we only consider deep learning techniques, both standard and specifically designed for handling sequence data in general and time series in particular. Among standard approaches, we use",
            "title": ""
        },
        {
            "id": "5277768",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "this kind of memory architecture. We believe that this work increases the relevance of our proposal and shows how latency is one of the main problems to solve to have NVM architectures fully in use. \uf0b7 Also in section \"Prefetch Based on Hidden Markov Model\" we explain the on-chip memory hierarchy based on virtual address highlighting its advantages and disadvantages. Here we show that LLC analysis that relies on virtual addresses instead of physical addresses reduces drastically access frequency while providing enough information for a good prefetcher to catch algorithmic behavior and produce good prefetch accuracy. We've also added a comparation between the literature proposals and our HMM model highlighting our main differential proposal. Reviewer #1 Comments Latest approaches can be included in the related work \uf0b7 In related work we have included three recent works in hardware prefetching related to different features of LLC, big data and machine learning, irregular workloads and fabric memory integration: \u2022 [Choi21]: LLC prefetcher targeted to big data and machine learning processing that adapt granularities entailing cache lines and page granularity. Includes a DRAM buffer with an access history table and several prefetch engines to deal with irregular memory access patterns. \u2022 [Talati21]: hardware-software codesign solution to deal with very irregular workloads that uses static information from programs and dynamic from hardware to extract program semantic information used to generate prefetch requests. Our proposal succeeds in capturing irregular data access patterns without the use of compiler information from running programs. \u2022 [Kommareddy20]: decoupling of memory from computing is currently under",
            "retrieval next text": "research because improves data bandwidth when used with novel interconnection technologies. This kind of memory organization eases the task of integration between different technologies. The problem is the latency of memory access so the use of prefetch techniques has a very high impact in performance. \uf0b7 Also a more updated bibliography is included related to the use of hardware prefetching in modern processors. This is added in the introduction. We have included a work of hardware prefetching in the Intel Xeon Phi ([Sodari16]) and other in the IBM BlueGene/Q ([Haring12]). \uf0b7 Analysis or similar proposals like Domino [Bakhshlipour18] compared with our proposal is also included to explain how HMM Prefetch deals with more complex data relations on each of the groups it can simultenously identify (section \"Prefetch Based on Hidden Markov Model\"). Provide proof for the performance of HMM when complexity increases. In the section \"Evaluation\" we have improved the explanations and also we have created an specific section for multicore evaluation including new experiments with sixteen cores. We have also developed a new model to assess how OS-managed multiprogramming influences our prefetcher's ability to manage complexity. This model has a 4 core processor running 4 benchmarks each so multiprogrammed is needed to context switch between processes. With this new simulation model the complexity that time sharing adds to the off-chip access pattern can be evaluated. These modifications have been included to provide proofs ot the behavior of HMM Prefetch on more real and complex environments. We believe that now the evidence of how HMM works",
            "retrieval score": "1.5517921",
            "text": "this kind of memory architecture. We believe that this work increases the relevance of our proposal and shows how latency is one of the main problems to solve to have NVM architectures fully in use. \uf0b7 Also in section \"Prefetch Based on Hidden Markov Model\" we explain the on-chip memory hierarchy based on virtual address highlighting its advantages and disadvantages. Here we show that LLC analysis that relies on virtual addresses instead of physical addresses reduces drastically access frequency while providing enough information for a good prefetcher to catch algorithmic behavior and produce good prefetch accuracy. We've also added a comparation between the literature proposals and our HMM model highlighting our main differential proposal. Reviewer #1 Comments Latest approaches can be included in the related work \uf0b7 In related work we have included three recent works in hardware prefetching related to different features of LLC, big data and machine learning, irregular workloads and fabric memory integration: \u2022 [Choi21: LLC prefetcher targeted to big data and machine learning processing that adapt granularities entailing cache lines and page granularity. Includes a DRAM buffer with an access history table and several prefetch engines to deal with irregular memory access patterns. \u2022 [Talati21: hardware-software codesign solution to deal with very irregular workloads that uses static information from programs and dynamic from hardware to extract program semantic information used to generate prefetch requests. Our proposal succeeds in capturing irregular data access patterns without the use of compiler information from running programs. \u2022 [Kommareddy20: decoupling of memory from computing is currently under",
            "title": ""
        },
        {
            "id": "6120686",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "workloads. INTRODUCTION Modern large-scale web services rely on caching extensively; inmemory key-value (KV) caches are placed between front-end services and back-end storage systems to achieve high throughput and overcome the long latency gap.In-memory KV caches are widely used and discussed in industry and research communities; Memcached [29] and Redis [23] are two in-memory caching solutions commonly deployed in many production environments.Large web service providers like Facebook and Twitter also developed their general-purpose caching frameworks, Cachelib [4] and Pelikan [42], respectively, to handle their caching use cases. The performance of these caching systems is largely impacted by its replacement/eviction policy, i.e., the algorithm that decides whether an item should be cached or evicted.The Least Recently Used (LRU) is one of the most commonly known replacement policies, which evicts an item based on the item's access recency.Despite its simplicity, the LRU policy has proven quite effective in many caching systems [4,9,32,41].There are also many other advanced eviction policies, such as [2,6,27,33,43], which make eviction decisions based on a combination of item's metadata.The effectiveness of an eviction policy primarily depends on two factors: First, the caching workload; With the rise of cloud and datadriven services, the diversity of in-memory caching workloads has grown drastically compared to the past [1,4,10,41].Many existing studies have shown that no existing heuristic-based eviction policies can consistently outperform others under every caching use case [27,33,43].As a result, many special purposes caching frameworks have adopted different replacement policies to accommodate different use cases [13,26,39,42]. Second, the underlying cache's structure and the storage medium also constrain the",
            "retrieval next text": "choice of replacement policy.For example, flashbased cache suffers from application-level write amplification (ALWA); even though FIFO does not deliver a good hit ratio, using FIFO helps avoid metadata updates on reads which reduces ALWA [15].Due to this, FIFO is often the top choice for replacement on a flash-based cache. Memcached and Redis use replacement policies based on more conservative heuristics to prevent poor performance on unpredictable extreme workload patterns.The early versions of Memcached used the traditional doubly-linked list implementation of LRU as default replacement [12].The LRU policy helps the cache retain the most recent data; however, the serialized LRU update procedures severely hindered Memcached's multithreaded scalability, especially on write-heavy workloads.To address the thread contention problem, the later version of Memcached (after 1.5.0)added a multi-queue LRU and asynchronous updates, named Segmented LRU (SegLRU), which significantly improved Memcached's performance on write-heavy workloads [12].Similar multi-list approaches are observed in other caching systems such as [4,35,43].On the other hand, Redis uses random sampling to approximate the true LRU replacement.On cache eviction, the random samplingbased LRU, or KLRU for short, randomly selects K objects from the cache and evicts the least recently used object among the K objects [22].Such sampling-based technique is commonly used in priority function-based replacement policies [2,6,22,31].Although both Memcached and Redis use similar replacement heuristics, they are implemented based on very different approaches, which motivates the need for a thorough comparison of their impacts on the in-memory caching system's performance.In this work, we implement Redis-like KLRU in Memcached.Then, we present a detailed comparison based on their impact on",
            "retrieval score": "1.5432034",
            "text": "workloads. INTRODUCTION Modern large-scale web services rely on caching extensively; inmemory key-value (KV) caches are placed between front-end services and back-end storage systems to achieve high throughput and overcome the long latency gap.In-memory KV caches are widely used and discussed in industry and research communities; Memcached and Redis are two in-memory caching solutions commonly deployed in many production environments.Large web service providers like Facebook and Twitter also developed their general-purpose caching frameworks, Cachelib and Pelikan, respectively, to handle their caching use cases. The performance of these caching systems is largely impacted by its replacement/eviction policy, i.e., the algorithm that decides whether an item should be cached or evicted.The Least Recently Used (LRU) is one of the most commonly known replacement policies, which evicts an item based on the item's access recency.Despite its simplicity, the LRU policy has proven quite effective in many caching systems,9,32,41.There are also many other advanced eviction policies, such as,6,27,33,43, which make eviction decisions based on a combination of item's metadata.The effectiveness of an eviction policy primarily depends on two factors: First, the caching workload; With the rise of cloud and datadriven services, the diversity of in-memory caching workloads has grown drastically compared to the past,4,10,41.Many existing studies have shown that no existing heuristic-based eviction policies can consistently outperform others under every caching use case,33,43.As a result, many special purposes caching frameworks have adopted different replacement policies to accommodate different use cases,26,39,42. Second, the underlying cache's structure and the storage medium also constrain the",
            "title": ""
        },
        {
            "id": "10164463",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "by prior figures in this section. Also, when comparing Figures 32(c) and 32(f) it should be noted that owing to our choice of normalized units for bandwidth, the hit rate is always one when all clients have the same (constant) bandwidth above one (Figure 32(c)) whereas bandwidth variations in the national speedtest dataset (Figure 32(f)) result in significant periods of bandwidth below one even for average values substantially larger than one. Again, in practice, we expect clients sharing the same cache to see bandwidth variation between these two extremes, with operators likely to strive towards increasingly stable network conditions for streaming clients [20]. RELATED WORK Broadly, the related work can be split into works that consider the head movements during viewing of 360\u00b0videos, client-side techniques to provide the best possible QoE (e.g., through adaptive prefetching based on expected viewing directions), and caching of HAS videos. While some recent works have considered optimized cache management policies for 360\u00b0videos [25,29,32], none of these works provide a data-driven characterization of the caching opportunities that would be observed with traditional caching policies that simply cache the requested tiles when the clients apply adaptive prefetching techniques. Head-movement characterization: Some recent works have collected datasets and characterized the 360\u00b0viewer behavior [1,2,9,11,12,26,34]. However, most of these datasets use relatively short video segments and do not capture changes in behavior over time or across classes of videos. The primary exception, and the work most closely related to ours, is the work by Almquist et al. [1], as we use their dataset. In their work,",
            "retrieval next text": "they present a category-based characterization of the head movements over time, and analyze how changes in viewing behavior depend on the time window considered, but do not consider overlapping viewports of users watching the same video or other similarity metrics of users' viewing directions. Coverage and overlap metrics, such as those introduced here, are valuable in identifying potential caching and bandwidth saving opportunities. Client-side techniques: Motivated by HMDs allowing different projection and quality adaptive download techniques [52], various techniques have been proposed to adaptively download different qualities for different viewing directions [2,18,34,39,40,53]. These typically combine user head movement tracking/prediction [2,34,35,48] and bandwidth management [18,31]. For example, Bao et al. [2] show that view-dependent 360\u00b0transmission schemes with motion prediction can reduce bandwidth consumption by 45% at the cost of only a small performance degradation. Similarly, Hosseini and Swaminathan [18] present an adaptive tile-based streaming solution and show that large bandwidth savings (72%) can be achieved with only small quality degradation. Graf et al. [17] studied the impact of projection techniques, quantization parameters, and tile patterns on the playback experience and resource requirements. Others have considered tradeoffs that try to address variations and uncertainties in the user's bandwidth and viewing direction simultaneously [1,34,41,50]. For example, Sun et al. [41] use simulations and experiments that capture the bandwidth variations, Qian et al. [34] have implemented and tested a tile-based solution on a real network, whereas Almquist et al. [1] note that HAS clients typically try to maintain a steady buffer (to protect against stalls) and consider the optimized prefetch-aggressiveness",
            "retrieval score": "1.4591223",
            "text": "by prior figures in this section. Also, when comparing Figures 32(c) and 32(f) it should be noted that owing to our choice of normalized units for bandwidth, the hit rate is always one when all clients have the same (constant) bandwidth above one (Figure 32(c)) whereas bandwidth variations in the national speedtest dataset (Figure 32(f)) result in significant periods of bandwidth below one even for average values substantially larger than one. Again, in practice, we expect clients sharing the same cache to see bandwidth variation between these two extremes, with operators likely to strive towards increasingly stable network conditions for streaming clients. RELATED WORK Broadly, the related work can be split into works that consider the head movements during viewing of 360\u00b0videos, client-side techniques to provide the best possible QoE (e.g., through adaptive prefetching based on expected viewing directions), and caching of HAS videos. While some recent works have considered optimized cache management policies for 360\u00b0videos,29,32, none of these works provide a data-driven characterization of the caching opportunities that would be observed with traditional caching policies that simply cache the requested tiles when the clients apply adaptive prefetching techniques. Head-movement characterization: Some recent works have collected datasets and characterized the 360\u00b0viewer behavior,2,9,11,12,26,34. However, most of these datasets use relatively short video segments and do not capture changes in behavior over time or across classes of videos. The primary exception, and the work most closely related to ours, is the work by Almquist et al., as we use their dataset. In their work,",
            "title": ""
        },
        {
            "id": "9087067",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "Spects 2005 -decision on Manuscript Id Spects-05-nts-025 Reviewer: 1 Comments to the Author This paper have technical weak points.First the background and references are not adequate!The locality characteristics are found not only with characteristics of 2-tier architecture but also with 3-tier etc. Thus locality characteristics are examined not only in a temporal context but also in client temporal context particularly for web based streams... Secondly, authros do not evaluate through exaustive simulations the correllation bettween locality characteristics and self similar traffic for demand-driven caching schemes. However this paper has a positive point that it hosts many results...But I believe that these results do not figure out any significance... I recommend that authors should exploit circular explanations to concepts reaching readers without the topic's background.",
            "retrieval next text": "Techniques of data retrieval for scientometric research in the ISI citation indexes The usefulness of the retrieval software of the hosts DIMDI, STN, DIALOG and ORBIT and the ISI CD-ROM for scientometric studies in Citation Index databases is exam ined. The frequency distribution of publications and citations. the co-author and co-citation analysis, and the method of bibliographic coupling are used for examples. Practical prob lems in the work with the online Citation Index are described.",
            "retrieval score": "1.4910431",
            "text": "Spects 2005 -decision on Manuscript Id Spects-05-nts-025 Reviewer: 1 Comments to the Author This paper have technical weak points.First the background and references are not adequate!The locality characteristics are found not only with characteristics of 2-tier architecture but also with 3-tier etc. Thus locality characteristics are examined not only in a temporal context but also in client temporal context particularly for web based streams... Secondly, authros do not evaluate through exaustive simulations the correllation bettween locality characteristics and self similar traffic for demand-driven caching schemes. However this paper has a positive point that it hosts many results...But I believe that these results do not figure out any significance... I recommend that authors should exploit circular explanations to concepts reaching readers without the topic's background.",
            "title": ""
        },
        {
            "id": "8871776",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "provides both CPU-GPU with a definite cache space. On the other hand, the reuse cache is able to achieve within 32% data bus utilization for 4 of the 5 workload mixes. VI. CONCLUSION LLC Management is an important problem in today's heterogeneous processors. The reference stream observed by the shared-LLC (SLLC) from the CPU/GPU exhibits little temporal locality, but instead, it exhibits reuse locality. As a consequence, a high proportion of the SLLC lines is useless because the lines will not be requested again before being evicted, and most hits are expected to come from a small subset of already reused lines. This proportion could be particularly high for the GPU accesses. In this work, we looked to minimize counterproductive insertions and evictions in the SLLC by implementing a reuse cache. We evaluate our proposal by running heterogeneous CPU-GPU workloads on the gem5 AMD APU model. We use static partitioning and GPU LLC bypassing as our baseline comparison. Our major observations are: (i) Static-partitioning performs best when the GPU application working set fits into LLC and the CPU application is not cache-sensitive (ii) For the reuse cache, a performance of within 0.8% (or better when GPU application is cache-sensitive) was achievable for most cases, providing average 40% reduction in area (iii) A tag-to-data cache ratio of 2:1 (where data-array is half of a conventional cache) is a good starting point for reuse-cache design space exploration (iv) Simple LLC bypassing degrades performance for GPU in most cases, though it could improve CPU IPC. Static-partitioning hard-partitions the cache,",
            "retrieval next text": "while GPU LLC-bypassing does not provide space to GPU in LLC. We think reuse cache incorporates properties from both, hence could adapt to a wider range of applications. VII. FUTURE WORK In this paper, we have achieved comparable or slightly better application performance with reuse cache in most cases, at significant area reduction. Further reuse cache performance improvements might be closely linked with data/tag replacement policies and memory-access rate (and thread-awareness) in heterogeneous systems. Potential performance improvement might also be obtained by adding a \"Tag-only\" coherence state in the reuse cache coherence protocol, and increasing the reuse hysteresis. Reuse cache could be combined with cache compression techniques where we have more tags than data blocks, thereby reducing data-array size further.",
            "retrieval score": "1.4404079",
            "text": "provides both CPU-GPU with a definite cache space. On the other hand, the reuse cache is able to achieve within 32% data bus utilization for 4 of the 5 workload mixes. VI. CONCLUSION LLC Management is an important problem in today's heterogeneous processors. The reference stream observed by the shared-LLC (SLLC) from the CPU/GPU exhibits little temporal locality, but instead, it exhibits reuse locality. As a consequence, a high proportion of the SLLC lines is useless because the lines will not be requested again before being evicted, and most hits are expected to come from a small subset of already reused lines. This proportion could be particularly high for the GPU accesses. In this work, we looked to minimize counterproductive insertions and evictions in the SLLC by implementing a reuse cache. We evaluate our proposal by running heterogeneous CPU-GPU workloads on the gem5 AMD APU model. We use static partitioning and GPU LLC bypassing as our baseline comparison. Our major observations are: (i) Static-partitioning performs best when the GPU application working set fits into LLC and the CPU application is not cache-sensitive (ii) For the reuse cache, a performance of within 0.8% (or better when GPU application is cache-sensitive) was achievable for most cases, providing average 40% reduction in area (iii) A tag-to-data cache ratio of 2:1 (where data-array is half of a conventional cache) is a good starting point for reuse-cache design space exploration (iv) Simple LLC bypassing degrades performance for GPU in most cases, though it could improve CPU IPC. Static-partitioning hard-partitions the cache,",
            "title": ""
        },
        {
            "id": "367989",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "of the same length. Empirically, as prediction errors are dominantly smaller than the actual TTFT, CacheGen can almost always pick the right compression level to meet a given TTFT target. Related work Faster LLM serving: The landscape of LLM systems research is fast evolving. Besides speeding up LLM training (e.g., [86,91]), many recent efforts, including ours, focus on the LLM serving systems. Some try to parallelize the generation of multiple tokens with smaller approximating models [59,74] and accelerate the inference of Mix-of-Expert LLMs on edge devices [111]. Others quantize LLM model weights (e.g., [22]), reduce memory I/O of GPU on-chip SRAM [39], and reduce self-attention computation complexity by k-means [87]. New scheduling strategies are also proposed for LLM services [105,112]. Nonetheless, a common approach to speeding up without modifying the LLMs is by caching KV features of repeatedly used input sequences, such as contexts [38,63,67,74,80,82,89,104,105,118]. However, caching KV features results in a dramatically increased amount of cached data, which requires a bandwidth-efficient KV/context-loading system like CacheGen. Longer LLM input (context): Though the benefits of longer contexts are still under scrutiny [66,87,93], many recent efforts aim at enabling LLMs to accept ever longer contexts. The main challenge is to hold the attention matrices of more tokens without needing a bigger GPU memory. They enable longer input sequences by dynamically offloading less used parts of the attention matrices [89], using external knowledge via KNN [106], retraining the self-attention module to only attend to top-k keys to approximate the full attention [21,26], mapping long inputs into smaller latent space",
            "retrieval next text": "[47], using local windowed self-attention [25], or using dilated attention [42] or sparse attention [113] to scale the input length to even 1 billion tokens. However, the longer inputs (contexts) also inflate the KV cache, and this work complements these efforts by focusing on fast remote loading of KV cache. Context shortening/pruning: More relevant to our work is the emerging literature on shortening and pruning long contexts. These efforts select the most important text segments and prune out the others to reduce context length, based on the similarity between the user query and the potentially relevant documents (e.g., [28]), or drop tokens that are less attended to by the current prompt [67,118]. Different from them, CacheGen compresses the KV cache into bitstreams, rather than shortening the context in its original form (i.e., text or tensor). Furthermore, unlike [28,67,118], CacheGen's KV compression does not need to know the query (prompt). Recently, some work retrains LLM models to directly take in contexts that are rewritten by gisting [76] or autoencoding [45]. Different from them, CacheGen does not need to train a new LLM model and also achieves better compression rate with less information loss by leveraging unique distributional properties of KV features. Finally, we show CacheGen achieves better delay-quality tradeoffs than existing context compressors ( \u00a75.4). and can potentially be combined with them to further improve the delay-quality tradeoffs. Tensor compression: CacheGen's KV encoder can be seen as a special tensor compression technique tailored for KV features produced by LLMs. General tensor compression has been intensively studied, e.g.,",
            "retrieval score": "1.4546101",
            "text": "of the same length. Empirically, as prediction errors are dominantly smaller than the actual TTFT, CacheGen can almost always pick the right compression level to meet a given TTFT target. Related work Faster LLM serving: The landscape of LLM systems research is fast evolving. Besides speeding up LLM training (e.g.,,91), many recent efforts, including ours, focus on the LLM serving systems. Some try to parallelize the generation of multiple tokens with smaller approximating models,74 and accelerate the inference of Mix-of-Expert LLMs on edge devices. Others quantize LLM model weights (e.g.,), reduce memory I/O of GPU on-chip SRAM, and reduce self-attention computation complexity by k-means. New scheduling strategies are also proposed for LLM services,112. Nonetheless, a common approach to speeding up without modifying the LLMs is by caching KV features of repeatedly used input sequences, such as contexts,63,67,74,80,82,89,104,105,118. However, caching KV features results in a dramatically increased amount of cached data, which requires a bandwidth-efficient KV/context-loading system like CacheGen. Longer LLM input (context): Though the benefits of longer contexts are still under scrutiny,87,93, many recent efforts aim at enabling LLMs to accept ever longer contexts. The main challenge is to hold the attention matrices of more tokens without needing a bigger GPU memory. They enable longer input sequences by dynamically offloading less used parts of the attention matrices, using external knowledge via KNN, retraining the self-attention module to only attend to top-k keys to approximate the full attention,26, mapping long inputs into smaller latent space",
            "title": ""
        },
        {
            "id": "10164431",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "for each. The prefetching policy must address a prefetch-aggressiveness tradeoff [1] and balance the use of a larger buffer (to protect against stalls) against making prefetching decisions closer to the time of playback (improving predictions of future viewing directions). To address this problem various head-movement prediction techniques have been proposed and evaluated [2,35,48]. However, prior work has not considered the implications of tiling and associated quality-adaptive prefetching techniques for 360\u00b0video on the performance of content caches. This paper presents new trace-based analysis methods that incorporate users' viewports (the area of the full 360\u00b0view the user actually sees), a first characterization of the cross-user similarities of the users' viewports, and a trace-based analysis of the potential bandwidth savings that cachingbased techniques may offer under different conditions. The paper has three main parts, with the second and third parts building on the prior parts. Furthermore, each part includes both novel methodological contributions and a trace-based characterization or analysis providing insights that can help guide the design of more effective caching policies and other related system solutions. First, we focus on similarities in the viewports among different users when at identical playback points within the same 360\u00b0video. Here, we define basic similarity metrics such as the overlap in viewports for two or more users and study how the overlap between a viewport and the aggregate view cover from prior user views increases with the number of such users and how this differs between different video categories. This analysis provides insight into inherent similarities in viewing behavior, and the metrics",
            "retrieval next text": "are not affected by the details of how video data is delivered. Second, we extend the metrics and analysis to evaluate the impact of chunk granularity on the insights from the first part of the paper. This analysis is important to understand how similarities in viewing direction would impact caching performance. For example, consider the case where two users have significantly different viewing directions at a particular time instant, and yet, over the time duration of a chunk, they have essentially the same viewport cover. Finally, we present simulation results using a novel simulation model. The model captures steady-state performance of many independent sessions, while using only a limited number of traces. We simulate a proxy cache and evaluate the cache hit rates observed when using prefetching algorithms that attempt to adaptively select which tiles to download and the quality of each to optimize the user's quality of experience. The observed cache hit rates (analyzed in Section 5) reflect not only viewing direction similarities, both at identical time instants (analyzed in Section 3) and over the time duration of a chunk (analyzed in Section 4), but also differences in chunk quality selections caused by bandwidth variations and uncertainties in viewing direction. The simulation results provide quantitative example comparisons and deliver insights into how viewing direction similarities and potential caching performance depend on the nature of the 360\u00b0video content. Throughout our analysis we place particular focus on insights regarding differences seen when comparing categories of 360\u00b0videos. For this analysis, we use head-movement traces collected for different 360\u00b0categories",
            "retrieval score": "1.4498613",
            "text": "for each. The prefetching policy must address a prefetch-aggressiveness tradeoff and balance the use of a larger buffer (to protect against stalls) against making prefetching decisions closer to the time of playback (improving predictions of future viewing directions). To address this problem various head-movement prediction techniques have been proposed and evaluated,35,48. However, prior work has not considered the implications of tiling and associated quality-adaptive prefetching techniques for 360\u00b0video on the performance of content caches. This paper presents new trace-based analysis methods that incorporate users' viewports (the area of the full 360\u00b0view the user actually sees), a first characterization of the cross-user similarities of the users' viewports, and a trace-based analysis of the potential bandwidth savings that cachingbased techniques may offer under different conditions. The paper has three main parts, with the second and third parts building on the prior parts. Furthermore, each part includes both novel methodological contributions and a trace-based characterization or analysis providing insights that can help guide the design of more effective caching policies and other related system solutions. First, we focus on similarities in the viewports among different users when at identical playback points within the same 360\u00b0video. Here, we define basic similarity metrics such as the overlap in viewports for two or more users and study how the overlap between a viewport and the aggregate view cover from prior user views increases with the number of such users and how this differs between different video categories. This analysis provides insight into inherent similarities in viewing behavior, and the metrics",
            "title": ""
        },
        {
            "paperId": "http://arxiv.org/abs/2411.17981v1",
            "url": "http://arxiv.org/abs/2411.17981v1",
            "title": "Engineering Trustworthy Software: A Mission for LLMs",
            "year": "2024",
            "publicationDate": "2024-11-27 01:30:44+00:00",
            "abstract": "LLMs are transforming software engineering by accelerating development,\nreducing complexity, and cutting costs. When fully integrated into the software\nlifecycle they will drive design, development and deployment while facilitating\nearly bug detection, continuous improvement, and rapid resolution of critical\nissues. However, trustworthy LLM-driven software engineering requires\naddressing multiple challenges such as accuracy, scalability, bias, and\nexplainability.",
            "text": "LLMs are transforming software engineering by accelerating development,\nreducing complexity, and cutting costs. When fully integrated into the software\nlifecycle they will drive design, development and deployment while facilitating\nearly bug detection, continuous improvement, and rapid resolution of critical\nissues. However, trustworthy LLM-driven software engineering requires\naddressing multiple challenges such as accuracy, scalability, bias, and\nexplainability.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2405.17147v1",
            "url": "http://arxiv.org/abs/2405.17147v1",
            "title": "Large Language Models (LLMs): Deployment, Tokenomics and Sustainability",
            "year": "2024",
            "publicationDate": "2024-05-27 13:16:29+00:00",
            "abstract": "The rapid advancement of Large Language Models (LLMs) has significantly\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\nintroduced comprehensive multi-modality capabilities. In this paper, we first\nexplored the deployment strategies, economic considerations, and sustainability\nchallenges associated with the state-of-the-art LLMs. More specifically, we\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\nand fine-tuning, highlighting their respective advantages and limitations.\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\ninference. Additionally, for the tokenomics of LLM services, we examined the\nbalance between performance and cost from the quality of experience (QoE)'s\nperspective of end users. Lastly, we envisioned the future hybrid architecture\nof LLM processing and its corresponding sustainability concerns, particularly\nin the environmental carbon footprint impact. Through these discussions, we\nprovided a comprehensive overview of the operational and strategic\nconsiderations essential for the responsible development and deployment of\nLLMs.",
            "text": "The rapid advancement of Large Language Models (LLMs) has significantly\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\nintroduced comprehensive multi-modality capabilities. In this paper, we first\nexplored the deployment strategies, economic considerations, and sustainability\nchallenges associated with the state-of-the-art LLMs. More specifically, we\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\nand fine-tuning, highlighting their respective advantages and limitations.\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\ninference. Additionally, for the tokenomics of LLM services, we examined the\nbalance between performance and cost from the quality of experience (QoE)'s\nperspective of end users. Lastly, we envisioned the future hybrid architecture\nof LLM processing and its corresponding sustainability concerns, particularly\nin the environmental carbon footprint impact. Through these discussions, we\nprovided a comprehensive overview of the operational and strategic\nconsiderations essential for the responsible development and deployment of\nLLMs.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2412.05693v1",
            "url": "http://arxiv.org/abs/2412.05693v1",
            "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression",
            "year": "2024",
            "publicationDate": "2024-12-07 16:41:54+00:00",
            "abstract": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
            "text": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2505.09999v2",
            "url": "http://arxiv.org/abs/2505.09999v2",
            "title": "ServeGen: Workload Characterization and Generation of Large Language Model Serving in Production",
            "year": "2025",
            "publicationDate": "2025-05-15 06:24:08+00:00",
            "abstract": "With the widespread adoption of Large Language Models (LLMs), serving LLM\ninference requests has become an increasingly important task, attracting active\nresearch advancements. Practical workloads play an essential role in this\nprocess: they are critical for motivating and benchmarking serving techniques\nand systems. However, the existing understanding of real-world LLM serving\nworkloads is limited due to the lack of a comprehensive workload\ncharacterization. Prior analyses remain insufficient in scale and scope, thus\nfailing to fully capture intricate workload characteristics.\n  In this paper, we fill the gap with an in-depth characterization of LLM\nserving workloads collected from our worldwide cloud inference serving service,\ncovering not only language models but also emerging multimodal and reasoning\nmodels, and unveiling important new findings in each case. Moreover, based on\nour findings, we propose ServeGen, a principled framework for generating\nrealistic LLM serving workloads by composing them on a per-client basis. A\npractical use case in production validates that ServeGen avoids 50%\nunder-provisioning compared to naive workload generation, demonstrating\nServeGen's advantage in performance benchmarking. ServeGen is available at\nhttps://github.com/alibaba/ServeGen.",
            "text": "With the widespread adoption of Large Language Models (LLMs), serving LLM\ninference requests has become an increasingly important task, attracting active\nresearch advancements. Practical workloads play an essential role in this\nprocess: they are critical for motivating and benchmarking serving techniques\nand systems. However, the existing understanding of real-world LLM serving\nworkloads is limited due to the lack of a comprehensive workload\ncharacterization. Prior analyses remain insufficient in scale and scope, thus\nfailing to fully capture intricate workload characteristics.\n  In this paper, we fill the gap with an in-depth characterization of LLM\nserving workloads collected from our worldwide cloud inference serving service,\ncovering not only language models but also emerging multimodal and reasoning\nmodels, and unveiling important new findings in each case. Moreover, based on\nour findings, we propose ServeGen, a principled framework for generating\nrealistic LLM serving workloads by composing them on a per-client basis. A\npractical use case in production validates that ServeGen avoids 50%\nunder-provisioning compared to naive workload generation, demonstrating\nServeGen's advantage in performance benchmarking. ServeGen is available at\nhttps://github.com/alibaba/ServeGen.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2402.10517v4",
            "url": "http://arxiv.org/abs/2402.10517v4",
            "title": "Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs",
            "year": "2024",
            "publicationDate": "2024-02-16 09:06:06+00:00",
            "abstract": "Recently, considerable efforts have been directed towards compressing Large\nLanguage Models (LLMs), which showcase groundbreaking capabilities across\ndiverse applications but entail significant deployment costs due to their large\nsizes. Meanwhile, much less attention has been given to mitigating the costs\nassociated with deploying multiple LLMs of varying sizes despite its practical\nsignificance. Thus, this paper introduces \\emph{any-precision LLM}, extending\nthe concept of any-precision DNN to LLMs. Addressing challenges in\nany-precision LLM, we propose a lightweight method for any-precision\nquantization of LLMs, leveraging a post-training quantization framework, and\ndevelop a specialized software engine for its efficient serving. As a result,\nour solution significantly reduces the high costs of deploying multiple,\ndifferent-sized LLMs by overlaying LLMs quantized to varying bit-widths, such\nas 3, 4, ..., $n$ bits, into a memory footprint comparable to a single $n$-bit\nLLM. All the supported LLMs with varying bit-widths demonstrate\nstate-of-the-art model quality and inference throughput, proving itself to be a\ncompelling option for deployment of multiple, different-sized LLMs. Our code is\nopen-sourced and available online.",
            "text": "Recently, considerable efforts have been directed towards compressing Large\nLanguage Models (LLMs), which showcase groundbreaking capabilities across\ndiverse applications but entail significant deployment costs due to their large\nsizes. Meanwhile, much less attention has been given to mitigating the costs\nassociated with deploying multiple LLMs of varying sizes despite its practical\nsignificance. Thus, this paper introduces \\emph{any-precision LLM}, extending\nthe concept of any-precision DNN to LLMs. Addressing challenges in\nany-precision LLM, we propose a lightweight method for any-precision\nquantization of LLMs, leveraging a post-training quantization framework, and\ndevelop a specialized software engine for its efficient serving. As a result,\nour solution significantly reduces the high costs of deploying multiple,\ndifferent-sized LLMs by overlaying LLMs quantized to varying bit-widths, such\nas 3, 4, ..., $n$ bits, into a memory footprint comparable to a single $n$-bit\nLLM. All the supported LLMs with varying bit-widths demonstrate\nstate-of-the-art model quality and inference throughput, proving itself to be a\ncompelling option for deployment of multiple, different-sized LLMs. Our code is\nopen-sourced and available online.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2407.11550v4",
            "url": "http://arxiv.org/abs/2407.11550v4",
            "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference",
            "year": "2024",
            "publicationDate": "2024-07-16 09:53:32+00:00",
            "abstract": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods.",
            "text": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2502.00840v1",
            "url": "http://arxiv.org/abs/2502.00840v1",
            "title": "Activation Approximations Can Incur Safety Vulnerabilities Even in Aligned LLMs: Comprehensive Analysis and Defense",
            "year": "2025",
            "publicationDate": "2025-02-02 16:25:48+00:00",
            "abstract": "Large Language Models (LLMs) have showcased remarkable capabilities across\nvarious domains. Accompanying the evolving capabilities and expanding\ndeployment scenarios of LLMs, their deployment challenges escalate due to their\nsheer scale and the advanced yet complex activation designs prevalent in\nnotable model series, such as Llama, Gemma, and Mistral. These challenges have\nbecome particularly pronounced in resource-constrained deployment scenarios,\nwhere mitigating inference efficiency bottlenecks is imperative. Among various\nrecent efforts, activation approximation has emerged as a promising avenue for\npursuing inference efficiency, sometimes considered indispensable in\napplications such as private inference. Despite achieving substantial speedups\nwith minimal impact on utility, even appearing sound and practical for\nreal-world deployment, the safety implications of activation approximations\nremain unclear. In this work, we fill this critical gap in LLM safety by\nconducting the first systematic safety evaluation of activation approximations.\nOur safety vetting spans seven sota techniques across three popular categories,\nrevealing consistent safety degradation across ten safety-aligned LLMs.",
            "text": "Large Language Models (LLMs) have showcased remarkable capabilities across\nvarious domains. Accompanying the evolving capabilities and expanding\ndeployment scenarios of LLMs, their deployment challenges escalate due to their\nsheer scale and the advanced yet complex activation designs prevalent in\nnotable model series, such as Llama, Gemma, and Mistral. These challenges have\nbecome particularly pronounced in resource-constrained deployment scenarios,\nwhere mitigating inference efficiency bottlenecks is imperative. Among various\nrecent efforts, activation approximation has emerged as a promising avenue for\npursuing inference efficiency, sometimes considered indispensable in\napplications such as private inference. Despite achieving substantial speedups\nwith minimal impact on utility, even appearing sound and practical for\nreal-world deployment, the safety implications of activation approximations\nremain unclear. In this work, we fill this critical gap in LLM safety by\nconducting the first systematic safety evaluation of activation approximations.\nOur safety vetting spans seven sota techniques across three popular categories,\nrevealing consistent safety degradation across ten safety-aligned LLMs.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2505.09082v1",
            "url": "http://arxiv.org/abs/2505.09082v1",
            "title": "CEC-Zero: Chinese Error Correction Solution Based on LLM",
            "year": "2025",
            "publicationDate": "2025-05-14 02:35:47+00:00",
            "abstract": "Recent advancements in large language models (LLMs) demonstrate exceptional\nChinese text processing capabilities, particularly in Chinese Spelling\nCorrection (CSC). While LLMs outperform traditional BERT-based models in\naccuracy and robustness, challenges persist in reliability and generalization.\nThis paper proposes CEC-Zero, a novel reinforcement learning (RL) framework\nenabling LLMs to self-correct through autonomous error strategy learning\nwithout external supervision. By integrating RL with LLMs' generative power,\nthe method eliminates dependency on annotated data or auxiliary models.\nExperiments reveal RL-enhanced LLMs achieve industry-viable accuracy and\nsuperior cross-domain generalization, offering a scalable solution for\nreliability optimization in Chinese NLP applications. This breakthrough\nfacilitates LLM deployment in practical Chinese text correction scenarios while\nestablishing a new paradigm for self-improving language models.",
            "text": "Recent advancements in large language models (LLMs) demonstrate exceptional\nChinese text processing capabilities, particularly in Chinese Spelling\nCorrection (CSC). While LLMs outperform traditional BERT-based models in\naccuracy and robustness, challenges persist in reliability and generalization.\nThis paper proposes CEC-Zero, a novel reinforcement learning (RL) framework\nenabling LLMs to self-correct through autonomous error strategy learning\nwithout external supervision. By integrating RL with LLMs' generative power,\nthe method eliminates dependency on annotated data or auxiliary models.\nExperiments reveal RL-enhanced LLMs achieve industry-viable accuracy and\nsuperior cross-domain generalization, offering a scalable solution for\nreliability optimization in Chinese NLP applications. This breakthrough\nfacilitates LLM deployment in practical Chinese text correction scenarios while\nestablishing a new paradigm for self-improving language models.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2410.01805v2",
            "url": "http://arxiv.org/abs/2410.01805v2",
            "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads on Consumer-Grade Devices",
            "year": "2024",
            "publicationDate": "2024-10-02 17:59:52+00:00",
            "abstract": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.",
            "text": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2408.12757v2",
            "url": "http://arxiv.org/abs/2408.12757v2",
            "title": "NanoFlow: Towards Optimal Large Language Model Serving Throughput",
            "year": "2024",
            "publicationDate": "2024-08-22 23:00:40+00:00",
            "abstract": "Large Language Models (LLMs) have resulted in a surging demand for\nplanet-scale serving systems, where tens of thousands of GPUs continuously\nserve hundreds of millions of users. Consequently, throughput has emerged as a\nkey metric that determines serving systems' performance. Due to large model\nsizes and memory-intensive self-attention, LLM serving has been commonly\nassumed to be memory-bound. Through a detailed analysis, we show that despite\nhaving memory-intensive components, end-to-end LLM serving is compute bound for\nmost common workloads and LLMs. Alas, most existing serving engines fall short\nfrom optimal compute utilization, because the heterogeneous operations that\ncomprise LLM serving--compute, memory, networking--are executed sequentially\nwithin a device.\n  We propose NanoFlow, a novel serving framework that exploits intra-device\nparallelism, which overlaps the usage of heterogeneous resources within a\nsingle device. NanoFlow splits inputs into smaller nano-batches and duplicates\noperations to operate on each portion independently, enabling overlapping.\nNanoFlow automatically identifies the number, size, ordering, and GPU resource\nallocation of nano-batches to minimize the execution time, while considering\nthe interference of concurrent operations. We evaluate NanoFlow's end-to-end\nserving throughput on several popular models such as LLaMA-2-70B, Mixtral 8x7B,\nLLaMA-3-8B, etc. With practical workloads, NanoFlow provides 1.91x throughput\nboost compared to state-of-the-art serving systems achieving 50% to 72% of\noptimal throughput across popular models.",
            "text": "Large Language Models (LLMs) have resulted in a surging demand for\nplanet-scale serving systems, where tens of thousands of GPUs continuously\nserve hundreds of millions of users. Consequently, throughput has emerged as a\nkey metric that determines serving systems' performance. Due to large model\nsizes and memory-intensive self-attention, LLM serving has been commonly\nassumed to be memory-bound. Through a detailed analysis, we show that despite\nhaving memory-intensive components, end-to-end LLM serving is compute bound for\nmost common workloads and LLMs. Alas, most existing serving engines fall short\nfrom optimal compute utilization, because the heterogeneous operations that\ncomprise LLM serving--compute, memory, networking--are executed sequentially\nwithin a device.\n  We propose NanoFlow, a novel serving framework that exploits intra-device\nparallelism, which overlaps the usage of heterogeneous resources within a\nsingle device. NanoFlow splits inputs into smaller nano-batches and duplicates\noperations to operate on each portion independently, enabling overlapping.\nNanoFlow automatically identifies the number, size, ordering, and GPU resource\nallocation of nano-batches to minimize the execution time, while considering\nthe interference of concurrent operations. We evaluate NanoFlow's end-to-end\nserving throughput on several popular models such as LLaMA-2-70B, Mixtral 8x7B,\nLLaMA-3-8B, etc. With practical workloads, NanoFlow provides 1.91x throughput\nboost compared to state-of-the-art serving systems achieving 50% to 72% of\noptimal throughput across popular models.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2402.06262v2",
            "url": "http://arxiv.org/abs/2402.06262v2",
            "title": "On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference",
            "year": "2024",
            "publicationDate": "2024-02-09 09:20:59+00:00",
            "abstract": "Despite the recent success associated with Large Language Models (LLMs), they\nare notably cost-prohibitive to deploy in resource-constrained environments due\nto their excessive memory and computational demands. In addition to model\nparameters, the key-value cache is also stored in GPU memory, growing linearly\nwith batch size and sequence length. As a remedy, recent works have proposed\nvarious eviction policies for maintaining the overhead of key-value cache under\na given budget. This paper embarks on the efficacy of existing eviction\npolicies in terms of importance score calculation and eviction scope\nconstruction. We identify the deficiency of prior policies in these two aspects\nand introduce RoCo, a robust cache omission policy based on temporal attention\nscores and robustness measures. Extensive experimentation spanning prefilling\nand auto-regressive decoding stages validates the superiority of RoCo. Finally,\nwe release EasyKV, a versatile software package dedicated to user-friendly\nkey-value constrained generative inference. Code available at\nhttps://github.com/DRSY/EasyKV.",
            "text": "Despite the recent success associated with Large Language Models (LLMs), they\nare notably cost-prohibitive to deploy in resource-constrained environments due\nto their excessive memory and computational demands. In addition to model\nparameters, the key-value cache is also stored in GPU memory, growing linearly\nwith batch size and sequence length. As a remedy, recent works have proposed\nvarious eviction policies for maintaining the overhead of key-value cache under\na given budget. This paper embarks on the efficacy of existing eviction\npolicies in terms of importance score calculation and eviction scope\nconstruction. We identify the deficiency of prior policies in these two aspects\nand introduce RoCo, a robust cache omission policy based on temporal attention\nscores and robustness measures. Extensive experimentation spanning prefilling\nand auto-regressive decoding stages validates the superiority of RoCo. Finally,\nwe release EasyKV, a versatile software package dedicated to user-friendly\nkey-value constrained generative inference. Code available at\nhttps://github.com/DRSY/EasyKV.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2410.12876v3",
            "url": "http://arxiv.org/abs/2410.12876v3",
            "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
            "year": "2024",
            "publicationDate": "2024-10-15 05:01:19+00:00",
            "abstract": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
            "text": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2403.01136v1",
            "url": "http://arxiv.org/abs/2403.01136v1",
            "title": "LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization",
            "year": "2024",
            "publicationDate": "2024-03-02 08:40:07+00:00",
            "abstract": "Recent breakthroughs in Large-scale language models (LLMs) have demonstrated\nimpressive performance on various tasks. The immense sizes of LLMs have led to\nvery high resource demand and cost for running the models. Though the models\nare largely served using uniform high-caliber GPUs nowadays, utilizing a\nheterogeneous cluster with a mix of available high- and low-capacity GPUs can\npotentially substantially reduce the serving cost. There is a lack of designs\nto support efficient LLM serving using a heterogeneous cluster, while the\ncurrent solutions focus on model partition and uniform compression among\nhomogeneous devices. This paper proposes LLM-PQ, a system that advocates\nadaptive model quantization and phase-aware partition to improve LLM serving\nefficiency on heterogeneous GPU clusters. We carefully decide on\nmixed-precision model quantization together with phase-aware model partition\nand micro-batch sizing in distributed LLM serving with an efficient algorithm,\nto greatly enhance inference throughput while fulfilling user-specified model\nquality targets. Extensive experiments on production inference workloads in 11\ndifferent clusters demonstrate that LLM-PQ achieves up to 2.88x (2.26x on\naverage) throughput improvement in inference, showing great advantages over\nstate-of-the-art works.",
            "text": "Recent breakthroughs in Large-scale language models (LLMs) have demonstrated\nimpressive performance on various tasks. The immense sizes of LLMs have led to\nvery high resource demand and cost for running the models. Though the models\nare largely served using uniform high-caliber GPUs nowadays, utilizing a\nheterogeneous cluster with a mix of available high- and low-capacity GPUs can\npotentially substantially reduce the serving cost. There is a lack of designs\nto support efficient LLM serving using a heterogeneous cluster, while the\ncurrent solutions focus on model partition and uniform compression among\nhomogeneous devices. This paper proposes LLM-PQ, a system that advocates\nadaptive model quantization and phase-aware partition to improve LLM serving\nefficiency on heterogeneous GPU clusters. We carefully decide on\nmixed-precision model quantization together with phase-aware model partition\nand micro-batch sizing in distributed LLM serving with an efficient algorithm,\nto greatly enhance inference throughput while fulfilling user-specified model\nquality targets. Extensive experiments on production inference workloads in 11\ndifferent clusters demonstrate that LLM-PQ achieves up to 2.88x (2.26x on\naverage) throughput improvement in inference, showing great advantages over\nstate-of-the-art works.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2502.08381v1",
            "url": "http://arxiv.org/abs/2502.08381v1",
            "title": "The MoE-Empowered Edge LLMs Deployment: Architecture, Challenges, and Opportunities",
            "year": "2025",
            "publicationDate": "2025-02-12 13:16:07+00:00",
            "abstract": "The powerfulness of LLMs indicates that deploying various LLMs with different\nscales and architectures on end, edge, and cloud to satisfy different\nrequirements and adaptive heterogeneous hardware is the critical way to achieve\nubiquitous intelligence for 6G. However, the massive parameter scale of LLMs\nposes significant challenges in deploying them on edge devices due to high\ncomputational and storage demands. Considering that the sparse activation in\nMixture of Experts (MoE) is effective on scalable and dynamic allocation of\ncomputational and communications resources at the edge, this paper proposes a\nnovel MoE-empowered collaborative deployment framework for edge LLMs, denoted\nas CoEL. This framework fully leverages the properties of MoE architecture and\nencompasses four key aspects: Perception, Deployment, Compression, and\nUpdating. Edge servers broadcast their resource status and the specific\nresource requirements of LLMs to their neighbors. Then, utilizing this data,\ntwo sophisticated deployment strategies are proposed for satisfying varying\nmodel scales, ensuring that each model is deployed effectively. One for\ndeploying LLMs on a single edge device through intra-device resource\ncollaboration, and another for a distributed deployment across multiple edge\ndevices via inter-device resource collaboration. Furthermore, both the models\nand the intermediate data are compressed for reducing memory footprint by\nquantization and reducing the volume of intermediate data by token fusion and\npruning. Finally, given the dynamic of network topology, resource status, and\nuser requirements, the deployment strategies are regularly updated to maintain\nits relevance and effectiveness. This paper also delineates the challenges and\npotential research directions for the deployment of edge LLMs.",
            "text": "The powerfulness of LLMs indicates that deploying various LLMs with different\nscales and architectures on end, edge, and cloud to satisfy different\nrequirements and adaptive heterogeneous hardware is the critical way to achieve\nubiquitous intelligence for 6G. However, the massive parameter scale of LLMs\nposes significant challenges in deploying them on edge devices due to high\ncomputational and storage demands. Considering that the sparse activation in\nMixture of Experts (MoE) is effective on scalable and dynamic allocation of\ncomputational and communications resources at the edge, this paper proposes a\nnovel MoE-empowered collaborative deployment framework for edge LLMs, denoted\nas CoEL. This framework fully leverages the properties of MoE architecture and\nencompasses four key aspects: Perception, Deployment, Compression, and\nUpdating. Edge servers broadcast their resource status and the specific\nresource requirements of LLMs to their neighbors. Then, utilizing this data,\ntwo sophisticated deployment strategies are proposed for satisfying varying\nmodel scales, ensuring that each model is deployed effectively. One for\ndeploying LLMs on a single edge device through intra-device resource\ncollaboration, and another for a distributed deployment across multiple edge\ndevices via inter-device resource collaboration. Furthermore, both the models\nand the intermediate data are compressed for reducing memory footprint by\nquantization and reducing the volume of intermediate data by token fusion and\npruning. Finally, given the dynamic of network topology, resource status, and\nuser requirements, the deployment strategies are regularly updated to maintain\nits relevance and effectiveness. This paper also delineates the challenges and\npotential research directions for the deployment of edge LLMs.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2408.00008v2",
            "url": "http://arxiv.org/abs/2408.00008v2",
            "title": "ScaleLLM: A Resource-Frugal LLM Serving Framework by Optimizing End-to-End Efficiency",
            "year": "2024",
            "publicationDate": "2024-07-23 23:37:29+00:00",
            "abstract": "Large language models (LLMs) have surged in popularity and are extensively\nused in commercial applications, where the efficiency of model serving is\ncrucial for the user experience. Most current research focuses on optimizing\nindividual sub-procedures, e.g. local inference and communication, however,\nthere is no comprehensive framework that provides a holistic system view for\noptimizing LLM serving in an end-to-end manner. In this work, we conduct a\ndetailed analysis to identify major bottlenecks that impact end-to-end latency\nin LLM serving systems. Our analysis reveals that a comprehensive LLM serving\nendpoint must address a series of efficiency bottlenecks that extend beyond LLM\ninference. We then propose ScaleLLM, an optimized system for resource-efficient\nLLM serving. Our extensive experiments reveal that with 64 concurrent requests,\nScaleLLM achieves a 4.3x speed up over vLLM and outperforms state-of-the-arts\nwith 1.5x higher throughput.",
            "text": "Large language models (LLMs) have surged in popularity and are extensively\nused in commercial applications, where the efficiency of model serving is\ncrucial for the user experience. Most current research focuses on optimizing\nindividual sub-procedures, e.g. local inference and communication, however,\nthere is no comprehensive framework that provides a holistic system view for\noptimizing LLM serving in an end-to-end manner. In this work, we conduct a\ndetailed analysis to identify major bottlenecks that impact end-to-end latency\nin LLM serving systems. Our analysis reveals that a comprehensive LLM serving\nendpoint must address a series of efficiency bottlenecks that extend beyond LLM\ninference. We then propose ScaleLLM, an optimized system for resource-efficient\nLLM serving. Our extensive experiments reveal that with 64 concurrent requests,\nScaleLLM achieves a 4.3x speed up over vLLM and outperforms state-of-the-arts\nwith 1.5x higher throughput.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2504.20101v2",
            "url": "http://arxiv.org/abs/2504.20101v2",
            "title": "GenTorrent: Scaling Large Language Model Serving with An Overley Network",
            "year": "2025",
            "publicationDate": "2025-04-27 01:08:25+00:00",
            "abstract": "While significant progress has been made in research and development on\nopen-source and cost-efficient large-language models (LLMs), serving\nscalability remains a critical challenge, particularly for small organizations\nand individuals seeking to deploy and test their LLM innovations. Inspired by\npeer-to-peer networks that leverage decentralized overlay nodes to increase\nthroughput and availability, we propose GenTorrent, an LLM serving overlay that\nharnesses computing resources from decentralized contributors. We identify four\nkey research problems inherent to enabling such a decentralized infrastructure:\n1) overlay network organization; 2) LLM communication privacy; 3) overlay\nforwarding for resource efficiency; and 4) verification of serving quality.\nThis work presents the first systematic study of these fundamental problems in\nthe context of decentralized LLM serving. Evaluation results from a prototype\nimplemented on a set of decentralized nodes demonstrate that GenTorrent\nachieves a latency reduction of over 50% compared to the baseline design\nwithout overlay forwarding. Furthermore, the security features introduce\nminimal overhead to serving latency and throughput. We believe this work\npioneers a new direction for democratizing and scaling future AI serving\ncapabilities.",
            "text": "While significant progress has been made in research and development on\nopen-source and cost-efficient large-language models (LLMs), serving\nscalability remains a critical challenge, particularly for small organizations\nand individuals seeking to deploy and test their LLM innovations. Inspired by\npeer-to-peer networks that leverage decentralized overlay nodes to increase\nthroughput and availability, we propose GenTorrent, an LLM serving overlay that\nharnesses computing resources from decentralized contributors. We identify four\nkey research problems inherent to enabling such a decentralized infrastructure:\n1) overlay network organization; 2) LLM communication privacy; 3) overlay\nforwarding for resource efficiency; and 4) verification of serving quality.\nThis work presents the first systematic study of these fundamental problems in\nthe context of decentralized LLM serving. Evaluation results from a prototype\nimplemented on a set of decentralized nodes demonstrate that GenTorrent\nachieves a latency reduction of over 50% compared to the baseline design\nwithout overlay forwarding. Furthermore, the security features introduce\nminimal overhead to serving latency and throughput. We believe this work\npioneers a new direction for democratizing and scaling future AI serving\ncapabilities.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2309.16739v4",
            "url": "http://arxiv.org/abs/2309.16739v4",
            "title": "Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities",
            "year": "2023",
            "publicationDate": "2023-09-28 06:22:59+00:00",
            "abstract": "Large language models (LLMs), which have shown remarkable capabilities, are\nrevolutionizing AI development and potentially shaping our future. However,\ngiven their multimodality, the status quo cloud-based deployment faces some\ncritical challenges: 1) long response time; 2) high bandwidth costs; and 3) the\nviolation of data privacy. 6G mobile edge computing (MEC) systems may resolve\nthese pressing issues. In this article, we explore the potential of deploying\nLLMs at the 6G edge. We start by introducing killer applications powered by\nmultimodal LLMs, including robotics and healthcare, to highlight the need for\ndeploying LLMs in the vicinity of end users. Then, we identify the critical\nchallenges for LLM deployment at the edge and envision the 6G MEC architecture\nfor LLMs. Furthermore, we delve into two design aspects, i.e., edge training\nand edge inference for LLMs. In both aspects, considering the inherent resource\nlimitations at the edge, we discuss various cutting-edge techniques, including\nsplit learning/inference, parameter-efficient fine-tuning, quantization, and\nparameter-sharing inference, to facilitate the efficient deployment of LLMs.\nThis article serves as a position paper for thoroughly identifying the\nmotivation, challenges, and pathway for empowering LLMs at the 6G edge.",
            "text": "Large language models (LLMs), which have shown remarkable capabilities, are\nrevolutionizing AI development and potentially shaping our future. However,\ngiven their multimodality, the status quo cloud-based deployment faces some\ncritical challenges: 1) long response time; 2) high bandwidth costs; and 3) the\nviolation of data privacy. 6G mobile edge computing (MEC) systems may resolve\nthese pressing issues. In this article, we explore the potential of deploying\nLLMs at the 6G edge. We start by introducing killer applications powered by\nmultimodal LLMs, including robotics and healthcare, to highlight the need for\ndeploying LLMs in the vicinity of end users. Then, we identify the critical\nchallenges for LLM deployment at the edge and envision the 6G MEC architecture\nfor LLMs. Furthermore, we delve into two design aspects, i.e., edge training\nand edge inference for LLMs. In both aspects, considering the inherent resource\nlimitations at the edge, we discuss various cutting-edge techniques, including\nsplit learning/inference, parameter-efficient fine-tuning, quantization, and\nparameter-sharing inference, to facilitate the efficient deployment of LLMs.\nThis article serves as a position paper for thoroughly identifying the\nmotivation, challenges, and pathway for empowering LLMs at the 6G edge.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2502.13176v2",
            "url": "http://arxiv.org/abs/2502.13176v2",
            "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
            "year": "2025",
            "publicationDate": "2025-02-18 04:08:29+00:00",
            "abstract": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
            "text": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2401.07886v2",
            "url": "http://arxiv.org/abs/2401.07886v2",
            "title": "Learned Best-Effort LLM Serving",
            "year": "2024",
            "publicationDate": "2024-01-15 18:28:17+00:00",
            "abstract": "Many applications must provide low-latency LLM service to users or risk\nunacceptable user experience. However, over-provisioning resources to serve\nfluctuating request patterns is often prohibitively expensive. In this work, we\npresent a best-effort serving system that employs deep reinforcement learning\nto adjust service quality based on the task distribution and system load. Our\nbest-effort system can maintain availability with over 10x higher client\nrequest rates, serves above 96% of peak performance 4.1x more often, and serves\nabove 98% of peak performance 2.3x more often than static serving on\nunpredictable workloads. Our learned router is robust to shifts in both the\narrival and task distribution. Compared to static serving, learned best-effort\nserving allows for cost-efficient serving through increased hardware utility.\nAdditionally, we argue that learned best-effort LLM serving is applicable in\nwide variety of settings and provides application developers great flexibility\nto meet their specific needs.",
            "text": "Many applications must provide low-latency LLM service to users or risk\nunacceptable user experience. However, over-provisioning resources to serve\nfluctuating request patterns is often prohibitively expensive. In this work, we\npresent a best-effort serving system that employs deep reinforcement learning\nto adjust service quality based on the task distribution and system load. Our\nbest-effort system can maintain availability with over 10x higher client\nrequest rates, serves above 96% of peak performance 4.1x more often, and serves\nabove 98% of peak performance 2.3x more often than static serving on\nunpredictable workloads. Our learned router is robust to shifts in both the\narrival and task distribution. Compared to static serving, learned best-effort\nserving allows for cost-efficient serving through increased hardware utility.\nAdditionally, we argue that learned best-effort LLM serving is applicable in\nwide variety of settings and provides application developers great flexibility\nto meet their specific needs.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2304.10268v5",
            "url": "http://arxiv.org/abs/2304.10268v5",
            "title": "BackCache: Mitigating Contention-Based Cache Timing Attacks by Hiding Cache Line Evictions",
            "year": "2023",
            "publicationDate": "2023-04-20 12:47:11+00:00",
            "abstract": "Caches are used to reduce the speed differential between the CPU and memory\nto improve the performance of modern processors. However, attackers can use\ncontention-based cache timing attacks to steal sensitive information from\nvictim processes through carefully designed cache eviction sets. And L1 data\ncache attacks are widely exploited and pose a significant privacy and\nconfidentiality threat. Existing hardware-based countermeasures mainly focus on\ncache partitioning, randomization, and cache line flushing, which unfortunately\neither incur high overhead or can be circumvented by sophisticated attacks. In\nthis paper, we propose a novel hardware-software co-design called BackCache\nwith the idea of always achieving cache hits instead of cache misses to\nmitigate contention-based cache timing attacks on the L1 data cache. BackCache\nplaces the evicted cache lines from the L1 data cache into a fully-associative\nbackup cache to hide the evictions. To improve the security of BackCache, we\nintroduce a randomly used replacement policy (RURP) and a dynamic backup cache\nresizing mechanism. We also present a theoretical security analysis to\ndemonstrate the effectiveness of BackCache. Our evaluation on the gem5\nsimulator shows that BackCache can degrade the performance by 2.61%, 2.66%, and\n3.36% For OS kernel, single-thread, and multi-thread benchmarks.",
            "text": "Caches are used to reduce the speed differential between the CPU and memory\nto improve the performance of modern processors. However, attackers can use\ncontention-based cache timing attacks to steal sensitive information from\nvictim processes through carefully designed cache eviction sets. And L1 data\ncache attacks are widely exploited and pose a significant privacy and\nconfidentiality threat. Existing hardware-based countermeasures mainly focus on\ncache partitioning, randomization, and cache line flushing, which unfortunately\neither incur high overhead or can be circumvented by sophisticated attacks. In\nthis paper, we propose a novel hardware-software co-design called BackCache\nwith the idea of always achieving cache hits instead of cache misses to\nmitigate contention-based cache timing attacks on the L1 data cache. BackCache\nplaces the evicted cache lines from the L1 data cache into a fully-associative\nbackup cache to hide the evictions. To improve the security of BackCache, we\nintroduce a randomly used replacement policy (RURP) and a dynamic backup cache\nresizing mechanism. We also present a theoretical security analysis to\ndemonstrate the effectiveness of BackCache. Our evaluation on the gem5\nsimulator shows that BackCache can degrade the performance by 2.61%, 2.66%, and\n3.36% For OS kernel, single-thread, and multi-thread benchmarks.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2410.14257v1",
            "url": "http://arxiv.org/abs/2410.14257v1",
            "title": "Revisiting SLO and Goodput Metrics in LLM Serving",
            "year": "2024",
            "publicationDate": "2024-10-18 08:05:37+00:00",
            "abstract": "Large language models (LLMs) have achieved remarkable performance and are\nwidely deployed in various applications, while the serving of LLM inference has\nraised concerns about user experience and serving throughput. Accordingly,\nservice level objectives (SLOs) and goodput-the number of requests that meet\nSLOs per second-are introduced to evaluate the performance of LLM serving.\nHowever, existing metrics fail to capture the nature of user experience. We\nobserve two ridiculous phenomena in existing metrics: 1) delaying token\ndelivery can smooth the tail time between tokens (tail TBT) of a request and 2)\ndropping the request that fails to meet the SLOs midway can improve goodput.\n  In this paper, we revisit SLO and goodput metrics in LLM serving and propose\na unified metric framework smooth goodput including SLOs and goodput to reflect\nthe nature of user experience in LLM serving. The framework can adapt to\nspecific goals of different tasks by setting parameters. We re-evaluate the\nperformance of different LLM serving systems under multiple workloads based on\nthis unified framework and provide possible directions for future optimization\nof existing strategies. We hope that this framework can provide a unified\nstandard for evaluating LLM serving and foster researches in the field of LLM\nserving optimization to move in a cohesive direction.",
            "text": "Large language models (LLMs) have achieved remarkable performance and are\nwidely deployed in various applications, while the serving of LLM inference has\nraised concerns about user experience and serving throughput. Accordingly,\nservice level objectives (SLOs) and goodput-the number of requests that meet\nSLOs per second-are introduced to evaluate the performance of LLM serving.\nHowever, existing metrics fail to capture the nature of user experience. We\nobserve two ridiculous phenomena in existing metrics: 1) delaying token\ndelivery can smooth the tail time between tokens (tail TBT) of a request and 2)\ndropping the request that fails to meet the SLOs midway can improve goodput.\n  In this paper, we revisit SLO and goodput metrics in LLM serving and propose\na unified metric framework smooth goodput including SLOs and goodput to reflect\nthe nature of user experience in LLM serving. The framework can adapt to\nspecific goals of different tasks by setting parameters. We re-evaluate the\nperformance of different LLM serving systems under multiple workloads based on\nthis unified framework and provide possible directions for future optimization\nof existing strategies. We hope that this framework can provide a unified\nstandard for evaluating LLM serving and foster researches in the field of LLM\nserving optimization to move in a cohesive direction.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2404.08509v2",
            "url": "http://arxiv.org/abs/2404.08509v2",
            "title": "Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction",
            "year": "2024",
            "publicationDate": "2024-04-12 14:46:15+00:00",
            "abstract": "Large language models (LLMs) have been driving a new wave of interactive AI\napplications across numerous domains. However, efficiently serving LLM\ninference requests is challenging due to their unpredictable execution times\noriginating from the autoregressive nature of generative models. Existing LLM\nserving systems exploit first-come-first-serve (FCFS) scheduling, suffering\nfrom head-of-line blocking issues. To address the non-deterministic nature of\nLLMs and enable efficient interactive LLM serving, we present a speculative\nshortest-job-first (SSJF) scheduler that uses a light proxy model to predict\nLLM output sequence lengths. Our open-source SSJF implementation does not\nrequire changes to memory management or batching strategies. Evaluations on\nreal-world datasets and production workload traces show that SSJF reduces\naverage job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x\ncompared to FCFS schedulers, across no batching, dynamic batching, and\ncontinuous batching settings.",
            "text": "Large language models (LLMs) have been driving a new wave of interactive AI\napplications across numerous domains. However, efficiently serving LLM\ninference requests is challenging due to their unpredictable execution times\noriginating from the autoregressive nature of generative models. Existing LLM\nserving systems exploit first-come-first-serve (FCFS) scheduling, suffering\nfrom head-of-line blocking issues. To address the non-deterministic nature of\nLLMs and enable efficient interactive LLM serving, we present a speculative\nshortest-job-first (SSJF) scheduler that uses a light proxy model to predict\nLLM output sequence lengths. Our open-source SSJF implementation does not\nrequire changes to memory management or batching strategies. Evaluations on\nreal-world datasets and production workload traces show that SSJF reduces\naverage job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x\ncompared to FCFS schedulers, across no batching, dynamic batching, and\ncontinuous batching settings.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2407.09486v1",
            "url": "http://arxiv.org/abs/2407.09486v1",
            "title": "ENOVA: Autoscaling towards Cost-effective and Stable Serverless LLM Serving",
            "year": "2024",
            "publicationDate": "2024-05-17 09:48:31+00:00",
            "abstract": "Since the increasing popularity of large language model (LLM) backend\nsystems, it is common and necessary to deploy stable serverless serving of LLM\non multi-GPU clusters with autoscaling. However, there exist challenges because\nthe diversity and co-location of applications in multi-GPU clusters will lead\nto low service quality and GPU utilization. To address them, we build ENOVA, a\ndeployment, monitoring and autoscaling service towards serverless LLM serving.\nENOVA deconstructs the execution process of LLM service comprehensively, based\non which ENOVA designs a configuration recommendation module for automatic\ndeployment on any GPU clusters and a performance detection module for\nautoscaling. On top of them, ENOVA implements a deployment execution engine for\nmulti-GPU cluster scheduling. The experiment results show that ENOVA\nsignificantly outperforms other state-of-the-art methods and is suitable for\nwide deployment in large online systems.",
            "text": "Since the increasing popularity of large language model (LLM) backend\nsystems, it is common and necessary to deploy stable serverless serving of LLM\non multi-GPU clusters with autoscaling. However, there exist challenges because\nthe diversity and co-location of applications in multi-GPU clusters will lead\nto low service quality and GPU utilization. To address them, we build ENOVA, a\ndeployment, monitoring and autoscaling service towards serverless LLM serving.\nENOVA deconstructs the execution process of LLM service comprehensively, based\non which ENOVA designs a configuration recommendation module for automatic\ndeployment on any GPU clusters and a performance detection module for\nautoscaling. On top of them, ENOVA implements a deployment execution engine for\nmulti-GPU cluster scheduling. The experiment results show that ENOVA\nsignificantly outperforms other state-of-the-art methods and is suitable for\nwide deployment in large online systems.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2505.14468v1",
            "url": "http://arxiv.org/abs/2505.14468v1",
            "title": "ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs",
            "year": "2025",
            "publicationDate": "2025-05-20 15:04:17+00:00",
            "abstract": "Serverless computing has grown rapidly for serving Large Language Model (LLM)\ninference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid\nscaling. However, our analysis reveals that current serverless can effectively\nserve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to\nthree key limitations: 1) massive parameter redundancy among functions where\n99% of weights are unnecessarily duplicated, 2) costly artifact loading latency\nbeyond LLM loading, and 3) magnified resource contention when serving multiple\nLoRA LLMs. These inefficiencies lead to massive GPU wastage, increased\nTime-To-First-Token (TTFT), and high monetary costs.\n  We propose ServerlessLoRA, a novel serverless inference system designed for\nfaster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM\nsharing across isolated LoRA functions to reduce redundancy. We design a\npre-loading method that pre-loads comprehensive LoRA artifacts to minimize\ncold-start latency. Furthermore, ServerlessLoRA employs contention aware\nbatching and offloading to mitigate GPU resource conflicts during bursty\nworkloads. Experiment on industrial workloads demonstrates that ServerlessLoRA\nreduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to\nstate-of-the-art LLM inference solutions.",
            "text": "Serverless computing has grown rapidly for serving Large Language Model (LLM)\ninference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid\nscaling. However, our analysis reveals that current serverless can effectively\nserve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to\nthree key limitations: 1) massive parameter redundancy among functions where\n99% of weights are unnecessarily duplicated, 2) costly artifact loading latency\nbeyond LLM loading, and 3) magnified resource contention when serving multiple\nLoRA LLMs. These inefficiencies lead to massive GPU wastage, increased\nTime-To-First-Token (TTFT), and high monetary costs.\n  We propose ServerlessLoRA, a novel serverless inference system designed for\nfaster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM\nsharing across isolated LoRA functions to reduce redundancy. We design a\npre-loading method that pre-loads comprehensive LoRA artifacts to minimize\ncold-start latency. Furthermore, ServerlessLoRA employs contention aware\nbatching and offloading to mitigate GPU resource conflicts during bursty\nworkloads. Experiment on industrial workloads demonstrates that ServerlessLoRA\nreduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to\nstate-of-the-art LLM inference solutions.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2407.12391v1",
            "url": "http://arxiv.org/abs/2407.12391v1",
            "title": "LLM Inference Serving: Survey of Recent Advances and Opportunities",
            "year": "2024",
            "publicationDate": "2024-07-17 08:11:47+00:00",
            "abstract": "This survey offers a comprehensive overview of recent advancements in Large\nLanguage Model (LLM) serving systems, focusing on research since the year 2023.\nWe specifically examine system-level enhancements that improve performance and\nefficiency without altering the core LLM decoding mechanisms. By selecting and\nreviewing high-quality papers from prestigious ML and system venues, we\nhighlight key innovations and practical considerations for deploying and\nscaling LLMs in real-world production environments. This survey serves as a\nvaluable resource for LLM practitioners seeking to stay abreast of the latest\ndevelopments in this rapidly evolving field.",
            "text": "This survey offers a comprehensive overview of recent advancements in Large\nLanguage Model (LLM) serving systems, focusing on research since the year 2023.\nWe specifically examine system-level enhancements that improve performance and\nefficiency without altering the core LLM decoding mechanisms. By selecting and\nreviewing high-quality papers from prestigious ML and system venues, we\nhighlight key innovations and practical considerations for deploying and\nscaling LLMs in real-world production environments. This survey serves as a\nvaluable resource for LLM practitioners seeking to stay abreast of the latest\ndevelopments in this rapidly evolving field.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1801.00390v1",
            "url": "http://arxiv.org/abs/1801.00390v1",
            "title": "Time Aware Least Recent Used (TLRU) Cache Management Policy in ICN",
            "year": "2018",
            "publicationDate": "2018-01-01 04:13:53+00:00",
            "abstract": "The information centric networks (ICN) can be viewed as a network of caches.\nConversely, ICN type of cache networks has distinctive features e.g, contents\npopularity, usability time of content and other factors inflicts some diverse\nrequirements for cache eviction policies. In this paper we defined four\nimportant characteristics of a suitable eviction policy for ICN. We analysed\nwell known eviction policies in view of defined characteristics. Based upon\nanalysis we propose a new eviction scheme which is well suitable for ICN type\nof cache networks.",
            "text": "The information centric networks (ICN) can be viewed as a network of caches.\nConversely, ICN type of cache networks has distinctive features e.g, contents\npopularity, usability time of content and other factors inflicts some diverse\nrequirements for cache eviction policies. In this paper we defined four\nimportant characteristics of a suitable eviction policy for ICN. We analysed\nwell known eviction policies in view of defined characteristics. Based upon\nanalysis we propose a new eviction scheme which is well suitable for ICN type\nof cache networks.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1901.01187v1",
            "url": "http://arxiv.org/abs/1901.01187v1",
            "title": "PopNetCod: A Popularity-based Caching Policy for Network Coding enabled Named Data Networking",
            "year": "2019",
            "publicationDate": "2019-01-04 16:02:41+00:00",
            "abstract": "In this paper, we propose PopNetCod, a popularity-based caching policy for\nnetwork coding enabled Named Data Networking. PopNetCod is a distributed\ncaching policy, in which each router measures the local popularity of the\ncontent objects by analyzing the requests that it receives. It then uses this\ninformation to decide which Data packets to cache or evict from its content\nstore. Since network coding is used, partial caching of content objects is\nsupported, which facilitates the management of the content store. The routers\ndecide the Data packets that they cache or evict in an online manner when they\nreceive requests for Data packets. This allows the most popular Data packets to\nbe cached closer to the network edges. The evaluation of PopNetCod shows an\nimproved cache-hit rate compared to the widely used Leave Copy Everywhere\nplacement policy and the Least Recently Used eviction policy. The improved\ncache-hit rate helps the clients to achieve higher goodput, while it also\nreduces the load on the source servers.",
            "text": "In this paper, we propose PopNetCod, a popularity-based caching policy for\nnetwork coding enabled Named Data Networking. PopNetCod is a distributed\ncaching policy, in which each router measures the local popularity of the\ncontent objects by analyzing the requests that it receives. It then uses this\ninformation to decide which Data packets to cache or evict from its content\nstore. Since network coding is used, partial caching of content objects is\nsupported, which facilitates the management of the content store. The routers\ndecide the Data packets that they cache or evict in an online manner when they\nreceive requests for Data packets. This allows the most popular Data packets to\nbe cached closer to the network edges. The evaluation of PopNetCod shows an\nimproved cache-hit rate compared to the widely used Leave Copy Everywhere\nplacement policy and the Least Recently Used eviction policy. The improved\ncache-hit rate helps the clients to achieve higher goodput, while it also\nreduces the load on the source servers.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2505.09999v2",
            "url": "http://arxiv.org/abs/2505.09999v2",
            "title": "ServeGen: Workload Characterization and Generation of Large Language Model Serving in Production",
            "year": "2025",
            "publicationDate": "2025-05-15 06:24:08+00:00",
            "abstract": "With the widespread adoption of Large Language Models (LLMs), serving LLM\ninference requests has become an increasingly important task, attracting active\nresearch advancements. Practical workloads play an essential role in this\nprocess: they are critical for motivating and benchmarking serving techniques\nand systems. However, the existing understanding of real-world LLM serving\nworkloads is limited due to the lack of a comprehensive workload\ncharacterization. Prior analyses remain insufficient in scale and scope, thus\nfailing to fully capture intricate workload characteristics.\n  In this paper, we fill the gap with an in-depth characterization of LLM\nserving workloads collected from our worldwide cloud inference serving service,\ncovering not only language models but also emerging multimodal and reasoning\nmodels, and unveiling important new findings in each case. Moreover, based on\nour findings, we propose ServeGen, a principled framework for generating\nrealistic LLM serving workloads by composing them on a per-client basis. A\npractical use case in production validates that ServeGen avoids 50%\nunder-provisioning compared to naive workload generation, demonstrating\nServeGen's advantage in performance benchmarking. ServeGen is available at\nhttps://github.com/alibaba/ServeGen.",
            "text": "With the widespread adoption of Large Language Models (LLMs), serving LLM\ninference requests has become an increasingly important task, attracting active\nresearch advancements. Practical workloads play an essential role in this\nprocess: they are critical for motivating and benchmarking serving techniques\nand systems. However, the existing understanding of real-world LLM serving\nworkloads is limited due to the lack of a comprehensive workload\ncharacterization. Prior analyses remain insufficient in scale and scope, thus\nfailing to fully capture intricate workload characteristics.\n  In this paper, we fill the gap with an in-depth characterization of LLM\nserving workloads collected from our worldwide cloud inference serving service,\ncovering not only language models but also emerging multimodal and reasoning\nmodels, and unveiling important new findings in each case. Moreover, based on\nour findings, we propose ServeGen, a principled framework for generating\nrealistic LLM serving workloads by composing them on a per-client basis. A\npractical use case in production validates that ServeGen avoids 50%\nunder-provisioning compared to naive workload generation, demonstrating\nServeGen's advantage in performance benchmarking. ServeGen is available at\nhttps://github.com/alibaba/ServeGen.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1811.01670v2",
            "url": "http://arxiv.org/abs/1811.01670v2",
            "title": "Fast and exact analysis for LRU caches",
            "year": "2018",
            "publicationDate": "2018-11-05 13:34:21+00:00",
            "abstract": "For applications in worst-case execution time analysis and in security, it is\ndesirable to statically classify memory accesses into those that result in\ncache hits, and those that result in cache misses. Among cache replacement\npolicies, the least recently used (LRU) policy has been studied the most and is\nconsidered to be the most predictable. The state-of-the-art in LRU cache\nanalysis presents a tradeoff between precision and analysis efficiency: The\nclassical approach to analyzing programs running on LRU caches, an abstract\ninterpretation based on a range abstraction, is very fast but can be imprecise.\nAn exact analysis was recently presented, but, as a last resort, it calls a\nmodel checker, which is expensive. In this paper, we develop an analysis based\non abstract interpretation that comes close to the efficiency of the classical\napproach, while achieving exact classification of all memory accesses as the\nmodel-checking approach. Compared with the model-checking approach we observe\nspeedups of several orders of magnitude. As a secondary contribution we show\nthat LRU cache analysis problems are in general NP-complete.",
            "text": "For applications in worst-case execution time analysis and in security, it is\ndesirable to statically classify memory accesses into those that result in\ncache hits, and those that result in cache misses. Among cache replacement\npolicies, the least recently used (LRU) policy has been studied the most and is\nconsidered to be the most predictable. The state-of-the-art in LRU cache\nanalysis presents a tradeoff between precision and analysis efficiency: The\nclassical approach to analyzing programs running on LRU caches, an abstract\ninterpretation based on a range abstraction, is very fast but can be imprecise.\nAn exact analysis was recently presented, but, as a last resort, it calls a\nmodel checker, which is expensive. In this paper, we develop an analysis based\non abstract interpretation that comes close to the efficiency of the classical\napproach, while achieving exact classification of all memory accesses as the\nmodel-checking approach. Compared with the model-checking approach we observe\nspeedups of several orders of magnitude. As a secondary contribution we show\nthat LRU cache analysis problems are in general NP-complete.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1512.00727v2",
            "url": "http://arxiv.org/abs/1512.00727v2",
            "title": "TinyLFU: A Highly Efficient Cache Admission Policy",
            "year": "2015",
            "publicationDate": "2015-12-02 15:05:46+00:00",
            "abstract": "This paper proposes to use a frequency based cache admission policy in order\nto boost the effectiveness of caches subject to skewed access distributions.\nGiven a newly accessed item and an eviction candidate from the cache, our\nscheme decides, based on the recent access history, whether it is worth\nadmitting the new item into the cache at the expense of the eviction candidate.\n  Realizing this concept is enabled through a novel approximate LFU structure\ncalled TinyLFU, which maintains an approximate representation of the access\nfrequency of a large sample of recently accessed items. TinyLFU is very compact\nand light-weight as it builds upon Bloom filter theory.\n  We study the properties of TinyLFU through simulations of both synthetic\nworkloads as well as multiple real traces from several sources. These\nsimulations demonstrate the performance boost obtained by enhancing various\nreplacement policies with the TinyLFU eviction policy. Also, a new combined\nreplacement and eviction policy scheme nicknamed W-TinyLFU is presented.\nW-TinyLFU is demonstrated to obtain equal or better hit-ratios than other state\nof the art replacement policies on these traces. It is the only scheme to\nobtain such good results on all traces.",
            "text": "This paper proposes to use a frequency based cache admission policy in order\nto boost the effectiveness of caches subject to skewed access distributions.\nGiven a newly accessed item and an eviction candidate from the cache, our\nscheme decides, based on the recent access history, whether it is worth\nadmitting the new item into the cache at the expense of the eviction candidate.\n  Realizing this concept is enabled through a novel approximate LFU structure\ncalled TinyLFU, which maintains an approximate representation of the access\nfrequency of a large sample of recently accessed items. TinyLFU is very compact\nand light-weight as it builds upon Bloom filter theory.\n  We study the properties of TinyLFU through simulations of both synthetic\nworkloads as well as multiple real traces from several sources. These\nsimulations demonstrate the performance boost obtained by enhancing various\nreplacement policies with the TinyLFU eviction policy. Also, a new combined\nreplacement and eviction policy scheme nicknamed W-TinyLFU is presented.\nW-TinyLFU is demonstrated to obtain equal or better hit-ratios than other state\nof the art replacement policies on these traces. It is the only scheme to\nobtain such good results on all traces.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1810.06930v1",
            "url": "http://arxiv.org/abs/1810.06930v1",
            "title": "Feedforward Neural Networks for Caching: Enough or Too Much?",
            "year": "2018",
            "publicationDate": "2018-10-16 11:26:18+00:00",
            "abstract": "We propose a caching policy that uses a feedforward neural network (FNN) to\npredict content popularity. Our scheme outperforms popular eviction policies\nlike LRU or ARC, but also a new policy relying on the more complex recurrent\nneural networks. At the same time, replacing the FNN predictor with a naive\nlinear estimator does not degrade caching performance significantly,\nquestioning then the role of neural networks for these applications.",
            "text": "We propose a caching policy that uses a feedforward neural network (FNN) to\npredict content popularity. Our scheme outperforms popular eviction policies\nlike LRU or ARC, but also a new policy relying on the more complex recurrent\nneural networks. At the same time, replacing the FNN predictor with a naive\nlinear estimator does not degrade caching performance significantly,\nquestioning then the role of neural networks for these applications.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2408.12757v2",
            "url": "http://arxiv.org/abs/2408.12757v2",
            "title": "NanoFlow: Towards Optimal Large Language Model Serving Throughput",
            "year": "2024",
            "publicationDate": "2024-08-22 23:00:40+00:00",
            "abstract": "Large Language Models (LLMs) have resulted in a surging demand for\nplanet-scale serving systems, where tens of thousands of GPUs continuously\nserve hundreds of millions of users. Consequently, throughput has emerged as a\nkey metric that determines serving systems' performance. Due to large model\nsizes and memory-intensive self-attention, LLM serving has been commonly\nassumed to be memory-bound. Through a detailed analysis, we show that despite\nhaving memory-intensive components, end-to-end LLM serving is compute bound for\nmost common workloads and LLMs. Alas, most existing serving engines fall short\nfrom optimal compute utilization, because the heterogeneous operations that\ncomprise LLM serving--compute, memory, networking--are executed sequentially\nwithin a device.\n  We propose NanoFlow, a novel serving framework that exploits intra-device\nparallelism, which overlaps the usage of heterogeneous resources within a\nsingle device. NanoFlow splits inputs into smaller nano-batches and duplicates\noperations to operate on each portion independently, enabling overlapping.\nNanoFlow automatically identifies the number, size, ordering, and GPU resource\nallocation of nano-batches to minimize the execution time, while considering\nthe interference of concurrent operations. We evaluate NanoFlow's end-to-end\nserving throughput on several popular models such as LLaMA-2-70B, Mixtral 8x7B,\nLLaMA-3-8B, etc. With practical workloads, NanoFlow provides 1.91x throughput\nboost compared to state-of-the-art serving systems achieving 50% to 72% of\noptimal throughput across popular models.",
            "text": "Large Language Models (LLMs) have resulted in a surging demand for\nplanet-scale serving systems, where tens of thousands of GPUs continuously\nserve hundreds of millions of users. Consequently, throughput has emerged as a\nkey metric that determines serving systems' performance. Due to large model\nsizes and memory-intensive self-attention, LLM serving has been commonly\nassumed to be memory-bound. Through a detailed analysis, we show that despite\nhaving memory-intensive components, end-to-end LLM serving is compute bound for\nmost common workloads and LLMs. Alas, most existing serving engines fall short\nfrom optimal compute utilization, because the heterogeneous operations that\ncomprise LLM serving--compute, memory, networking--are executed sequentially\nwithin a device.\n  We propose NanoFlow, a novel serving framework that exploits intra-device\nparallelism, which overlaps the usage of heterogeneous resources within a\nsingle device. NanoFlow splits inputs into smaller nano-batches and duplicates\noperations to operate on each portion independently, enabling overlapping.\nNanoFlow automatically identifies the number, size, ordering, and GPU resource\nallocation of nano-batches to minimize the execution time, while considering\nthe interference of concurrent operations. We evaluate NanoFlow's end-to-end\nserving throughput on several popular models such as LLaMA-2-70B, Mixtral 8x7B,\nLLaMA-3-8B, etc. With practical workloads, NanoFlow provides 1.91x throughput\nboost compared to state-of-the-art serving systems achieving 50% to 72% of\noptimal throughput across popular models.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2403.01136v1",
            "url": "http://arxiv.org/abs/2403.01136v1",
            "title": "LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization",
            "year": "2024",
            "publicationDate": "2024-03-02 08:40:07+00:00",
            "abstract": "Recent breakthroughs in Large-scale language models (LLMs) have demonstrated\nimpressive performance on various tasks. The immense sizes of LLMs have led to\nvery high resource demand and cost for running the models. Though the models\nare largely served using uniform high-caliber GPUs nowadays, utilizing a\nheterogeneous cluster with a mix of available high- and low-capacity GPUs can\npotentially substantially reduce the serving cost. There is a lack of designs\nto support efficient LLM serving using a heterogeneous cluster, while the\ncurrent solutions focus on model partition and uniform compression among\nhomogeneous devices. This paper proposes LLM-PQ, a system that advocates\nadaptive model quantization and phase-aware partition to improve LLM serving\nefficiency on heterogeneous GPU clusters. We carefully decide on\nmixed-precision model quantization together with phase-aware model partition\nand micro-batch sizing in distributed LLM serving with an efficient algorithm,\nto greatly enhance inference throughput while fulfilling user-specified model\nquality targets. Extensive experiments on production inference workloads in 11\ndifferent clusters demonstrate that LLM-PQ achieves up to 2.88x (2.26x on\naverage) throughput improvement in inference, showing great advantages over\nstate-of-the-art works.",
            "text": "Recent breakthroughs in Large-scale language models (LLMs) have demonstrated\nimpressive performance on various tasks. The immense sizes of LLMs have led to\nvery high resource demand and cost for running the models. Though the models\nare largely served using uniform high-caliber GPUs nowadays, utilizing a\nheterogeneous cluster with a mix of available high- and low-capacity GPUs can\npotentially substantially reduce the serving cost. There is a lack of designs\nto support efficient LLM serving using a heterogeneous cluster, while the\ncurrent solutions focus on model partition and uniform compression among\nhomogeneous devices. This paper proposes LLM-PQ, a system that advocates\nadaptive model quantization and phase-aware partition to improve LLM serving\nefficiency on heterogeneous GPU clusters. We carefully decide on\nmixed-precision model quantization together with phase-aware model partition\nand micro-batch sizing in distributed LLM serving with an efficient algorithm,\nto greatly enhance inference throughput while fulfilling user-specified model\nquality targets. Extensive experiments on production inference workloads in 11\ndifferent clusters demonstrate that LLM-PQ achieves up to 2.88x (2.26x on\naverage) throughput improvement in inference, showing great advantages over\nstate-of-the-art works.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2408.00008v2",
            "url": "http://arxiv.org/abs/2408.00008v2",
            "title": "ScaleLLM: A Resource-Frugal LLM Serving Framework by Optimizing End-to-End Efficiency",
            "year": "2024",
            "publicationDate": "2024-07-23 23:37:29+00:00",
            "abstract": "Large language models (LLMs) have surged in popularity and are extensively\nused in commercial applications, where the efficiency of model serving is\ncrucial for the user experience. Most current research focuses on optimizing\nindividual sub-procedures, e.g. local inference and communication, however,\nthere is no comprehensive framework that provides a holistic system view for\noptimizing LLM serving in an end-to-end manner. In this work, we conduct a\ndetailed analysis to identify major bottlenecks that impact end-to-end latency\nin LLM serving systems. Our analysis reveals that a comprehensive LLM serving\nendpoint must address a series of efficiency bottlenecks that extend beyond LLM\ninference. We then propose ScaleLLM, an optimized system for resource-efficient\nLLM serving. Our extensive experiments reveal that with 64 concurrent requests,\nScaleLLM achieves a 4.3x speed up over vLLM and outperforms state-of-the-arts\nwith 1.5x higher throughput.",
            "text": "Large language models (LLMs) have surged in popularity and are extensively\nused in commercial applications, where the efficiency of model serving is\ncrucial for the user experience. Most current research focuses on optimizing\nindividual sub-procedures, e.g. local inference and communication, however,\nthere is no comprehensive framework that provides a holistic system view for\noptimizing LLM serving in an end-to-end manner. In this work, we conduct a\ndetailed analysis to identify major bottlenecks that impact end-to-end latency\nin LLM serving systems. Our analysis reveals that a comprehensive LLM serving\nendpoint must address a series of efficiency bottlenecks that extend beyond LLM\ninference. We then propose ScaleLLM, an optimized system for resource-efficient\nLLM serving. Our extensive experiments reveal that with 64 concurrent requests,\nScaleLLM achieves a 4.3x speed up over vLLM and outperforms state-of-the-arts\nwith 1.5x higher throughput.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2504.20101v2",
            "url": "http://arxiv.org/abs/2504.20101v2",
            "title": "GenTorrent: Scaling Large Language Model Serving with An Overley Network",
            "year": "2025",
            "publicationDate": "2025-04-27 01:08:25+00:00",
            "abstract": "While significant progress has been made in research and development on\nopen-source and cost-efficient large-language models (LLMs), serving\nscalability remains a critical challenge, particularly for small organizations\nand individuals seeking to deploy and test their LLM innovations. Inspired by\npeer-to-peer networks that leverage decentralized overlay nodes to increase\nthroughput and availability, we propose GenTorrent, an LLM serving overlay that\nharnesses computing resources from decentralized contributors. We identify four\nkey research problems inherent to enabling such a decentralized infrastructure:\n1) overlay network organization; 2) LLM communication privacy; 3) overlay\nforwarding for resource efficiency; and 4) verification of serving quality.\nThis work presents the first systematic study of these fundamental problems in\nthe context of decentralized LLM serving. Evaluation results from a prototype\nimplemented on a set of decentralized nodes demonstrate that GenTorrent\nachieves a latency reduction of over 50% compared to the baseline design\nwithout overlay forwarding. Furthermore, the security features introduce\nminimal overhead to serving latency and throughput. We believe this work\npioneers a new direction for democratizing and scaling future AI serving\ncapabilities.",
            "text": "While significant progress has been made in research and development on\nopen-source and cost-efficient large-language models (LLMs), serving\nscalability remains a critical challenge, particularly for small organizations\nand individuals seeking to deploy and test their LLM innovations. Inspired by\npeer-to-peer networks that leverage decentralized overlay nodes to increase\nthroughput and availability, we propose GenTorrent, an LLM serving overlay that\nharnesses computing resources from decentralized contributors. We identify four\nkey research problems inherent to enabling such a decentralized infrastructure:\n1) overlay network organization; 2) LLM communication privacy; 3) overlay\nforwarding for resource efficiency; and 4) verification of serving quality.\nThis work presents the first systematic study of these fundamental problems in\nthe context of decentralized LLM serving. Evaluation results from a prototype\nimplemented on a set of decentralized nodes demonstrate that GenTorrent\nachieves a latency reduction of over 50% compared to the baseline design\nwithout overlay forwarding. Furthermore, the security features introduce\nminimal overhead to serving latency and throughput. We believe this work\npioneers a new direction for democratizing and scaling future AI serving\ncapabilities.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2502.02750v1",
            "url": "http://arxiv.org/abs/2502.02750v1",
            "title": "Cache is King: Smart Page Eviction with eBPF",
            "year": "2025",
            "publicationDate": "2025-02-04 22:37:17+00:00",
            "abstract": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
            "text": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2308.03388v1",
            "url": "http://arxiv.org/abs/2308.03388v1",
            "title": "Optimal Design of Lines Replaceable Units",
            "year": "2023",
            "publicationDate": "2023-08-07 08:16:11+00:00",
            "abstract": "A Line Replaceable Unit (LRU) is a collection of connected parts in a system\nthat is replaced when any part of the LRU fails. Companies use LRUs as a\nmechanism to reduce downtime of systems following a failure. The design of LRUs\ndetermines how fast a replacement is performed, so a smart design reduces\nreplacement and downtime cost. A firm must purchase/repair a LRU upon failure,\nand large LRUs are more expensive to purchase/repair. Hence, a firm seeks to\ndesign LRUs such that the average costs per time unit are minimized. We\nformalize this problem in a new model that captures how parts in a system are\nconnected, and how they are disassembled from the system. Our model optimizes\nthe design of LRUs such that the replacement (and downtime) costs and LRU\npurchase/repair costs are minimized. We present a set partitioning formulation\nfor which we prove a rare result: the optimal solution is integer, despite a\nnon--integral feasible polyhedron. Secondly, we formulate our problem as a\nbinary linear program. The paper concludes by numerically comparing the\ncomputation times of both formulations and illustrates the effects of various\nparameters on the model's outcome.",
            "text": "A Line Replaceable Unit (LRU) is a collection of connected parts in a system\nthat is replaced when any part of the LRU fails. Companies use LRUs as a\nmechanism to reduce downtime of systems following a failure. The design of LRUs\ndetermines how fast a replacement is performed, so a smart design reduces\nreplacement and downtime cost. A firm must purchase/repair a LRU upon failure,\nand large LRUs are more expensive to purchase/repair. Hence, a firm seeks to\ndesign LRUs such that the average costs per time unit are minimized. We\nformalize this problem in a new model that captures how parts in a system are\nconnected, and how they are disassembled from the system. Our model optimizes\nthe design of LRUs such that the replacement (and downtime) costs and LRU\npurchase/repair costs are minimized. We present a set partitioning formulation\nfor which we prove a rare result: the optimal solution is integer, despite a\nnon--integral feasible polyhedron. Secondly, we formulate our problem as a\nbinary linear program. The paper concludes by numerically comparing the\ncomputation times of both formulations and illustrates the effects of various\nparameters on the model's outcome.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2401.07886v2",
            "url": "http://arxiv.org/abs/2401.07886v2",
            "title": "Learned Best-Effort LLM Serving",
            "year": "2024",
            "publicationDate": "2024-01-15 18:28:17+00:00",
            "abstract": "Many applications must provide low-latency LLM service to users or risk\nunacceptable user experience. However, over-provisioning resources to serve\nfluctuating request patterns is often prohibitively expensive. In this work, we\npresent a best-effort serving system that employs deep reinforcement learning\nto adjust service quality based on the task distribution and system load. Our\nbest-effort system can maintain availability with over 10x higher client\nrequest rates, serves above 96% of peak performance 4.1x more often, and serves\nabove 98% of peak performance 2.3x more often than static serving on\nunpredictable workloads. Our learned router is robust to shifts in both the\narrival and task distribution. Compared to static serving, learned best-effort\nserving allows for cost-efficient serving through increased hardware utility.\nAdditionally, we argue that learned best-effort LLM serving is applicable in\nwide variety of settings and provides application developers great flexibility\nto meet their specific needs.",
            "text": "Many applications must provide low-latency LLM service to users or risk\nunacceptable user experience. However, over-provisioning resources to serve\nfluctuating request patterns is often prohibitively expensive. In this work, we\npresent a best-effort serving system that employs deep reinforcement learning\nto adjust service quality based on the task distribution and system load. Our\nbest-effort system can maintain availability with over 10x higher client\nrequest rates, serves above 96% of peak performance 4.1x more often, and serves\nabove 98% of peak performance 2.3x more often than static serving on\nunpredictable workloads. Our learned router is robust to shifts in both the\narrival and task distribution. Compared to static serving, learned best-effort\nserving allows for cost-efficient serving through increased hardware utility.\nAdditionally, we argue that learned best-effort LLM serving is applicable in\nwide variety of settings and provides application developers great flexibility\nto meet their specific needs.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2304.10268v5",
            "url": "http://arxiv.org/abs/2304.10268v5",
            "title": "BackCache: Mitigating Contention-Based Cache Timing Attacks by Hiding Cache Line Evictions",
            "year": "2023",
            "publicationDate": "2023-04-20 12:47:11+00:00",
            "abstract": "Caches are used to reduce the speed differential between the CPU and memory\nto improve the performance of modern processors. However, attackers can use\ncontention-based cache timing attacks to steal sensitive information from\nvictim processes through carefully designed cache eviction sets. And L1 data\ncache attacks are widely exploited and pose a significant privacy and\nconfidentiality threat. Existing hardware-based countermeasures mainly focus on\ncache partitioning, randomization, and cache line flushing, which unfortunately\neither incur high overhead or can be circumvented by sophisticated attacks. In\nthis paper, we propose a novel hardware-software co-design called BackCache\nwith the idea of always achieving cache hits instead of cache misses to\nmitigate contention-based cache timing attacks on the L1 data cache. BackCache\nplaces the evicted cache lines from the L1 data cache into a fully-associative\nbackup cache to hide the evictions. To improve the security of BackCache, we\nintroduce a randomly used replacement policy (RURP) and a dynamic backup cache\nresizing mechanism. We also present a theoretical security analysis to\ndemonstrate the effectiveness of BackCache. Our evaluation on the gem5\nsimulator shows that BackCache can degrade the performance by 2.61%, 2.66%, and\n3.36% For OS kernel, single-thread, and multi-thread benchmarks.",
            "text": "Caches are used to reduce the speed differential between the CPU and memory\nto improve the performance of modern processors. However, attackers can use\ncontention-based cache timing attacks to steal sensitive information from\nvictim processes through carefully designed cache eviction sets. And L1 data\ncache attacks are widely exploited and pose a significant privacy and\nconfidentiality threat. Existing hardware-based countermeasures mainly focus on\ncache partitioning, randomization, and cache line flushing, which unfortunately\neither incur high overhead or can be circumvented by sophisticated attacks. In\nthis paper, we propose a novel hardware-software co-design called BackCache\nwith the idea of always achieving cache hits instead of cache misses to\nmitigate contention-based cache timing attacks on the L1 data cache. BackCache\nplaces the evicted cache lines from the L1 data cache into a fully-associative\nbackup cache to hide the evictions. To improve the security of BackCache, we\nintroduce a randomly used replacement policy (RURP) and a dynamic backup cache\nresizing mechanism. We also present a theoretical security analysis to\ndemonstrate the effectiveness of BackCache. Our evaluation on the gem5\nsimulator shows that BackCache can degrade the performance by 2.61%, 2.66%, and\n3.36% For OS kernel, single-thread, and multi-thread benchmarks.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2410.14257v1",
            "url": "http://arxiv.org/abs/2410.14257v1",
            "title": "Revisiting SLO and Goodput Metrics in LLM Serving",
            "year": "2024",
            "publicationDate": "2024-10-18 08:05:37+00:00",
            "abstract": "Large language models (LLMs) have achieved remarkable performance and are\nwidely deployed in various applications, while the serving of LLM inference has\nraised concerns about user experience and serving throughput. Accordingly,\nservice level objectives (SLOs) and goodput-the number of requests that meet\nSLOs per second-are introduced to evaluate the performance of LLM serving.\nHowever, existing metrics fail to capture the nature of user experience. We\nobserve two ridiculous phenomena in existing metrics: 1) delaying token\ndelivery can smooth the tail time between tokens (tail TBT) of a request and 2)\ndropping the request that fails to meet the SLOs midway can improve goodput.\n  In this paper, we revisit SLO and goodput metrics in LLM serving and propose\na unified metric framework smooth goodput including SLOs and goodput to reflect\nthe nature of user experience in LLM serving. The framework can adapt to\nspecific goals of different tasks by setting parameters. We re-evaluate the\nperformance of different LLM serving systems under multiple workloads based on\nthis unified framework and provide possible directions for future optimization\nof existing strategies. We hope that this framework can provide a unified\nstandard for evaluating LLM serving and foster researches in the field of LLM\nserving optimization to move in a cohesive direction.",
            "text": "Large language models (LLMs) have achieved remarkable performance and are\nwidely deployed in various applications, while the serving of LLM inference has\nraised concerns about user experience and serving throughput. Accordingly,\nservice level objectives (SLOs) and goodput-the number of requests that meet\nSLOs per second-are introduced to evaluate the performance of LLM serving.\nHowever, existing metrics fail to capture the nature of user experience. We\nobserve two ridiculous phenomena in existing metrics: 1) delaying token\ndelivery can smooth the tail time between tokens (tail TBT) of a request and 2)\ndropping the request that fails to meet the SLOs midway can improve goodput.\n  In this paper, we revisit SLO and goodput metrics in LLM serving and propose\na unified metric framework smooth goodput including SLOs and goodput to reflect\nthe nature of user experience in LLM serving. The framework can adapt to\nspecific goals of different tasks by setting parameters. We re-evaluate the\nperformance of different LLM serving systems under multiple workloads based on\nthis unified framework and provide possible directions for future optimization\nof existing strategies. We hope that this framework can provide a unified\nstandard for evaluating LLM serving and foster researches in the field of LLM\nserving optimization to move in a cohesive direction.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2404.08509v2",
            "url": "http://arxiv.org/abs/2404.08509v2",
            "title": "Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction",
            "year": "2024",
            "publicationDate": "2024-04-12 14:46:15+00:00",
            "abstract": "Large language models (LLMs) have been driving a new wave of interactive AI\napplications across numerous domains. However, efficiently serving LLM\ninference requests is challenging due to their unpredictable execution times\noriginating from the autoregressive nature of generative models. Existing LLM\nserving systems exploit first-come-first-serve (FCFS) scheduling, suffering\nfrom head-of-line blocking issues. To address the non-deterministic nature of\nLLMs and enable efficient interactive LLM serving, we present a speculative\nshortest-job-first (SSJF) scheduler that uses a light proxy model to predict\nLLM output sequence lengths. Our open-source SSJF implementation does not\nrequire changes to memory management or batching strategies. Evaluations on\nreal-world datasets and production workload traces show that SSJF reduces\naverage job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x\ncompared to FCFS schedulers, across no batching, dynamic batching, and\ncontinuous batching settings.",
            "text": "Large language models (LLMs) have been driving a new wave of interactive AI\napplications across numerous domains. However, efficiently serving LLM\ninference requests is challenging due to their unpredictable execution times\noriginating from the autoregressive nature of generative models. Existing LLM\nserving systems exploit first-come-first-serve (FCFS) scheduling, suffering\nfrom head-of-line blocking issues. To address the non-deterministic nature of\nLLMs and enable efficient interactive LLM serving, we present a speculative\nshortest-job-first (SSJF) scheduler that uses a light proxy model to predict\nLLM output sequence lengths. Our open-source SSJF implementation does not\nrequire changes to memory management or batching strategies. Evaluations on\nreal-world datasets and production workload traces show that SSJF reduces\naverage job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x\ncompared to FCFS schedulers, across no batching, dynamic batching, and\ncontinuous batching settings.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1801.02436v2",
            "url": "http://arxiv.org/abs/1801.02436v2",
            "title": "Asymptotic Miss Ratio of LRU Caching with Consistent Hashing",
            "year": "2018",
            "publicationDate": "2018-01-08 14:27:19+00:00",
            "abstract": "To efficiently scale data caching infrastructure to support emerging big data\napplications, many caching systems rely on consistent hashing to group a large\nnumber of servers to form a cooperative cluster. These servers are organized\ntogether according to a random hash function. They jointly provide a unified\nbut distributed hash table to serve swift and voluminous data item requests.\nDifferent from the single least-recently-used (LRU) server that has already\nbeen extensively studied, theoretically characterizing a cluster that consists\nof multiple LRU servers remains yet to be explored. These servers are not\nsimply added together; the random hashing complicates the behavior. To this\nend, we derive the asymptotic miss ratio of data item requests on a LRU cluster\nwith consistent hashing. We show that these individual cache spaces on\ndifferent servers can be effectively viewed as if they could be pooled together\nto form a single virtual LRU cache space parametrized by an appropriate cache\nsize. This equivalence can be established rigorously under the condition that\nthe cache sizes of the individual servers are large. For typical data caching\nsystems this condition is common. Our theoretical framework provides a\nconvenient abstraction that can directly apply the results from the simpler\nsingle LRU cache to the more complex LRU cluster with consistent hashing.",
            "text": "To efficiently scale data caching infrastructure to support emerging big data\napplications, many caching systems rely on consistent hashing to group a large\nnumber of servers to form a cooperative cluster. These servers are organized\ntogether according to a random hash function. They jointly provide a unified\nbut distributed hash table to serve swift and voluminous data item requests.\nDifferent from the single least-recently-used (LRU) server that has already\nbeen extensively studied, theoretically characterizing a cluster that consists\nof multiple LRU servers remains yet to be explored. These servers are not\nsimply added together; the random hashing complicates the behavior. To this\nend, we derive the asymptotic miss ratio of data item requests on a LRU cluster\nwith consistent hashing. We show that these individual cache spaces on\ndifferent servers can be effectively viewed as if they could be pooled together\nto form a single virtual LRU cache space parametrized by an appropriate cache\nsize. This equivalence can be established rigorously under the condition that\nthe cache sizes of the individual servers are large. For typical data caching\nsystems this condition is common. Our theoretical framework provides a\nconvenient abstraction that can directly apply the results from the simpler\nsingle LRU cache to the more complex LRU cluster with consistent hashing.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1904.06278v1",
            "url": "http://arxiv.org/abs/1904.06278v1",
            "title": "RELOAD+REFRESH: Abusing Cache Replacement Policies to Perform Stealthy Cache Attacks",
            "year": "2019",
            "publicationDate": "2019-04-12 15:30:45+00:00",
            "abstract": "Caches have become the prime method for unintended information extraction\nacross logical isolation boundaries. Even Spectre and Meltdown rely on the\ncache side channel, as it provides great resolution and is widely available on\nall major CPU platforms. As a consequence, several methods to stop cache\nattacks by detecting them have been proposed. Detection is strongly aided by\nthe fact that observing cache activity of co-resident processes is not possible\nwithout altering the cache state and thereby forcing evictions on the observed\nprocesses. In this work, we show that this widely held assumption is incorrect.\nThrough clever usage of the cache replacement policy it is possible to track a\nvictims process cache accesses without forcing evictions on the victim's data.\nHence, online detection mechanisms that rely on these evictions can be\ncircumvented as they do not detect be the introduced RELOAD+REFRESH attack. The\nattack requires a profound understanding of the cache replacement policy. We\npresent a methodology to recover the replacement policy and apply it to the\nlast five generations of Intel processors. We further show empirically that the\nperformance of RELOAD+REFRESH on cryptographic implementations is comparable to\nthat of other widely used cache attacks, while its detectability becomes\nextremely difficult, due to the negligible effect on the victims cache access\npattern.",
            "text": "Caches have become the prime method for unintended information extraction\nacross logical isolation boundaries. Even Spectre and Meltdown rely on the\ncache side channel, as it provides great resolution and is widely available on\nall major CPU platforms. As a consequence, several methods to stop cache\nattacks by detecting them have been proposed. Detection is strongly aided by\nthe fact that observing cache activity of co-resident processes is not possible\nwithout altering the cache state and thereby forcing evictions on the observed\nprocesses. In this work, we show that this widely held assumption is incorrect.\nThrough clever usage of the cache replacement policy it is possible to track a\nvictims process cache accesses without forcing evictions on the victim's data.\nHence, online detection mechanisms that rely on these evictions can be\ncircumvented as they do not detect be the introduced RELOAD+REFRESH attack. The\nattack requires a profound understanding of the cache replacement policy. We\npresent a methodology to recover the replacement policy and apply it to the\nlast five generations of Intel processors. We further show empirically that the\nperformance of RELOAD+REFRESH on cryptographic implementations is comparable to\nthat of other widely used cache attacks, while its detectability becomes\nextremely difficult, due to the negligible effect on the victims cache access\npattern.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1606.09206v1",
            "url": "http://arxiv.org/abs/1606.09206v1",
            "title": "Performance of spatial Multi-LRU caching under traffic with temporal locality",
            "year": "2016",
            "publicationDate": "2016-06-29 18:11:21+00:00",
            "abstract": "In this work a novel family of decentralised caching policies for wireless\nnetworks is introduced, referred to as spatial multi-LRU. These improve\ncache-hit probability by exploiting multi-coverage. Two variations are\nproposed, the multi-LRU-One and -All, which differ in the number of replicas\ninserted in the covering edge-caches. The evaluation is done under spatial\ntraffic that exhibits temporal locality, with varying content catalogue and\ndependent demands. The performance metric is hit probability and the policies\nare compared to (1) the single-LRU and (2) an upper bound for all centralised\npolicies with periodic popularity updates. Numerical results show the multi-LRU\npolicies outperform both comparison policies. The reason is their passive\nadaptability to popularity changes. Between the -One and -All variation, which\none is preferable strongly depends on the available storage space and on\ntraffic characteristics. The performance also depends on the popularity shape.",
            "text": "In this work a novel family of decentralised caching policies for wireless\nnetworks is introduced, referred to as spatial multi-LRU. These improve\ncache-hit probability by exploiting multi-coverage. Two variations are\nproposed, the multi-LRU-One and -All, which differ in the number of replicas\ninserted in the covering edge-caches. The evaluation is done under spatial\ntraffic that exhibits temporal locality, with varying content catalogue and\ndependent demands. The performance metric is hit probability and the policies\nare compared to (1) the single-LRU and (2) an upper bound for all centralised\npolicies with periodic popularity updates. Numerical results show the multi-LRU\npolicies outperform both comparison policies. The reason is their passive\nadaptability to popularity changes. Between the -One and -All variation, which\none is preferable strongly depends on the available storage space and on\ntraffic characteristics. The performance also depends on the popularity shape.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1602.07623v1",
            "url": "http://arxiv.org/abs/1602.07623v1",
            "title": "Spatial multi-LRU Caching for Wireless Networks with Coverage Overlaps",
            "year": "2016",
            "publicationDate": "2016-02-24 18:08:43+00:00",
            "abstract": "This article introduces a novel family of decentralised caching policies,\napplicable to wireless networks with finite storage at the edge-nodes\n(stations). These policies are based on the Least-Recently-Used replacement\nprinciple, and are, here, referred to as spatial multi-LRU. Based on these,\ncache inventories are updated in a way that provides content diversity to users\nwho are covered by, and thus have access to, more than one station. Two\nvariations are proposed, namely the multi-LRU-One and -All, which differ in the\nnumber of replicas inserted in the involved caches. By introducing spatial\napproximations, we propose a Che-like method to predict the hit probability,\nwhich gives very accurate results under the Independent Reference Model (IRM).\nIt is shown that the performance of multi-LRU increases the more the\nmulti-coverage areas increase, and it approaches the performance of other\nproposed centralised policies, when multi-coverage is sufficient. For IRM\ntraffic multi-LRU-One outperforms multi-LRU-All, whereas when the traffic\nexhibits temporal locality the -All variation can perform better.",
            "text": "This article introduces a novel family of decentralised caching policies,\napplicable to wireless networks with finite storage at the edge-nodes\n(stations). These policies are based on the Least-Recently-Used replacement\nprinciple, and are, here, referred to as spatial multi-LRU. Based on these,\ncache inventories are updated in a way that provides content diversity to users\nwho are covered by, and thus have access to, more than one station. Two\nvariations are proposed, namely the multi-LRU-One and -All, which differ in the\nnumber of replicas inserted in the involved caches. By introducing spatial\napproximations, we propose a Che-like method to predict the hit probability,\nwhich gives very accurate results under the Independent Reference Model (IRM).\nIt is shown that the performance of multi-LRU increases the more the\nmulti-coverage areas increase, and it approaches the performance of other\nproposed centralised policies, when multi-coverage is sufficient. For IRM\ntraffic multi-LRU-One outperforms multi-LRU-All, whereas when the traffic\nexhibits temporal locality the -All variation can perform better.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2505.14468v1",
            "url": "http://arxiv.org/abs/2505.14468v1",
            "title": "ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs",
            "year": "2025",
            "publicationDate": "2025-05-20 15:04:17+00:00",
            "abstract": "Serverless computing has grown rapidly for serving Large Language Model (LLM)\ninference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid\nscaling. However, our analysis reveals that current serverless can effectively\nserve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to\nthree key limitations: 1) massive parameter redundancy among functions where\n99% of weights are unnecessarily duplicated, 2) costly artifact loading latency\nbeyond LLM loading, and 3) magnified resource contention when serving multiple\nLoRA LLMs. These inefficiencies lead to massive GPU wastage, increased\nTime-To-First-Token (TTFT), and high monetary costs.\n  We propose ServerlessLoRA, a novel serverless inference system designed for\nfaster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM\nsharing across isolated LoRA functions to reduce redundancy. We design a\npre-loading method that pre-loads comprehensive LoRA artifacts to minimize\ncold-start latency. Furthermore, ServerlessLoRA employs contention aware\nbatching and offloading to mitigate GPU resource conflicts during bursty\nworkloads. Experiment on industrial workloads demonstrates that ServerlessLoRA\nreduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to\nstate-of-the-art LLM inference solutions.",
            "text": "Serverless computing has grown rapidly for serving Large Language Model (LLM)\ninference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid\nscaling. However, our analysis reveals that current serverless can effectively\nserve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to\nthree key limitations: 1) massive parameter redundancy among functions where\n99% of weights are unnecessarily duplicated, 2) costly artifact loading latency\nbeyond LLM loading, and 3) magnified resource contention when serving multiple\nLoRA LLMs. These inefficiencies lead to massive GPU wastage, increased\nTime-To-First-Token (TTFT), and high monetary costs.\n  We propose ServerlessLoRA, a novel serverless inference system designed for\nfaster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM\nsharing across isolated LoRA functions to reduce redundancy. We design a\npre-loading method that pre-loads comprehensive LoRA artifacts to minimize\ncold-start latency. Furthermore, ServerlessLoRA employs contention aware\nbatching and offloading to mitigate GPU resource conflicts during bursty\nworkloads. Experiment on industrial workloads demonstrates that ServerlessLoRA\nreduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to\nstate-of-the-art LLM inference solutions.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1710.02254v2",
            "url": "http://arxiv.org/abs/1710.02254v2",
            "title": "Lattice Recurrent Unit: Improving Convergence and Statistical Efficiency for Sequence Modeling",
            "year": "2017",
            "publicationDate": "2017-10-06 01:52:14+00:00",
            "abstract": "Recurrent neural networks have shown remarkable success in modeling\nsequences. However low resource situations still adversely affect the\ngeneralizability of these models. We introduce a new family of models, called\nLattice Recurrent Units (LRU), to address the challenge of learning deep\nmulti-layer recurrent models with limited resources. LRU models achieve this\ngoal by creating distinct (but coupled) flow of information inside the units: a\nfirst flow along time dimension and a second flow along depth dimension. It\nalso offers a symmetry in how information can flow horizontally and vertically.\nWe analyze the effects of decoupling three different components of our LRU\nmodel: Reset Gate, Update Gate and Projected State. We evaluate this family on\nnew LRU models on computational convergence rates and statistical efficiency.\nOur experiments are performed on four publicly-available datasets, comparing\nwith Grid-LSTM and Recurrent Highway networks. Our results show that LRU has\nbetter empirical computational convergence rates and statistical efficiency\nvalues, along with learning more accurate language models.",
            "text": "Recurrent neural networks have shown remarkable success in modeling\nsequences. However low resource situations still adversely affect the\ngeneralizability of these models. We introduce a new family of models, called\nLattice Recurrent Units (LRU), to address the challenge of learning deep\nmulti-layer recurrent models with limited resources. LRU models achieve this\ngoal by creating distinct (but coupled) flow of information inside the units: a\nfirst flow along time dimension and a second flow along depth dimension. It\nalso offers a symmetry in how information can flow horizontally and vertically.\nWe analyze the effects of decoupling three different components of our LRU\nmodel: Reset Gate, Update Gate and Projected State. We evaluate this family on\nnew LRU models on computational convergence rates and statistical efficiency.\nOur experiments are performed on four publicly-available datasets, comparing\nwith Grid-LSTM and Recurrent Highway networks. Our results show that LRU has\nbetter empirical computational convergence rates and statistical efficiency\nvalues, along with learning more accurate language models.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1905.08348v2",
            "url": "http://arxiv.org/abs/1905.08348v2",
            "title": "Leaking Information Through Cache LRU States",
            "year": "2019",
            "publicationDate": "2019-05-20 21:11:13+00:00",
            "abstract": "The Least-Recently Used cache replacement policy and its variants are widely\ndeployed in modern processors. This paper shows for the first time in detail\nthat the LRU states of caches can be used to leak information: any access to a\ncache by a sender will modify the LRU state, and the receiver is able to\nobserve this through a timing measurement. This paper presents LRU timing-based\nchannels both when the sender and the receiver have shared memory, e.g., shared\nlibrary data pages, and when they are separate processes without shared memory.\nIn addition, the new LRU timing-based channels are demonstrated on both Intel\nand AMD processors in scenarios where the sender and the receiver are sharing\nthe cache in both hyper-threaded setting and time-sliced setting. The\ntransmission rate of the LRU channels can be up to 600Kbps per cache set in the\nhyper-threaded setting. Different from the majority of existing cache channels\nwhich require the sender to trigger cache misses, the new LRU channels work\nwith the sender only having cache hits, making the channel faster and more\nstealthy. This paper also demonstrates that the new LRU channels can be used in\ntransient execution attacks, e.g., Spectre. Further, this paper shows that the\nLRU channels pose threats to existing secure cache designs, and this work\ndemonstrates the LRU channels affect the secure PL cache. The paper finishes by\ndiscussing and evaluating possible defenses.",
            "text": "The Least-Recently Used cache replacement policy and its variants are widely\ndeployed in modern processors. This paper shows for the first time in detail\nthat the LRU states of caches can be used to leak information: any access to a\ncache by a sender will modify the LRU state, and the receiver is able to\nobserve this through a timing measurement. This paper presents LRU timing-based\nchannels both when the sender and the receiver have shared memory, e.g., shared\nlibrary data pages, and when they are separate processes without shared memory.\nIn addition, the new LRU timing-based channels are demonstrated on both Intel\nand AMD processors in scenarios where the sender and the receiver are sharing\nthe cache in both hyper-threaded setting and time-sliced setting. The\ntransmission rate of the LRU channels can be up to 600Kbps per cache set in the\nhyper-threaded setting. Different from the majority of existing cache channels\nwhich require the sender to trigger cache misses, the new LRU channels work\nwith the sender only having cache hits, making the channel faster and more\nstealthy. This paper also demonstrates that the new LRU channels can be used in\ntransient execution attacks, e.g., Spectre. Further, this paper shows that the\nLRU channels pose threats to existing secure cache designs, and this work\ndemonstrates the LRU channels affect the secure PL cache. The paper finishes by\ndiscussing and evaluating possible defenses.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2407.12391v1",
            "url": "http://arxiv.org/abs/2407.12391v1",
            "title": "LLM Inference Serving: Survey of Recent Advances and Opportunities",
            "year": "2024",
            "publicationDate": "2024-07-17 08:11:47+00:00",
            "abstract": "This survey offers a comprehensive overview of recent advancements in Large\nLanguage Model (LLM) serving systems, focusing on research since the year 2023.\nWe specifically examine system-level enhancements that improve performance and\nefficiency without altering the core LLM decoding mechanisms. By selecting and\nreviewing high-quality papers from prestigious ML and system venues, we\nhighlight key innovations and practical considerations for deploying and\nscaling LLMs in real-world production environments. This survey serves as a\nvaluable resource for LLM practitioners seeking to stay abreast of the latest\ndevelopments in this rapidly evolving field.",
            "text": "This survey offers a comprehensive overview of recent advancements in Large\nLanguage Model (LLM) serving systems, focusing on research since the year 2023.\nWe specifically examine system-level enhancements that improve performance and\nefficiency without altering the core LLM decoding mechanisms. By selecting and\nreviewing high-quality papers from prestigious ML and system venues, we\nhighlight key innovations and practical considerations for deploying and\nscaling LLMs in real-world production environments. This survey serves as a\nvaluable resource for LLM practitioners seeking to stay abreast of the latest\ndevelopments in this rapidly evolving field.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1310.3584v1",
            "url": "http://arxiv.org/abs/1310.3584v1",
            "title": "Selection Policy: Fighting against Filter Effect in Network of Caches",
            "year": "2013",
            "publicationDate": "2013-10-14 07:46:59+00:00",
            "abstract": "Many Information Centric Networking (ICN) proposals use a network of caches\nto bring the contents closer to the consumers, reduce the load on producers and\ndecrease the unnecessary retransmission for ISPs. Nevertheless, the existing\ncache management scheme for the network of caches obtain poor performance. The\nmain reason for performance degradation in a network of caches is the filter\neffect of the replacement policy. A cache serves the requests that generate\ncache-hits and forwards the requests that generate cache-misses. This filtering\nchanges the pattern of requests and leads to decreased hit ratios in the\nsubsequent caches. In this paper, we propose a coordinated caching scheme to\nsolve the filter effect problem by introducing the selection policy. This\npolicy manages a cache such that: i) the cache obtains a high hit ratio ii) the\nmissed requests from the cache can be used by subsequent caches to obtain a\nhigh hit ratio. Our coordinated selection scheme achieves an overall hit ratio\nof a network of caches equivalent to that of edge routers with big caches.\nMoreover, our scheme decreases the average number of evictions per cache slot\nby four order of magnitude compared to the LRU universal caching.",
            "text": "Many Information Centric Networking (ICN) proposals use a network of caches\nto bring the contents closer to the consumers, reduce the load on producers and\ndecrease the unnecessary retransmission for ISPs. Nevertheless, the existing\ncache management scheme for the network of caches obtain poor performance. The\nmain reason for performance degradation in a network of caches is the filter\neffect of the replacement policy. A cache serves the requests that generate\ncache-hits and forwards the requests that generate cache-misses. This filtering\nchanges the pattern of requests and leads to decreased hit ratios in the\nsubsequent caches. In this paper, we propose a coordinated caching scheme to\nsolve the filter effect problem by introducing the selection policy. This\npolicy manages a cache such that: i) the cache obtains a high hit ratio ii) the\nmissed requests from the cache can be used by subsequent caches to obtain a\nhigh hit ratio. Our coordinated selection scheme achieves an overall hit ratio\nof a network of caches equivalent to that of edge routers with big caches.\nMoreover, our scheme decreases the average number of evictions per cache slot\nby four order of magnitude compared to the LRU universal caching.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1510.03362v1",
            "url": "http://arxiv.org/abs/1510.03362v1",
            "title": "On the Smoothness of Paging Algorithms",
            "year": "2015",
            "publicationDate": "2015-10-12 16:49:13+00:00",
            "abstract": "We study the smoothness of paging algorithms. How much can the number of page\nfaults increase due to a perturbation of the request sequence? We call a paging\nalgorithm smooth if the maximal increase in page faults is proportional to the\nnumber of changes in the request sequence. We also introduce quantitative\nsmoothness notions that measure the smoothness of an algorithm. We derive lower\nand upper bounds on the smoothness of deterministic and randomized\ndemand-paging and competitive algorithms. Among strongly-competitive\ndeterministic algorithms LRU matches the lower bound, while FIFO matches the\nupper bound.\n  Well-known randomized algorithms like Partition, Equitable, or Mark are shown\nnot to be smooth. We introduce two new randomized algorithms, called\nSmoothed-LRU and LRU-Random. Smoothed- LRU allows to sacrifice competitiveness\nfor smoothness, where the trade-off is controlled by a parameter. LRU-Random is\nat least as competitive as any deterministic algorithm while smoother.",
            "text": "We study the smoothness of paging algorithms. How much can the number of page\nfaults increase due to a perturbation of the request sequence? We call a paging\nalgorithm smooth if the maximal increase in page faults is proportional to the\nnumber of changes in the request sequence. We also introduce quantitative\nsmoothness notions that measure the smoothness of an algorithm. We derive lower\nand upper bounds on the smoothness of deterministic and randomized\ndemand-paging and competitive algorithms. Among strongly-competitive\ndeterministic algorithms LRU matches the lower bound, while FIFO matches the\nupper bound.\n  Well-known randomized algorithms like Partition, Equitable, or Mark are shown\nnot to be smooth. We introduce two new randomized algorithms, called\nSmoothed-LRU and LRU-Random. Smoothed- LRU allows to sacrifice competitiveness\nfor smoothness, where the trade-off is controlled by a parameter. LRU-Random is\nat least as competitive as any deterministic algorithm while smoother.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2212.13671v1",
            "url": "http://arxiv.org/abs/2212.13671v1",
            "title": "Optimizing Replacement Policies for Content Delivery Network Caching: Beyond Belady to Attain A Seemingly Unattainable Byte Miss Ratio",
            "year": "2022",
            "publicationDate": "2022-12-28 02:55:11+00:00",
            "abstract": "When facing objects/files of differing sizes in content delivery networks\n(CDNs) caches, pursuing an optimal object miss ratio (OMR) by approximating\nBelady no longer ensures an optimal byte miss ratio (BMR), creating confusion\nabout how to achieve a superior BMR in CDNs. To address this issue, we\nexperimentally observe that there exists a time window to delay the eviction of\nthe object with the longest reuse distance to improve BMR without increasing\nOMR. As a result, we introduce a deep reinforcement learning (RL) model to\ncapture this time window by dynamically monitoring the changes in OMR and BMR,\nand implementing a BMR-friendly policy in the time window. Based on this\npolicy, we propose a Belady and Size Eviction (LRU-BaSE) algorithm, reducing\nBMR while maintaining OMR. To make LRU-BaSE efficient and practical, we address\nthe feedback delay problem of RL with a two-pronged approach. On the one hand,\nour observation of a rear section of the LRU cache queue containing most of the\neviction candidates allows LRU-BaSE to shorten the decision region. On the\nother hand, the request distribution on CDNs makes it feasible to divide the\nlearning region into multiple sub-regions that are each learned with reduced\ntime and increased accuracy. In real CDN systems, compared to LRU, LRU-BaSE can\nreduce \"backing to OS\" traffic and access latency by 30.05\\% and 17.07\\%,\nrespectively, on average. The results on the simulator confirm that LRU-BaSE\noutperforms the state-of-the-art cache replacement policies, where LRU-BaSE's\nBMR is 0.63\\% and 0.33\\% less than that of Belady and Practical Flow-based\nOffline Optimal (PFOO), respectively, on average. In addition, compared to\nLearning Relaxed Belady (LRB), LRU-BaSE can yield relatively stable performance\nwhen facing workload drift.",
            "text": "When facing objects/files of differing sizes in content delivery networks\n(CDNs) caches, pursuing an optimal object miss ratio (OMR) by approximating\nBelady no longer ensures an optimal byte miss ratio (BMR), creating confusion\nabout how to achieve a superior BMR in CDNs. To address this issue, we\nexperimentally observe that there exists a time window to delay the eviction of\nthe object with the longest reuse distance to improve BMR without increasing\nOMR. As a result, we introduce a deep reinforcement learning (RL) model to\ncapture this time window by dynamically monitoring the changes in OMR and BMR,\nand implementing a BMR-friendly policy in the time window. Based on this\npolicy, we propose a Belady and Size Eviction (LRU-BaSE) algorithm, reducing\nBMR while maintaining OMR. To make LRU-BaSE efficient and practical, we address\nthe feedback delay problem of RL with a two-pronged approach. On the one hand,\nour observation of a rear section of the LRU cache queue containing most of the\neviction candidates allows LRU-BaSE to shorten the decision region. On the\nother hand, the request distribution on CDNs makes it feasible to divide the\nlearning region into multiple sub-regions that are each learned with reduced\ntime and increased accuracy. In real CDN systems, compared to LRU, LRU-BaSE can\nreduce \"backing to OS\" traffic and access latency by 30.05\\% and 17.07\\%,\nrespectively, on average. The results on the simulator confirm that LRU-BaSE\noutperforms the state-of-the-art cache replacement policies, where LRU-BaSE's\nBMR is 0.63\\% and 0.33\\% less than that of Belady and Practical Flow-based\nOffline Optimal (PFOO), respectively, on average. In addition, compared to\nLearning Relaxed Belady (LRB), LRU-BaSE can yield relatively stable performance\nwhen facing workload drift.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1702.04078v4",
            "url": "http://arxiv.org/abs/1702.04078v4",
            "title": "A Cache Management Scheme for Efficient Content Eviction and Replication in Cache Networks",
            "year": "2017",
            "publicationDate": "2017-02-14 04:31:46+00:00",
            "abstract": "To cope with the ongoing changing demands of the internet, 'in-network\ncaching' has been presented as an application solution for two decades. With\nthe advent of information-centric network (ICN) architecture, 'in-network\ncaching' becomes a network level solution. Some unique features of ICNs, e.g.,\nrapidly changing cache states, higher request arrival rates, smaller cache\nsizes, and other factors, impose diverse requirements on the content eviction\npolicies. In particular, eviction policies should be fast and lightweight. In\nthis study, we propose cache replication and eviction schemes, Conditional\nLeave Cope Everywhere (CLCE) and Least Frequent Recently Used (LFRU), which are\nwell suited for the ICN type of cache networks (CNs). The CLCE replication\nscheme reduces the redundant caching of contents; hence improves the cache\nspace utilization. LFRU approximates the Least Frequently Used (LFU) scheme\ncoupled with the Least Recently Used (LRU) scheme and is practically\nimplementable for rapidly changing cache networks like ICNs.",
            "text": "To cope with the ongoing changing demands of the internet, 'in-network\ncaching' has been presented as an application solution for two decades. With\nthe advent of information-centric network (ICN) architecture, 'in-network\ncaching' becomes a network level solution. Some unique features of ICNs, e.g.,\nrapidly changing cache states, higher request arrival rates, smaller cache\nsizes, and other factors, impose diverse requirements on the content eviction\npolicies. In particular, eviction policies should be fast and lightweight. In\nthis study, we propose cache replication and eviction schemes, Conditional\nLeave Cope Everywhere (CLCE) and Least Frequent Recently Used (LFRU), which are\nwell suited for the ICN type of cache networks (CNs). The CLCE replication\nscheme reduces the redundant caching of contents; hence improves the cache\nspace utilization. LFRU approximates the Least Frequently Used (LFU) scheme\ncoupled with the Least Recently Used (LRU) scheme and is practically\nimplementable for rapidly changing cache networks like ICNs.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2402.15425v1",
            "url": "http://arxiv.org/abs/2402.15425v1",
            "title": "Prime+Retouch: When Cache is Locked and Leaked",
            "year": "2024",
            "publicationDate": "2024-02-23 16:34:49+00:00",
            "abstract": "Caches on the modern commodity CPUs have become one of the major sources of\nside-channel leakages and been abused as a new attack vector. To thwart the\ncache-based side-channel attacks, two types of countermeasures have been\nproposed: detection-based ones that limit the amount of microarchitectural\ntraces an attacker can leave, and cache prefetching-and-locking techniques that\nclaim to prevent such leakage by disallowing evictions on sensitive data. In\nthis paper, we present the Prime+Retouch attack that completely bypasses these\ndefense schemes by accurately inferring the cache activities with the metadata\nof the cache replacement policy. Prime+Retouch has three noticeable properties:\n1) it incurs no eviction on the victim's data, allowing us to bypass the two\nknown mitigation schemes, 2) it requires minimal synchronization of only one\nmemory access to the attacker's pre-primed cache lines, and 3) it leaks data\nvia non-shared memory, yet because underlying eviction metadata is shared.\n  We demonstrate Prime+Retouch in two architectures: predominant Intel x86 and\nemerging Apple M1. We elucidate how Prime+Retouch can break the T-table\nimplementation of AES with robust cache side-channel mitigations such as Cloak,\nunder both normal and SGX-protected environments. We also manifest feasibility\nof the Prime+Retouch attack on the M1 platform imposing more restrictions where\nthe precise measurement tools such as core clock cycle timer and performance\ncounters are inaccessible to the attacker. Furthermore, we first demystify\nundisclosed cache architecture and its eviction policy of L1 data cache on\nApple M1 architecture. We also devise a user-space noise-free cache monitoring\ntool by repurposing Intel TSX.",
            "text": "Caches on the modern commodity CPUs have become one of the major sources of\nside-channel leakages and been abused as a new attack vector. To thwart the\ncache-based side-channel attacks, two types of countermeasures have been\nproposed: detection-based ones that limit the amount of microarchitectural\ntraces an attacker can leave, and cache prefetching-and-locking techniques that\nclaim to prevent such leakage by disallowing evictions on sensitive data. In\nthis paper, we present the Prime+Retouch attack that completely bypasses these\ndefense schemes by accurately inferring the cache activities with the metadata\nof the cache replacement policy. Prime+Retouch has three noticeable properties:\n1) it incurs no eviction on the victim's data, allowing us to bypass the two\nknown mitigation schemes, 2) it requires minimal synchronization of only one\nmemory access to the attacker's pre-primed cache lines, and 3) it leaks data\nvia non-shared memory, yet because underlying eviction metadata is shared.\n  We demonstrate Prime+Retouch in two architectures: predominant Intel x86 and\nemerging Apple M1. We elucidate how Prime+Retouch can break the T-table\nimplementation of AES with robust cache side-channel mitigations such as Cloak,\nunder both normal and SGX-protected environments. We also manifest feasibility\nof the Prime+Retouch attack on the M1 platform imposing more restrictions where\nthe precise measurement tools such as core clock cycle timer and performance\ncounters are inaccessible to the attacker. Furthermore, we first demystify\nundisclosed cache architecture and its eviction policy of L1 data cache on\nApple M1 architecture. We also devise a user-space noise-free cache monitoring\ntool by repurposing Intel TSX.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2112.09981v1",
            "url": "http://arxiv.org/abs/2112.09981v1",
            "title": "Multi-step LRU: SIMD-based Cache Replacement for Lower Overhead and Higher Precision",
            "year": "2021",
            "publicationDate": "2021-12-18 18:26:07+00:00",
            "abstract": "A key-value cache is a key component of many services to provide low-latency\nand high-throughput data accesses to a huge amount of data. To improve the\nend-to-end performance of such services, a key-value cache must achieve a high\ncache hit ratio with high throughput. In this paper, we propose a new cache\nreplacement algorithm, multi-step LRU, which achieves high throughput by\nefficiently exploiting SIMD instructions without using per-item additional\nmemory (LRU metadata) to record information such as the last access timestamp.\nFor a small set of items that can fit within a vector register, SIMD-based LRU\nmanagement without LRU metadata is known (in-vector LRU). It remembers the\naccess history by reordering items in one vector using vector shuffle\ninstruction. In-vector LRU alone cannot be used for a caching system since it\ncan manage only few items. Set-associative cache is a straightforward way to\nbuild a large cache using in-vector LRU as a building block. However, a naive\nset-associative cache based on in-vector LRU has a poorer cache hit ratio than\nthe original LRU although it can achieve a high throughput. Our multi-step LRU\nenhances naive set-associative cache based on in-vector LRU for improving cache\naccuracy by taking both access frequency and access recency of items into\naccount while keeping the efficiency by SIMD instructions. Our results indicate\nthat multi-step LRU outperforms the original LRU and GCLOCK algorithms in terms\nof both execution speed and cache hit ratio. Multi-step LRU improves the cache\nhit ratios over the original LRU by implicitly taking access frequency of items\nas well as access recency into account. The cache hit ratios of multi-step LRU\nare similar to those of ARC, which achieves a higher a cache hit ratio in a\ntradeoff for using more LRU metadata.",
            "text": "A key-value cache is a key component of many services to provide low-latency\nand high-throughput data accesses to a huge amount of data. To improve the\nend-to-end performance of such services, a key-value cache must achieve a high\ncache hit ratio with high throughput. In this paper, we propose a new cache\nreplacement algorithm, multi-step LRU, which achieves high throughput by\nefficiently exploiting SIMD instructions without using per-item additional\nmemory (LRU metadata) to record information such as the last access timestamp.\nFor a small set of items that can fit within a vector register, SIMD-based LRU\nmanagement without LRU metadata is known (in-vector LRU). It remembers the\naccess history by reordering items in one vector using vector shuffle\ninstruction. In-vector LRU alone cannot be used for a caching system since it\ncan manage only few items. Set-associative cache is a straightforward way to\nbuild a large cache using in-vector LRU as a building block. However, a naive\nset-associative cache based on in-vector LRU has a poorer cache hit ratio than\nthe original LRU although it can achieve a high throughput. Our multi-step LRU\nenhances naive set-associative cache based on in-vector LRU for improving cache\naccuracy by taking both access frequency and access recency of items into\naccount while keeping the efficiency by SIMD instructions. Our results indicate\nthat multi-step LRU outperforms the original LRU and GCLOCK algorithms in terms\nof both execution speed and cache hit ratio. Multi-step LRU improves the cache\nhit ratios over the original LRU by implicitly taking access frequency of items\nas well as access recency into account. The cache hit ratios of multi-step LRU\nare similar to those of ARC, which achieves a higher a cache hit ratio in a\ntradeoff for using more LRU metadata.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1801.00390v1",
            "url": "http://arxiv.org/abs/1801.00390v1",
            "title": "Time Aware Least Recent Used (TLRU) Cache Management Policy in ICN",
            "year": "2018",
            "publicationDate": "2018-01-01 04:13:53+00:00",
            "abstract": "The information centric networks (ICN) can be viewed as a network of caches.\nConversely, ICN type of cache networks has distinctive features e.g, contents\npopularity, usability time of content and other factors inflicts some diverse\nrequirements for cache eviction policies. In this paper we defined four\nimportant characteristics of a suitable eviction policy for ICN. We analysed\nwell known eviction policies in view of defined characteristics. Based upon\nanalysis we propose a new eviction scheme which is well suitable for ICN type\nof cache networks.",
            "text": "The information centric networks (ICN) can be viewed as a network of caches.\nConversely, ICN type of cache networks has distinctive features e.g, contents\npopularity, usability time of content and other factors inflicts some diverse\nrequirements for cache eviction policies. In this paper we defined four\nimportant characteristics of a suitable eviction policy for ICN. We analysed\nwell known eviction policies in view of defined characteristics. Based upon\nanalysis we propose a new eviction scheme which is well suitable for ICN type\nof cache networks.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1901.01187v1",
            "url": "http://arxiv.org/abs/1901.01187v1",
            "title": "PopNetCod: A Popularity-based Caching Policy for Network Coding enabled Named Data Networking",
            "year": "2019",
            "publicationDate": "2019-01-04 16:02:41+00:00",
            "abstract": "In this paper, we propose PopNetCod, a popularity-based caching policy for\nnetwork coding enabled Named Data Networking. PopNetCod is a distributed\ncaching policy, in which each router measures the local popularity of the\ncontent objects by analyzing the requests that it receives. It then uses this\ninformation to decide which Data packets to cache or evict from its content\nstore. Since network coding is used, partial caching of content objects is\nsupported, which facilitates the management of the content store. The routers\ndecide the Data packets that they cache or evict in an online manner when they\nreceive requests for Data packets. This allows the most popular Data packets to\nbe cached closer to the network edges. The evaluation of PopNetCod shows an\nimproved cache-hit rate compared to the widely used Leave Copy Everywhere\nplacement policy and the Least Recently Used eviction policy. The improved\ncache-hit rate helps the clients to achieve higher goodput, while it also\nreduces the load on the source servers.",
            "text": "In this paper, we propose PopNetCod, a popularity-based caching policy for\nnetwork coding enabled Named Data Networking. PopNetCod is a distributed\ncaching policy, in which each router measures the local popularity of the\ncontent objects by analyzing the requests that it receives. It then uses this\ninformation to decide which Data packets to cache or evict from its content\nstore. Since network coding is used, partial caching of content objects is\nsupported, which facilitates the management of the content store. The routers\ndecide the Data packets that they cache or evict in an online manner when they\nreceive requests for Data packets. This allows the most popular Data packets to\nbe cached closer to the network edges. The evaluation of PopNetCod shows an\nimproved cache-hit rate compared to the widely used Leave Copy Everywhere\nplacement policy and the Least Recently Used eviction policy. The improved\ncache-hit rate helps the clients to achieve higher goodput, while it also\nreduces the load on the source servers.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2505.05772v1",
            "url": "http://arxiv.org/abs/2505.05772v1",
            "title": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM",
            "year": "2025",
            "publicationDate": "2025-05-09 04:17:05+00:00",
            "abstract": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures.",
            "text": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1512.00727v2",
            "url": "http://arxiv.org/abs/1512.00727v2",
            "title": "TinyLFU: A Highly Efficient Cache Admission Policy",
            "year": "2015",
            "publicationDate": "2015-12-02 15:05:46+00:00",
            "abstract": "This paper proposes to use a frequency based cache admission policy in order\nto boost the effectiveness of caches subject to skewed access distributions.\nGiven a newly accessed item and an eviction candidate from the cache, our\nscheme decides, based on the recent access history, whether it is worth\nadmitting the new item into the cache at the expense of the eviction candidate.\n  Realizing this concept is enabled through a novel approximate LFU structure\ncalled TinyLFU, which maintains an approximate representation of the access\nfrequency of a large sample of recently accessed items. TinyLFU is very compact\nand light-weight as it builds upon Bloom filter theory.\n  We study the properties of TinyLFU through simulations of both synthetic\nworkloads as well as multiple real traces from several sources. These\nsimulations demonstrate the performance boost obtained by enhancing various\nreplacement policies with the TinyLFU eviction policy. Also, a new combined\nreplacement and eviction policy scheme nicknamed W-TinyLFU is presented.\nW-TinyLFU is demonstrated to obtain equal or better hit-ratios than other state\nof the art replacement policies on these traces. It is the only scheme to\nobtain such good results on all traces.",
            "text": "This paper proposes to use a frequency based cache admission policy in order\nto boost the effectiveness of caches subject to skewed access distributions.\nGiven a newly accessed item and an eviction candidate from the cache, our\nscheme decides, based on the recent access history, whether it is worth\nadmitting the new item into the cache at the expense of the eviction candidate.\n  Realizing this concept is enabled through a novel approximate LFU structure\ncalled TinyLFU, which maintains an approximate representation of the access\nfrequency of a large sample of recently accessed items. TinyLFU is very compact\nand light-weight as it builds upon Bloom filter theory.\n  We study the properties of TinyLFU through simulations of both synthetic\nworkloads as well as multiple real traces from several sources. These\nsimulations demonstrate the performance boost obtained by enhancing various\nreplacement policies with the TinyLFU eviction policy. Also, a new combined\nreplacement and eviction policy scheme nicknamed W-TinyLFU is presented.\nW-TinyLFU is demonstrated to obtain equal or better hit-ratios than other state\nof the art replacement policies on these traces. It is the only scheme to\nobtain such good results on all traces.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2505.24221v1",
            "url": "http://arxiv.org/abs/2505.24221v1",
            "title": "FOCUS: Boosting Schema-aware Access for KV Stores via Hierarchical Data Management",
            "year": "2025",
            "publicationDate": "2025-05-30 05:17:44+00:00",
            "abstract": "Persistent key-value (KV) stores are critical infrastructure for\ndata-intensive applications. Leveraging high-performance Non-Volatile Memory\n(NVM) to enhance KV stores has gained traction. However, previous work has\nprimarily focused on optimizing KV stores themselves, without adequately\naddressing their integration into applications. Consequently, existing\napplications, represented by NewSQL databases, still resort to a flat mapping\napproach, which simply maps structured records into flat KV pairs to use KV\nstores. Such semantic mismatch may cause significant I/O amplification and I/O\nsplitting under production workloads, harming the performance. To this end, we\npropose FOCUS, a log-structured KV store optimized for fine-grained\nhierarchical data organization and schema-aware access. FOCUS introduces a\nhierarchical KV model to provide native support for upper-layer structured\ndata. We implemented FOCUS from scratch. Experiments show that FOCUS can\nincrease throughput by 2.1-5.9x compared to mainstream NVM-backed KV stores\nunder YCSB SQL workloads.",
            "text": "Persistent key-value (KV) stores are critical infrastructure for\ndata-intensive applications. Leveraging high-performance Non-Volatile Memory\n(NVM) to enhance KV stores has gained traction. However, previous work has\nprimarily focused on optimizing KV stores themselves, without adequately\naddressing their integration into applications. Consequently, existing\napplications, represented by NewSQL databases, still resort to a flat mapping\napproach, which simply maps structured records into flat KV pairs to use KV\nstores. Such semantic mismatch may cause significant I/O amplification and I/O\nsplitting under production workloads, harming the performance. To this end, we\npropose FOCUS, a log-structured KV store optimized for fine-grained\nhierarchical data organization and schema-aware access. FOCUS introduces a\nhierarchical KV model to provide native support for upper-layer structured\ndata. We implemented FOCUS from scratch. Experiments show that FOCUS can\nincrease throughput by 2.1-5.9x compared to mainstream NVM-backed KV stores\nunder YCSB SQL workloads.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2106.03840v2",
            "url": "http://arxiv.org/abs/2106.03840v2",
            "title": "Balancing Garbage Collection vs I/O Amplification using hybrid Key-Value Placement in LSM-based Key-Value Stores",
            "year": "2021",
            "publicationDate": "2021-06-07 17:55:32+00:00",
            "abstract": "Key-value (KV) separation is a technique that introduces randomness in the\nI/O access patterns to reduce I/O amplification in LSM-based key-value stores\nfor fast storage devices (NVMe). KV separation has a significant drawback that\nmakes it less attractive: Delete and especially update operations that are\nimportant in modern workloads result in frequent and expensive garbage\ncollection (GC) in the value log. In this paper, we design and implement\nParallax, which proposes hybrid KV placement that reduces GC overhead\nsignificantly and maximizes the benefits of using a log. We first model the\nbenefits of KV separation for different KV pair sizes. We use this model to\nclassify KV pairs in three categories small, medium, and large. Then, Parallax\nuses different approaches for each KV category: It always places large values\nin a log and small values in place. For medium values it uses a mixed strategy\nthat combines the benefits of using a log and eliminates GC overhead as\nfollows: It places medium values in a log for all but the last few (typically\none or two) levels in the LSM structure, where it performs a full compaction,\nmerges values in place, and reclaims log space without the need for GC. We\nevaluate Parallax against RocksDB that places all values in place and BlobDB\nthat always performs KV separation. We find that Parallax increases throughput\nby up to 12.4x and 17.83x, decreases I/O amplification by up to 27.1x and 26x,\nand increases CPU efficiency by up to 18.7x and 28x respectively, for all but\nscan-based YCSB workloads.",
            "text": "Key-value (KV) separation is a technique that introduces randomness in the\nI/O access patterns to reduce I/O amplification in LSM-based key-value stores\nfor fast storage devices (NVMe). KV separation has a significant drawback that\nmakes it less attractive: Delete and especially update operations that are\nimportant in modern workloads result in frequent and expensive garbage\ncollection (GC) in the value log. In this paper, we design and implement\nParallax, which proposes hybrid KV placement that reduces GC overhead\nsignificantly and maximizes the benefits of using a log. We first model the\nbenefits of KV separation for different KV pair sizes. We use this model to\nclassify KV pairs in three categories small, medium, and large. Then, Parallax\nuses different approaches for each KV category: It always places large values\nin a log and small values in place. For medium values it uses a mixed strategy\nthat combines the benefits of using a log and eliminates GC overhead as\nfollows: It places medium values in a log for all but the last few (typically\none or two) levels in the LSM structure, where it performs a full compaction,\nmerges values in place, and reclaims log space without the need for GC. We\nevaluate Parallax against RocksDB that places all values in place and BlobDB\nthat always performs KV separation. We find that Parallax increases throughput\nby up to 12.4x and 17.83x, decreases I/O amplification by up to 27.1x and 26x,\nand increases CPU efficiency by up to 18.7x and 28x respectively, for all but\nscan-based YCSB workloads.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1810.06930v1",
            "url": "http://arxiv.org/abs/1810.06930v1",
            "title": "Feedforward Neural Networks for Caching: Enough or Too Much?",
            "year": "2018",
            "publicationDate": "2018-10-16 11:26:18+00:00",
            "abstract": "We propose a caching policy that uses a feedforward neural network (FNN) to\npredict content popularity. Our scheme outperforms popular eviction policies\nlike LRU or ARC, but also a new policy relying on the more complex recurrent\nneural networks. At the same time, replacing the FNN predictor with a naive\nlinear estimator does not degrade caching performance significantly,\nquestioning then the role of neural networks for these applications.",
            "text": "We propose a caching policy that uses a feedforward neural network (FNN) to\npredict content popularity. Our scheme outperforms popular eviction policies\nlike LRU or ARC, but also a new policy relying on the more complex recurrent\nneural networks. At the same time, replacing the FNN predictor with a naive\nlinear estimator does not degrade caching performance significantly,\nquestioning then the role of neural networks for these applications.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2411.11091v1",
            "url": "http://arxiv.org/abs/2411.11091v1",
            "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage Engines",
            "year": "2024",
            "publicationDate": "2024-11-17 14:47:15+00:00",
            "abstract": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
            "text": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2503.14647v1",
            "url": "http://arxiv.org/abs/2503.14647v1",
            "title": "Towards More Economical Context-Augmented LLM Generation by Reusing Stored KV Cache",
            "year": "2025",
            "publicationDate": "2025-03-18 18:52:03+00:00",
            "abstract": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing.",
            "text": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2407.00023v2",
            "url": "http://arxiv.org/abs/2407.00023v2",
            "title": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving",
            "year": "2024",
            "publicationDate": "2024-05-08 06:30:58+00:00",
            "abstract": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency.",
            "text": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1811.10000v2",
            "url": "http://arxiv.org/abs/1811.10000v2",
            "title": "Enabling Efficient Updates in KV Storage via Hashing: Design and Performance Evaluation",
            "year": "2018",
            "publicationDate": "2018-11-25 12:52:26+00:00",
            "abstract": "Persistent key-value (KV) stores mostly build on the Log-Structured Merge\n(LSM) tree for high write performance, yet the LSM-tree suffers from the\ninherently high I/O amplification. KV separation mitigates I/O amplification by\nstoring only keys in the LSM-tree and values in separate storage. However, the\ncurrent KV separation design remains inefficient under update-intensive\nworkloads due to its high garbage collection (GC) overhead in value storage. We\npropose HashKV, which aims for high update performance atop KV separation under\nupdate-intensive workloads. HashKV uses hash-based data grouping, which\ndeterministically maps values to storage space so as to make both updates and\nGC efficient. We further relax the restriction of such deterministic mappings\nvia simple but useful design extensions. We extensively evaluate various design\naspects of HashKV. We show that HashKV achieves 4.6x update throughput and\n53.4% less write traffic compared to the current KV separation design. In\naddition, we demonstrate that we can integrate the design of HashKV with\nstate-of-the-art KV stores and improve their respective performance.",
            "text": "Persistent key-value (KV) stores mostly build on the Log-Structured Merge\n(LSM) tree for high write performance, yet the LSM-tree suffers from the\ninherently high I/O amplification. KV separation mitigates I/O amplification by\nstoring only keys in the LSM-tree and values in separate storage. However, the\ncurrent KV separation design remains inefficient under update-intensive\nworkloads due to its high garbage collection (GC) overhead in value storage. We\npropose HashKV, which aims for high update performance atop KV separation under\nupdate-intensive workloads. HashKV uses hash-based data grouping, which\ndeterministically maps values to storage space so as to make both updates and\nGC efficient. We further relax the restriction of such deterministic mappings\nvia simple but useful design extensions. We extensively evaluate various design\naspects of HashKV. We show that HashKV achieves 4.6x update throughput and\n53.4% less write traffic compared to the current KV separation design. In\naddition, we demonstrate that we can integrate the design of HashKV with\nstate-of-the-art KV stores and improve their respective performance.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2502.02750v1",
            "url": "http://arxiv.org/abs/2502.02750v1",
            "title": "Cache is King: Smart Page Eviction with eBPF",
            "year": "2025",
            "publicationDate": "2025-02-04 22:37:17+00:00",
            "abstract": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
            "text": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2412.03131v2",
            "url": "http://arxiv.org/abs/2412.03131v2",
            "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
            "year": "2024",
            "publicationDate": "2024-12-04 08:51:23+00:00",
            "abstract": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
            "text": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2505.17694v1",
            "url": "http://arxiv.org/abs/2505.17694v1",
            "title": "FlashForge: Ultra-Efficient Prefix-Aware Attention for LLM Decoding",
            "year": "2025",
            "publicationDate": "2025-05-23 10:03:28+00:00",
            "abstract": "Prefix-sharing among multiple prompts presents opportunities to combine the\noperations of the shared prefix, while attention computation in the decode\nstage, which becomes a critical bottleneck with increasing context lengths, is\na memory-intensive process requiring heavy memory access on the key-value (KV)\ncache of the prefixes. Therefore, in this paper, we explore the potential of\nprefix-sharing in the attention computation of the decode stage. However, the\ntree structure of the prefix-sharing mechanism presents significant challenges\nfor attention computation in efficiently processing shared KV cache access\npatterns while managing complex dependencies and balancing irregular workloads.\nTo address the above challenges, we propose a dedicated attention kernel to\ncombine the memory access of shared prefixes in the decoding stage, namely\nFlashForge. FlashForge delivers two key innovations: a novel shared-prefix\nattention kernel that optimizes memory hierarchy and exploits both intra-block\nand inter-block parallelism, and a comprehensive workload balancing mechanism\nthat efficiently estimates cost, divides tasks, and schedules execution.\nExperimental results show that FlashForge achieves an average 1.9x speedup and\n120.9x memory access reduction compared to the state-of-the-art FlashDecoding\nkernel regarding attention computation in the decode stage and 3.8x end-to-end\ntime per output token compared to the vLLM.",
            "text": "Prefix-sharing among multiple prompts presents opportunities to combine the\noperations of the shared prefix, while attention computation in the decode\nstage, which becomes a critical bottleneck with increasing context lengths, is\na memory-intensive process requiring heavy memory access on the key-value (KV)\ncache of the prefixes. Therefore, in this paper, we explore the potential of\nprefix-sharing in the attention computation of the decode stage. However, the\ntree structure of the prefix-sharing mechanism presents significant challenges\nfor attention computation in efficiently processing shared KV cache access\npatterns while managing complex dependencies and balancing irregular workloads.\nTo address the above challenges, we propose a dedicated attention kernel to\ncombine the memory access of shared prefixes in the decoding stage, namely\nFlashForge. FlashForge delivers two key innovations: a novel shared-prefix\nattention kernel that optimizes memory hierarchy and exploits both intra-block\nand inter-block parallelism, and a comprehensive workload balancing mechanism\nthat efficiently estimates cost, divides tasks, and schedules execution.\nExperimental results show that FlashForge achieves an average 1.9x speedup and\n120.9x memory access reduction compared to the state-of-the-art FlashDecoding\nkernel regarding attention computation in the decode stage and 3.8x end-to-end\ntime per output token compared to the vLLM.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2304.10268v5",
            "url": "http://arxiv.org/abs/2304.10268v5",
            "title": "BackCache: Mitigating Contention-Based Cache Timing Attacks by Hiding Cache Line Evictions",
            "year": "2023",
            "publicationDate": "2023-04-20 12:47:11+00:00",
            "abstract": "Caches are used to reduce the speed differential between the CPU and memory\nto improve the performance of modern processors. However, attackers can use\ncontention-based cache timing attacks to steal sensitive information from\nvictim processes through carefully designed cache eviction sets. And L1 data\ncache attacks are widely exploited and pose a significant privacy and\nconfidentiality threat. Existing hardware-based countermeasures mainly focus on\ncache partitioning, randomization, and cache line flushing, which unfortunately\neither incur high overhead or can be circumvented by sophisticated attacks. In\nthis paper, we propose a novel hardware-software co-design called BackCache\nwith the idea of always achieving cache hits instead of cache misses to\nmitigate contention-based cache timing attacks on the L1 data cache. BackCache\nplaces the evicted cache lines from the L1 data cache into a fully-associative\nbackup cache to hide the evictions. To improve the security of BackCache, we\nintroduce a randomly used replacement policy (RURP) and a dynamic backup cache\nresizing mechanism. We also present a theoretical security analysis to\ndemonstrate the effectiveness of BackCache. Our evaluation on the gem5\nsimulator shows that BackCache can degrade the performance by 2.61%, 2.66%, and\n3.36% For OS kernel, single-thread, and multi-thread benchmarks.",
            "text": "Caches are used to reduce the speed differential between the CPU and memory\nto improve the performance of modern processors. However, attackers can use\ncontention-based cache timing attacks to steal sensitive information from\nvictim processes through carefully designed cache eviction sets. And L1 data\ncache attacks are widely exploited and pose a significant privacy and\nconfidentiality threat. Existing hardware-based countermeasures mainly focus on\ncache partitioning, randomization, and cache line flushing, which unfortunately\neither incur high overhead or can be circumvented by sophisticated attacks. In\nthis paper, we propose a novel hardware-software co-design called BackCache\nwith the idea of always achieving cache hits instead of cache misses to\nmitigate contention-based cache timing attacks on the L1 data cache. BackCache\nplaces the evicted cache lines from the L1 data cache into a fully-associative\nbackup cache to hide the evictions. To improve the security of BackCache, we\nintroduce a randomly used replacement policy (RURP) and a dynamic backup cache\nresizing mechanism. We also present a theoretical security analysis to\ndemonstrate the effectiveness of BackCache. Our evaluation on the gem5\nsimulator shows that BackCache can degrade the performance by 2.61%, 2.66%, and\n3.36% For OS kernel, single-thread, and multi-thread benchmarks.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2401.04287v1",
            "url": "http://arxiv.org/abs/2401.04287v1",
            "title": "What Is an App Store? The Software Engineering Perspective",
            "year": "2024",
            "publicationDate": "2024-01-08 23:56:45+00:00",
            "abstract": "\"App stores\" are online software stores where end users may browse, purchase,\ndownload, and install software applications. By far, the best known app stores\nare associated with mobile platforms, such as Google Play for Android and\nApple's App Store for iOS. The ubiquity of smartphones has led to mobile app\nstores becoming a touchstone experience of modern living. However, most of app\nstore research has concentrated on properties of the apps rather than the\nstores themselves. Today, there is a rich diversity of app stores and these\nstores have largely been overlooked by researchers: app stores exist on many\ndistinctive platforms, are aimed at different classes of users, and have\ndifferent end-goals beyond simply selling a standalone app to a smartphone\nuser.\n  We survey and characterize the broader dimensionality of app stores, and\nexplore how and why they influence software development practices, such as\nsystem design and release management. We begin by collecting a set of app store\nexamples from web search queries. By analyzing and curating the results, we\nderive a set of features common to app stores. We then build a dimensional\nmodel of app stores based on these features, and we fit each app store from our\nweb search result set into this model. Next, we performed unsupervised\nclustering to the app stores to find their natural groupings. Our results\nsuggest that app stores have become an essential stakeholder in modern software\ndevelopment. They control the distribution channel to end users and ensure that\nthe applications are of suitable quality; in turn, this leads to developers\nadhering to various store guidelines when creating their applications. However,\nwe found the app stores operational model could vary widely between stores, and\nthis variability could in turn affect the generalizability of existing\nunderstanding of app stores.",
            "text": "\"App stores\" are online software stores where end users may browse, purchase,\ndownload, and install software applications. By far, the best known app stores\nare associated with mobile platforms, such as Google Play for Android and\nApple's App Store for iOS. The ubiquity of smartphones has led to mobile app\nstores becoming a touchstone experience of modern living. However, most of app\nstore research has concentrated on properties of the apps rather than the\nstores themselves. Today, there is a rich diversity of app stores and these\nstores have largely been overlooked by researchers: app stores exist on many\ndistinctive platforms, are aimed at different classes of users, and have\ndifferent end-goals beyond simply selling a standalone app to a smartphone\nuser.\n  We survey and characterize the broader dimensionality of app stores, and\nexplore how and why they influence software development practices, such as\nsystem design and release management. We begin by collecting a set of app store\nexamples from web search queries. By analyzing and curating the results, we\nderive a set of features common to app stores. We then build a dimensional\nmodel of app stores based on these features, and we fit each app store from our\nweb search result set into this model. Next, we performed unsupervised\nclustering to the app stores to find their natural groupings. Our results\nsuggest that app stores have become an essential stakeholder in modern software\ndevelopment. They control the distribution channel to end users and ensure that\nthe applications are of suitable quality; in turn, this leads to developers\nadhering to various store guidelines when creating their applications. However,\nwe found the app stores operational model could vary widely between stores, and\nthis variability could in turn affect the generalizability of existing\nunderstanding of app stores.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1904.06278v1",
            "url": "http://arxiv.org/abs/1904.06278v1",
            "title": "RELOAD+REFRESH: Abusing Cache Replacement Policies to Perform Stealthy Cache Attacks",
            "year": "2019",
            "publicationDate": "2019-04-12 15:30:45+00:00",
            "abstract": "Caches have become the prime method for unintended information extraction\nacross logical isolation boundaries. Even Spectre and Meltdown rely on the\ncache side channel, as it provides great resolution and is widely available on\nall major CPU platforms. As a consequence, several methods to stop cache\nattacks by detecting them have been proposed. Detection is strongly aided by\nthe fact that observing cache activity of co-resident processes is not possible\nwithout altering the cache state and thereby forcing evictions on the observed\nprocesses. In this work, we show that this widely held assumption is incorrect.\nThrough clever usage of the cache replacement policy it is possible to track a\nvictims process cache accesses without forcing evictions on the victim's data.\nHence, online detection mechanisms that rely on these evictions can be\ncircumvented as they do not detect be the introduced RELOAD+REFRESH attack. The\nattack requires a profound understanding of the cache replacement policy. We\npresent a methodology to recover the replacement policy and apply it to the\nlast five generations of Intel processors. We further show empirically that the\nperformance of RELOAD+REFRESH on cryptographic implementations is comparable to\nthat of other widely used cache attacks, while its detectability becomes\nextremely difficult, due to the negligible effect on the victims cache access\npattern.",
            "text": "Caches have become the prime method for unintended information extraction\nacross logical isolation boundaries. Even Spectre and Meltdown rely on the\ncache side channel, as it provides great resolution and is widely available on\nall major CPU platforms. As a consequence, several methods to stop cache\nattacks by detecting them have been proposed. Detection is strongly aided by\nthe fact that observing cache activity of co-resident processes is not possible\nwithout altering the cache state and thereby forcing evictions on the observed\nprocesses. In this work, we show that this widely held assumption is incorrect.\nThrough clever usage of the cache replacement policy it is possible to track a\nvictims process cache accesses without forcing evictions on the victim's data.\nHence, online detection mechanisms that rely on these evictions can be\ncircumvented as they do not detect be the introduced RELOAD+REFRESH attack. The\nattack requires a profound understanding of the cache replacement policy. We\npresent a methodology to recover the replacement policy and apply it to the\nlast five generations of Intel processors. We further show empirically that the\nperformance of RELOAD+REFRESH on cryptographic implementations is comparable to\nthat of other widely used cache attacks, while its detectability becomes\nextremely difficult, due to the negligible effect on the victims cache access\npattern.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1605.03325v1",
            "url": "http://arxiv.org/abs/1605.03325v1",
            "title": "Multi-class Vector AutoRegressive Models for Multi-store Sales Data",
            "year": "2016",
            "publicationDate": "2016-05-11 08:32:13+00:00",
            "abstract": "Retailers use the Vector AutoRegressive (VAR) model as a standard tool to\nestimate the effects of prices, promotions and sales in one product category on\nthe sales of another product category. Besides, these price, promotion and\nsales data are available for not just one store, but a whole chain of stores.\nWe propose to study cross-category effects using a multi-class VAR model: we\njointly estimate cross-category effects for several distinct but related VAR\nmodels, one for each store. Our methodology encourages effects to be similar\nacross stores, while still allowing for small differences between stores to\naccount for store heterogeneity. Moreover, our estimator is sparse: unimportant\neffects are estimated as exactly zero, which facilitates the interpretation of\nthe results. A simulation study shows that the proposed multi-class estimator\nimproves estimation accuracy by borrowing strength across classes. Finally, we\nprovide three visual tools showing (i) the clustering of stores on identical\ncross-category effects, (ii) the networks of product categories and (iii) the\nsimilarity matrices of shared cross-category effects across stores.",
            "text": "Retailers use the Vector AutoRegressive (VAR) model as a standard tool to\nestimate the effects of prices, promotions and sales in one product category on\nthe sales of another product category. Besides, these price, promotion and\nsales data are available for not just one store, but a whole chain of stores.\nWe propose to study cross-category effects using a multi-class VAR model: we\njointly estimate cross-category effects for several distinct but related VAR\nmodels, one for each store. Our methodology encourages effects to be similar\nacross stores, while still allowing for small differences between stores to\naccount for store heterogeneity. Moreover, our estimator is sparse: unimportant\neffects are estimated as exactly zero, which facilitates the interpretation of\nthe results. A simulation study shows that the proposed multi-class estimator\nimproves estimation accuracy by borrowing strength across classes. Finally, we\nprovide three visual tools showing (i) the clustering of stores on identical\ncross-category effects, (ii) the networks of product categories and (iii) the\nsimilarity matrices of shared cross-category effects across stores.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1310.3584v1",
            "url": "http://arxiv.org/abs/1310.3584v1",
            "title": "Selection Policy: Fighting against Filter Effect in Network of Caches",
            "year": "2013",
            "publicationDate": "2013-10-14 07:46:59+00:00",
            "abstract": "Many Information Centric Networking (ICN) proposals use a network of caches\nto bring the contents closer to the consumers, reduce the load on producers and\ndecrease the unnecessary retransmission for ISPs. Nevertheless, the existing\ncache management scheme for the network of caches obtain poor performance. The\nmain reason for performance degradation in a network of caches is the filter\neffect of the replacement policy. A cache serves the requests that generate\ncache-hits and forwards the requests that generate cache-misses. This filtering\nchanges the pattern of requests and leads to decreased hit ratios in the\nsubsequent caches. In this paper, we propose a coordinated caching scheme to\nsolve the filter effect problem by introducing the selection policy. This\npolicy manages a cache such that: i) the cache obtains a high hit ratio ii) the\nmissed requests from the cache can be used by subsequent caches to obtain a\nhigh hit ratio. Our coordinated selection scheme achieves an overall hit ratio\nof a network of caches equivalent to that of edge routers with big caches.\nMoreover, our scheme decreases the average number of evictions per cache slot\nby four order of magnitude compared to the LRU universal caching.",
            "text": "Many Information Centric Networking (ICN) proposals use a network of caches\nto bring the contents closer to the consumers, reduce the load on producers and\ndecrease the unnecessary retransmission for ISPs. Nevertheless, the existing\ncache management scheme for the network of caches obtain poor performance. The\nmain reason for performance degradation in a network of caches is the filter\neffect of the replacement policy. A cache serves the requests that generate\ncache-hits and forwards the requests that generate cache-misses. This filtering\nchanges the pattern of requests and leads to decreased hit ratios in the\nsubsequent caches. In this paper, we propose a coordinated caching scheme to\nsolve the filter effect problem by introducing the selection policy. This\npolicy manages a cache such that: i) the cache obtains a high hit ratio ii) the\nmissed requests from the cache can be used by subsequent caches to obtain a\nhigh hit ratio. Our coordinated selection scheme achieves an overall hit ratio\nof a network of caches equivalent to that of edge routers with big caches.\nMoreover, our scheme decreases the average number of evictions per cache slot\nby four order of magnitude compared to the LRU universal caching.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1905.10677v3",
            "url": "http://arxiv.org/abs/1905.10677v3",
            "title": "An Exploratory Study on Machine Learning Model Stores",
            "year": "2019",
            "publicationDate": "2019-05-25 21:22:02+00:00",
            "abstract": "Recent advances in Artificial Intelligence, especially in Machine Learning\n(ML), have brought applications previously considered as science fiction (e.g.,\nvirtual personal assistants and autonomous cars) into the reach of millions of\neveryday users. Since modern ML technologies like deep learning require\nconsiderable technical expertise and resource to build custom models, reusing\nexisting models trained by experts has become essential. This is why in the\npast year model stores have been introduced, which, similar to mobile app\nstores, offer organizations and developers access to pre-trained models and/or\ntheir code to train, evaluate, and predict samples. This paper conducts an\nexploratory study on three popular model stores (AWS marketplace, Wolfram\nneural net repository, and ModelDepot) that compares the information elements\n(features and policies) provided by model stores to those used by the two\npopular mobile app stores (Google Play and Apple's App Store). We have found\nthat the model information elements vary among the different model stores, with\n65% elements shared by all three studied stores. Model stores share five\ninformation elements with mobile app stores, while eight elements are unique to\nmodel stores and four elements unique to app stores. Only few models were\navailable on multiple model stores. Our findings allow to better understand the\ndifferences between ML models and \"regular\" source code components or\napplications, and provide inspiration to identify software engineering\npractices (e.g., in requirements and delivery) specific to ML applications.",
            "text": "Recent advances in Artificial Intelligence, especially in Machine Learning\n(ML), have brought applications previously considered as science fiction (e.g.,\nvirtual personal assistants and autonomous cars) into the reach of millions of\neveryday users. Since modern ML technologies like deep learning require\nconsiderable technical expertise and resource to build custom models, reusing\nexisting models trained by experts has become essential. This is why in the\npast year model stores have been introduced, which, similar to mobile app\nstores, offer organizations and developers access to pre-trained models and/or\ntheir code to train, evaluate, and predict samples. This paper conducts an\nexploratory study on three popular model stores (AWS marketplace, Wolfram\nneural net repository, and ModelDepot) that compares the information elements\n(features and policies) provided by model stores to those used by the two\npopular mobile app stores (Google Play and Apple's App Store). We have found\nthat the model information elements vary among the different model stores, with\n65% elements shared by all three studied stores. Model stores share five\ninformation elements with mobile app stores, while eight elements are unique to\nmodel stores and four elements unique to app stores. Only few models were\navailable on multiple model stores. Our findings allow to better understand the\ndifferences between ML models and \"regular\" source code components or\napplications, and provide inspiration to identify software engineering\npractices (e.g., in requirements and delivery) specific to ML applications.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2012.06966v1",
            "url": "http://arxiv.org/abs/2012.06966v1",
            "title": "A Dual-Store Structure for Knowledge Graphs",
            "year": "2020",
            "publicationDate": "2020-12-13 05:33:19+00:00",
            "abstract": "To effectively manage increasing knowledge graphs in various domains, a hot\nresearch topic, knowledge graph storage management, has emerged. Existing\nmethods are classified to relational stores and native graph stores. Relational\nstores are able to store large-scale knowledge graphs and convenient in\nupdating knowledge, but the query performance weakens obviously when the\nselectivity of a knowledge graph query is large. Native graph stores are\nefficient in processing complex knowledge graph queries due to its index-free\nadjacent property, but they are inapplicable to manage a large-scale knowledge\ngraph due to limited storage budgets or inflexible updating process. Motivated\nby this, we propose a dual-store structure which leverages a graph store to\naccelerate the complex query process in the relational store. However, it is\nchallenging to determine what data to transfer from relational store to graph\nstore at what time. To address this problem, we formulate it as a Markov\nDecision Process and derive a physical design tuner DOTIL based on\nreinforcement learning. With DOTIL, the dual-store structure is adaptive to\ndynamic changing workloads. Experimental results on real knowledge graphs\ndemonstrate that our proposed dual-store structure improves query performance\nup to average 43.72% compared with the most commonly used relational stores.",
            "text": "To effectively manage increasing knowledge graphs in various domains, a hot\nresearch topic, knowledge graph storage management, has emerged. Existing\nmethods are classified to relational stores and native graph stores. Relational\nstores are able to store large-scale knowledge graphs and convenient in\nupdating knowledge, but the query performance weakens obviously when the\nselectivity of a knowledge graph query is large. Native graph stores are\nefficient in processing complex knowledge graph queries due to its index-free\nadjacent property, but they are inapplicable to manage a large-scale knowledge\ngraph due to limited storage budgets or inflexible updating process. Motivated\nby this, we propose a dual-store structure which leverages a graph store to\naccelerate the complex query process in the relational store. However, it is\nchallenging to determine what data to transfer from relational store to graph\nstore at what time. To address this problem, we formulate it as a Markov\nDecision Process and derive a physical design tuner DOTIL based on\nreinforcement learning. With DOTIL, the dual-store structure is adaptive to\ndynamic changing workloads. Experimental results on real knowledge graphs\ndemonstrate that our proposed dual-store structure improves query performance\nup to average 43.72% compared with the most commonly used relational stores.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/0708.0399v1",
            "url": "http://arxiv.org/abs/0708.0399v1",
            "title": "Diffusion induced decoherence of stored optical vortices",
            "year": "2007",
            "publicationDate": "2007-08-02 19:43:44+00:00",
            "abstract": "We study the coherence properties of optical vortices stored in atomic\nensembles. In the presence of thermal diffusion, the topological nature of\nstored optical vortices is found not to guarantee slow decoherence. Instead the\nstored vortex state has decoherence surprisingly larger than the stored\nGaussian mode. Generally, the less phase gradient, the more robust for stored\ncoherence against diffusion. Furthermore, calculation of coherence factor shows\nthat the center of stored vortex becomes completely incoherent once diffusion\nbegins and, when reading laser is applied, the optical intensity at the center\nof the vortex becomes nonzero. Its implication for quantum information is\ndiscussed. Comparison of classical diffusion and quantum diffusion is also\npresented.",
            "text": "We study the coherence properties of optical vortices stored in atomic\nensembles. In the presence of thermal diffusion, the topological nature of\nstored optical vortices is found not to guarantee slow decoherence. Instead the\nstored vortex state has decoherence surprisingly larger than the stored\nGaussian mode. Generally, the less phase gradient, the more robust for stored\ncoherence against diffusion. Furthermore, calculation of coherence factor shows\nthat the center of stored vortex becomes completely incoherent once diffusion\nbegins and, when reading laser is applied, the optical intensity at the center\nof the vortex becomes nonzero. Its implication for quantum information is\ndiscussed. Comparison of classical diffusion and quantum diffusion is also\npresented.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1702.04078v4",
            "url": "http://arxiv.org/abs/1702.04078v4",
            "title": "A Cache Management Scheme for Efficient Content Eviction and Replication in Cache Networks",
            "year": "2017",
            "publicationDate": "2017-02-14 04:31:46+00:00",
            "abstract": "To cope with the ongoing changing demands of the internet, 'in-network\ncaching' has been presented as an application solution for two decades. With\nthe advent of information-centric network (ICN) architecture, 'in-network\ncaching' becomes a network level solution. Some unique features of ICNs, e.g.,\nrapidly changing cache states, higher request arrival rates, smaller cache\nsizes, and other factors, impose diverse requirements on the content eviction\npolicies. In particular, eviction policies should be fast and lightweight. In\nthis study, we propose cache replication and eviction schemes, Conditional\nLeave Cope Everywhere (CLCE) and Least Frequent Recently Used (LFRU), which are\nwell suited for the ICN type of cache networks (CNs). The CLCE replication\nscheme reduces the redundant caching of contents; hence improves the cache\nspace utilization. LFRU approximates the Least Frequently Used (LFU) scheme\ncoupled with the Least Recently Used (LRU) scheme and is practically\nimplementable for rapidly changing cache networks like ICNs.",
            "text": "To cope with the ongoing changing demands of the internet, 'in-network\ncaching' has been presented as an application solution for two decades. With\nthe advent of information-centric network (ICN) architecture, 'in-network\ncaching' becomes a network level solution. Some unique features of ICNs, e.g.,\nrapidly changing cache states, higher request arrival rates, smaller cache\nsizes, and other factors, impose diverse requirements on the content eviction\npolicies. In particular, eviction policies should be fast and lightweight. In\nthis study, we propose cache replication and eviction schemes, Conditional\nLeave Cope Everywhere (CLCE) and Least Frequent Recently Used (LFRU), which are\nwell suited for the ICN type of cache networks (CNs). The CLCE replication\nscheme reduces the redundant caching of contents; hence improves the cache\nspace utilization. LFRU approximates the Least Frequently Used (LFU) scheme\ncoupled with the Least Recently Used (LRU) scheme and is practically\nimplementable for rapidly changing cache networks like ICNs.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2301.05941v1",
            "url": "http://arxiv.org/abs/2301.05941v1",
            "title": "Improving Confidentiality for NFT Referenced Data Stores",
            "year": "2023",
            "publicationDate": "2023-01-14 16:04:21+00:00",
            "abstract": "A non-fungible token (NFT) references a data store location, typically, using\na URL or another unique identifier. At the minimum, a NFT is expected to\nguarantee ownership and control over the tokenised asset. However, information\nstored on a third party data store may be copied and stolen. We propose a\nsolution to give control back to the information owner by storing encrypted\ncontent on the data store and providing additional security against hacks and\nzero day exploits. The content on our data store is never decrypted or returned\nto its owner for decryption during rekeying. Also, the key size in our protocol\ndoes not increase with each rekeying. With this, we reduce the synchronisation\nsteps and maintain a bounded key size.",
            "text": "A non-fungible token (NFT) references a data store location, typically, using\na URL or another unique identifier. At the minimum, a NFT is expected to\nguarantee ownership and control over the tokenised asset. However, information\nstored on a third party data store may be copied and stolen. We propose a\nsolution to give control back to the information owner by storing encrypted\ncontent on the data store and providing additional security against hacks and\nzero day exploits. The content on our data store is never decrypted or returned\nto its owner for decryption during rekeying. Also, the key size in our protocol\ndoes not increase with each rekeying. With this, we reduce the synchronisation\nsteps and maintain a bounded key size.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2404.15670v1",
            "url": "http://arxiv.org/abs/2404.15670v1",
            "title": "HTAP Databases: A Survey",
            "year": "2024",
            "publicationDate": "2024-04-24 05:59:30+00:00",
            "abstract": "Since Gartner coined the term, Hybrid Transactional and Analytical Processing\n(HTAP), numerous HTAP databases have been proposed to combine transactions with\nanalytics in order to enable real-time data analytics for various\ndata-intensive applications. HTAP databases typically process the mixed\nworkloads of transactions and analytical queries in a unified system by\nleveraging both a row store and a column store. As there are different storage\narchitectures and processing techniques to satisfy various requirements of\ndiverse applications, it is critical to summarize the pros and cons of these\nkey techniques. This paper offers a comprehensive survey of HTAP databases. We\nmainly classify state-of-the-art HTAP databases according to four storage\narchitectures: (a) Primary Row Store and In-Memory Column Store; (b)\nDistributed Row Store and Column Store Replica; (c) Primary Row Store and\nDistributed In-Memory Column Store; and (d) Primary Column Store and Delta Row\nStore. We then review the key techniques in HTAP databases, including hybrid\nworkload processing, data organization, data synchronization, query\noptimization, and resource scheduling. We also discuss existing HTAP\nbenchmarks. Finally, we provide the research challenges and opportunities for\nHTAP techniques.",
            "text": "Since Gartner coined the term, Hybrid Transactional and Analytical Processing\n(HTAP), numerous HTAP databases have been proposed to combine transactions with\nanalytics in order to enable real-time data analytics for various\ndata-intensive applications. HTAP databases typically process the mixed\nworkloads of transactions and analytical queries in a unified system by\nleveraging both a row store and a column store. As there are different storage\narchitectures and processing techniques to satisfy various requirements of\ndiverse applications, it is critical to summarize the pros and cons of these\nkey techniques. This paper offers a comprehensive survey of HTAP databases. We\nmainly classify state-of-the-art HTAP databases according to four storage\narchitectures: (a) Primary Row Store and In-Memory Column Store; (b)\nDistributed Row Store and Column Store Replica; (c) Primary Row Store and\nDistributed In-Memory Column Store; and (d) Primary Column Store and Delta Row\nStore. We then review the key techniques in HTAP databases, including hybrid\nworkload processing, data organization, data synchronization, query\noptimization, and resource scheduling. We also discuss existing HTAP\nbenchmarks. Finally, we provide the research challenges and opportunities for\nHTAP techniques.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1806.05563v1",
            "url": "http://arxiv.org/abs/1806.05563v1",
            "title": "Data-Driven Analytics for Benchmarking and Optimizing Retail Store Performance",
            "year": "2018",
            "publicationDate": "2018-06-14 14:07:30+00:00",
            "abstract": "Growing competitiveness and increasing availability of data is generating\ntremendous interest in data-driven analytics across industries. In the retail\nsector, stores need targeted guidance to improve both the efficiency and\neffectiveness of individual stores based on their specific locations,\ndemographics, and environment. We propose an effective data-driven framework\nfor internal benchmarking that can lead to targeted guidance for individual\nstores. In particular, we propose an objective method for segmenting stores\nusing a model-based clustering technique that accounts for similarity in store\nperformance dynamics. The proposed method relies on an effective Finite Mixture\nof Regressions technique based on competitive learning for carrying out the\nmodel-based clustering with `must-link' constraints and modeling store\nperformance. We also propose an optimization framework to derive tailored\nrecommendations for individual stores within store clusters that jointly\nimproves profitability for the store while also improving sales to satisfy\nfranchiser requirements. We validate the methods using synthetic experiments as\nwell as a real-world automotive dealership network study for a leading global\nautomotive manufacturer.",
            "text": "Growing competitiveness and increasing availability of data is generating\ntremendous interest in data-driven analytics across industries. In the retail\nsector, stores need targeted guidance to improve both the efficiency and\neffectiveness of individual stores based on their specific locations,\ndemographics, and environment. We propose an effective data-driven framework\nfor internal benchmarking that can lead to targeted guidance for individual\nstores. In particular, we propose an objective method for segmenting stores\nusing a model-based clustering technique that accounts for similarity in store\nperformance dynamics. The proposed method relies on an effective Finite Mixture\nof Regressions technique based on competitive learning for carrying out the\nmodel-based clustering with `must-link' constraints and modeling store\nperformance. We also propose an optimization framework to derive tailored\nrecommendations for individual stores within store clusters that jointly\nimproves profitability for the store while also improving sales to satisfy\nfranchiser requirements. We validate the methods using synthetic experiments as\nwell as a real-world automotive dealership network study for a leading global\nautomotive manufacturer.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2111.11517v1",
            "url": "http://arxiv.org/abs/2111.11517v1",
            "title": "Columnar Formats for Schemaless LSM-based Document Stores",
            "year": "2021",
            "publicationDate": "2021-11-22 20:28:59+00:00",
            "abstract": "In the last decade, document store database systems have gained more traction\nfor storing and querying large volumes of semi-structured data. However, the\nflexibility of the document stores' data models has limited their ability to\nstore data in a columnar-major layout - making them less performant for\nanalytical workloads than column store relational databases. In this paper, we\npropose several techniques based on piggy-backing on Log-Structured Merge (LSM)\ntree events and tailored to document stores to store document data in a\ncolumnar layout. We first extend the Dremel format, a popular on-disk columnar\nformat for semi-structured data, to comply with document stores' flexible data\nmodel. We then introduce two columnar layouts for organizing and storing data\nin LSM-based storage. We also highlight the potential of using query\ncompilation techniques for document stores, where values' types are known only\nat runtime. We have implemented and evaluated our techniques to measure their\nimpact on storage, data ingestion, and query performance in Apache AsterixDB.\nOur experiments show significant performance gains, improving the query\nexecution time by orders of magnitude while minimally impacting ingestion\nperformance.",
            "text": "In the last decade, document store database systems have gained more traction\nfor storing and querying large volumes of semi-structured data. However, the\nflexibility of the document stores' data models has limited their ability to\nstore data in a columnar-major layout - making them less performant for\nanalytical workloads than column store relational databases. In this paper, we\npropose several techniques based on piggy-backing on Log-Structured Merge (LSM)\ntree events and tailored to document stores to store document data in a\ncolumnar layout. We first extend the Dremel format, a popular on-disk columnar\nformat for semi-structured data, to comply with document stores' flexible data\nmodel. We then introduce two columnar layouts for organizing and storing data\nin LSM-based storage. We also highlight the potential of using query\ncompilation techniques for document stores, where values' types are known only\nat runtime. We have implemented and evaluated our techniques to measure their\nimpact on storage, data ingestion, and query performance in Apache AsterixDB.\nOur experiments show significant performance gains, improving the query\nexecution time by orders of magnitude while minimally impacting ingestion\nperformance.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/2402.15425v1",
            "url": "http://arxiv.org/abs/2402.15425v1",
            "title": "Prime+Retouch: When Cache is Locked and Leaked",
            "year": "2024",
            "publicationDate": "2024-02-23 16:34:49+00:00",
            "abstract": "Caches on the modern commodity CPUs have become one of the major sources of\nside-channel leakages and been abused as a new attack vector. To thwart the\ncache-based side-channel attacks, two types of countermeasures have been\nproposed: detection-based ones that limit the amount of microarchitectural\ntraces an attacker can leave, and cache prefetching-and-locking techniques that\nclaim to prevent such leakage by disallowing evictions on sensitive data. In\nthis paper, we present the Prime+Retouch attack that completely bypasses these\ndefense schemes by accurately inferring the cache activities with the metadata\nof the cache replacement policy. Prime+Retouch has three noticeable properties:\n1) it incurs no eviction on the victim's data, allowing us to bypass the two\nknown mitigation schemes, 2) it requires minimal synchronization of only one\nmemory access to the attacker's pre-primed cache lines, and 3) it leaks data\nvia non-shared memory, yet because underlying eviction metadata is shared.\n  We demonstrate Prime+Retouch in two architectures: predominant Intel x86 and\nemerging Apple M1. We elucidate how Prime+Retouch can break the T-table\nimplementation of AES with robust cache side-channel mitigations such as Cloak,\nunder both normal and SGX-protected environments. We also manifest feasibility\nof the Prime+Retouch attack on the M1 platform imposing more restrictions where\nthe precise measurement tools such as core clock cycle timer and performance\ncounters are inaccessible to the attacker. Furthermore, we first demystify\nundisclosed cache architecture and its eviction policy of L1 data cache on\nApple M1 architecture. We also devise a user-space noise-free cache monitoring\ntool by repurposing Intel TSX.",
            "text": "Caches on the modern commodity CPUs have become one of the major sources of\nside-channel leakages and been abused as a new attack vector. To thwart the\ncache-based side-channel attacks, two types of countermeasures have been\nproposed: detection-based ones that limit the amount of microarchitectural\ntraces an attacker can leave, and cache prefetching-and-locking techniques that\nclaim to prevent such leakage by disallowing evictions on sensitive data. In\nthis paper, we present the Prime+Retouch attack that completely bypasses these\ndefense schemes by accurately inferring the cache activities with the metadata\nof the cache replacement policy. Prime+Retouch has three noticeable properties:\n1) it incurs no eviction on the victim's data, allowing us to bypass the two\nknown mitigation schemes, 2) it requires minimal synchronization of only one\nmemory access to the attacker's pre-primed cache lines, and 3) it leaks data\nvia non-shared memory, yet because underlying eviction metadata is shared.\n  We demonstrate Prime+Retouch in two architectures: predominant Intel x86 and\nemerging Apple M1. We elucidate how Prime+Retouch can break the T-table\nimplementation of AES with robust cache side-channel mitigations such as Cloak,\nunder both normal and SGX-protected environments. We also manifest feasibility\nof the Prime+Retouch attack on the M1 platform imposing more restrictions where\nthe precise measurement tools such as core clock cycle timer and performance\ncounters are inaccessible to the attacker. Furthermore, we first demystify\nundisclosed cache architecture and its eviction policy of L1 data cache on\nApple M1 architecture. We also devise a user-space noise-free cache monitoring\ntool by repurposing Intel TSX.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1208.4169v1",
            "url": "http://arxiv.org/abs/1208.4169v1",
            "title": "A Storage Advisor for Hybrid-Store Databases",
            "year": "2012",
            "publicationDate": "2012-08-21 02:53:09+00:00",
            "abstract": "With the SAP HANA database, SAP offers a high-performance in-memory\nhybrid-store database. Hybrid-store databases---that is, databases supporting\nrow- and column-oriented data management---are getting more and more prominent.\nWhile the columnar management offers high-performance capabilities for\nanalyzing large quantities of data, the row-oriented store can handle\ntransactional point queries as well as inserts and updates more efficiently. To\neffectively take advantage of both stores at the same time the novel question\nwhether to store the given data row- or column-oriented arises. We tackle this\nproblem with a storage advisor tool that supports database administrators at\nthis decision. Our proposed storage advisor recommends the optimal store based\non data and query characteristics; its core is a cost model to estimate and\ncompare query execution times for the different stores. Besides a per-table\ndecision, our tool also considers to horizontally and vertically partition the\ndata and manage the partitions on different stores. We evaluated the storage\nadvisor for the use in the SAP HANA database; we show the recommendation\nquality as well as the benefit of having the data in the optimal store with\nrespect to increased query performance.",
            "text": "With the SAP HANA database, SAP offers a high-performance in-memory\nhybrid-store database. Hybrid-store databases---that is, databases supporting\nrow- and column-oriented data management---are getting more and more prominent.\nWhile the columnar management offers high-performance capabilities for\nanalyzing large quantities of data, the row-oriented store can handle\ntransactional point queries as well as inserts and updates more efficiently. To\neffectively take advantage of both stores at the same time the novel question\nwhether to store the given data row- or column-oriented arises. We tackle this\nproblem with a storage advisor tool that supports database administrators at\nthis decision. Our proposed storage advisor recommends the optimal store based\non data and query characteristics; its core is a cost model to estimate and\ncompare query execution times for the different stores. Besides a per-table\ndecision, our tool also considers to horizontally and vertically partition the\ndata and manage the partitions on different stores. We evaluated the storage\nadvisor for the use in the SAP HANA database; we show the recommendation\nquality as well as the benefit of having the data in the optimal store with\nrespect to increased query performance.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1801.00390v1",
            "url": "http://arxiv.org/abs/1801.00390v1",
            "title": "Time Aware Least Recent Used (TLRU) Cache Management Policy in ICN",
            "year": "2018",
            "publicationDate": "2018-01-01 04:13:53+00:00",
            "abstract": "The information centric networks (ICN) can be viewed as a network of caches.\nConversely, ICN type of cache networks has distinctive features e.g, contents\npopularity, usability time of content and other factors inflicts some diverse\nrequirements for cache eviction policies. In this paper we defined four\nimportant characteristics of a suitable eviction policy for ICN. We analysed\nwell known eviction policies in view of defined characteristics. Based upon\nanalysis we propose a new eviction scheme which is well suitable for ICN type\nof cache networks.",
            "text": "The information centric networks (ICN) can be viewed as a network of caches.\nConversely, ICN type of cache networks has distinctive features e.g, contents\npopularity, usability time of content and other factors inflicts some diverse\nrequirements for cache eviction policies. In this paper we defined four\nimportant characteristics of a suitable eviction policy for ICN. We analysed\nwell known eviction policies in view of defined characteristics. Based upon\nanalysis we propose a new eviction scheme which is well suitable for ICN type\nof cache networks.",
            "citationCount": -1,
            "citation_counts": -1
        },
        {
            "paperId": "http://arxiv.org/abs/1901.01187v1",
            "url": "http://arxiv.org/abs/1901.01187v1",
            "title": "PopNetCod: A Popularity-based Caching Policy for Network Coding enabled Named Data Networking",
            "year": "2019",
            "publicationDate": "2019-01-04 16:02:41+00:00",
            "abstract": "In this paper, we propose PopNetCod, a popularity-based caching policy for\nnetwork coding enabled Named Data Networking. PopNetCod is a distributed\ncaching policy, in which each router measures the local popularity of the\ncontent objects by analyzing the requests that it receives. It then uses this\ninformation to decide which Data packets to cache or evict from its content\nstore. Since network coding is used, partial caching of content objects is\nsupported, which facilitates the management of the content store. The routers\ndecide the Data packets that they cache or evict in an online manner when they\nreceive requests for Data packets. This allows the most popular Data packets to\nbe cached closer to the network edges. The evaluation of PopNetCod shows an\nimproved cache-hit rate compared to the widely used Leave Copy Everywhere\nplacement policy and the Least Recently Used eviction policy. The improved\ncache-hit rate helps the clients to achieve higher goodput, while it also\nreduces the load on the source servers.",
            "text": "In this paper, we propose PopNetCod, a popularity-based caching policy for\nnetwork coding enabled Named Data Networking. PopNetCod is a distributed\ncaching policy, in which each router measures the local popularity of the\ncontent objects by analyzing the requests that it receives. It then uses this\ninformation to decide which Data packets to cache or evict from its content\nstore. Since network coding is used, partial caching of content objects is\nsupported, which facilitates the management of the content store. The routers\ndecide the Data packets that they cache or evict in an online manner when they\nreceive requests for Data packets. This allows the most popular Data packets to\nbe cached closer to the network edges. The evaluation of PopNetCod shows an\nimproved cache-hit rate compared to the widely used Leave Copy Everywhere\nplacement policy and the Least Recently Used eviction policy. The improved\ncache-hit rate helps the clients to achieve higher goodput, while it also\nreduces the load on the source servers.",
            "citationCount": -1,
            "citation_counts": -1
        }
    ],
    "answer": "",
    "input": "\nYour task is to write a Related Works section for an academic paper given the paper's abstract. Your response should provide the Related Works section and references. Only include references that are published before 2025-06-03T08:51:38+00:00. Mention them in a separate reference list at the end and cite them properly in the Related Works section. Here is the paper abstract:\nServing large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity..\n",
    "original_ctxs": [
        {
            "id": "6846355",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "a priori, the pre-allocation is still inefficient: As the entire chunk is reserved during the request's lifetime, other shorter requests cannot utilize any part of the chunk that is currently unused.Besides, external memory fragmentation can also be significant, since the preallocated size can be different for each request.Indeed, our profiling results in Fig. 2 show that only 20.4% -38.2% of the KV cache memory is used to store the actual token states in the existing systems. Second, the existing systems cannot exploit the opportunities for memory sharing.LLM services often use advanced decoding algorithms, such as parallel sampling and beam search, that generate multiple outputs per request.In these scenarios, the request consists of multiple sequences that can partially share their KV cache.However, memory sharing is not possible in the existing systems because the KV cache of the sequences is stored in separate contiguous spaces. To address the above limitations, we propose PagedAttention, an attention algorithm inspired by the operating system's (OS) solution to memory fragmentation and sharing: virtual memory with paging.PagedAttention divides the request's KV cache into blocks, each of which can contain the attention keys and values of a fixed number of tokens.In PagedAttention, the blocks for the KV cache are not necessarily stored in contiguous space.Therefore, we can manage the KV cache in a more flexible way as in OS's virtual memory: one can think of blocks as pages, tokens as bytes, and requests as processes.This design alleviates internal fragmentation by using relatively small blocks and allocating them on demand.Moreover, it eliminates external fragmentation as",
            "retrieval next text": "all blocks have the same size.Finally, it enables memory sharing at the granularity of a block, across the different sequences associated with the same request or even across the different requests. In this work, we build vLLM, a high-throughput distributed LLM serving engine on top of PagedAttention that achieves near-zero waste in KV cache memory.vLLM uses block-level memory management and preemptive request scheduling that are co-designed with PagedAttention.vLLM supports popular LLMs such as GPT [5], OPT [62], and LLaMA [52] with varying sizes, including the ones exceeding the memory capacity of a single GPU.Our evaluations on various models and workloads show that vLLM improves the LLM serving throughput by 2-4\u00d7 compared to the state-of-the-art systems [31,60], without affecting the model accuracy at all.The improvements are more pronounced with longer sequences, larger models, and more complex decoding algorithms ( \u00a74.3).In summary, we make the following contributions: \u2022 We identify the challenges in memory allocation in serving LLMs and quantify their impact on serving performance. Background In this section, we describe the generation and serving procedures of typical LLMs and the iteration-level scheduling used in LLM serving. Transformer-Based Large Language Models The task of language modeling is to model the probability of a list of tokens ( 1 , . . ., ).Since language has a natural sequential ordering, it is common to factorize the joint probability over the whole sequence as the product of conditional probabilities (a.k.a.autoregressive decomposition [3]): Transformers [53] have become the de facto standard architecture for modeling the probability above at a large scale.The",
            "retrieval score": "1.5632744",
            "text": "a priori, the pre-allocation is still inefficient: As the entire chunk is reserved during the request's lifetime, other shorter requests cannot utilize any part of the chunk that is currently unused.Besides, external memory fragmentation can also be significant, since the preallocated size can be different for each request.Indeed, our profiling results in Fig. 2 show that only 20.4% -38.2% of the KV cache memory is used to store the actual token states in the existing systems. Second, the existing systems cannot exploit the opportunities for memory sharing.LLM services often use advanced decoding algorithms, such as parallel sampling and beam search, that generate multiple outputs per request.In these scenarios, the request consists of multiple sequences that can partially share their KV cache.However, memory sharing is not possible in the existing systems because the KV cache of the sequences is stored in separate contiguous spaces. To address the above limitations, we propose PagedAttention, an attention algorithm inspired by the operating system's (OS) solution to memory fragmentation and sharing: virtual memory with paging.PagedAttention divides the request's KV cache into blocks, each of which can contain the attention keys and values of a fixed number of tokens.In PagedAttention, the blocks for the KV cache are not necessarily stored in contiguous space.Therefore, we can manage the KV cache in a more flexible way as in OS's virtual memory: one can think of blocks as pages, tokens as bytes, and requests as processes.This design alleviates internal fragmentation by using relatively small blocks and allocating them on demand.Moreover, it eliminates external fragmentation as",
            "title": ""
        },
        {
            "id": "6846356",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "all blocks have the same size.Finally, it enables memory sharing at the granularity of a block, across the different sequences associated with the same request or even across the different requests. In this work, we build vLLM, a high-throughput distributed LLM serving engine on top of PagedAttention that achieves near-zero waste in KV cache memory.vLLM uses block-level memory management and preemptive request scheduling that are co-designed with PagedAttention.vLLM supports popular LLMs such as GPT [5], OPT [62], and LLaMA [52] with varying sizes, including the ones exceeding the memory capacity of a single GPU.Our evaluations on various models and workloads show that vLLM improves the LLM serving throughput by 2-4\u00d7 compared to the state-of-the-art systems [31,60], without affecting the model accuracy at all.The improvements are more pronounced with longer sequences, larger models, and more complex decoding algorithms ( \u00a74.3).In summary, we make the following contributions: \u2022 We identify the challenges in memory allocation in serving LLMs and quantify their impact on serving performance. Background In this section, we describe the generation and serving procedures of typical LLMs and the iteration-level scheduling used in LLM serving. Transformer-Based Large Language Models The task of language modeling is to model the probability of a list of tokens ( 1 , . . ., ).Since language has a natural sequential ordering, it is common to factorize the joint probability over the whole sequence as the product of conditional probabilities (a.k.a.autoregressive decomposition [3]): Transformers [53] have become the de facto standard architecture for modeling the probability above at a large scale.The",
            "retrieval next text": "most important component of a Transformer-based language model is its self-attention layers.For an input hidden state sequence ( 1 , . . ., ) \u2208 R \u00d7 , a self-attention layer first applies linear transformations on each position to get the query, key, and value vectors: Then, the self-attention layer computes the attention score by multiplying the query vector at one position with all the key vectors before it and compute the output as the weighted average over the value vectors: Besides the computation in Eq. 4, all other components in the Transformer model, including the embedding layer, feed-forward layer, layer normalization [2], residual connection [22], output logit computation, and the query, key, and value transformation in Eq. 2, are all applied independently position-wise in a form of = ( ). LLM Service & Autoregressive Generation Once trained, LLMs are often deployed as a conditional generation service (e.g., completion API [34] or chatbot [19,35]).A request to an LLM service provides a list of input prompt tokens ( 1 , . . ., ), and the LLM service generates a list of output tokens ( +1 , . . ., + ) according to Eq. 1.We refer to the concatenation of the prompt and output lists as sequence. Due to the decomposition in Eq. 1, the LLM can only sample and generate new tokens one by one, and the generation process of each new token depends on all the previous tokens in that sequence, specifically their key and value vectors.In this sequential generation process, the key and",
            "retrieval score": "1.559174",
            "text": "all blocks have the same size.Finally, it enables memory sharing at the granularity of a block, across the different sequences associated with the same request or even across the different requests. In this work, we build vLLM, a high-throughput distributed LLM serving engine on top of PagedAttention that achieves near-zero waste in KV cache memory.vLLM uses block-level memory management and preemptive request scheduling that are co-designed with PagedAttention.vLLM supports popular LLMs such as GPT, OPT, and LLaMA with varying sizes, including the ones exceeding the memory capacity of a single GPU.Our evaluations on various models and workloads show that vLLM improves the LLM serving throughput by 2-4\u00d7 compared to the state-of-the-art systems,60, without affecting the model accuracy at all.The improvements are more pronounced with longer sequences, larger models, and more complex decoding algorithms ( \u00a74.3).In summary, we make the following contributions: \u2022 We identify the challenges in memory allocation in serving LLMs and quantify their impact on serving performance. Background In this section, we describe the generation and serving procedures of typical LLMs and the iteration-level scheduling used in LLM serving. Transformer-Based Large Language Models The task of language modeling is to model the probability of a list of tokens ( 1 , . . ., ).Since language has a natural sequential ordering, it is common to factorize the joint probability over the whole sequence as the product of conditional probabilities (a.k.a.autoregressive decomposition): Transformers have become the de facto standard architecture for modeling the probability above at a large scale.The",
            "title": ""
        },
        {
            "id": "5277768",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "this kind of memory architecture. We believe that this work increases the relevance of our proposal and shows how latency is one of the main problems to solve to have NVM architectures fully in use. \uf0b7 Also in section \"Prefetch Based on Hidden Markov Model\" we explain the on-chip memory hierarchy based on virtual address highlighting its advantages and disadvantages. Here we show that LLC analysis that relies on virtual addresses instead of physical addresses reduces drastically access frequency while providing enough information for a good prefetcher to catch algorithmic behavior and produce good prefetch accuracy. We've also added a comparation between the literature proposals and our HMM model highlighting our main differential proposal. Reviewer #1 Comments Latest approaches can be included in the related work \uf0b7 In related work we have included three recent works in hardware prefetching related to different features of LLC, big data and machine learning, irregular workloads and fabric memory integration: \u2022 [Choi21]: LLC prefetcher targeted to big data and machine learning processing that adapt granularities entailing cache lines and page granularity. Includes a DRAM buffer with an access history table and several prefetch engines to deal with irregular memory access patterns. \u2022 [Talati21]: hardware-software codesign solution to deal with very irregular workloads that uses static information from programs and dynamic from hardware to extract program semantic information used to generate prefetch requests. Our proposal succeeds in capturing irregular data access patterns without the use of compiler information from running programs. \u2022 [Kommareddy20]: decoupling of memory from computing is currently under",
            "retrieval next text": "research because improves data bandwidth when used with novel interconnection technologies. This kind of memory organization eases the task of integration between different technologies. The problem is the latency of memory access so the use of prefetch techniques has a very high impact in performance. \uf0b7 Also a more updated bibliography is included related to the use of hardware prefetching in modern processors. This is added in the introduction. We have included a work of hardware prefetching in the Intel Xeon Phi ([Sodari16]) and other in the IBM BlueGene/Q ([Haring12]). \uf0b7 Analysis or similar proposals like Domino [Bakhshlipour18] compared with our proposal is also included to explain how HMM Prefetch deals with more complex data relations on each of the groups it can simultenously identify (section \"Prefetch Based on Hidden Markov Model\"). Provide proof for the performance of HMM when complexity increases. In the section \"Evaluation\" we have improved the explanations and also we have created an specific section for multicore evaluation including new experiments with sixteen cores. We have also developed a new model to assess how OS-managed multiprogramming influences our prefetcher's ability to manage complexity. This model has a 4 core processor running 4 benchmarks each so multiprogrammed is needed to context switch between processes. With this new simulation model the complexity that time sharing adds to the off-chip access pattern can be evaluated. These modifications have been included to provide proofs ot the behavior of HMM Prefetch on more real and complex environments. We believe that now the evidence of how HMM works",
            "retrieval score": "1.5517921",
            "text": "this kind of memory architecture. We believe that this work increases the relevance of our proposal and shows how latency is one of the main problems to solve to have NVM architectures fully in use. \uf0b7 Also in section \"Prefetch Based on Hidden Markov Model\" we explain the on-chip memory hierarchy based on virtual address highlighting its advantages and disadvantages. Here we show that LLC analysis that relies on virtual addresses instead of physical addresses reduces drastically access frequency while providing enough information for a good prefetcher to catch algorithmic behavior and produce good prefetch accuracy. We've also added a comparation between the literature proposals and our HMM model highlighting our main differential proposal. Reviewer #1 Comments Latest approaches can be included in the related work \uf0b7 In related work we have included three recent works in hardware prefetching related to different features of LLC, big data and machine learning, irregular workloads and fabric memory integration: \u2022 [Choi21: LLC prefetcher targeted to big data and machine learning processing that adapt granularities entailing cache lines and page granularity. Includes a DRAM buffer with an access history table and several prefetch engines to deal with irregular memory access patterns. \u2022 [Talati21: hardware-software codesign solution to deal with very irregular workloads that uses static information from programs and dynamic from hardware to extract program semantic information used to generate prefetch requests. Our proposal succeeds in capturing irregular data access patterns without the use of compiler information from running programs. \u2022 [Kommareddy20: decoupling of memory from computing is currently under",
            "title": ""
        },
        {
            "id": "13461128",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "being accessed twice.While simple, heuristic-based policies are insufficient for CDN caching because access patterns for various services vary greatly, and these policies usually work well for some access patterns but poorly for others [24,32].Figure 1 summarizes six realistic CDN workloads (such as cloud album, e-commerce, and video) in the production CDN deployment we investigate.We present the object reuse time distribution for all six workloads; we refer object reuse time as the number of requests between two consecutive references to the same object, an important metric for characterizing cache access patterns.As can be seen, the reuse time distribution of each service varies greatly; this constrains simple heuristic-based admission policies from performing well in all workloads in a CDN setup, as demonstrated abundantly in the literature [28,32,42]. Second, \"learned admission policies\" [6,9,37] are a recent new type of approach for CDN.Instead of manually specifying rules, this approach trains a model with sample accesses, and the model uses the features of cache accesses to make the admission decision for each cache miss.Learned policies can automatically adapt to different access patterns, making them appealing to CDNs.Current learned policies, however, are insufficient for large-scale CDN deployment due to the dynamic nature of cache sizes allocated to each service. Reuse-Time in CDN Cache In a CDN cache, the file reuse time has a significant impact on the cache hit ratio, and predicting the reuse time of the currently missed file will substantially help make the right selection of cache admission.For the convenience of exposition, we first introduce the definition of reuse time.",
            "retrieval next text": "We define the reuse time of a file as the amount of files between two consecutive references to the same file.For example, assuming the access sequence is A-B-C-B-D-B-A-D, the reuse time of A is 5, as the files referenced between the two consecutive accesses to file A are file B, C, B, D and B. If the object reuse time is known exactly, the cache hit ratio will be optimal [5].Reuse time is also particularly important in cache admission.There are a large number of objects in the cache that have not been accessed again after being written to the cache (the reuse time of the object is greater than the cache lifetime 4 of the object).If these objects can be prevented from being written to the cache in advance, more hot objects can be stored in the cache, while also reducing the number of cache write operations and extending the life of SSDs.However, the multitenant scenario of CDN and the dynamically varying mixed workloads make it almost impossible SLAP 32:5 Fig. 2. Useful reuse time with different cache sizes.\"Usefulness\" means the object will be reused before eviction; as demonstrated, the useful reuse time varies with cache size.to accurately predict the reuse time.The prediction will be substantially simplified if the reuse time is classified into a specific range and only the category of the reuse time is predicted.Therefore, SLAP uses segmented reuse time in admission policy (see Section 3.2.1 for details).As shown in Figure 2, if the current cache size is A, then the reuse time in",
            "retrieval score": "1.5504428",
            "text": "being accessed twice.While simple, heuristic-based policies are insufficient for CDN caching because access patterns for various services vary greatly, and these policies usually work well for some access patterns but poorly for others,32.Figure 1 summarizes six realistic CDN workloads (such as cloud album, e-commerce, and video) in the production CDN deployment we investigate.We present the object reuse time distribution for all six workloads; we refer object reuse time as the number of requests between two consecutive references to the same object, an important metric for characterizing cache access patterns.As can be seen, the reuse time distribution of each service varies greatly; this constrains simple heuristic-based admission policies from performing well in all workloads in a CDN setup, as demonstrated abundantly in the literature,32,42. Second, \"learned admission policies\",9,37 are a recent new type of approach for CDN.Instead of manually specifying rules, this approach trains a model with sample accesses, and the model uses the features of cache accesses to make the admission decision for each cache miss.Learned policies can automatically adapt to different access patterns, making them appealing to CDNs.Current learned policies, however, are insufficient for large-scale CDN deployment due to the dynamic nature of cache sizes allocated to each service. Reuse-Time in CDN Cache In a CDN cache, the file reuse time has a significant impact on the cache hit ratio, and predicting the reuse time of the currently missed file will substantially help make the right selection of cache admission.For the convenience of exposition, we first introduce the definition of reuse time.",
            "title": ""
        },
        {
            "id": "6120686",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "workloads. INTRODUCTION Modern large-scale web services rely on caching extensively; inmemory key-value (KV) caches are placed between front-end services and back-end storage systems to achieve high throughput and overcome the long latency gap.In-memory KV caches are widely used and discussed in industry and research communities; Memcached [29] and Redis [23] are two in-memory caching solutions commonly deployed in many production environments.Large web service providers like Facebook and Twitter also developed their general-purpose caching frameworks, Cachelib [4] and Pelikan [42], respectively, to handle their caching use cases. The performance of these caching systems is largely impacted by its replacement/eviction policy, i.e., the algorithm that decides whether an item should be cached or evicted.The Least Recently Used (LRU) is one of the most commonly known replacement policies, which evicts an item based on the item's access recency.Despite its simplicity, the LRU policy has proven quite effective in many caching systems [4,9,32,41].There are also many other advanced eviction policies, such as [2,6,27,33,43], which make eviction decisions based on a combination of item's metadata.The effectiveness of an eviction policy primarily depends on two factors: First, the caching workload; With the rise of cloud and datadriven services, the diversity of in-memory caching workloads has grown drastically compared to the past [1,4,10,41].Many existing studies have shown that no existing heuristic-based eviction policies can consistently outperform others under every caching use case [27,33,43].As a result, many special purposes caching frameworks have adopted different replacement policies to accommodate different use cases [13,26,39,42]. Second, the underlying cache's structure and the storage medium also constrain the",
            "retrieval next text": "choice of replacement policy.For example, flashbased cache suffers from application-level write amplification (ALWA); even though FIFO does not deliver a good hit ratio, using FIFO helps avoid metadata updates on reads which reduces ALWA [15].Due to this, FIFO is often the top choice for replacement on a flash-based cache. Memcached and Redis use replacement policies based on more conservative heuristics to prevent poor performance on unpredictable extreme workload patterns.The early versions of Memcached used the traditional doubly-linked list implementation of LRU as default replacement [12].The LRU policy helps the cache retain the most recent data; however, the serialized LRU update procedures severely hindered Memcached's multithreaded scalability, especially on write-heavy workloads.To address the thread contention problem, the later version of Memcached (after 1.5.0)added a multi-queue LRU and asynchronous updates, named Segmented LRU (SegLRU), which significantly improved Memcached's performance on write-heavy workloads [12].Similar multi-list approaches are observed in other caching systems such as [4,35,43].On the other hand, Redis uses random sampling to approximate the true LRU replacement.On cache eviction, the random samplingbased LRU, or KLRU for short, randomly selects K objects from the cache and evicts the least recently used object among the K objects [22].Such sampling-based technique is commonly used in priority function-based replacement policies [2,6,22,31].Although both Memcached and Redis use similar replacement heuristics, they are implemented based on very different approaches, which motivates the need for a thorough comparison of their impacts on the in-memory caching system's performance.In this work, we implement Redis-like KLRU in Memcached.Then, we present a detailed comparison based on their impact on",
            "retrieval score": "1.5432034",
            "text": "workloads. INTRODUCTION Modern large-scale web services rely on caching extensively; inmemory key-value (KV) caches are placed between front-end services and back-end storage systems to achieve high throughput and overcome the long latency gap.In-memory KV caches are widely used and discussed in industry and research communities; Memcached and Redis are two in-memory caching solutions commonly deployed in many production environments.Large web service providers like Facebook and Twitter also developed their general-purpose caching frameworks, Cachelib and Pelikan, respectively, to handle their caching use cases. The performance of these caching systems is largely impacted by its replacement/eviction policy, i.e., the algorithm that decides whether an item should be cached or evicted.The Least Recently Used (LRU) is one of the most commonly known replacement policies, which evicts an item based on the item's access recency.Despite its simplicity, the LRU policy has proven quite effective in many caching systems,9,32,41.There are also many other advanced eviction policies, such as,6,27,33,43, which make eviction decisions based on a combination of item's metadata.The effectiveness of an eviction policy primarily depends on two factors: First, the caching workload; With the rise of cloud and datadriven services, the diversity of in-memory caching workloads has grown drastically compared to the past,4,10,41.Many existing studies have shown that no existing heuristic-based eviction policies can consistently outperform others under every caching use case,33,43.As a result, many special purposes caching frameworks have adopted different replacement policies to accommodate different use cases,26,39,42. Second, the underlying cache's structure and the storage medium also constrain the",
            "title": ""
        },
        {
            "id": "11456247",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient. Relevant code can be found at https://github.com/hdong920/LESS.",
            "retrieval next text": "High rate of tuberculin conversion in Indochinese refugees. From January 1979 through December 1980, a total of 664 Indochinese refugees were screened for tuberculosis in Monroe County, New York; 307 (46%) had positive 5-tuberculin unit PPD Mantoux tests. When 217 refugees with initially negative tests were retested at 60 days, 94 (43%) had converted from a negative to a positive test. Although none of the converters had evidence of active tuberculosis disease, 90 (96%) were given isoniazid preventive therapy. This conversion phenomenon has continued over time, discounting an effect of local, national, or international policy changes. The conversions do not seem to be a function of antigen, tester, or reader; criteria for testing, retesting, or reading; home country, camp country, or length of time in camp; or history of prior BCG vaccination, tuberculosis, or immunization exposure, current illness, or testing intervals since arrival in the United States and subsequent testing. Further studies to evaluate boosting or anergy are in progress to assess this potential public health problem.",
            "retrieval score": "1.5416313",
            "text": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient. Relevant code can be found at https://github.com/hdong920/LESS.",
            "title": ""
        },
        {
            "id": "9360507",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "the focus of this paper. Here we assume elastic resources, where cache eviction is not needed to make space for a new insertion, but rather to reduce cost by removing objects that are not expected to be requested again soon. A TTL-based eviction policy is a good heuristic for such purposes. Cloud service providers already provide elastic provisioning at varying granularities for computation and storage, and in the context of trends such as serverless computing, in-memory caching, and multi-access edge computing, we believe that support for fine-grained elasticity may increase in the future. In the past, selective cache insertion policies have been shown valuable in reducing cache pollution due to ephemeral content popularity and the long tail of one-timers observed in edge networks [6,20,24,27]. However, prior work has not bounded or optimized the worst-case delivery costs of such policies. In this paper, we first present novel worst-case bounds for the optimal cost and competitive cost-ratios of different variations of these policies. Second, we derive explicit average cost expressions and cost ratio bounds for these policies under arbitrary interrequest time distributions, assuming independent and identically distributed request times, as well as for specific short-tailed (deterministic, Erlang, and exponential) and heavy-tailed (Pareto) inter-request time distributions. Our analysis includes comparisons against both optimal offline policy bounds and, for the case when hazard rates are increasing or constant, optimal online policy bounds; all derived here. Finally, we present numeric and tracebased evaluations and provide insights into the relative cost performance of the policies. Our analysis reveals that window-based cache on",
            "retrieval next text": "M th request cache insertion policies can substantially outperform policies that do not take into account the recency of prior object requests when making cache insertion decisions. With window-based cache on M th request policies a counter is maintained for each uncached object that has been requested at least once within the last W time units. A newly allocated counter is initialized to one, and the counter is incremented by one whenever the object is referenced within W time units of its most recent previous request. The object is inserted into the cache whenever the counter reaches M. Our results show that a single parameter version of this policy can be used beneficially, in which W = T , and that the best worst-case bounds are achieved by selecting the window size W = T equal to the time that it takes to accumulate a cache storage cost (for that object) equal to the remote bandwidth cost R associated with a cache miss (for that object). With these protocol settings, the worst-case bounds of the window-based cache on M t h request policies have a competitive ratio of M + 1 (compared to the optimal offline policy). While these ratios at first may appear discouraging for larger M, our average case analysis for different inter-request time distributions clearly shows substantial cost benefits of using intermediate M such as 2-4, with the best choice depending on where in the parameter region the system operates. For less popular objects a slightly larger M (e.g., M = 4) may",
            "retrieval score": "1.5380677",
            "text": "the focus of this paper. Here we assume elastic resources, where cache eviction is not needed to make space for a new insertion, but rather to reduce cost by removing objects that are not expected to be requested again soon. A TTL-based eviction policy is a good heuristic for such purposes. Cloud service providers already provide elastic provisioning at varying granularities for computation and storage, and in the context of trends such as serverless computing, in-memory caching, and multi-access edge computing, we believe that support for fine-grained elasticity may increase in the future. In the past, selective cache insertion policies have been shown valuable in reducing cache pollution due to ephemeral content popularity and the long tail of one-timers observed in edge networks,20,24,27. However, prior work has not bounded or optimized the worst-case delivery costs of such policies. In this paper, we first present novel worst-case bounds for the optimal cost and competitive cost-ratios of different variations of these policies. Second, we derive explicit average cost expressions and cost ratio bounds for these policies under arbitrary interrequest time distributions, assuming independent and identically distributed request times, as well as for specific short-tailed (deterministic, Erlang, and exponential) and heavy-tailed (Pareto) inter-request time distributions. Our analysis includes comparisons against both optimal offline policy bounds and, for the case when hazard rates are increasing or constant, optimal online policy bounds; all derived here. Finally, we present numeric and tracebased evaluations and provide insights into the relative cost performance of the policies. Our analysis reveals that window-based cache on",
            "title": ""
        },
        {
            "id": "4290933",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "single GPU workload; they also propose GPU performance and power models for effective workload consolidation on GPUs. Joulemeter [15] that is initially designed as a tool for power usage measurement of virtual machines aims to consolidate multiple workloads on fewer servers for improving resource utilization and power costs. Analyzing the effect of last level processor cache (LLC) on workload consolidation -another topic of interest we investigate in this work-is also covered by several studies. For example, authors of [16] study the behavior of consolidated workloads particularly on sharing caches across a variety of configurations. In [10] authors also study shared resource monitoring to understand resource usage and ways to improve overall throughput as well as quality of service of a data center. A mathematical model has also been proposed to predict the effect of cache contention on the performance of consolidated workloads [17]. After close examination of all these works, we noticed several shortcomings and decided to cover them in this article; thus, we can highlight our contribution in this work through the following items. The first difference of our work with previously reported studies mainly lies in the way we characterize data-intensive applications with two main parameters: file size and request size; such characterization is inspired by well-known filesystem benchmarking tools, namely Iometer [18], IOzone [19], TestDFSIO [20], and Bonnie++ [21]. Our second contribution is related to the Hadoop distributed file system that has been never properly covered in previous studies -to the best of our knowledge. We believe this is the first work that",
            "retrieval next text": "thoroughly analyses inter-relationship between workload consolidation, throughput degradation, and LLC contention for dataintensive applications employing HDFS. Our third contribution is to propose mathematical models for different aspects of this study based on imperial results from TestDFSIO [20]. III. SINGLE WORKLOAD ON SINGLE SERVER In this section, we measure the throughput of a single workload on a single physical server. Here, we show that throughput is a function of file size (FS) and file operation request size (RS) of the workload. RS is the amount of data that workload reads/writes from/to a file in a single file operation. Our experimental results show that increasing FS beyond LLC size noticeably degrades the throughput of workload. A. Workload Characterization We conduct a series of experiments on two physical servers to capture the effect of FS and RS on throughput. The experiments are based on the intuition that data-intensive workloads can be characterized by FS and RS [18][19][20][21]. As expected, the throughput curves for all servers follow the same pattern by varying FS and RS for both read and write operation. Table I shows the experimental setup for these two servers (M 1 and M 2 ). On all physical servers, system file cache and file buffering are always activated in operating systems; thus, workloads always interact with system file cache rather than system disk. Through enabling this feature write-back cache always delays flushing file data until triggered by cache manager -usually at predefined time intervals. Note that although system file cache is a feature of native filesystem not that",
            "retrieval score": "1.5272238",
            "text": "single GPU workload; they also propose GPU performance and power models for effective workload consolidation on GPUs. Joulemeter that is initially designed as a tool for power usage measurement of virtual machines aims to consolidate multiple workloads on fewer servers for improving resource utilization and power costs. Analyzing the effect of last level processor cache (LLC) on workload consolidation -another topic of interest we investigate in this work-is also covered by several studies. For example, authors of study the behavior of consolidated workloads particularly on sharing caches across a variety of configurations. In authors also study shared resource monitoring to understand resource usage and ways to improve overall throughput as well as quality of service of a data center. A mathematical model has also been proposed to predict the effect of cache contention on the performance of consolidated workloads. After close examination of all these works, we noticed several shortcomings and decided to cover them in this article; thus, we can highlight our contribution in this work through the following items. The first difference of our work with previously reported studies mainly lies in the way we characterize data-intensive applications with two main parameters: file size and request size; such characterization is inspired by well-known filesystem benchmarking tools, namely Iometer, IOzone, TestDFSIO, and Bonnie++. Our second contribution is related to the Hadoop distributed file system that has been never properly covered in previous studies -to the best of our knowledge. We believe this is the first work that",
            "title": ""
        },
        {
            "id": "6846352",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "Efficient Memory Management for Large Language Model Serving with PagedAttention High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm Introduction The emergence of large language models (LLMs) like GPT [5,37] and PaLM [9] have enabled new applications such as programming assistants [6,18] and universal chatbots [19,35] that are starting to profoundly impact our work and daily routines.Many cloud companies [34,44] are racing to provide these applications as hosted services.However, running these applications is very expensive, requiring a large number of hardware accelerators such as GPUs.According to recent estimates, processing an LLM request can be 10\u00d7 more expensive than a traditional",
            "retrieval next text": "keyword query [43].Given these high costs, increasing the throughput-and hence reducing Throughput (token/s) Figure 1.Left: Memory layout when serving an LLM with 13B parameters on NVIDIA A100.The parameters (gray) persist in GPU memory throughout serving.The memory for the KV cache (red) is (de)allocated per serving request. A small amount of memory (yellow) is used ephemerally for activation.Right: vLLM smooths out the rapid growth curve of KV cache memory seen in existing systems [31,60], leading to a notable boost in serving throughput. the cost per request-of LLM serving systems is becoming more important.At the core of LLMs lies an autoregressive Transformer model [53].This model generates words (tokens), one at a time, based on the input (prompt) and the previous sequence of the output's tokens it has generated so far.For each request, this expensive process is repeated until the model outputs a termination token.This sequential generation process makes the workload memory-bound, underutilizing the computation power of GPUs and limiting the serving throughput. Improving the throughput is possible by batching multiple requests together.However, to process many requests in a batch, the memory space for each request should be efficiently managed.For example, Fig. 1 (left) illustrates the memory distribution for a 13B-parameter LLM on an NVIDIA A100 GPU with 40GB RAM.Approximately 65% of the memory is allocated for the model weights, which remain static during serving.Close to 30% of the memory is used to store the dynamic states of the requests.For Transformers, these states consist of the key and value tensors associated with the attention mechanism, commonly referred to as",
            "retrieval score": "1.5250677",
            "text": "Efficient Memory Management for Large Language Model Serving with PagedAttention High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm Introduction The emergence of large language models (LLMs) like GPT,37 and PaLM have enabled new applications such as programming assistants,18 and universal chatbots,35 that are starting to profoundly impact our work and daily routines.Many cloud companies,44 are racing to provide these applications as hosted services.However, running these applications is very expensive, requiring a large number of hardware accelerators such as GPUs.According to recent estimates, processing an LLM request can be 10\u00d7 more expensive than a traditional",
            "title": ""
        },
        {
            "id": "4425502",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "to underscore their features. Main Ideas of CBTs To get insights, we now discuss the essential ideas of architecture-level cache management which are used by various CBTs.Note that these ideas are not mutually exclusive. 2. Classifying accesses/warps for guiding bypassing: Some works classify the accesses, misses or warps into different categories to selectively bypass certain categories.Ahn et al. [13] classify the writes into dead-on-arrival fills, dead-value fills and closing writes (refer to Section 5.1).Wang et al. [53] classify the LLC write accesses into core-write (write to LLC through a higher-level write-through cache or eviction of dirty data from a higher-level writeback cache), write due to prefetch-miss and due to demand miss (refer to Section 6.1).Similarly, Chaudhuri et al. [45] classify cache blocks based on the number of reuses seen by it and its state at the time of eviction from L2, etc.In the work of Wang et al. [67], the LLC blocks which are frequently written back to memory in an access interval are termed as frequent writeback blocks and remaining blocks (either dirty or clean) are termed as infrequent writeback blocks. Collins et al. [10] classify the misses into conflict and capacity (which includes compulsory) misses.Tyson et al. [61] classify the misses based on whether they fetch useful or dead-on-arrival data.For GPUs, Wang et al. [68] classify the warps into locality warps and thrashing warps depending on the reuse pattern shown by them.Liang et al. [69] classify access patterns as partial or full sharing (few or all threads share the same data, respectively) and streaming",
            "retrieval next text": "pattern. Adaptive bypassing: Since bypassing all requests degrades performance, some techniques perform bypass only when no invalid block is available [21,45,64] or a no-reuse block is available [14,20] [9,10,36,49,65,66,72] or only tags [43,56] of the bypassed blocks.Access to the cache is avoided for the blocks found in these buffers and with effective bypassing algorithms, the size of these buffers are expected to be small [43,49].The bypassed blocks stored in the buffer may be moved to the main cache only if they show temporal reuse [9,49,73,74].Chou et al. [37] buffer tags of recently accessed adjacent DRAM cache lines.On a miss to last level SRAM cache, the request is first searched in this buffer and a hit result avoids the need of miss probe in DRAM cache.6. Granularity: Most techniques make prediction at the granularity of a block of size 64 B or 128 B. Stacked-DRAM cache designs may use 64 B block size [37] to reduce cache pollution or 4 KB block size [30,48] to reduce metadata overhead.By comparison, Alves et al. [23] predict when a sub-block (8 B) is dead, while Johnson and Hwu [65] make prediction at the level of a macroblock (1 KB) which consists of multiple adjacent blocks.Lee et al. [48] also discuss bypassing at superpage (2 MB to 1 GB) level (refer to Section 6.2).Khairy et al. [58] disable the entire cache and thus, all data bypass the cache (refer to Section 4.7).Use of larger granularity allows lowering the metadata overhead at the cost of reducing the accuracy of information collected about",
            "retrieval score": "1.5244057",
            "text": "to underscore their features. Main Ideas of CBTs To get insights, we now discuss the essential ideas of architecture-level cache management which are used by various CBTs.Note that these ideas are not mutually exclusive. 2. Classifying accesses/warps for guiding bypassing: Some works classify the accesses, misses or warps into different categories to selectively bypass certain categories.Ahn et al. classify the writes into dead-on-arrival fills, dead-value fills and closing writes (refer to Section 5.1).Wang et al. classify the LLC write accesses into core-write (write to LLC through a higher-level write-through cache or eviction of dirty data from a higher-level writeback cache), write due to prefetch-miss and due to demand miss (refer to Section 6.1).Similarly, Chaudhuri et al. classify cache blocks based on the number of reuses seen by it and its state at the time of eviction from L2, etc.In the work of Wang et al., the LLC blocks which are frequently written back to memory in an access interval are termed as frequent writeback blocks and remaining blocks (either dirty or clean) are termed as infrequent writeback blocks. Collins et al. classify the misses into conflict and capacity (which includes compulsory) misses.Tyson et al. classify the misses based on whether they fetch useful or dead-on-arrival data.For GPUs, Wang et al. classify the warps into locality warps and thrashing warps depending on the reuse pattern shown by them.Liang et al. classify access patterns as partial or full sharing (few or all threads share the same data, respectively) and streaming",
            "title": ""
        },
        {
            "id": "11986382",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "the analysis of LRU eviction policy spanning 15 years up to 2015.All that work done has been done for LRU only, whereas LFU is an alternative expiration policy, where few papers exist [1].The point being, a pragmatic and arguably more accurate eviction policy can be coded into 15 lines of code within an existing simulator.This abstract, and accompanying tool is intended to bring theory and simulation closer together, as there are clearly advantages to both approaches.Given a highly dynamical system such as network caching, we propose, there is a role for good visualization tools as well.The simulator is publicly available, with documentation from1 . Figure 1 : Figure 1: Inspection of a caching simulation. Figure 2 : Figure 2: Input data to a workload characterization. Figure 3 : Figure 3: Example hitrate performance of a networked cache.",
            "retrieval next text": "Collision dynamics of Skyrmions in a two-component Bose-Einstein condensate The dynamics of Skyrmions in a two-component Bose-Einstein condensate are numerically investigated in the mean-field theory. When two Skyrmions collide with each other, they are first united and then scattered into various states. For head-on collisions, Skyrmions with unit winding number are scattered. The collision dynamics with an impact parameter are shown to depend on the relative phase. These dynamic processes are characterized by integer winding numbers. I. INTRODUCTION A quantized vortex in a superfluid is a topological excitation that reflects the U(1) manifold of the orderparameter space. For multicomponent superfluids with larger degrees of freedom in the order-parameter space, there exist a rich variety of topological excitations, such as spin vortices, monopoles, and Skyrmions [1], which have been realized in a superfluid 3 He [2], Bose-Einstein condensates (BECs) of ultracold gases [3][4][5][6], and excitonpolariton superfluids [7,8]. When two or more topological excitations are generated in proximity to each other, they can exhibit interesting dynamics. The simplest example is a quantized vortex-antivortex pair (called a vortex dipole), which travels at a constant velocity in a uniform system. Such a topological object in a BEC has been studied theoretically [9,10] and realized experimentally [11][12][13][14]. A pair of quantized vortices with the same circulation rotate around one another [15,16]. When two quantized vortex lines approach each other, they interact with one another [17], and a reconnection occurs [18,19]. Two copropagating quantized vortex rings show leapfrogging dynamics [20,21], such as those in classical fluids, and when they collide with each",
            "retrieval score": "1.523025",
            "text": "the analysis of LRU eviction policy spanning 15 years up to 2015.All that work done has been done for LRU only, whereas LFU is an alternative expiration policy, where few papers exist.The point being, a pragmatic and arguably more accurate eviction policy can be coded into 15 lines of code within an existing simulator.This abstract, and accompanying tool is intended to bring theory and simulation closer together, as there are clearly advantages to both approaches.Given a highly dynamical system such as network caching, we propose, there is a role for good visualization tools as well.The simulator is publicly available, with documentation from1 . Figure 1 : Figure 1: Inspection of a caching simulation. Figure 2 : Figure 2: Input data to a workload characterization. Figure 3 : Figure 3: Example hitrate performance of a networked cache.",
            "title": ""
        },
        {
            "id": "2816832",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "of admission and eviction candidates. An efficient ML adaptation approach is evaluated by [25], which adapts to LRU-and LFU-friendly workloads. Performance analysis of multi-layer machine learning algorithms is an open future research topic. Nonetheless, the knapsack bounds of Sections 3 and 4 are generally applicable for basic as well as advanced ML strategies. GENERAL BOUNDS ON OPTIMUM CACHING The previous bounds on hit and value ratios do not cover all types of correlated request sequences. When decisions on cache updates can anticipate the request sequence with knowledge about future requests, then Belady's algorithm [7][86] [90] achieves the maximum cache hit ratio for arbitrary request pattern. Even if clairvoyant caching is unrealistic for most applications, Belady's upper bound is a common reference for caching performance. The bound indicates the potential for improvement by prediction or by using partial information about future requests [55]. In case of program code optimization, the data reference sequence is often known as a precondition to apply a clairvoyant caching strategy [68], but future requests are usually unknown in web caching. Belady's algorithm follows a \"farthest next request first\" eviction principle. Therefore, objects in the cache are sorted due to the time until their next request. In case of a cache hit, the next request time and the implied new object rank is updated. Upon a cache miss, the requested object is inserted into the cache, if and only if its next request time comes before the next request time of a cached object. In this way, Belady's algorithm performs content selection based",
            "retrieval next text": "on the next request times of the objects as score criterion. However, Belady's bound only applies for unit object size and for the hit count. Extensions for web caches with objects of different size and regarding caching values have not been addressed before 2018 [11] [59], as to the authors' knowledge. Extended knapsack solutions to obtain maximum caching value are NP-hard, but standard knapsack heuristics are used to obtain upper and lower bounds around the maximum. 2D-Knapsack Solution for Maximum Caching Value We specify optimum clairvoyant caching with size and value per data as a 2-dimensional (2D-)knapsack solution [19][59] \u2022 with a storage dimension limited by the cache size M and \u2022 with a time dimension, which refers to the index of the request sequence r1 , r2 , \u2026, rR of length R. A request rn is a hit, if the requested object Orn was stored in the cache during the inter-request interval I = (m, n) since its previous request rm (Or m = Orn and Or p \u2260 Or m for m < p < n). Therefore, space must be blocked in the 2D-knapsack to realize a cache hit in the format of a rectangle, with size nm in time and sr n in the storage dimension (sr n : size of Orn ). In general, a rectangle (nm) \u00d7 sr n represents the required cache occupation to be placed in the 2D-knapsack per cache hit. Then the maximum hit count for clairvoyant caching is equal to the maximum number of rectangles which",
            "retrieval score": "1.5226508",
            "text": "of admission and eviction candidates. An efficient ML adaptation approach is evaluated by, which adapts to LRU-and LFU-friendly workloads. Performance analysis of multi-layer machine learning algorithms is an open future research topic. Nonetheless, the knapsack bounds of Sections 3 and 4 are generally applicable for basic as well as advanced ML strategies. GENERAL BOUNDS ON OPTIMUM CACHING The previous bounds on hit and value ratios do not cover all types of correlated request sequences. When decisions on cache updates can anticipate the request sequence with knowledge about future requests, then Belady's algorithm achieves the maximum cache hit ratio for arbitrary request pattern. Even if clairvoyant caching is unrealistic for most applications, Belady's upper bound is a common reference for caching performance. The bound indicates the potential for improvement by prediction or by using partial information about future requests. In case of program code optimization, the data reference sequence is often known as a precondition to apply a clairvoyant caching strategy, but future requests are usually unknown in web caching. Belady's algorithm follows a \"farthest next request first\" eviction principle. Therefore, objects in the cache are sorted due to the time until their next request. In case of a cache hit, the next request time and the implied new object rank is updated. Upon a cache miss, the requested object is inserted into the cache, if and only if its next request time comes before the next request time of a cached object. In this way, Belady's algorithm performs content selection based",
            "title": ""
        },
        {
            "id": "1291256",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "scheduler overheads in this scenario.The extent of the interference typically governs what kind of SLAs (99 percentile, worst-case performance scenarios) storage service providers can offer to their tenants.KF-3 and KF-4: Kyber and BFQ can provide good bounded performance in the presence of interference when the CPU is not the bottleneck, however, BFQ suffers from performance scalability overheads.Hence, we conclude that overall Kyber is the best fit Linux I/O schedulers for SSDs ( \u00a75). (RQ4) How do a scheduler configuration parameters affect the schedulers' behavior on competing workloads?The Linux I/O schedulers provide tunable parameters, which affect the schedulers' behavior.Based on our empirical findings, we further present a detailed analysis of the Kyber scheduler which is specifically designed for modern SSDs.It has two unique configuration parameters: read and write target latencies.We perform a configuration space exploration for Kyber.KF-5: Kyber can be configured to prioritize latency or total throughput by tuning its read and write target latencies, but not both ( \u00a76). Our key contributions in this work include: \u2022 To the best of our knowledge, this is the first-of-its-kind systematic study about overhead quantification and characterization of state-of-the-practice I/O schedulers with modern NVMe SSDs, exploring their performance, scalability, and interference patterns, resulting in 23 observations and 5 key findings.\u2022 We explore the configuration space of Kyber, an SSD-optimized scheduler.We report that Kyber has the least amount of CPU overhead, and it can provide bounded performance in the presence of read/write interference. BENCHMARKING ENVIRONMENT In this section, we present details about the benchmarking environment, workloads used, and selected Linux",
            "retrieval next text": "storage schedulers. Hardware and Software We use fio [4] as the workload generator with the io_uring I/O engine [14,19,51].Our setup is able to deliver a peak random read performance of 3.4 Million IOPS with the Linux storage stack under Linux v6.3.8 (5.9 Million IOPS with SPDK that by-passes the kernel), and an average read latency of 68 s (4KiB, with a queue depth of 1, QD=1); hence, creating a unique opportunity to study I/O schedulers in this high-performance I/O setup.We use three metrics to evaluate the performance of the Linux I/O schedulers: throughput, latency, and CPU usage.We measure the throughput as I/O operations completed per second (IOPS).Latency measurements focus on the tail latency where we report 99 percentile latencies (P99) with a complete CDF distribution.The average CPU usage is measured using fio, which reports \"the CPU time used by the process/run time\".CPU usage = 1 means that the process uses a whole CPU core.fio gets the resource usage with the Linux getrusage system call [6].We precondition the flash SSDs according to [18], by sequentially writing the entire device, then randomly writing 4KiB blocks with a total of 2TiB data.Each device can deliver 1 MIOPS@4KiB random read according to the specification.However, we only get \u223c770 KIOPS with 4KiB random read workload after the preconditioning. Workload Patterns and Methodology We focus on two kinds of applications in this work, latency-sensitive and throughput-bound.We use L-app to represent latency-sensitive applications such as database queries and T-app to represent throughputbound applications such as batch processing jobs like Map-Reduce.We use three kinds",
            "retrieval score": "1.5196912",
            "text": "scheduler overheads in this scenario.The extent of the interference typically governs what kind of SLAs (99 percentile, worst-case performance scenarios) storage service providers can offer to their tenants.KF-3 and KF-4: Kyber and BFQ can provide good bounded performance in the presence of interference when the CPU is not the bottleneck, however, BFQ suffers from performance scalability overheads.Hence, we conclude that overall Kyber is the best fit Linux I/O schedulers for SSDs ( \u00a75). (RQ4) How do a scheduler configuration parameters affect the schedulers' behavior on competing workloads?The Linux I/O schedulers provide tunable parameters, which affect the schedulers' behavior.Based on our empirical findings, we further present a detailed analysis of the Kyber scheduler which is specifically designed for modern SSDs.It has two unique configuration parameters: read and write target latencies.We perform a configuration space exploration for Kyber.KF-5: Kyber can be configured to prioritize latency or total throughput by tuning its read and write target latencies, but not both ( \u00a76). Our key contributions in this work include: \u2022 To the best of our knowledge, this is the first-of-its-kind systematic study about overhead quantification and characterization of state-of-the-practice I/O schedulers with modern NVMe SSDs, exploring their performance, scalability, and interference patterns, resulting in 23 observations and 5 key findings.\u2022 We explore the configuration space of Kyber, an SSD-optimized scheduler.We report that Kyber has the least amount of CPU overhead, and it can provide bounded performance in the presence of read/write interference. BENCHMARKING ENVIRONMENT In this section, we present details about the benchmarking environment, workloads used, and selected Linux",
            "title": ""
        },
        {
            "id": "6846354",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "KV cache [41], which represent the context from earlier tokens to generate new output tokens in sequence.The percentage of memory is used for other data, including activations -the ephemeral tensors created when evaluating the LLM.Since the model weights are constant and the activations only occupy a small fraction of the GPU memory, the way the KV cache is managed is critical in determining the maximum batch size.When managed inefficiently, the KV cache memory can significantly limit the batch size and consequently the throughput of the LLM, as illustrated in Fig. 1 (right). In this paper, we observe that existing LLM serving systems [31,60] fall short of managing the KV cache memory efficiently.This is mainly because they store the KV cache of a request in contiguous memory space, as most deep learning frameworks [33,39] require tensors to be stored in contiguous memory.However, unlike the tensors in the traditional deep learning workloads, the KV cache has unique characteristics: it dynamically grows and shrinks over time as the model generates new tokens, and its lifetime and length are not known a priori.These characteristics make the existing systems' approach significantly inefficient in two ways: First, the existing systems [31,60] suffer from internal and external memory fragmentation.To store the KV cache of a request in contiguous space, they pre-allocate a contiguous chunk of memory with the request's maximum length (e.g., 2048 tokens).This can result in severe internal fragmentation, since the request's actual length can be much shorter than its maximum length (e.g., Fig. 11).Moreover, even if the actual length is known",
            "retrieval next text": "a priori, the pre-allocation is still inefficient: As the entire chunk is reserved during the request's lifetime, other shorter requests cannot utilize any part of the chunk that is currently unused.Besides, external memory fragmentation can also be significant, since the preallocated size can be different for each request.Indeed, our profiling results in Fig. 2 show that only 20.4% -38.2% of the KV cache memory is used to store the actual token states in the existing systems. Second, the existing systems cannot exploit the opportunities for memory sharing.LLM services often use advanced decoding algorithms, such as parallel sampling and beam search, that generate multiple outputs per request.In these scenarios, the request consists of multiple sequences that can partially share their KV cache.However, memory sharing is not possible in the existing systems because the KV cache of the sequences is stored in separate contiguous spaces. To address the above limitations, we propose PagedAttention, an attention algorithm inspired by the operating system's (OS) solution to memory fragmentation and sharing: virtual memory with paging.PagedAttention divides the request's KV cache into blocks, each of which can contain the attention keys and values of a fixed number of tokens.In PagedAttention, the blocks for the KV cache are not necessarily stored in contiguous space.Therefore, we can manage the KV cache in a more flexible way as in OS's virtual memory: one can think of blocks as pages, tokens as bytes, and requests as processes.This design alleviates internal fragmentation by using relatively small blocks and allocating them on demand.Moreover, it eliminates external fragmentation as",
            "retrieval score": "1.5183241",
            "text": "KV cache, which represent the context from earlier tokens to generate new output tokens in sequence.The percentage of memory is used for other data, including activations -the ephemeral tensors created when evaluating the LLM.Since the model weights are constant and the activations only occupy a small fraction of the GPU memory, the way the KV cache is managed is critical in determining the maximum batch size.When managed inefficiently, the KV cache memory can significantly limit the batch size and consequently the throughput of the LLM, as illustrated in Fig. 1 (right). In this paper, we observe that existing LLM serving systems,60 fall short of managing the KV cache memory efficiently.This is mainly because they store the KV cache of a request in contiguous memory space, as most deep learning frameworks,39 require tensors to be stored in contiguous memory.However, unlike the tensors in the traditional deep learning workloads, the KV cache has unique characteristics: it dynamically grows and shrinks over time as the model generates new tokens, and its lifetime and length are not known a priori.These characteristics make the existing systems' approach significantly inefficient in two ways: First, the existing systems,60 suffer from internal and external memory fragmentation.To store the KV cache of a request in contiguous space, they pre-allocate a contiguous chunk of memory with the request's maximum length (e.g., 2048 tokens).This can result in severe internal fragmentation, since the request's actual length can be much shorter than its maximum length (e.g., Fig. 11).Moreover, even if the actual length is known",
            "title": ""
        },
        {
            "id": "9048321",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "It's Time to Revisit LRU vs. FIFO We revisit the question of the effectiveness of the popular LRU cache eviction policy versus the FIFO heuristic which attempts to give an LRU like behavior. Several past works have considered this question and commonly stipulated that while FIFO is much easier to implement, the improved hit ratio of LRU outweighs this. We claim that two main trends call for a reevaluation: new caches such as front-ends to cloud storage have very large scales and this makes managing cache metadata in RAM no longer feasible; and new workloads have emerged that possess different characteristics. We model the overall cost of running LRU and FIFO in a very large scale cache and evaluate this cost using a number of publicly available traces. Our main evaluation workload is a new set of traces that we collected from a large public cloud object storage service and on this new trace FIFO exhibits better overall cost than LRU. We hope that these observations reignite the evaluation of cache eviction policies under new circumstances and that the new traces, that we intend to make public, serve as a testing ground for such work.",
            "retrieval next text": "The Biology of Bone Metastasis. Bone metastasis, or the development of secondary tumors within the bone of cancer patients, is a debilitating and incurable disease. Despite its morbidity, the biology of bone metastasis represents one of the most complex and intriguing of all oncogenic processes. This complexity derives from the intricately organized bone microenvironment in which the various stages of hematopoiesis, osteogenesis, and osteolysis are jointly regulated but spatially restricted. Disseminated tumor cells (DTCs) from various common malignancies such as breast, prostate, lung, and kidney cancers or myeloma are uniquely primed to subvert these endogenous bone stromal elements to grow into pathological osteolytic or osteoblastic lesions. This colonization process can be separated into three key steps: seeding, dormancy, and outgrowth. Targeting the processes of dormancy and initial outgrowth offers the most therapeutic promise. Here, we discuss the concepts of the bone metastasis niche, from controlling tumor-cell survival to growth into clinically detectable disease.",
            "retrieval score": "1.5168312",
            "text": "It's Time to Revisit LRU vs. FIFO We revisit the question of the effectiveness of the popular LRU cache eviction policy versus the FIFO heuristic which attempts to give an LRU like behavior. Several past works have considered this question and commonly stipulated that while FIFO is much easier to implement, the improved hit ratio of LRU outweighs this. We claim that two main trends call for a reevaluation: new caches such as front-ends to cloud storage have very large scales and this makes managing cache metadata in RAM no longer feasible; and new workloads have emerged that possess different characteristics. We model the overall cost of running LRU and FIFO in a very large scale cache and evaluate this cost using a number of publicly available traces. Our main evaluation workload is a new set of traces that we collected from a large public cloud object storage service and on this new trace FIFO exhibits better overall cost than LRU. We hope that these observations reignite the evaluation of cache eviction policies under new circumstances and that the new traces, that we intend to make public, serve as a testing ground for such work.",
            "title": ""
        },
        {
            "id": "9699213",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "1(c) plots the access frequency distribution for all objects larger than 10 MB. About 30% of large objects are accessed at least 10 times, and the object popularity shows a long-tail distribution, with the most popular objects absorbing more than 10 4 accesses. Figure 1(d) shows the temporal reuse patterns of the large object workloads. Around 37%-46% large objects are reused within 1 hour since the last time they were accessed. The strong temporal locality patterns underscore the benefit for caching large objects for web applications. Building a Memory Cache on Cloud Functions: Opportunities and Challenges The above observations lead to an important question to the storage system designers and cluster administrators: can we build a new cloud caching model that relieves the tension between performance and cost while serving large objects in a cost-effective manner? We argue that what is missing is a truly elastic cloud storage service model that charges tenants in a request driven mode instead of capacity usage, which the emerging serverless computing naturally enables, with the following desirable properties: Pay-Per-Use Pricing: FaaS providers (including AWS Lambda [2], Google Cloud Functions [7], Microsoft Azure Functions [4], and IBM Cloud Functions [8]) charge users at a fine granularity -for example, AWS Lambda bills on a perinvocation basis ($0.02 per 1 million invocations) and charges (CPU and memory bundle) resource usage by rounding up the function's execution time to the nearest 100 milliseconds with a rate of $0.0000166667 per second for each GB of RAM. Note the function startup cost is not billed, and",
            "retrieval next text": "does not count for its execution time. Large object IMOC workloads can take advantage of this fine-grained pay-as-you-go pricing model to keep the tenant's monetary costs low. Short-Term Caching: More importantly, FaaS providers keep functions \"warm\" by caching their state in memory for a short period of time to mitigate the \"cold-start\" penalty 1 [15,43,54]. Functions that are not invoked for a while can be reclaimed by the provider, and the state stored in the functions is lost. The duration of the \"warm\" period may vary (ranging from tens of minutes to longer than 6 hours as observed in \u00a74.1) for AWS Lambda, and largely depends on how frequently the Lambda function gets invoked. 1 \"Cold start\" refers to the first-ever invocation of a function instance. Ideally, a cloud tenant can leverage the above properties naturally enabled by a FaaS provider to build an opportunistic IMOC on a serverless platform. As such, a naive design would simply invoke a cloud function and store objects into the function's memory until the function is reclaimed by the provider, and then re-insert the objects into a new function. This approach is appealing for several reasons. First and foremost, it inherently redefines the pay-as-you-go pricing model in the context of storage (in our case memory cache storage) by realizing a new form of memory elasticity -the memory capacity used to cache an object is billed only when there is a request hitting that object. This significantly differentiates the proposed cache model against conventional cloud storage or cache services, which start",
            "retrieval score": "1.5168048",
            "text": "1(c) plots the access frequency distribution for all objects larger than 10 MB. About 30% of large objects are accessed at least 10 times, and the object popularity shows a long-tail distribution, with the most popular objects absorbing more than 10 4 accesses. Figure 1(d) shows the temporal reuse patterns of the large object workloads. Around 37%-46% large objects are reused within 1 hour since the last time they were accessed. The strong temporal locality patterns underscore the benefit for caching large objects for web applications. Building a Memory Cache on Cloud Functions: Opportunities and Challenges The above observations lead to an important question to the storage system designers and cluster administrators: can we build a new cloud caching model that relieves the tension between performance and cost while serving large objects in a cost-effective manner? We argue that what is missing is a truly elastic cloud storage service model that charges tenants in a request driven mode instead of capacity usage, which the emerging serverless computing naturally enables, with the following desirable properties: Pay-Per-Use Pricing: FaaS providers (including AWS Lambda, Google Cloud Functions, Microsoft Azure Functions, and IBM Cloud Functions) charge users at a fine granularity -for example, AWS Lambda bills on a perinvocation basis ($0.02 per 1 million invocations) and charges (CPU and memory bundle) resource usage by rounding up the function's execution time to the nearest 100 milliseconds with a rate of $0.0000166667 per second for each GB of RAM. Note the function startup cost is not billed, and",
            "title": ""
        },
        {
            "id": "10035787",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "the online paging problem, where there is a system with a small fast memory (the cache) and a big slow memory. Data is organized into blocks called pages, with up to k different pages fitting in the cache at once. A page request results in either a cache hit (if the page is already in the cache) or a cache miss (if not). On a cache miss, the requested page must be brought into the cache. If the cache is already full, then some page in it must be evicted. A cache replacement policy is an algorithm for making these eviction decisions. Any systems textbook will recommend aspiring to the least recently used (LRU) policy, which evicts the page whose most recent reference is furthest in the past. The same textbook will explain why: Real-world page request sequences tend to exhibit locality of reference, meaning that recently requested pages are likely to be requested again soon. The LRU policy uses the recent past as a prediction for the near future. Empirically, it typically suffers fewer cache misses than competing policies like first-in first-out (FIFO). Worst-case analysis, straightforwardly applied, provides no useful insights about the performance of different cache replacement policies. For every deterministic policy and cache size k, there is a pathological page request sequence that triggers a page fault rate of 100%, even though the optimal clairvoyant replacement policy (known as B\u00e9l\u00e1dy's furthest-in-the-future algorithm) would have a page fault rate of at most 1/k (Exercise 1). This observation is troublesome both for its absurdly pessimistic",
            "retrieval next text": "performance prediction and for its failure to differentiate between competing replacement policies (like LRU vs. FIFO). One solution, described in Section 3, is to choose an appropriately fine-grained parameterization of the input space and to assess and compare algorithms using parameterized guarantees. The Cons of Worst-Case Analysis We should celebrate the fact that worst-case analysis works so well for so many fundamental computational problems, while at the same time recognizing that the cherrypicked successes highlighted in undergraduate algorithms can paint a potentially misleading picture about the range of its practical relevance. The preceding four examples highlight the chief weaknesses of the worst-case analysis framework. 1. Overly pessimistic performance predictions. By design, worst-case analysis gives a pessimistic estimate of an algorithm's empirical performance. In the preceding four examples, the gap between the two is embarrassingly large. 2. Can rank algorithms inaccurately. Overly pessimistic performance summaries can derail worst-case analysis from identifying the right algorithm to use in practice. In the online paging problem, it cannot distinguish between the FIFO and LRU policies; for linear programming, it implicitly suggests that the ellipsoid method is superior to the simplex method. 3. No data model. If worst-case analysis has an implicit model of data, then it's the \"Murphy's Law\" data model, where the instance to be solved is an adversarially selected function of the chosen algorithm. 7 Outside of security applications, this algorithm-dependent model of data is a rather paranoid and incoherent way to think about a computational problem. In many applications, the algorithm of choice is superior precisely because",
            "retrieval score": "1.5086011",
            "text": "the online paging problem, where there is a system with a small fast memory (the cache) and a big slow memory. Data is organized into blocks called pages, with up to k different pages fitting in the cache at once. A page request results in either a cache hit (if the page is already in the cache) or a cache miss (if not). On a cache miss, the requested page must be brought into the cache. If the cache is already full, then some page in it must be evicted. A cache replacement policy is an algorithm for making these eviction decisions. Any systems textbook will recommend aspiring to the least recently used (LRU) policy, which evicts the page whose most recent reference is furthest in the past. The same textbook will explain why: Real-world page request sequences tend to exhibit locality of reference, meaning that recently requested pages are likely to be requested again soon. The LRU policy uses the recent past as a prediction for the near future. Empirically, it typically suffers fewer cache misses than competing policies like first-in first-out (FIFO). Worst-case analysis, straightforwardly applied, provides no useful insights about the performance of different cache replacement policies. For every deterministic policy and cache size k, there is a pathological page request sequence that triggers a page fault rate of 100%, even though the optimal clairvoyant replacement policy (known as B\u00e9l\u00e1dy's furthest-in-the-future algorithm) would have a page fault rate of at most 1/k (Exercise 1). This observation is troublesome both for its absurdly pessimistic",
            "title": ""
        },
        {
            "id": "4715307",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.",
            "retrieval next text": "An improved algorithm for fault location in distribution network Fault location is a difficult problem when a single-phase earth fault occurs in distribution network in China. The signal injection method is the most effective way of localizing the single-phase earth fault at present. But it can not automatically localize fault presently. An algorithm of automatically localizing the fault point based on the signal injection method is proposed. It takes the signal detectors fixed to FTU along the fault feeder as nodes. The assigned positive direction is that the injected current is from bus-bar to fault point. The network describing matrix C is formed by the assigned positive directions. The information vector S of injected signal is constructed according to the detected results from each detector of the injected signal. The zero sequence fault current information vector F is constructed according to the information of zero sequence fault current from each FTU. The fault information vector G is formed by integrating vector S with vector F. The fault judgment matrix D is formed by adding the elements of vector G to the diagonal of the describing matrix C. The failure criterion and the algorithmic process of automatically localizing the fault point are proposed according to the fault judgment matrix D. Thus the automatic fault location is implemented. The algorithm is simple. And the automatic fault location based on the signal injection method is implemented. This algorithm applies in the tree fault feeders which have branches or not. Case simulation results show that the proposed algorithm is correct,",
            "retrieval score": "1.5081",
            "text": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.",
            "title": ""
        },
        {
            "id": "6846353",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "keyword query [43].Given these high costs, increasing the throughput-and hence reducing Throughput (token/s) Figure 1.Left: Memory layout when serving an LLM with 13B parameters on NVIDIA A100.The parameters (gray) persist in GPU memory throughout serving.The memory for the KV cache (red) is (de)allocated per serving request. A small amount of memory (yellow) is used ephemerally for activation.Right: vLLM smooths out the rapid growth curve of KV cache memory seen in existing systems [31,60], leading to a notable boost in serving throughput. the cost per request-of LLM serving systems is becoming more important.At the core of LLMs lies an autoregressive Transformer model [53].This model generates words (tokens), one at a time, based on the input (prompt) and the previous sequence of the output's tokens it has generated so far.For each request, this expensive process is repeated until the model outputs a termination token.This sequential generation process makes the workload memory-bound, underutilizing the computation power of GPUs and limiting the serving throughput. Improving the throughput is possible by batching multiple requests together.However, to process many requests in a batch, the memory space for each request should be efficiently managed.For example, Fig. 1 (left) illustrates the memory distribution for a 13B-parameter LLM on an NVIDIA A100 GPU with 40GB RAM.Approximately 65% of the memory is allocated for the model weights, which remain static during serving.Close to 30% of the memory is used to store the dynamic states of the requests.For Transformers, these states consist of the key and value tensors associated with the attention mechanism, commonly referred to as",
            "retrieval next text": "KV cache [41], which represent the context from earlier tokens to generate new output tokens in sequence.The percentage of memory is used for other data, including activations -the ephemeral tensors created when evaluating the LLM.Since the model weights are constant and the activations only occupy a small fraction of the GPU memory, the way the KV cache is managed is critical in determining the maximum batch size.When managed inefficiently, the KV cache memory can significantly limit the batch size and consequently the throughput of the LLM, as illustrated in Fig. 1 (right). In this paper, we observe that existing LLM serving systems [31,60] fall short of managing the KV cache memory efficiently.This is mainly because they store the KV cache of a request in contiguous memory space, as most deep learning frameworks [33,39] require tensors to be stored in contiguous memory.However, unlike the tensors in the traditional deep learning workloads, the KV cache has unique characteristics: it dynamically grows and shrinks over time as the model generates new tokens, and its lifetime and length are not known a priori.These characteristics make the existing systems' approach significantly inefficient in two ways: First, the existing systems [31,60] suffer from internal and external memory fragmentation.To store the KV cache of a request in contiguous space, they pre-allocate a contiguous chunk of memory with the request's maximum length (e.g., 2048 tokens).This can result in severe internal fragmentation, since the request's actual length can be much shorter than its maximum length (e.g., Fig. 11).Moreover, even if the actual length is known",
            "retrieval score": "1.5080912",
            "text": "keyword query.Given these high costs, increasing the throughput-and hence reducing Throughput (token/s) Figure 1.Left: Memory layout when serving an LLM with 13B parameters on NVIDIA A100.The parameters (gray) persist in GPU memory throughout serving.The memory for the KV cache (red) is (de)allocated per serving request. A small amount of memory (yellow) is used ephemerally for activation.Right: vLLM smooths out the rapid growth curve of KV cache memory seen in existing systems,60, leading to a notable boost in serving throughput. the cost per request-of LLM serving systems is becoming more important.At the core of LLMs lies an autoregressive Transformer model.This model generates words (tokens), one at a time, based on the input (prompt) and the previous sequence of the output's tokens it has generated so far.For each request, this expensive process is repeated until the model outputs a termination token.This sequential generation process makes the workload memory-bound, underutilizing the computation power of GPUs and limiting the serving throughput. Improving the throughput is possible by batching multiple requests together.However, to process many requests in a batch, the memory space for each request should be efficiently managed.For example, Fig. 1 (left) illustrates the memory distribution for a 13B-parameter LLM on an NVIDIA A100 GPU with 40GB RAM.Approximately 65% of the memory is allocated for the model weights, which remain static during serving.Close to 30% of the memory is used to store the dynamic states of the requests.For Transformers, these states consist of the key and value tensors associated with the attention mechanism, commonly referred to as",
            "title": ""
        },
        {
            "id": "7096500",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "times which makes it reusable. Motivated by this, caching at the edge of the network has been identified as a promising approach to meet the high demand of reusable content [2], by the users. The key idea behind proactive caching is to store likely-to-be-requested content at the network edge nodes according to some caching policies (e.g. most popular content, random caching, coded caching) during off-peak hours [3]- [6]. When users request content that is already cached in their nearby nodes, the content delivery delay can be greatly reduced and the throughput The associate editor coordinating the review of this manuscript and approving it for publication was Anton Kos . of users requesting non-reusable content can vastly increase. Furthermore, users have different secrecy requirements, thus it is important to analyze the impact of caching on the system performance under secrecy constraints in heterogeneous traffic conditions. In this work, we consider a network scenario, where two users have different secrecy requirements. A dedicated user receives external traffic with secrecy requirements while a non-dedicated user without secrecy requirements requests reusable (cacheable) content that can be stored at the edge node's (helper) cache. If the non-dedicated user requests for content that cannot be found at the helper's cache, then it can be served by the core network server through a base station. We consider the presence of a passive eavesdropper, which is not part of the network. The eavesdropper intends to decode the transmissions under secrecy requirements. When the helper needs to serve multiple users simultaneously, the decoding process and",
            "retrieval next text": "capability of the users can impact the system performance. Hence, it is important to investigate how the simultaneous transmission from the helper to different users can enhance or impede secure communication under different decoding schemes at the users. To measure the secrecy performance of the system, the notion of physical layer secrecy is considered which can exploit the randomness present in the wireless environment. A. RELATED WORK In [7], it was shown that is possible to send messages securely over a noisy channel without using any key between the legitimate nodes. The random nature of the physical wireless channel was studied for the case of the wiretap channel, where an eavesdropper tries to decode the messages intended to the appropriate receiver. A secure communication was maintained between the transmitter and the dedicated receiver without the need of cryptographic or other security coding techniques. The problem of secure communication over multiuser scenarios has been studied extensively under different settings [8]- [12]. The impact of fading on secure communication has been explored under various settings in [13]- [16]. It has been demonstrated that fading wireless channels can facilitate secure communication in contrast to the case of Gaussian wiretap channel [13], [15]. There is also a connection between the secure communication problem considered in this work and digital watermarking. Many of the communication techniques used for reliable and secure communication can be useful for watermarking. In the watermarking process, embedding information (known as a watermark) to the underlying signal results in limited distortion to the original signal. The work",
            "retrieval score": "1.4994099",
            "text": "times which makes it reusable. Motivated by this, caching at the edge of the network has been identified as a promising approach to meet the high demand of reusable content, by the users. The key idea behind proactive caching is to store likely-to-be-requested content at the network edge nodes according to some caching policies (e.g. most popular content, random caching, coded caching) during off-peak hours-. When users request content that is already cached in their nearby nodes, the content delivery delay can be greatly reduced and the throughput The associate editor coordinating the review of this manuscript and approving it for publication was Anton Kos . of users requesting non-reusable content can vastly increase. Furthermore, users have different secrecy requirements, thus it is important to analyze the impact of caching on the system performance under secrecy constraints in heterogeneous traffic conditions. In this work, we consider a network scenario, where two users have different secrecy requirements. A dedicated user receives external traffic with secrecy requirements while a non-dedicated user without secrecy requirements requests reusable (cacheable) content that can be stored at the edge node's (helper) cache. If the non-dedicated user requests for content that cannot be found at the helper's cache, then it can be served by the core network server through a base station. We consider the presence of a passive eavesdropper, which is not part of the network. The eavesdropper intends to decode the transmissions under secrecy requirements. When the helper needs to serve multiple users simultaneously, the decoding process and",
            "title": ""
        },
        {
            "id": "5277774",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "We have included three more similar recent works (years 2020 and 2021) in the \"Related work\" section. These works show how the latency is today an important problem in NVM architectures due to the mix of the very irregular data access patterns and the high inherent latency of NVM modules. Different hardware prefetching techniques appear in current literature showing the insterest of this topic in the research community. Related work: improve the contrast with current works to highlight novelty of our work We have included current researcho work covering the same topic. We compare the characteristics and limitations of the diferent works against our proposal showing how we deal with some of those limitations by allowing a more complex and complete prefetch solution. The \"Related work\" section should be described in the introduction This section is described in the \"Introduction\". Reviewer #3 Comments Impact of HMM in the production final price. The impact of our prefetch technique in the final production price is an important issue. Sometimes as researchers we are too focused in the design and technical topics and we forget that contributions to the state-of-the-art should also be applied in order to be useful to society. We appreciate the reviewer #3 comments and we have included, in subsection \"Prefetcher Implementation\", an study of the cost of an HMM implementation on an actual processor.",
            "retrieval next text": "Expression, Localization, and Effect of High Salt Intake on Electroneutral Na+/HCO3\u2013 Cotransporter NBCn2 in Rat Small Intestine: Implication in Intestinal NaCl Absorption The electroneutral Na+/HCO3\u2013 cotransporter NBCn2 (SLC4A10) of solute carrier family 4 (SLC4) plays important physiological and pathological roles in the body. Our previous study showed that NBCn2 is expressed on the protein level in the small intestine of rat. Here, by reverse-transcription polymerase chain reaction (PCR), we identified a novel full-length NBCn2 variant, i.e., NBCn2-K, from rat small intestine. By pHi measurement with Xenopus oocytes, the activity of NBCn2-K is not significantly different from NBCn2-G. By western blotting, NBCn2 and the Na+/H+ exchanger NHE3 (SLC9A3) are predominantly expressed in the jejunum of rat small intestine. By immunofluorescence, NBCn2 and NHE3 are localized at the apical domain of the jejunum. NaCl overload decreases the expression of NBCn2 by 56% and that of NHE3 by 40% in the small intestine. We propose that NBCn2 is involved in the transepithelial NaCl absorption in the small intestine, and that the down-regulation of NBCn2 by NaCl represents an adaptive response to high salt intake in rat. INTRODUCTION Na + /HCO 3 \u2212 cotransporter NBCn2, the product of SLC4A10 gene, plays significant physiological and pathological roles in the body. In human, genetic abnormality in locus 2q24 spanning SLC4A10 is associated with complex epilepsy, mental retardation, autism spectra, cognitive disabilities, and hearing impairment (Sebat et al., 2007;Gurnett et al., 2008;Krepischi et al., 2010;Belengeanu et al., 2014;Nilsson et al., 2017;Zhao et al., 2018). In mouse, genetic disruption of Slc4a10 reduces neuronal excitability,",
            "retrieval score": "1.4986959",
            "text": "We have included three more similar recent works (years 2020 and 2021) in the \"Related work\" section. These works show how the latency is today an important problem in NVM architectures due to the mix of the very irregular data access patterns and the high inherent latency of NVM modules. Different hardware prefetching techniques appear in current literature showing the insterest of this topic in the research community. Related work: improve the contrast with current works to highlight novelty of our work We have included current researcho work covering the same topic. We compare the characteristics and limitations of the diferent works against our proposal showing how we deal with some of those limitations by allowing a more complex and complete prefetch solution. The \"Related work\" section should be described in the introduction This section is described in the \"Introduction\". Reviewer #3 Comments Impact of HMM in the production final price. The impact of our prefetch technique in the final production price is an important issue. Sometimes as researchers we are too focused in the design and technical topics and we forget that contributions to the state-of-the-art should also be applied in order to be useful to society. We appreciate the reviewer #3 comments and we have included, in subsection \"Prefetcher Implementation\", an study of the cost of an HMM implementation on an actual processor.",
            "title": ""
        },
        {
            "id": "9358270",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "A Learned Cache Eviction Framework with Minimal Overhead Recent work shows the effectiveness of Machine Learning (ML) to reduce cache miss ratios by making better eviction decisions than heuristics. However, state-of-the-art ML caches require many predictions to make an eviction decision, making them impractical for high-throughput caching systems. This paper introduces Machine learning At the Tail (MAT), a framework to build efficient ML-based caching systems by integrating an ML module with a traditional cache system based on a heuristic algorithm. MAT treats the heuristic algorithm as a filter to receive high-quality samples to train an ML model and likely candidate objects for evictions. We evaluate MAT on 8 production workloads, spanning storage, in-memory caching, and CDNs. The simulation experiments show MAT reduces the number of costly ML predictions-per-eviction from 63 to 2, while achieving comparable miss ratios to the state-of-the-art ML cache system. We compare a MAT prototype system with an LRU-based caching system in the same setting and show that they achieve similar request rates. Introduction Software caching systems are ubiquitous in modern computing infrastructure. Examples of large-scale use cases include include content delivery networks (CDNs), in-memory caches, and storage systems. CDNs protect expensive and scarce Internet backbone bandwidth and are expected to serve 72% of Internet traffic by 2022 [16]. In-memory caches protect computationally expensive services are extensively used in the data centers of Facebook [32] and Twitter [42]. Storage caches reduce the data movement of large objects in the network and an essential part of cloud services [22]. Caching systems seek to minimize",
            "retrieval next text": "their miss ratio, i.e., the fraction of requests not served by the cache. The lower the miss ratios, the lower the load on backend servers and Internet traffic (for CDNs). To decide which objects to keep in the cache, current caching systems [3, 6, 12, 32] rely on heuristic algorithms, such as Least Recently Used (LRU), and First In First out (FIFO), and Least Frequently Used (LFU). Recent work [8,13,35,38,40] shows that machine learning based eviction algorithms (ML-based caching systems) significantly outperform these heuristics by using a history of past access patterns to predict future access patterns. These accurate predictions reduce miss ratios by up to 25% compared to heuristic caches [35]. Bringing ML-based caching systems from research to production faces a key challenge due to their computational overhead and hardware cost. In particular, ML-based caching systems are not yet applicable in systems with high throughput demands [10,23] or when CPU resources are scarce due to being coloated with other applications [12]. The overhead of ML-based caches is significantly higher than heuristic caching systems for two reasons. First, MLbased caching systems need to update the model online frequently to retrain with more recent access patterns. For example, a state-of-the-art ML-based caching system for CDNs, LRB [35], uses all cache requests to generate training entries, which leads to a large training data volume and a slow training process. Second, ML-based caches require running many predictions to find an object to evict. For example, LRB samples 64 eviction candidates randomly within the cache to run predictions. Running 64",
            "retrieval score": "1.4974463",
            "text": "A Learned Cache Eviction Framework with Minimal Overhead Recent work shows the effectiveness of Machine Learning (ML) to reduce cache miss ratios by making better eviction decisions than heuristics. However, state-of-the-art ML caches require many predictions to make an eviction decision, making them impractical for high-throughput caching systems. This paper introduces Machine learning At the Tail (MAT), a framework to build efficient ML-based caching systems by integrating an ML module with a traditional cache system based on a heuristic algorithm. MAT treats the heuristic algorithm as a filter to receive high-quality samples to train an ML model and likely candidate objects for evictions. We evaluate MAT on 8 production workloads, spanning storage, in-memory caching, and CDNs. The simulation experiments show MAT reduces the number of costly ML predictions-per-eviction from 63 to 2, while achieving comparable miss ratios to the state-of-the-art ML cache system. We compare a MAT prototype system with an LRU-based caching system in the same setting and show that they achieve similar request rates. Introduction Software caching systems are ubiquitous in modern computing infrastructure. Examples of large-scale use cases include include content delivery networks (CDNs), in-memory caches, and storage systems. CDNs protect expensive and scarce Internet backbone bandwidth and are expected to serve 72% of Internet traffic by 2022. In-memory caches protect computationally expensive services are extensively used in the data centers of Facebook and Twitter. Storage caches reduce the data movement of large objects in the network and an essential part of cloud services. Caching systems seek to minimize",
            "title": ""
        },
        {
            "id": "8833196",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "sequence lengths can vary significantly across different LLM inference requests. Finally, we focused on sequence lengths of up to 3K, and P:D ratio in the range of 1-200. We believe that these are representative of many real-world deployments. However, there has also been an increased interest in supporting very long sequences (e.g., 10s-100s of thousands [18]). Such large sequence lengths may pose new challenges as the cost of attention grows quadratically with the number of tokens. We are actively investigating these challenges. Related Work In this section, we provide a brief summary of related work along two dimensions: systems optimizations and model innovations. Systems Optimizations Memory management: In auto-regressive decoding, the number of tokens that need to be generated for a given request is not known apriori. Therefore, conventional systems pre-allocate memory for the KV cache based on a conservative estimation of the maximum number of tokens. Recently, vLLM showed that this approach is inefficient and proposed a framework -motivated by the virtual memory abstraction -that enables incremental memory allocation for KV caches [20]. This helps improve the batch size, especially when the number of tokens varies significantly across different requests. FlexGen [42] focuses on improving the throughput of offline LLM inference in resource-constrained scenarios e.g., running a large model on a single GPU. Toward this goal, FlexGen employs a judicious combination of memory offloading, quantization, and scheduling. Optimizing (self-)attention: In [40], the authors propose an algorithm to reduce the memory requirement of self-attention from O(n 2 ) to O(1), with respect to the sequence length.",
            "retrieval next text": "FlashAttention [29] proposed a tiling-based algorithm that speeds up attention computation by minimizing the number of bytes read/written between different levels of GPU memory. Follow-up work [28] on FlashAttention further improved it along parallelism and work partitioning [28]. In our experiments, we found the xformers memory efficient attention implementation [21] to be the most efficient. Kernel-level optimizations: FasterTransformer [6] proposed optimized layers for the transformer's encoder and decoder blocks. These are based on low-level GPU optimizations such as kernel fusion. We expect that such low-level optimizations would equally benefit SARATHI as well. Scheduling optimizations: Orca proposed an iteration-level scheduling framework that avoids wasting compute due to token padding that was used earlier to batch together requests with different sequence lengths [48]. Further, Orca reduces latency by returning the response as soon as a request's endof-sequence token gets generated. FastServe proposed a preemptive scheduling framework to minimize the job completion times [46]. Some other scheduling frameworks include Triton [13] and Clipper [27] that separate the serving layer from the execution engine of the model. Our current work focuses on optimizing the execution layer and can be used with different scheduling policies proposed by such systems. The optimizations proposed by several of the prior works can complement our optimizations e.g., more optimized attention implementations will enable scaling SARATHI to longer sequence lengths and dynamic memory allocation will help in supporting larger batch sizes and so on. Model Innovations A significant body of work around model innovations has attempted to address the shortcomings of transformer-based language models or to",
            "retrieval score": "1.4969716",
            "text": "sequence lengths can vary significantly across different LLM inference requests. Finally, we focused on sequence lengths of up to 3K, and P:D ratio in the range of 1-200. We believe that these are representative of many real-world deployments. However, there has also been an increased interest in supporting very long sequences (e.g., 10s-100s of thousands). Such large sequence lengths may pose new challenges as the cost of attention grows quadratically with the number of tokens. We are actively investigating these challenges. Related Work In this section, we provide a brief summary of related work along two dimensions: systems optimizations and model innovations. Systems Optimizations Memory management: In auto-regressive decoding, the number of tokens that need to be generated for a given request is not known apriori. Therefore, conventional systems pre-allocate memory for the KV cache based on a conservative estimation of the maximum number of tokens. Recently, vLLM showed that this approach is inefficient and proposed a framework -motivated by the virtual memory abstraction -that enables incremental memory allocation for KV caches. This helps improve the batch size, especially when the number of tokens varies significantly across different requests. FlexGen focuses on improving the throughput of offline LLM inference in resource-constrained scenarios e.g., running a large model on a single GPU. Toward this goal, FlexGen employs a judicious combination of memory offloading, quantization, and scheduling. Optimizing (self-)attention: In, the authors propose an algorithm to reduce the memory requirement of self-attention from O(n 2 ) to O(1), with respect to the sequence length.",
            "title": ""
        },
        {
            "id": "4735765",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \\textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \\textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \\textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \\textit{throttLL'eM} achieves up to 43.8\\% lower energy consumption and an energy efficiency improvement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's Triton server.",
            "retrieval next text": "Structural and Electrical Studies of Polycrystalline Pr\ud835\udc65Y1\u2212\ud835\udc65FeO3 Materials Prepared Through Sol-Gel Route Objectives: Investigations on structural, morphological and electrical studies of Pr substituted YFeO3 samples synthesized through sol-gel route are presented in this paper. Methods: PrxY1-xFeO3 (x=0.0, 0.2, 0.4, 0.6, 0.8 and 1.0) samples are synthesized by sol-gel auto combustion method. Structural characterization of the samples is done by using X-ray diffraction (XRD), Field emission scanning electron microscope (FESEM) and Energy dispersive X-ray (EDX) techniques. Electrical properties are studied by J-E and Ln J-Ln E measurements. Findings: Rietveld refined XRD structural analysis shows sharp intense peaks indicating the crystalline nature of the samples and all the samples possess distorted orthorhombic structure with space group Pbnm. FESEM analysis shows non-uniform nature of grain size with increasing porosity. EDX data confirms elemental composition, weight and atomic percentage of the prepared samples which further confirms their purity. The leakage current density increases with increase in Pr doping. The slopes of Ln J-Ln E graphs of all the samples are in the range of 1.11 to 1.51 confirming that Ohmic conduction is predominant conduction mechanism. Novelty: Investigations on the effect of Pr substitution on electrical properties of single phase YFeO3\u00ad samples reveal that the substitution of Pr in YFeO3 decreases the ferroelectric nature of the samples. Keywords: Sol-gel method, XRD, FESEM, EDX, Electrical Properties",
            "retrieval score": "1.4963772",
            "text": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \\textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \\textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \\textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \\textit{throttLL'eM} achieves up to 43.8\\% lower energy consumption and an energy efficiency improvement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's Triton server.",
            "title": ""
        },
        {
            "id": "2228946",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "Can Increasing the Hit Ratio Hurt Cache Throughput? Software caches are an intrinsic component of almost every computer system. Consequently, caching algorithms, particularly eviction policies, are the topic of many papers. Almost all these prior papers evaluate the caching algorithm based on its hit ratio, namely the fraction of requests that are found in the cache, as opposed to disk. The hit ratio is viewed as a proxy for traditional performance metrics like system throughput or response time. Intuitively it makes sense that higher hit ratio should lead to higher throughput (and lower response time), since more requests are found in the cache (low access time) as opposed to the disk (high access time). This paper challenges this intuition. We show that increasing the hit ratio can actually hurt the throughput (and response time) for many caching algorithms. Our investigation follows a three-pronged approach involving (i) queueing modeling and analysis, (ii) implementation and measurement, and (iii) simulation to validate the accuracy of the queueing model. We also show that the phenomenon of throughput decreasing at higher hit ratios is likely to be more pronounced in future systems, where the trend is towards faster disks and higher numbers of cores per CPU.",
            "retrieval next text": "Cutting And Scribing Of Alumina Ceramic Using A CO2-Laser This investigation mainly reports about the practicable processing of alumuina ceramics (Al203) with a pulsed 150 W CO2-laser with a focus diameter of about 80 um- The optimal parameters like pulse length, pulse space and the laser power had been determined to work on plates of alumina ceramics with variable thickness of up to 2 mm.",
            "retrieval score": "1.4954271",
            "text": "Can Increasing the Hit Ratio Hurt Cache Throughput? Software caches are an intrinsic component of almost every computer system. Consequently, caching algorithms, particularly eviction policies, are the topic of many papers. Almost all these prior papers evaluate the caching algorithm based on its hit ratio, namely the fraction of requests that are found in the cache, as opposed to disk. The hit ratio is viewed as a proxy for traditional performance metrics like system throughput or response time. Intuitively it makes sense that higher hit ratio should lead to higher throughput (and lower response time), since more requests are found in the cache (low access time) as opposed to the disk (high access time). This paper challenges this intuition. We show that increasing the hit ratio can actually hurt the throughput (and response time) for many caching algorithms. Our investigation follows a three-pronged approach involving (i) queueing modeling and analysis, (ii) implementation and measurement, and (iii) simulation to validate the accuracy of the queueing model. We also show that the phenomenon of throughput decreasing at higher hit ratios is likely to be more pronounced in future systems, where the trend is towards faster disks and higher numbers of cores per CPU.",
            "title": ""
        },
        {
            "id": "7306926",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "to approximate the attention output. Besides, [21] realized exact efficient attention with wall-clock speed by optimizing the number of memory reads and writes. However, these approaches were evaluated mostly for training, focused on computation complexity, and did not address the KV-Cache memory usage introduced by auto-regressive language models. Recently, there is active research attempting to apply quantization or pruning in LLM [8][9][10][11][12][13][14]. However, they mostly focus on reducing the size of model weights. Flexgen [7] applies quantization and sparsification to the KV cache; however, the memory of the KV cache is not reduced regarding sequence lengths. It stores the quantized KV cache for all tokens in CPU memory and loads all attention keys from CPU memory to compute attention scores. The Persistence of Importance Hypothesis We first present one interesting observation upon which the persistence of importance hypothesis is derived in Section 3.1. In Section 3.2, we discuss the hypothesis in detail with empirical verification. Then, in Section 3.3, we provide theoretical intuition on the reason behind such model behaviors. Repetitive Attention Pattern. Observation. We are interested in the attention score from the position t over all the words that come before it in the sentence. In Figure 1, we provide three attention maps of a sentence randomly drawn from C4 [22] using OPT-6B. Each attention map is a discretized attention score calculated at a randomly decided position. We consider a score larger than 1 t as significant as 1 t indicates an averaging mixing score. High attention scores are marked with dark green. Result. High",
            "retrieval next text": "attention scores are observed at the same set of tokens from various positions in the sentence. In all three plots, we see dark green at sequence positions 27, 63, 98, 121, 152, and 177, suggesting that these tokens received high attention at all three positions. We observe similar model behavior at different transformer layers with different text inputs. More plots are in Appendix A. Implication. Even though small differences exist, repetitive attention patterns are evident in the attention maps. There exist specific tokens that keep receiving high attention. Meanwhile, these attention maps show sparsity: only a few tokens have high attention scores. The Persistence of Importance Hypothesis The repetitive attention pattern suggests that specific tokens are influential throughout the sequence. A stricter claim is that these tokens are the only ones that could be significant for a future step. Thus, we articulate the persistence of importance hypothesis. The Persistence of Importance Hypothesis. With a trained autoregressive language model, only pivotal tokens, which had a substantial influence at one previous step, will have a significant influence at a future step. If true, this hypothesis indicates the possibility of foreseeing what information in the previous sequences could be vital for future steps. This hypothesis is trivial when pivotal tokens include all tokens in the entire sentences. However, a much more interesting case is when pivotal tokens are a subset of previous words. This would enable us to reduce the size of the KV cache by throwing away the embedding of non-important tokens. Pivotal Token. One natural indication of",
            "retrieval score": "1.4947767",
            "text": "to approximate the attention output. Besides, realized exact efficient attention with wall-clock speed by optimizing the number of memory reads and writes. However, these approaches were evaluated mostly for training, focused on computation complexity, and did not address the KV-Cache memory usage introduced by auto-regressive language models. Recently, there is active research attempting to apply quantization or pruning in LLM. However, they mostly focus on reducing the size of model weights. Flexgen applies quantization and sparsification to the KV cache; however, the memory of the KV cache is not reduced regarding sequence lengths. It stores the quantized KV cache for all tokens in CPU memory and loads all attention keys from CPU memory to compute attention scores. The Persistence of Importance Hypothesis We first present one interesting observation upon which the persistence of importance hypothesis is derived in Section 3.1. In Section 3.2, we discuss the hypothesis in detail with empirical verification. Then, in Section 3.3, we provide theoretical intuition on the reason behind such model behaviors. Repetitive Attention Pattern. Observation. We are interested in the attention score from the position t over all the words that come before it in the sentence. In Figure 1, we provide three attention maps of a sentence randomly drawn from C4 using OPT-6B. Each attention map is a discretized attention score calculated at a randomly decided position. We consider a score larger than 1 t as significant as 1 t indicates an averaging mixing score. High attention scores are marked with dark green. Result. High",
            "title": ""
        },
        {
            "id": "9087067",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "Spects 2005 -decision on Manuscript Id Spects-05-nts-025 Reviewer: 1 Comments to the Author This paper have technical weak points.First the background and references are not adequate!The locality characteristics are found not only with characteristics of 2-tier architecture but also with 3-tier etc. Thus locality characteristics are examined not only in a temporal context but also in client temporal context particularly for web based streams... Secondly, authros do not evaluate through exaustive simulations the correllation bettween locality characteristics and self similar traffic for demand-driven caching schemes. However this paper has a positive point that it hosts many results...But I believe that these results do not figure out any significance... I recommend that authors should exploit circular explanations to concepts reaching readers without the topic's background.",
            "retrieval next text": "Techniques of data retrieval for scientometric research in the ISI citation indexes The usefulness of the retrieval software of the hosts DIMDI, STN, DIALOG and ORBIT and the ISI CD-ROM for scientometric studies in Citation Index databases is exam ined. The frequency distribution of publications and citations. the co-author and co-citation analysis, and the method of bibliographic coupling are used for examples. Practical prob lems in the work with the online Citation Index are described.",
            "retrieval score": "1.4910431",
            "text": "Spects 2005 -decision on Manuscript Id Spects-05-nts-025 Reviewer: 1 Comments to the Author This paper have technical weak points.First the background and references are not adequate!The locality characteristics are found not only with characteristics of 2-tier architecture but also with 3-tier etc. Thus locality characteristics are examined not only in a temporal context but also in client temporal context particularly for web based streams... Secondly, authros do not evaluate through exaustive simulations the correllation bettween locality characteristics and self similar traffic for demand-driven caching schemes. However this paper has a positive point that it hosts many results...But I believe that these results do not figure out any significance... I recommend that authors should exploit circular explanations to concepts reaching readers without the topic's background.",
            "title": ""
        },
        {
            "id": "8210999",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": ", or a same number?Cache hits could then utilize string matching or numeric ranges instead of vectors. Conclusion GPTCache is a caching solution tailored for LLM applications.It brings the following benefits to the LLM app developers: \u2022 Less costs: Most LLM services charge fees based on a combination of the number of requests and token count.GPTCache can effectively minimize expenses by caching query results, thereby reducing the number of requests and tokens sent to the LLM service. \u2022 Faster response times: LLMs utilize generative AI to produce responses in real-time, which can be time-consuming.However, when a similar query is cached, the response time greatly improves, as the result is retrieved directly from the cache without interaction with the LLM service.In most cases, GPT-Cache can also offer better query throughput than standard LLM services. \u2022 More scalable and available: LLM services often impose rate limits on the number of access requests within a given timeframe.If these limits are exceeded, additional requests are blocked until a cooldown period has elapsed, leading to service outages.GPTCache allows you to easily scale and handle increasing query volumes, ensuring consistent performance as your application's user base expands. By utilizing semantic similarity search and vector embeddings, GPTCache provides an effective caching solution that enhances performance, reduces costs, and improves scalability for applications that use large language models.Our initial experiments have shown great potential, and we plan to conduct more comprehensive evaluations on diverse real-world datasets and application scenarios. Table 1 : Results for Caching Hit and Miss Samples, Caching Mixed Positive and Negative",
            "retrieval next text": "Queries, and Hit Latency Table 2 : Results for Caching Hit and Miss Samples, Caching Mixed Positive and Negative Queries, and Hit Latency",
            "retrieval score": "1.4885232",
            "text": ", or a same number?Cache hits could then utilize string matching or numeric ranges instead of vectors. Conclusion GPTCache is a caching solution tailored for LLM applications.It brings the following benefits to the LLM app developers: \u2022 Less costs: Most LLM services charge fees based on a combination of the number of requests and token count.GPTCache can effectively minimize expenses by caching query results, thereby reducing the number of requests and tokens sent to the LLM service. \u2022 Faster response times: LLMs utilize generative AI to produce responses in real-time, which can be time-consuming.However, when a similar query is cached, the response time greatly improves, as the result is retrieved directly from the cache without interaction with the LLM service.In most cases, GPT-Cache can also offer better query throughput than standard LLM services. \u2022 More scalable and available: LLM services often impose rate limits on the number of access requests within a given timeframe.If these limits are exceeded, additional requests are blocked until a cooldown period has elapsed, leading to service outages.GPTCache allows you to easily scale and handle increasing query volumes, ensuring consistent performance as your application's user base expands. By utilizing semantic similarity search and vector embeddings, GPTCache provides an effective caching solution that enhances performance, reduces costs, and improves scalability for applications that use large language models.Our initial experiments have shown great potential, and we plan to conduct more comprehensive evaluations on diverse real-world datasets and application scenarios. Table 1 : Results for Caching Hit and Miss Samples, Caching Mixed Positive and Negative",
            "title": ""
        },
        {
            "id": "4820005",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "the broader context of relevant designs. Section 3 suggests several ways to implement k-way set associative caches efficiently in software. Section 5 shows an evaluation of the hit ratio obtained by limited associativity caches for a wide selection of caching algorithms and of their throughput compared to the Guava and Caffeine Java caching libraries. We conclude with a discussion in Section 6. Cache Management Policies A cache management scheme's design often involves certain (possibly implicit) assumptions on the characteristics of the \"typical\" workload. For example, the Least Recently Used (LRU) policy always admits new items to the cache and evicts the least recently used items [22]. LRU works well for recency biased workloads, where recently accessed items are most likely to be accessed again. Alternatively, the Least Frequently Used (LFU) policy, also called Perfect LFU, assumes that the access distribution is fixed over time, meaning that frequently accessed data items are more likely to be reaccessed. Hence, Perfect LFU evicts the least frequently used item from the cache and admits a new one if it is more frequent than the cache victim. For synthetic workloads with static access distributions, Perfect LFU is the optimal cache management policy [27]. Real workloads are dynamic, so practical LFU policies often apply aging mechanisms to the items' frequency. Such mechanisms can be calculating frequency on sliding windows [27], using exponential decay [10,9], or periodic aging [17]. Operation complexity is another essential factor in cache design. Both LRU and LFU need to maintain a priority queue or a heap, and can",
            "retrieval next text": "be implemented in constant time [29,34,13]. LRU and variants of LFU also maintain a hash table for fast access to items and their metadata. Another limitation of priority queue based LRU implementations is the high contention on the head of queue due to the need to update it on each cache access (both hits and misses). The realization that a good cache management mechanism needs to combine recency and frequency led to the development of more sophisticated approaches [20]. LRU-K [37] maintains the last K occurrences of each item and orders all items based on the recency of their last K th access. The drawbacks are significant memory and computational overheads. Also, most benefit comes when K = 2, with quickly diminishing returns for larger values of K. Low Inter-reference Recency Set (LIRS) [26] is a page replacement algorithm that attempts to directly predict the next occurrence of an item using a metric named reuse distance. To realize this, LIRS maintains two cache regions and a large number of ghost entries. FRD [38] can be viewed as a practical variant of LIRS, which overcomes many of the implementation related limitations of LIRS. Window-TinyLFU (W-TinyLFU) [17] also balances between recency and frequnecy using two cache regions, a window cache and the main cache. Yet, it controls which items are admitted to the main cache using an optimized counting Bloom filter [19] based admission filter and thereby avoids the need to maintain ghost entries. The relative size of the two caches is dynamically adjusted at runtime [16]. As",
            "retrieval score": "1.488075",
            "text": "the broader context of relevant designs. Section 3 suggests several ways to implement k-way set associative caches efficiently in software. Section 5 shows an evaluation of the hit ratio obtained by limited associativity caches for a wide selection of caching algorithms and of their throughput compared to the Guava and Caffeine Java caching libraries. We conclude with a discussion in Section 6. Cache Management Policies A cache management scheme's design often involves certain (possibly implicit) assumptions on the characteristics of the \"typical\" workload. For example, the Least Recently Used (LRU) policy always admits new items to the cache and evicts the least recently used items. LRU works well for recency biased workloads, where recently accessed items are most likely to be accessed again. Alternatively, the Least Frequently Used (LFU) policy, also called Perfect LFU, assumes that the access distribution is fixed over time, meaning that frequently accessed data items are more likely to be reaccessed. Hence, Perfect LFU evicts the least frequently used item from the cache and admits a new one if it is more frequent than the cache victim. For synthetic workloads with static access distributions, Perfect LFU is the optimal cache management policy. Real workloads are dynamic, so practical LFU policies often apply aging mechanisms to the items' frequency. Such mechanisms can be calculating frequency on sliding windows, using exponential decay,9, or periodic aging. Operation complexity is another essential factor in cache design. Both LRU and LFU need to maintain a priority queue or a heap, and can",
            "title": ""
        },
        {
            "id": "9115820",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management Transformer-based large language models (LLMs) demonstrate impressive performance across various natural language processing tasks. Serving LLM inference for generating long contents, however, poses a challenge due to the enormous memory footprint of the transient state, known as the key-value (KV) cache, which scales with the sequence length and batch size. In this paper, we present InfiniGen, a novel KV cache management framework tailored for long-text generation, which synergistically works with modern offloading-based inference systems. InfiniGen leverages the key insight that a few important tokens that are essential for computing the subsequent attention layer in the Transformer can be speculated by performing a minimal rehearsal with the inputs of the current layer and part of the query weight and key cache of the subsequent layer. This allows us to prefetch only the essential KV cache entries (without fetching them all), thereby mitigating the fetch overhead from the host memory in offloading-based LLM serving systems. Our evaluation on several representative LLMs shows that InfiniGen improves the overall performance of a modern offloading-based system by up to 3.00x compared to prior KV cache management methods while offering substantially better model accuracy.",
            "retrieval next text": "The Ideological Organization of Latin American Legislative Parties Are legislative party systems in Latin America organized along ideological lines? This article presents a cross-country analysis of legislators\u2019 positions on a variety of issues, such as government intervention in the economy, the value of democratic competition, attitudes toward cultural minorities, and views on international openness. The study is based on discriminant analysis of data from a 1997 survey of legislators in 12 countries. The purpose is to explore whether legislative parties in the region are ideologically organized and if so, to understand the substance and structure of the ideological dimensions that underlie legislative party systems. The analysis reveals variation in the programmatic organization of Latin American legislatures, emphasizing that political, cultural, and economic dimensions coalesce in different ways across countries to account for ideological disagreement among legislative parties.",
            "retrieval score": "1.4857873",
            "text": "InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management Transformer-based large language models (LLMs) demonstrate impressive performance across various natural language processing tasks. Serving LLM inference for generating long contents, however, poses a challenge due to the enormous memory footprint of the transient state, known as the key-value (KV) cache, which scales with the sequence length and batch size. In this paper, we present InfiniGen, a novel KV cache management framework tailored for long-text generation, which synergistically works with modern offloading-based inference systems. InfiniGen leverages the key insight that a few important tokens that are essential for computing the subsequent attention layer in the Transformer can be speculated by performing a minimal rehearsal with the inputs of the current layer and part of the query weight and key cache of the subsequent layer. This allows us to prefetch only the essential KV cache entries (without fetching them all), thereby mitigating the fetch overhead from the host memory in offloading-based LLM serving systems. Our evaluation on several representative LLMs shows that InfiniGen improves the overall performance of a modern offloading-based system by up to 3.00x compared to prior KV cache management methods while offering substantially better model accuracy.",
            "title": ""
        },
        {
            "id": "4835728",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "dominated by tightly-coupled parallel jobs and, more recently, by bag-of-tasks applications. The GWA includes traces collected from grid environments; differently from this work, these traces are dominated by bag-of-tasks applications and by virtual-machine lease-release data. Workload characterization, definition, and modeling: There is much related and relevant work in this area, from which we compare only with the closely related; other characterization work does not focus on comparing traces by domain and does not cover a set of characteristics as diverse as this work, leading to so many findings. Closest to this work, the Google cluster-traces have been analyzed from various points of view, e.g., [36,9,34]. Amvrosiadis et al. [3,4] compare the Google cluster traces with three other cluster traces, of 0.3-3 times the size and 3-60 times the duration, and find key differences; our work adds new views and quantitative data on diversity, through both survey and characterization techniques. Bharathi et al. [7] provide a characterization on workflow structures and the effect of workflow input sizes on said structures. Five scientific workflows are used to explain in detail the compositions of their data and computational dependencies. Using the characterization, a workflow generator generator for parameterized workflows is developed. Juve et al. [27] provide a characterization of six scientific workflows using workflow profiling tools that investigate resource consumption and computational characteristics of tasks. The teams of Feitelson and Iosup have provided many characterization and modeling studies for parallel [16], grid [22], and hostedbusiness [41] workloads; and Feitelson has written a seminal book on workload modeling [14]. In",
            "retrieval next text": "contrast, this work addresses in-depth the topic of workloads of workflows. Conclusion and Ongoing Work Responding to the stringent need for diverse workflow traces, in this work we propose the Workflow Trace Archive (WTA), which is an open-access archive containing workflow traces. We conduct a survey of how the systems community uses workflow traces, by systematically inspecting articles accepted in the last decade in peerreviewed conferences and journals. We find that, from all articles that use traces, less than 40% use realistic traces, and less than 15% use any open-access trace. Additionally, the community focuses primarily on scientific workloads, possibly due to the scarcity of traces from other domains. These findings suggest existing limits to the relevance and reproducibility of workflow-based studies and designs. We design and implement the WTA around five key requirements. At the core of the WTA is an unified trace format that, uniquely, supports both workflowand task-level NFRs. The archive contains a large and diverse set of traces, collected from 10 sources and encompassing over 48 million workflows and 2 billion CPU core hours. Finally, we provide deep insight into the WTA traces, through a statistical characterization revealing that: (1) there are large differences in workflow structures between scientific, industrial, and engineering workflows, (2) our two biggest traces-from Alibaba and Google-have the most stable arrival patterns in terms of tasks per hour, (3) industrial workflows tend to have the highest level of parallelism, (4) the level of parallelism per domain is clearly divided, (5) engineering workloads tend to have the most tasks",
            "retrieval score": "1.484954",
            "text": "dominated by tightly-coupled parallel jobs and, more recently, by bag-of-tasks applications. The GWA includes traces collected from grid environments; differently from this work, these traces are dominated by bag-of-tasks applications and by virtual-machine lease-release data. Workload characterization, definition, and modeling: There is much related and relevant work in this area, from which we compare only with the closely related; other characterization work does not focus on comparing traces by domain and does not cover a set of characteristics as diverse as this work, leading to so many findings. Closest to this work, the Google cluster-traces have been analyzed from various points of view, e.g.,,9,34. Amvrosiadis et al.,4 compare the Google cluster traces with three other cluster traces, of 0.3-3 times the size and 3-60 times the duration, and find key differences; our work adds new views and quantitative data on diversity, through both survey and characterization techniques. Bharathi et al. provide a characterization on workflow structures and the effect of workflow input sizes on said structures. Five scientific workflows are used to explain in detail the compositions of their data and computational dependencies. Using the characterization, a workflow generator generator for parameterized workflows is developed. Juve et al. provide a characterization of six scientific workflows using workflow profiling tools that investigate resource consumption and computational characteristics of tasks. The teams of Feitelson and Iosup have provided many characterization and modeling studies for parallel, grid, and hostedbusiness workloads; and Feitelson has written a seminal book on workload modeling. In",
            "title": ""
        },
        {
            "id": "14684224",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "eviction strategies when storage space faces the risk of overflow.Depending on the life of caching contents, these policies can be divided into two categories roughly: one is like first in first out (FIFO), least frequently used (LFU), least recently used (LRU) and randomized replacement, the contents would not be removed until no more memory is available; the other one is called time to live (TTL) strategy, where the eviction happens once the related timer expires.[21] presents analytic model for hit ratio in TTL-based cache requested by independent and identically distributed flows.It worth noting that in [21], the TTL-based cache policy is used for the consistency of dynamic contents instead of contents replacement.In [22], the authors introduce a TTL model for cache eviction and the timer is reset once related content cache hit happens. Cache dimensioning highlights how much storage space to be allocated.Benefit from the softwarization and virtualization technologies, the cache size in each CR or edge cloud can be managed in a more flexible and dynamical way, which makes the cache dimensioning decisions become an important feature in data caching.Technically, the cache hit ratio rises with the increasing of cache memory, and consequently eases the traffic congestion in the core network. However, excessive space allocation would waste the resource like energy to support the caching function.Hence there is a trade-off between cache size cost and network congestion.Economically, taking such scenario into consideration: a small content provider wants to rent service from a CDN provider such as Akamai or Huawei Cloud, and there is also a",
            "retrieval next text": "balance between investment saving and network performance.In [23], the proper cache size of individual CR in Content-Centric Network (CCN) is investigated via exploiting the network topology.In [24], the authors consider the effect of network traffic distribution and user behaviours when designing cache size. Content delivery considers how to transform the caching content to the requested user.The delivery traffic embraces single cache file downloading and video content steaming and the metrics for these two scenarios vary.Regarding file downloading, the content cannot be consumed until the delivery is completed.Therefore the downloading time of the entire file is viewed as a metric to reflect the quality of experience (QoE).For video steaming, especially for those large video splitted into several chunks, the delay limitation only works on the first chunk.In that case, delivering the first chunk in time and keep the smooth transmission of the rest chunks are the key aims [25].Apart from those measuring metrics, another problem in content delivery is the routing policy.In CCN [26], one implementation of ICN architecture, employs a flooding-based name routing protocol to publish the request among cached CRs.On one hand, flooding strategy simplifies the designing complexity and reduce the maintaining cost particularly in an unstable scenario; on the other hand, it costly wastes bandwidth resources.In [27], the authors discuss the optimal radius in scoped flooding.The deliver route is often considered jointly with where to cache problem, in which the objective function (1a) includes both deployment and routing cost. Layer 2 Caching Contrast to Layer 1 caching in wired connection, Layer 2 caching considers",
            "retrieval score": "1.4848287",
            "text": "eviction strategies when storage space faces the risk of overflow.Depending on the life of caching contents, these policies can be divided into two categories roughly: one is like first in first out (FIFO), least frequently used (LFU), least recently used (LRU) and randomized replacement, the contents would not be removed until no more memory is available; the other one is called time to live (TTL) strategy, where the eviction happens once the related timer expires. presents analytic model for hit ratio in TTL-based cache requested by independent and identically distributed flows.It worth noting that in, the TTL-based cache policy is used for the consistency of dynamic contents instead of contents replacement.In, the authors introduce a TTL model for cache eviction and the timer is reset once related content cache hit happens. Cache dimensioning highlights how much storage space to be allocated.Benefit from the softwarization and virtualization technologies, the cache size in each CR or edge cloud can be managed in a more flexible and dynamical way, which makes the cache dimensioning decisions become an important feature in data caching.Technically, the cache hit ratio rises with the increasing of cache memory, and consequently eases the traffic congestion in the core network. However, excessive space allocation would waste the resource like energy to support the caching function.Hence there is a trade-off between cache size cost and network congestion.Economically, taking such scenario into consideration: a small content provider wants to rent service from a CDN provider such as Akamai or Huawei Cloud, and there is also a",
            "title": ""
        },
        {
            "id": "6672795",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "Issues in File Caching and Virtual Memory Paging with Fast SCM Storage Article history: Received: 09 August, 2020 Accepted: 21 September, 2020 Online: 05 October, 2020 Storage-Class Memory (SCM) like Optane has advanced as a fast storage medium, and conventional memory management systems designed for the hard disk storage need to be reconsidered. In this article, we revisit the memory management system that adopts SCM as the underlying storage medium and discuss the issues in two layers: file caching and virtual memory paging. Our first observation shows that file caching in the SCM storage is profitable only if the cached data is referenced more than once, which is different from the file caching in hard disks, where a single hit is also beneficial. Our second observation in virtual memory paging shows that the page size in the SCM storage is sensitive to the memory system performance due to the influence of memory address translation and storage access cost. Our simulation studies show that the performance of paging systems can be improved by adjusting the page size appropriately considering application characteristics, storage types, and available memory capacities. However, the page size will not be a significant issue in mobile platforms like Android, where applications are killed before the memory space is exhausted, making situations simpler. We expect that the analysis shown in this article will be useful in configuring file caches and paging systems with the emerging SCM storage. Introduction With the large performance gap between hard disk drive (HDD) and dynamic random-access memory (DRAM), the main",
            "retrieval next text": "purpose of memory management in computing systems has been the minimization of disk I/Os [1,2]. The access latency of hard disks is more than tens of milliseconds, which is five to six orders of magnitude larger than DRAM's access latency. Meanwhile, due to the rapid improvement of storage access time by the adoption of flash-based solid state drive (SSD) and storage-class memory (SCM), the extremely large performance gap has been decreased [3][4][5]. The access latency of the flash storage is less than fifty milliseconds, and hence the performance gap of storage and memory becomes less than 3 orders of magnitude. Such trends have been speeded up by the commercialization of SCM whose access latency is just 1 or 2 orders of magnitude slower than DRAM [6,7]. A lot of patents related to the detailed architectures and algorithms of SCM management have been suggested, and Intel manufactured the commercial product of SCM, called Optane TM [8,9]. Owing to its desirable features like high performance, low energy consumption, and long write endurance, SCM is anticipated to be adopted in the storage systems like flash SSD and hard disks [10][11][12][13]. SCM can also be adopted in the main memory system because it allows byte-accesses like DRAM but consumes less energy because it is a non-volatile medium [14]. However, the access latency of SCM is longer than that of DRAM, and hence it is now considered as high-end storage or additional memory that can be used together with DRAM. Although SCM may be used as either memory or storage, this",
            "retrieval score": "1.4832245",
            "text": "Issues in File Caching and Virtual Memory Paging with Fast SCM Storage Article history: Received: 09 August, 2020 Accepted: 21 September, 2020 Online: 05 October, 2020 Storage-Class Memory (SCM) like Optane has advanced as a fast storage medium, and conventional memory management systems designed for the hard disk storage need to be reconsidered. In this article, we revisit the memory management system that adopts SCM as the underlying storage medium and discuss the issues in two layers: file caching and virtual memory paging. Our first observation shows that file caching in the SCM storage is profitable only if the cached data is referenced more than once, which is different from the file caching in hard disks, where a single hit is also beneficial. Our second observation in virtual memory paging shows that the page size in the SCM storage is sensitive to the memory system performance due to the influence of memory address translation and storage access cost. Our simulation studies show that the performance of paging systems can be improved by adjusting the page size appropriately considering application characteristics, storage types, and available memory capacities. However, the page size will not be a significant issue in mobile platforms like Android, where applications are killed before the memory space is exhausted, making situations simpler. We expect that the analysis shown in this article will be useful in configuring file caches and paging systems with the emerging SCM storage. Introduction With the large performance gap between hard disk drive (HDD) and dynamic random-access memory (DRAM), the main",
            "title": ""
        },
        {
            "id": "11796520",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "process for VoD services with recommendation systems, and instead, use trace data to evaluate the performance of the proposed policies. In addition, another key difference between the existing literature and this work is that we study the trade-off between bandwidth usage and quality of service, while most of the existing works (except [7]) focus only on the improvement in quality of service (cache hit-ratio) by pre-fetching content. In [5], the authors use trace data from a campus network gateway to analyze the performance of pre-fetching content to serve YouTube requests. A key observation in [5] that it is not necessary to pre-fetch complete videos to avoid start-up delays. Fetching a fraction of the video is often sufficient as the rest of the video can be fetched while the users' watch the initial part of the video. In [6], the authors compare the performance of pre-fetching+caching and the Least Recent Used (LRU) caching scheme which does not pre-fetch content, for Hulu (a VoD service) on a university network. In [7], trace data from a Swedish TV service provider is used to evaluate the benefits of pre-fetching episodes of shows that a specific user is watching in order to reduce latency. In [8], the authors study the setting where the requests arrive according to a known Markov process. They propose an MDP based pre-fetching scheme and prove its optimality. Although our work also assumes that the underlying request process is Markovian, unlike [8], our caching policy works without the knowledge of the transition probabilities. This is an important",
            "retrieval next text": "distinction, since for VoD services like YouTube with massive content catalogs, content popularity is often time-varying and unknown [17]. In [9], the authors study a pre-fetching and caching scheme for HTTP-based adaptive video streaming. They propose a pre-fetching and caching scheme to maximize the cache hit-ratio assuming the bandwidth between the local cache and the central server is limited. III. OUR REQUEST MODEL In this section, we discuss our model for the request process for VoD services with recommendation systems. A. Model Definition We construct a directed graph G(V, E), where the set V consists of all the videos offered by the VoD service and an edge e = {i, j} \u2208 E implies that Video j is one of the recommended videos for Video i. We then assign weights to edges. Each user's request process is a random walk on this weighted graph and therefore, the request arrival process is Markovian and can be completely described by a transition probability matrix. We use a subset of the properties discussed in Section II-A to construct this matrix and verify that the remaining properties discussed in Section II-A are satisfied by our Markovian model. Motivated by the fact the empirical studies like [10] have found that this graph is small-world, and the degree distribution follows the power law [16] we use the Barabasi-Albert model [18] to generate a random small-world graph. Refer to Figure 1 for a formal definition of the Barabasi-Albert model. 1: Initialize: Generate a connected graph of m nodes (v 1 , v 2",
            "retrieval score": "1.4826922",
            "text": "process for VoD services with recommendation systems, and instead, use trace data to evaluate the performance of the proposed policies. In addition, another key difference between the existing literature and this work is that we study the trade-off between bandwidth usage and quality of service, while most of the existing works (except) focus only on the improvement in quality of service (cache hit-ratio) by pre-fetching content. In, the authors use trace data from a campus network gateway to analyze the performance of pre-fetching content to serve YouTube requests. A key observation in that it is not necessary to pre-fetch complete videos to avoid start-up delays. Fetching a fraction of the video is often sufficient as the rest of the video can be fetched while the users' watch the initial part of the video. In, the authors compare the performance of pre-fetching+caching and the Least Recent Used (LRU) caching scheme which does not pre-fetch content, for Hulu (a VoD service) on a university network. In, trace data from a Swedish TV service provider is used to evaluate the benefits of pre-fetching episodes of shows that a specific user is watching in order to reduce latency. In, the authors study the setting where the requests arrive according to a known Markov process. They propose an MDP based pre-fetching scheme and prove its optimality. Although our work also assumes that the underlying request process is Markovian, unlike, our caching policy works without the knowledge of the transition probabilities. This is an important",
            "title": ""
        },
        {
            "id": "7981849",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "either cause premature eviction of a useful cache block, leading to an additional cache miss or cause delay in eviction of a dead block, leading to wastage of cache capacity. This calls for cache management mechanisms and policies that can tolerate variability in the reuse behavior of cache blocks to maximize cache e ciency. Our Proposals Aim of this thesis is: To design robust cache management mechanisms and policies for LLC that minimize cache misses in the face of variability in the reuse behavior of cache blocks, while keeping the cost and complexity of the hardware implementation low. To that end, we propose two cache management techniques, one domain-agnostic and one domain-specialized, that introduce robust mechanisms and policies to address variability in reuse prediction. The rest of the chapter provides a brief overview of both proposals. Domain-Agnostic Cache Management In this part of the thesis, we consider a conventional approach to cache management, namely domain-agnostic cache management, in which the LLC is managed completely in hardware. Such approach is quite attractive in practice as the cache management remains fully transparent to the application software. There has been a rich history of works that proposed various domain-agnostic techniques to improve cache e ciency [8,18,37,39,40,54,59,63,67,69,71,73,76,78,80,81,82,85,86,87,88,89,97,103,110]. The state-of-the-art techniques employ prediction mechanisms that seek to correctly identify as many dead blocks as possible and evict them immediately after their last use to reduce cache thrashing. These predictors all rely on some metric of temporal reuse to make their decisions regarding the end of a given block's useful life. Previous",
            "retrieval next text": "works have suggested hit count [81], last-touch PC [73], and number of references to the block's set since the last reference [59], among others, as metrics for determining whether the block is dead at a given point in time. However, we observe that existing metrics limit the accurate identi cation of dead blocks in the face of variability. For example, when the number of hits to a cache block is inconsistent across generations, a technique relying on this metric (i.e., hit count) would either prematurely classify the cache block dead or may not classify the cache block dead altogether until its eviction, both of which lead to cache ine ciency. This calls for robust metrics and policies that can tolerate inconsistencies. To that end, we propose Live Distance, a new metric of temporal reuse based on stack distance; stack distance of a given access to a cache block is de ned as the number of unique cache blocks accessed since the previous access to the cache block [112]. For a given generation of a cache block (from allocation to eviction), live distance is de ned as the largest observed stack distance in the generation. Live distance provides a conservative estimate of a cache block's useful lifetime. We introduce Leeway, a new domain-agnostic cache management technique that uses live distance as a metric for dead block predictions. Leeway uses code-data correlation to associate live distance for a group of blocks with a PC that brings the block into the cache. While live distance as a metric provides",
            "retrieval score": "1.4820393",
            "text": "either cause premature eviction of a useful cache block, leading to an additional cache miss or cause delay in eviction of a dead block, leading to wastage of cache capacity. This calls for cache management mechanisms and policies that can tolerate variability in the reuse behavior of cache blocks to maximize cache e ciency. Our Proposals Aim of this thesis is: To design robust cache management mechanisms and policies for LLC that minimize cache misses in the face of variability in the reuse behavior of cache blocks, while keeping the cost and complexity of the hardware implementation low. To that end, we propose two cache management techniques, one domain-agnostic and one domain-specialized, that introduce robust mechanisms and policies to address variability in reuse prediction. The rest of the chapter provides a brief overview of both proposals. Domain-Agnostic Cache Management In this part of the thesis, we consider a conventional approach to cache management, namely domain-agnostic cache management, in which the LLC is managed completely in hardware. Such approach is quite attractive in practice as the cache management remains fully transparent to the application software. There has been a rich history of works that proposed various domain-agnostic techniques to improve cache e ciency,18,37,39,40,54,59,63,67,69,71,73,76,78,80,81,82,85,86,87,88,89,97,103,110. The state-of-the-art techniques employ prediction mechanisms that seek to correctly identify as many dead blocks as possible and evict them immediately after their last use to reduce cache thrashing. These predictors all rely on some metric of temporal reuse to make their decisions regarding the end of a given block's useful life. Previous",
            "title": ""
        },
        {
            "id": "2802538",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. ous research studies examining the character of modern Internet traffic have indicated that caching has the potential to greatly reduce network load for a given traffic demand [2,3,4]. Indeed, Web caches are vital network elements, bringing popular content closer to the users, contributing to faster data delivery, and reducing network and server load within ISPs and at large stub networks. However, some studies question the effectiveness of Web caches [5,6], arguing that redundancy should be detected at a finer granularity, such as packets, instead of objects. These designs, also known as packet-level caches, can be significantly more efficient in eliminating repeated content transfers. Nevertheless, they present significant scalability and flexibility issues, such as managing large lookup indexes, performing per packet lookups at wire-speed, operating in more than one link and synchronizing lookup indexes. Most such weaknesses can potentially be addressed by Information-Centric Networking (ICN) [7]. ICN proposes a clean slate network architecture where all network operations concern information itself, in contrast to IP-based networking, where communication is endpoint-oriented. Most ICN initiatives adopt a model of receiver-driven content delivery of self-identified packets that can be temporarily cached by routers, allowing routers to satisfy future requests for the same content. Nevertheless, ICN caching has not",
            "retrieval next text": "yet met these expectations, receiving criticism for its efficiency [8,9], based on the debatable performance superiority of distributed in-network caching over independent caches at the network edge, as well as on the questionable support for packet-level caching by today's hardware. In this paper we introduce Object-oriented Packet Caching (OPC), a novel packet-level caching scheme for ICN architectures. OPC is designed to improve the performance of ICN packet caches by increasing the usable caching capacity of commodity routers, without requiring additional storage resources. Furthermore, OPC addresses the looped replacement and large object poisoning effects, two common issues with packet caches that can highly penalize the performance of ICN in-network caching. The remainder of this paper is organized as follows. In Section 2 we review work in packet-level caching and the issues raised by it in an ICN context. In Section 3 we explain how OPC works and how it addresses these challenges. In Section 4 we present an evaluation study of OPC, showing the gains achieved. We conclude and discuss future work in Section 5. Packet caches in IP Packet-level caching in IP networks requires detecting redundancy in arbitrary packets at wire-speeds. The computational cost for avoiding replication via, say, suppressing replicated data [10], deep packet inspection [11] and/or delta coding [12], has prevented Web caches from moving in this direction. Interest in packet-level caching was rejuvenated by a computationally efficient technique for finding redundancy in Web traffic [5], where Rabin fingerprints are used to detect similar, but not necessarily identical, information transfers in real time. As",
            "retrieval score": "1.4801118",
            "text": "notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. ous research studies examining the character of modern Internet traffic have indicated that caching has the potential to greatly reduce network load for a given traffic demand,3,4. Indeed, Web caches are vital network elements, bringing popular content closer to the users, contributing to faster data delivery, and reducing network and server load within ISPs and at large stub networks. However, some studies question the effectiveness of Web caches,6, arguing that redundancy should be detected at a finer granularity, such as packets, instead of objects. These designs, also known as packet-level caches, can be significantly more efficient in eliminating repeated content transfers. Nevertheless, they present significant scalability and flexibility issues, such as managing large lookup indexes, performing per packet lookups at wire-speed, operating in more than one link and synchronizing lookup indexes. Most such weaknesses can potentially be addressed by Information-Centric Networking (ICN). ICN proposes a clean slate network architecture where all network operations concern information itself, in contrast to IP-based networking, where communication is endpoint-oriented. Most ICN initiatives adopt a model of receiver-driven content delivery of self-identified packets that can be temporarily cached by routers, allowing routers to satisfy future requests for the same content. Nevertheless, ICN caching has not",
            "title": ""
        },
        {
            "id": "6672799",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "access latency. The reason is that a page fault handling latency is strongly related to the page size when the storage is SCM, which does not need seek movement. Second, even though a small page is efficient with respect to the latency of data access, it degrades the latency of address translation by increasing TLB miss counts. Due to this reason, deciding a page size needs to consider the trade-off relation of the data access latency and the address translation latency. Our simulation results exhibit that the performance of virtual memory paging can be improved by adjusting the page size appropriately considering application characteristics, storage types, and available memory sizes. However, the page size will not be a significant issue in mobile platforms like Android, where applications are killed before memory space is exhausted, making situations simpler. We expect that the analysis shown in this article will be useful in configuring file caches and paging systems with the emerging SCM storage. The remaining part of this article is organized as follows. In Section 2, we quantify the performance implication of file caching as the storage medium changes from hard disk to SCM. Section 3 anatomizes the virtual memory paging performances with highperformance SCM storage particularly focusing on page sizes. Section 4 presents the experimental results through conducting simulation experiments to observe the implications of SCM storage based memory management systems. Section 5 discusses the adoption of our model to the mobile application environments. Finally, we present the conclusion of this article in Section 6. File Caching",
            "retrieval next text": "for SCM Storage This section quantifies the efficiency of file caching as the storage medium changes from hard disk to SCM. To this end, we measure the latency of file system operations when DRAM is used as storage. Note that DRAM is volatile, but we use a certain area of DRAM as a storage partition just to see the effect of fast storage media. Note also that this situation implies the optimistic performance of SCM storage. We added a profiler to Ext4 for measuring the latency of directly accessing data from the DRAM storage and the latency of accessing data in the DRAM file cache. Figure 1 depicts the access latency of DRAM storage and DRAM file cache as the size of data accesses changes. In this graph, we measure each case 10 times and plot their average. As can be seen in the graph, the access latency from DRAM storage is 30% longer than accessing the same data from DRAM file cache. Although the same DRAM is used for cache and storage, the performance gap occurs due to the existence of software I/O stack. When considering these results, file caching can be still necessary to buffer the latency gap between storage and memory although their access latencies are identical. However, as the gap is very small, some conditions need to be met for file caching to be beneficial. When we use file caching, the accessed data should be stored into the cache, which requires additional latency. The access latency of DRAM storage shown in Figure",
            "retrieval score": "1.4788387",
            "text": "access latency. The reason is that a page fault handling latency is strongly related to the page size when the storage is SCM, which does not need seek movement. Second, even though a small page is efficient with respect to the latency of data access, it degrades the latency of address translation by increasing TLB miss counts. Due to this reason, deciding a page size needs to consider the trade-off relation of the data access latency and the address translation latency. Our simulation results exhibit that the performance of virtual memory paging can be improved by adjusting the page size appropriately considering application characteristics, storage types, and available memory sizes. However, the page size will not be a significant issue in mobile platforms like Android, where applications are killed before memory space is exhausted, making situations simpler. We expect that the analysis shown in this article will be useful in configuring file caches and paging systems with the emerging SCM storage. The remaining part of this article is organized as follows. In Section 2, we quantify the performance implication of file caching as the storage medium changes from hard disk to SCM. Section 3 anatomizes the virtual memory paging performances with highperformance SCM storage particularly focusing on page sizes. Section 4 presents the experimental results through conducting simulation experiments to observe the implications of SCM storage based memory management systems. Section 5 discusses the adoption of our model to the mobile application environments. Finally, we present the conclusion of this article in Section 6. File Caching",
            "title": ""
        },
        {
            "id": "4512133",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "One Queue Is All You Need: Resolving Head-of-Line Blocking in Large Language Model Serving $ $Large language models (LLMs) have become an increasingly important workload for cloud providers catering to both enterprise and consumer applications. LLM inference requests from these applications have end-to-end latency SLOs that must be adhered to in production settings. However, existing LLM serving systems focus on optimization objectives such as request serving throughput or request execution latency rather than the end-to-end latency SLOs. Achieving end-to-end SLOs for latency-sensitive requests is challenging due to head-of-line (HOL) blocking in the request queue, which results from bursty arrival rates and insufficient resources. To address the above challenge, we propose QLM, a multi-model queue management framework for LLM serving. QLM uses stochastic programming to orchestrate the actions of multiple LLM Serving Operations (LSOs) to reduce HOL blocking and maximize SLO attainment. Specifically, QLM uses the following LSOs: model swapping, request eviction, GPU-CPU state swapping, load balancing, and warm model start. Evaluation on heterogeneous GPU devices and models with real-world LLM serving dataset shows that QLM improves SLO attainment by 40-90% and throughput by 20-400% while maintaining or improving device utilization compared to other state-of-the-art LLM serving systems.",
            "retrieval next text": "Effect of childhood nutrition counselling on intelligence in adolescence: a 15-year follow-up of a cluster-randomised trial Abstract Objective The present study aimed to assess the effects of an early childhood nutrition counselling intervention on intelligence (as measured by the intelligence quotient (IQ)) at age 15\u201316 years. Design A single-blind, cluster-randomised trial. Setting In 1998, in Southern Brazil, mothers of children aged 18 months or younger were enrolled in a nutrition counselling intervention (n 424). Counselling included encouragement and promotion of exclusive breast-feeding until 6 months of age and continued breast-feeding supplemented by protein-, lipid- and carbohydrate-rich foods after age 6 months up to age 2 years. The control group received routine feeding advice. In 2013, the fourth round of follow-up of these individuals, at the age of 15\u201316 years, was undertaken. IQ was assessed using the short form of the Wechsler Adult Intelligence Scale (WAIS-III). Mental disorders (evaluated using the Development and Well-Being Assessment (DAWBA)) and self-reported school failure, smoking and alcohol use were also investigated. Adjusted analyses were conducted using a multilevel model in accordance with the sampling process. Subjects Adolescents, mean (sd) age of 15\u00b74 (0\u00b75) years (n 339). Results Mean (sd) total IQ score was lower in the intervention group than the control group (93\u00b74 (11\u00b74) and 95\u00b78 (11\u00b72), respectively) but the association did not persist after adjustment. The prevalence of any mental disorders was similar between intervention and control groups (23\u00b71 and 23\u00b75 %, respectively). There were no differences between groups regarding school failure, smoking and alcohol use. Conclusions Nutrition counselling intervention",
            "retrieval score": "1.4781084",
            "text": "One Queue Is All You Need: Resolving Head-of-Line Blocking in Large Language Model Serving $ $Large language models (LLMs) have become an increasingly important workload for cloud providers catering to both enterprise and consumer applications. LLM inference requests from these applications have end-to-end latency SLOs that must be adhered to in production settings. However, existing LLM serving systems focus on optimization objectives such as request serving throughput or request execution latency rather than the end-to-end latency SLOs. Achieving end-to-end SLOs for latency-sensitive requests is challenging due to head-of-line (HOL) blocking in the request queue, which results from bursty arrival rates and insufficient resources. To address the above challenge, we propose QLM, a multi-model queue management framework for LLM serving. QLM uses stochastic programming to orchestrate the actions of multiple LLM Serving Operations (LSOs) to reduce HOL blocking and maximize SLO attainment. Specifically, QLM uses the following LSOs: model swapping, request eviction, GPU-CPU state swapping, load balancing, and warm model start. Evaluation on heterogeneous GPU devices and models with real-world LLM serving dataset shows that QLM improves SLO attainment by 40-90% and throughput by 20-400% while maintaining or improving device utilization compared to other state-of-the-art LLM serving systems.",
            "title": ""
        },
        {
            "id": "13461126",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "make good admission decisions, however, most accesses (e.g., to popular objects) do not necessitate exact reuse time prediction; these times vary, but are always very short.Based on these findings, SLAP proposes a segmented reuse time model that predicts a reuse time range for each cache miss rather than an exact value.Specifically, after evaluating multiple options, SLAP selects a Long Short-Term Memory (LSTM) [13] model.Prediction accuracy is monitored during runtime, and retraining is triggered when workloads change.Second, deep learning models (like LSTM) have a high prediction latency.The SLAP LSTM architecture was delicately designed to allow a large portion of computation on recent access features to overlap the miss object fetching.SLAP was able to hide 98% of prediction latency by doing so.Finally, SLAP reuses common prediction computation on concurrent requests to reduce computation overhead, and uses trace sampling to reduce the training time of the LSTM model. We show that SLAP is effective at providing good CDN performance (e.g., hit rate), significantly reducing CDN write traffic (hence lower operational costs), and easily adapting to varying cache sizes.Our experiments with real-world traces show that: SLAP enables consistently better hit rate and significantly smaller write traffic, compared to all state-of-the-art learned and heuristic-based admission policies.For example, applying SLAP with the LRU replacement policy improves the SSDs lifetime by 104% -178% and reduces the cache write traffic by 38% -59%, outperforming all existing approaches.In addition, SLAP improves the hit rate by 3.2% -11.7%.Most importantly, SLAP requires no effort to deploy on cache instances of different size (for the same service) and",
            "retrieval next text": "to adapt to varying sizes, whereas other learned policies require significant re-training overheads. The rest of this paper is structured as follows.In Section 2, we introduce the background and motivation.In Section 3, we discuss the design of SLAP, including the segmented reuse time labeling model, and the admission policy architecture.We then present experimental results in Section 4. In Section 5, we discuss related work.Finally, we conclude in Section 6. BACKGROUND AND MOTIVATION We provide background on CDN and describe the goal of CDN cache admission policies.We explain why previous heuristic-based and learned cache admission policies are inadequate for large-scale production CDN deployments. CDN Cache Admission Policies CDNs are an essential component of internet infrastructure for many web services [33].Before accessing data (e.g., video, web page, etc.) over slow Wide-Area Networks (WANs), end users check a CDN cache for the content they are accessing.In production environments, web services such as video streaming pay CDN providers for caching; CDN providers maintain cluster of cache servers to serve these tenants.Flash SSDs are commonly used in CDN servers [1] due to their large capacity and low cost (compared to DRAM). CDN cache admission policies are critical for achieving a high cache hit rate for various services, as well as reducing write traffic to Flash SSDs, limiting SSD wear, and thus lowering operational costs.There have been dozens of cache admission policies designed to achieve these two goals.First, heuristic-based policies [14] define the rules that will be used to determine data admission.For example, in [22], an object is admitted into cache after",
            "retrieval score": "1.4755446",
            "text": "make good admission decisions, however, most accesses (e.g., to popular objects) do not necessitate exact reuse time prediction; these times vary, but are always very short.Based on these findings, SLAP proposes a segmented reuse time model that predicts a reuse time range for each cache miss rather than an exact value.Specifically, after evaluating multiple options, SLAP selects a Long Short-Term Memory (LSTM) model.Prediction accuracy is monitored during runtime, and retraining is triggered when workloads change.Second, deep learning models (like LSTM) have a high prediction latency.The SLAP LSTM architecture was delicately designed to allow a large portion of computation on recent access features to overlap the miss object fetching.SLAP was able to hide 98% of prediction latency by doing so.Finally, SLAP reuses common prediction computation on concurrent requests to reduce computation overhead, and uses trace sampling to reduce the training time of the LSTM model. We show that SLAP is effective at providing good CDN performance (e.g., hit rate), significantly reducing CDN write traffic (hence lower operational costs), and easily adapting to varying cache sizes.Our experiments with real-world traces show that: SLAP enables consistently better hit rate and significantly smaller write traffic, compared to all state-of-the-art learned and heuristic-based admission policies.For example, applying SLAP with the LRU replacement policy improves the SSDs lifetime by 104% -178% and reduces the cache write traffic by 38% -59%, outperforming all existing approaches.In addition, SLAP improves the hit rate by 3.2% -11.7%.Most importantly, SLAP requires no effort to deploy on cache instances of different size (for the same service) and",
            "title": ""
        },
        {
            "id": "3510373",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "HOC admissions, we argue that our approach can be flexibly extended to learn CDN eviction decisions with multiple objectives; we leave a systematic exploration for future work. CONCLUSION We presented Darwin, a CDN cache management system that uses a novel cache admission approach.Darwin is robust to traffic pattern changes, can optimize different caching objectives, and accommodates unrestricted action spaces.Darwin's offline clustering and expert prediction approaches provide crucial side information to its online phase, where a bandit selection algorithm quickly selects the right admission policy to use for the currently observed traffic pattern.Our evaluation shows that Darwin is highly effective at adaptation and at optimizing both hit rates and operational costs (such as disk writes and BMR) while offering high throughput at low overhead.Besides cache management, our novel learning paradigm of offline clustering and online expert selection is likely applicable to auto-tuning other system components and is the subject of future research.Ethics Statement: Our work uses synthetic traces or anonymized production CDN traces and raises no ethical concerns. A APPENDIX Appendices are supporting material that has not been peer-reviewed. A.1 Offline Clustering and Expert Sets We assume that each offline-collected traffic trace contains sequences of requests indexed by a triple of the ID, size, and timestamp associated with the requested object.Forming the clusters: For each trace, we compute a variety of statistics to use in clustering.In our setting, we found the following statistics to serve as useful features; we note Darwin allows the CDN server operators to use other features, too.The features are: (a).Average request size, (b).Vector",
            "retrieval next text": "of first average inter-arrival times: The \u210e inter-arrival time is the time elapsed between + 1 successive requests with the same ID, and (c).Vector of first average stack distances; the \u210e stack distance measures the cumulative size of all requests received between + 1 successive requests with the same ID.The averages above are over all the choices of object IDs; , are hyperparameters.Together, these statistics summarize the trace and serve as features to cluster traces using the -means clustering algorithm.The total number of clusters to be formed can also be tuned as necessary.Clusters to experts sets: We evaluate the HOC hit rate of each expert over all the traces offline.For each trace, we collect experts that achieve hit rates within = 1% of its best-performing expert to form the trace-level \"best expert set\".We then take the union of the trace-level best expert sets of all traces in a cluster to form the cluster-level best expert set. This offline process results in a map from the features to a set of experts that are best suited for these features.There are two reasons that motivate the association process: (a).The cluster-level best expert sets can potentially be much smaller than the total number of available experts, (b).The = 1% threshold above potentially captures the true best experts of the traces with similar features that were not present in the logs.Both these reasons will help accelerate learning in the online phase of the caching process. A.2 Proofs of Theorems in Section 4.2 In our analysis, we assume that rewards are",
            "retrieval score": "1.4737523",
            "text": "HOC admissions, we argue that our approach can be flexibly extended to learn CDN eviction decisions with multiple objectives; we leave a systematic exploration for future work. CONCLUSION We presented Darwin, a CDN cache management system that uses a novel cache admission approach.Darwin is robust to traffic pattern changes, can optimize different caching objectives, and accommodates unrestricted action spaces.Darwin's offline clustering and expert prediction approaches provide crucial side information to its online phase, where a bandit selection algorithm quickly selects the right admission policy to use for the currently observed traffic pattern.Our evaluation shows that Darwin is highly effective at adaptation and at optimizing both hit rates and operational costs (such as disk writes and BMR) while offering high throughput at low overhead.Besides cache management, our novel learning paradigm of offline clustering and online expert selection is likely applicable to auto-tuning other system components and is the subject of future research.Ethics Statement: Our work uses synthetic traces or anonymized production CDN traces and raises no ethical concerns. A APPENDIX Appendices are supporting material that has not been peer-reviewed. A.1 Offline Clustering and Expert Sets We assume that each offline-collected traffic trace contains sequences of requests indexed by a triple of the ID, size, and timestamp associated with the requested object.Forming the clusters: For each trace, we compute a variety of statistics to use in clustering.In our setting, we found the following statistics to serve as useful features; we note Darwin allows the CDN server operators to use other features, too.The features are: (a).Average request size, (b).Vector",
            "title": ""
        },
        {
            "id": "4830043",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "it will require deploying multiple, smaller caches. Additional help is expected from recommendation systems, whose effect is to shape the demand concentrating it around the most popular content items. Intuitively, having fewer, popular items to serve will improve caching performance. In this context, our paper targets three main questions. Vehicular demand. What is the data demand generated by today's vehicular users? Which apps and services represent the most significant contributions thereto? Caching architectures. Given a target hit ratio, what is the relationship between caching architecture and size of the caches we need to deploy? What is the impact of moving caches from core-level switches to individual base stations, on the total cache size, on the distance data must travel within the core network, and on the load thereof? What changes if a recommendation system is in place? Location-specific content. Content items consumed by future vehicular networks are expected to strongly depend on the location -augmented maps for self-driving vehicles being the most obvious example. What will be the impact of this kind of content on caching? We answer these questions using a set of real-world, large-scale measurement data, coming from users of the WeFi app [3]. Due to its crowd-sourced nature, our dataset includes data for: (i) multiple apps, including video (e.g., YouTube) and maps; (ii) multiple types of users, from pedestrian to vehicular ones; (iii) multiple network technologies, including 3G, LTE, and Wi-Fi; (iv) multiple operators. We describe our dataset, as well as the additional processing we need to perform in order to enhance the",
            "retrieval next text": "information it provides, in Section 2. Then, in Section 3 we explain how we model caching and caching architectures in our vehicular scenario. Section 4 presents numerical results and some relevant insights we obtain from them. Finally, Section 6 concludes the paper and sketches future work directions. Input data We describe the WeFi dataset we have access to in Section 2.1. Then in Section 2.2 we detail the processing steps we need, in order to extract further information that is not directly included therein. Finally, Section 2.3 explains how we complement the available information using other datasets and wellknown information. The WeFi dataset Our data comes from the users of an app called WeFi [3]. WeFi provides its users with information on the safest and fastest Wi-Fi access points available at the user's location. At the same time, it collects information about the user's location, connectivity and activity. WeFi reports over seven million downloads of the app globally, and over three billion daily records. In this work, we use the dataset relative to the city of Los Angeles -a vehicle-dominated environment. Its main features are summarized in Table 1. Each record contains the following information: \u2022 day, hour (a coarse-grained timestamp); \u2022 anonymized user identifier and GPS position; \u2022 network operator, cell ID, cell technology and local area (LAC) the user is connected to (if any); \u2022 Wi-Fi network (SSID) and access point (BSSID) the user is connected to (if any); \u2022 active app and amount of downloaded/uploaded data. If the location of the user or",
            "retrieval score": "1.4720025",
            "text": "it will require deploying multiple, smaller caches. Additional help is expected from recommendation systems, whose effect is to shape the demand concentrating it around the most popular content items. Intuitively, having fewer, popular items to serve will improve caching performance. In this context, our paper targets three main questions. Vehicular demand. What is the data demand generated by today's vehicular users? Which apps and services represent the most significant contributions thereto? Caching architectures. Given a target hit ratio, what is the relationship between caching architecture and size of the caches we need to deploy? What is the impact of moving caches from core-level switches to individual base stations, on the total cache size, on the distance data must travel within the core network, and on the load thereof? What changes if a recommendation system is in place? Location-specific content. Content items consumed by future vehicular networks are expected to strongly depend on the location -augmented maps for self-driving vehicles being the most obvious example. What will be the impact of this kind of content on caching? We answer these questions using a set of real-world, large-scale measurement data, coming from users of the WeFi app. Due to its crowd-sourced nature, our dataset includes data for: (i) multiple apps, including video (e.g., YouTube) and maps; (ii) multiple types of users, from pedestrian to vehicular ones; (iii) multiple network technologies, including 3G, LTE, and Wi-Fi; (iv) multiple operators. We describe our dataset, as well as the additional processing we need to perform in order to enhance the",
            "title": ""
        },
        {
            "id": "9683247",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "(or poorly) an algorithm was performing compared to others.When comparing write rates, the best performer has the lowest write rate, as this translates to the fewest writes/updates, which is one of the goals of non-datapath caching algorithms.In order to prevent small differences in metrics leading to large normalized differences, the results where the performance used for normalizing was less than 10% were not included.Caches for which the best performing algorithm produces extremely small hit-rates are unlikely to be of practical use in any case. Each measurable aspect of FOMO is given its own section.First, we discuss how FOMO is able to limit writes to the cache in a manner similar to the state-of-the-art nondatapath caching policies.Second, we discuss FOMO's hit rate performance compared to other non-datapath caching algorithms.Third, we compare the FOMO and TinyLFU admission policies using different cache replacement algorithms.Fourth, we analyzed how FOMO's Miss-History is able to detect patterns of reuse within the workload, using a particular workload for a case study.Finally, we discuss the adversarial workloads of Cloud-Cache and why FOMO and the other non-datapath caching algorithms see performance degradation compared to datapath cache algorithms. Non-Datapath Appropriate Write Rate First, we shall establish that the write rate of FOMO is similar to that of leading nondatapath algorithms and admission policies.The lower this measure, the less often a write operation is performed on the cache and the longer the cache's lifespan would then be. When compared to its peers, FOMO's write rate is primarily aligned with LARC, regardless of the internal cache replacement algorithm.As",
            "retrieval next text": "FOMO attempts to identify more opportunities to insert, we can see this affect the tail of its results, as seen in Figure 5. Ultimately, even with these additional writes, FOMO often writes similarly to LARC and more than TinyLFU. Consistent Hit Rate Performance Next, we shall establish that, when augmenting a datapath caching algorithm, FOMO is both able to improve or preserve hit rate performance of the internal cache replacement algorithm.In particular, we establish that, of the two admission policies (FOMO and TinyLFU), FOMO is better able to improve or preserve the hit rate of the internal cache replacement algorithm over TinyLFU by focusing more on the changing of working sets than the preservation of valuable items in the cache. When we compare FOMO's overall performance, as seen in Figure 6, to other nondatapath algorithms, we can see that its performance is similar to that of mARC and LARC, regardless of the internal cache replacement algorithm.In particular, we can see that FOMO(ARC) has the most consistent performance, having a smaller tail performance degradation compared to other versions of FOMO.TinyLFU, another admission policy, has the most variance in its performance compared to whichever algorithm performed the best. To simplify our observation of FOMO's performance in comparison to mARC, LARC, and TinyLFU, we focus upon a single version of both FOMO and TinyLFU that utilize ARC as their internal cache replacement algorithm.Figure 7 allows us to observe how well or poorly FOMO does in comparison to each algorithm on a more detailed basis.We can see that, when compared",
            "retrieval score": "1.4696848",
            "text": "(or poorly) an algorithm was performing compared to others.When comparing write rates, the best performer has the lowest write rate, as this translates to the fewest writes/updates, which is one of the goals of non-datapath caching algorithms.In order to prevent small differences in metrics leading to large normalized differences, the results where the performance used for normalizing was less than 10% were not included.Caches for which the best performing algorithm produces extremely small hit-rates are unlikely to be of practical use in any case. Each measurable aspect of FOMO is given its own section.First, we discuss how FOMO is able to limit writes to the cache in a manner similar to the state-of-the-art nondatapath caching policies.Second, we discuss FOMO's hit rate performance compared to other non-datapath caching algorithms.Third, we compare the FOMO and TinyLFU admission policies using different cache replacement algorithms.Fourth, we analyzed how FOMO's Miss-History is able to detect patterns of reuse within the workload, using a particular workload for a case study.Finally, we discuss the adversarial workloads of Cloud-Cache and why FOMO and the other non-datapath caching algorithms see performance degradation compared to datapath cache algorithms. Non-Datapath Appropriate Write Rate First, we shall establish that the write rate of FOMO is similar to that of leading nondatapath algorithms and admission policies.The lower this measure, the less often a write operation is performed on the cache and the longer the cache's lifespan would then be. When compared to its peers, FOMO's write rate is primarily aligned with LARC, regardless of the internal cache replacement algorithm.As",
            "title": ""
        },
        {
            "id": "1235885",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "terms of maximizing hit rates is one of the challenges faced in CDN. Authors in [2] analyzed the performance of a deployed CDN caching system by deriving its performance metrics using real video on demand (VoD) traces derived from an internet service provider (ISP) CDN caching system. In their analysis, caching system logs that contain a full report of anonymized requests from each user were used, along with the result of the caching operation and time needed to fulfill a particular request. The results of the analyses showed that, first, QoE is optimal if the time to serve is bounded, not if it is the lowest possible one. Also, there is not a straight proportional law between time to serve and resource size. Second, in the case of cache effectiveness, if the cache is serving an optimal number of users: if the per-flow, per-user hit ratio, both normalized to the resource size, are almost identical, then the cache is well performing, and adding more users will not lead to further benefits. Third, it remains the question of cache optimization by cache admission and cache eviction algorithms [2]. According to authors in [12], the research area of recommender systems is rather new compared to other disciplines in computer science. However, rapid growth of electronic commerce (e-commerce), social networks and broadcasting applications triggered and facilitated development of recommender approaches and techniques (content-based and collaborative filtering). Research studies on algorithms for collaborative filtering have proven to be effective in recommender systems on predicting popular items [10]. While user-based and",
            "retrieval next text": "item-based collaborative filtering methods are intuitive, matrix factorization (MF) techniques are effective since they aid to discover latent factors underlying interactions between users and items [8]. This aspect facilitates to determine what content is frequently requested and thus, to be in the cache, whether during cache admission or cache eviction. Cache replacement concept is suggested by authors in [2]. In collaborative filtering, a popular model is k-nearest neighbour (kNN) [11]. However, this technique has not shown to scale well with increasing amount of data. Recently, matrix factorization has become popular in recommender systems, used for both implicit and explicit feedback [12]. In earlier works on matrix factorization, singular value decomposition (SVD) was proposed to learn feature matrices [1]. But MF models learnt by SVD showed to be prone to overfitting. This lead to the proposition of regularized learning methods. [14]. The most popular approach in collaborative filtering is the use of model-based techniques consisting of matrix factorization (e.g., plain matrix factorization, matrix factorization with biases). These methods combine good scalability, predictive accuracy, and offer much flexibility for modeling various real-life situations [12]. Stochastic gradient descent-based optimization techniques have been efficient to find parameters of the model, i.e., the latent factors of users and items, minimizing the squared error or similar loss functions [16]. For our work, we proposed using matrix factorization techniques to predict popular content, potentially optimizing cache admission/eviction in content delivery networks. The Dataset The analysis was carried out on real-life CDN cache logs collected on multiple hosts in a large European telecommunication service",
            "retrieval score": "1.4695697",
            "text": "terms of maximizing hit rates is one of the challenges faced in CDN. Authors in analyzed the performance of a deployed CDN caching system by deriving its performance metrics using real video on demand (VoD) traces derived from an internet service provider (ISP) CDN caching system. In their analysis, caching system logs that contain a full report of anonymized requests from each user were used, along with the result of the caching operation and time needed to fulfill a particular request. The results of the analyses showed that, first, QoE is optimal if the time to serve is bounded, not if it is the lowest possible one. Also, there is not a straight proportional law between time to serve and resource size. Second, in the case of cache effectiveness, if the cache is serving an optimal number of users: if the per-flow, per-user hit ratio, both normalized to the resource size, are almost identical, then the cache is well performing, and adding more users will not lead to further benefits. Third, it remains the question of cache optimization by cache admission and cache eviction algorithms. According to authors in, the research area of recommender systems is rather new compared to other disciplines in computer science. However, rapid growth of electronic commerce (e-commerce), social networks and broadcasting applications triggered and facilitated development of recommender approaches and techniques (content-based and collaborative filtering). Research studies on algorithms for collaborative filtering have proven to be effective in recommender systems on predicting popular items. While user-based and",
            "title": ""
        },
        {
            "id": "8608258",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "queue up, time out, and retry, creating even more work and exacerbating performance problems. Unfortunately, publicly available information on communi-cation latencies in the cloud is scarce, impeding the decisionmaking process regarding critical timing assumptions.There are a plethora of websites and services that provide some information about average or median latency between cloud regions [5][6][7], but these services often do not provide regionlocal latencies or expose finer statistics on latency distributions to help engineers estimate how often, and by how much the communication latency may deviate from the average or median.As a result, most literature pulls these communication latency timing assumptions out of thin air.For example, the aforementioned Copilots [52] work runs on a dedicated cluster with average inter-node communication latency of 0.25ms, making 1 ms timeout a plausible assumption. In this work, we study the predictability of communication in the cloud to understand and empirically justify the timing assumptions engineers and designers make when working on cloud-native services and applications.To that extent, our contributions are two-fold.First, we present CLT, a simple opensource tool 1 to collect communication latency data across many cloud VMs.Second, we use CLT to study the communication latency patterns between VMs in three large cloud providers: Amazon Web Services (AWS), Google Compute Platform (GCP), and Microsoft Azure. Our tool, Cloud Latency Tester or CLT for short, is a simple echo-like application that can deploy to many VMs in different parts of the cloud, such as different placement groups, availability zones, or regions.The tool runs TCP traffic of configurable payload size and frequency between",
            "retrieval next text": "the VMs and collects the round-trip latency between all VM pairs.CLT also includes tools and scripts to process raw data and extract valuable statistics for desired pairs or sub-clusters of VMs (i.e., creating a latency histogram for all node pairs in the same availability zone). We used CLT to collect communication latency data in three large cloud providers.In particular, we look at the communication RTT between VMs in different deployment configurations, such as VMs in the same availability zone (AZ) or across AZs and regions.This data gives a lot of insights into the predictability of cloud communication in several common situations.For instance, we observe the potential for significant tail latency applications may experience in the cloud.The 99.99 th percentile tail latency for VMs in the same subnet of the same AZ is as much as 36\u00d7 higher than the average latency, while maximum RTT reaches as much as 2900\u00d7 the average.Considering that the 99.99 th percentile is not that rare (roughly every 10,000 th round-trip communication exchange), such high-tail latency can significantly impact applications relying on tight timing and communication latency assumptions.We also notice significant latency variations (as much as 7% change in 10 minutes) throughout the day across all tested clouds, suggesting a substantial impact from other cloud workloads/tenants on latency. 2 Background Latency in Cloud Systems The end-to-end communication latency in networked applications and systems consists of more than just the network latency between the nodes.Server hardware, virtualization stack, operating system, and the application itself may introduce additional overheads and jitter, as shown in",
            "retrieval score": "1.468946",
            "text": "queue up, time out, and retry, creating even more work and exacerbating performance problems. Unfortunately, publicly available information on communi-cation latencies in the cloud is scarce, impeding the decisionmaking process regarding critical timing assumptions.There are a plethora of websites and services that provide some information about average or median latency between cloud regions, but these services often do not provide regionlocal latencies or expose finer statistics on latency distributions to help engineers estimate how often, and by how much the communication latency may deviate from the average or median.As a result, most literature pulls these communication latency timing assumptions out of thin air.For example, the aforementioned Copilots work runs on a dedicated cluster with average inter-node communication latency of 0.25ms, making 1 ms timeout a plausible assumption. In this work, we study the predictability of communication in the cloud to understand and empirically justify the timing assumptions engineers and designers make when working on cloud-native services and applications.To that extent, our contributions are two-fold.First, we present CLT, a simple opensource tool 1 to collect communication latency data across many cloud VMs.Second, we use CLT to study the communication latency patterns between VMs in three large cloud providers: Amazon Web Services (AWS), Google Compute Platform (GCP), and Microsoft Azure. Our tool, Cloud Latency Tester or CLT for short, is a simple echo-like application that can deploy to many VMs in different parts of the cloud, such as different placement groups, availability zones, or regions.The tool runs TCP traffic of configurable payload size and frequency between",
            "title": ""
        },
        {
            "id": "7805596",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "policies that learn fast to maximize cache hits can mitigate the increasing costs of information transportation [54], and similar benefits can be expected for embedded and other computing systems [23]. This work aspires to advance our theoretical understanding of this fundamental problem and proposes new provably-optimal and computationally-efficient caching algorithms using a new modeling and solution approach based on optimistic learning. Motivation Common caching policies store the newly requested files and employ the Least-Recently-Used (LRU) [32], Least-Frequently-Used (LFU) [37] and other similar rules to evict files when the cache capacity is exhausted. Under certain statistical assumptions on the request trace, such policies maintain the cache at an optimal state, see [52,. However, with frequent addition of new content to libraries of online services and the high volatility of file popularity [28], these policies can perform arbitrarily bad. This has spurred intensive research efforts for policies that operate under more general conditions by learning on-the-fly the request distribution or adapting dynamically, e.g., with Reinforcement Learning, to observed requests; see Sec. 2. Nonetheless, these studies do not offer performance guarantees nor scale well for large libraries. The goal of this work is to design robust caching policies that are able to learn effective caching decisions with the aid of a prediction oracle of unknown quality (Fig. 1 left) even when the file requests are made in an adversarial fashion. To that end, we formulate the caching problem as an online convex optimization (OCO) problem [30]. At each slot = 1, 2, . . . , , a learner",
            "retrieval next text": "(the caching policy) selects a caching vector \u2208 X from the set of admissible cache states X \u2286 {0, 1} for a cache of size , where is the library size. Then, a 1-hot vector \u2208 {0, 1} with value 1 for the requested file is revealed, and the learner receives a reward of ( ) = \u27e8 , \u27e9 for cache hits. The reward is revealed only after committing , which naturally matches the dynamic caching operation where the cached files are decided before the next request arrives. Here, the learner makes no statistical assumptions and can follow any distribution, even one that is handpicked by an adversary. In the optimistic framework, the learner does not only consider its hit or miss performance so far when deciding , but also the predictor's performance and output ( Fig. 1 right). As customary in the online learning literature, we characterize the policy's performance by using the static regret metric: where \u2605 = argmax \u2208X =1 ( ) is the (typically unknown) best-in-hindsight cache decision that can be selected only with access to future requests. 1 The regret measures the accumulated reward gap between the online decisions { } and benchmark \u2605 . An algorithm is said to achieve sublinear regret when its average performance gap / vanishes as \u2192 \u221e. In this context, recent works have proposed caching policies that offer ( \u221a ) regret bound [12,44,45,51,53,64,65], which, in fact, is the optimal (as small as possible) achievable regret rate, see [50,Thm. 5.1], [12,Thm. 1]. Most of",
            "retrieval score": "1.4672046",
            "text": "policies that learn fast to maximize cache hits can mitigate the increasing costs of information transportation, and similar benefits can be expected for embedded and other computing systems. This work aspires to advance our theoretical understanding of this fundamental problem and proposes new provably-optimal and computationally-efficient caching algorithms using a new modeling and solution approach based on optimistic learning. Motivation Common caching policies store the newly requested files and employ the Least-Recently-Used (LRU), Least-Frequently-Used (LFU) and other similar rules to evict files when the cache capacity is exhausted. Under certain statistical assumptions on the request trace, such policies maintain the cache at an optimal state, see,. However, with frequent addition of new content to libraries of online services and the high volatility of file popularity, these policies can perform arbitrarily bad. This has spurred intensive research efforts for policies that operate under more general conditions by learning on-the-fly the request distribution or adapting dynamically, e.g., with Reinforcement Learning, to observed requests; see Sec. 2. Nonetheless, these studies do not offer performance guarantees nor scale well for large libraries. The goal of this work is to design robust caching policies that are able to learn effective caching decisions with the aid of a prediction oracle of unknown quality (Fig. 1 left) even when the file requests are made in an adversarial fashion. To that end, we formulate the caching problem as an online convex optimization (OCO) problem. At each slot = 1, 2, . . . , , a learner",
            "title": ""
        },
        {
            "id": "11204622",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "one multicast video stream. Another is to give more importance to prefix and evict the suffix of a video while giving room for only prefix. This approach has the advantage to save server resources as well as server access and network bandwidth, thus allowing the server to handle a large number of customers without sacrificing access latency. In this paper, we have proposed an adaptive dynamic replacement algorithm, which balances between LRU and LFU. It combines these two classical methods resulting in a good hit ratio and reducing the waiting time. The organization of the rest of the paper is as follows: In section 2 we present different replacement policy problem formulation. In Section 3 we discuss about related work. Section 4 presents an adaptive dynamic replacement policy. Section 5 presents algorithm and simulation results. Finally in section 6, we conclude the paper and refer to further work. REPLACEMENT POLICY PROBLEM A cache performs computations on data faster than the data that can be retrieved from the main memory. It attempts to accommodate the data rate at the CPU's demand rate. Three basic cache organizations have been defined at the level of cache memory [1,2]. Direct Mapped Each memory block is mapped to a unique cache block. This results in cache block repeatedly evicted even when there are empty slots. In this case, we do not require a replacement mechanism. Fully Associative A memory block is mapped to any one of the empty cache blocks, if one exists. If there is no empty cache block, a",
            "retrieval next text": "replacement policy is used to select one of the cache blocks for replacement. Set Associative This is a combination of fully-associative and direct-mapped schemes. Cache blocks are grouped into sets, finding a set is like the direct-mapped policy. Finding a block for replacement within the set is like the fully-associative policy. Hence, it is a compromise between the direct-mapped and fully associative placement policies. In general, this organization offers a good balance between hit ratios and implementation costs. The replacement policy determines how a memory block is mapped to a cache block. While choosing the blocks for replacement, it should ensure that the blocks likely to be referenced in the near future are retained in the cache. The replacement policy has considerable impact on the overall system performance in the selection process of a victim block in fully associative and set associative caches. Temporal locality refers to two accesses to a block of cache within a small period of time. The shorter the time between the first and last access to cache block the less likely it should be loaded from main memory. The optimization is brought by re-use of the block which has been brought to cache as often as possible. Cache replacement policies are used to optimize cache management. These policies are used to decide which item to keep and which to discard to make room for the new block. Some of the common replacement policies used are Belady's Min Least Recently Used (LRU): Replaces the block in the cache that has not been",
            "retrieval score": "1.4661967",
            "text": "one multicast video stream. Another is to give more importance to prefix and evict the suffix of a video while giving room for only prefix. This approach has the advantage to save server resources as well as server access and network bandwidth, thus allowing the server to handle a large number of customers without sacrificing access latency. In this paper, we have proposed an adaptive dynamic replacement algorithm, which balances between LRU and LFU. It combines these two classical methods resulting in a good hit ratio and reducing the waiting time. The organization of the rest of the paper is as follows: In section 2 we present different replacement policy problem formulation. In Section 3 we discuss about related work. Section 4 presents an adaptive dynamic replacement policy. Section 5 presents algorithm and simulation results. Finally in section 6, we conclude the paper and refer to further work. REPLACEMENT POLICY PROBLEM A cache performs computations on data faster than the data that can be retrieved from the main memory. It attempts to accommodate the data rate at the CPU's demand rate. Three basic cache organizations have been defined at the level of cache memory,2. Direct Mapped Each memory block is mapped to a unique cache block. This results in cache block repeatedly evicted even when there are empty slots. In this case, we do not require a replacement mechanism. Fully Associative A memory block is mapped to any one of the empty cache blocks, if one exists. If there is no empty cache block, a",
            "title": ""
        },
        {
            "id": "3531740",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "illustrates this behavior, grouping the blocks into three different categories: no reuse, just one reuse or more than one reuse (multiple reuse). As shown, our experimental results confirm that most lines in the LLC are dead. Notably, around 70% of the blocks do not receive any further access since the moment they enter the LLC. Only around 5% of the blocks just receives one further hit (i.e. one reuse) and around 25% exhibit more than one reuse. Consequently, getting blocks with just one use (the block fill, so no reuse) to bypass the LLC when they are evicted from the previous level caches, and just storing blocks with reuse (at least two LLC accesses), should allow to hold the small fraction of blocks with multiple reuses, increasing the LLC hit rate and improving system performance. Furthermore, Figure 3 shows that most LLC hits are to blocks having multiple reuses, which together with the aforementioned fact that most blocks inserted in the LLC do not experience any reuse, highly justify the idea of a content selector based on reuse detection between private caches and LLC. DASCA scheme In [1], the authors propose Dead Write Prediction Assisted STT-RAM Cache Architecture (DASCA) to predict and bypass dead writes (writes to data in last level caches not referenced again during the lifetime of corresponding cache blocks) for write energy reduction. In this work dead writes are classified into three categories: dead-on-arrival fills, dead-value fills and closing writes, as a theoretical model for redundant write elimination. On top of that they",
            "retrieval next text": "also present a dead write predictor based on a state-of-the-art dead block predictor [2]. Thus, DASCA bypasses a write operation to the SLLC only if it is predicted not to incur extra cache misses. Notably, DASCA adds a specific field to each line at the private levels to store the PC (program counter) of the instructions writing a block, being this PC only updated upon write operations. Also, a PC-signature table (prediction table) is included in the design in order to make the prediction about dead writes (this table is updated according to the mechanism shown in the Table 2 of the paper itself [1]). Specifically, the mechanism samples a few cache sets and keeps track of PC information only for those sets. Predictions are made via the predictor table, made up of saturating counters similar to those used in a bimodal branch predictor, being the counters indexed by the signatures stored in the sampler entries. Thus, this PC-based predictor correlates dead blocks with addresses of memory instructions (signatures), so that different signatures are used depending on the kind of dead write predicted. DESIGN In this section we first describe the baseline system we start from. Then we describe in detail the proposed design built on top of that. Baseline system The memory hierarchy used in the baseline multi-core system includes two private levels (L1 and L2) and a lastlevel cache shared among all the cores (SLLC). All caches are write-back, write-allocate and LRU. L1 and L2 are inclusive while the SLLC is non inclusive. The",
            "retrieval score": "1.4655993",
            "text": "illustrates this behavior, grouping the blocks into three different categories: no reuse, just one reuse or more than one reuse (multiple reuse). As shown, our experimental results confirm that most lines in the LLC are dead. Notably, around 70% of the blocks do not receive any further access since the moment they enter the LLC. Only around 5% of the blocks just receives one further hit (i.e. one reuse) and around 25% exhibit more than one reuse. Consequently, getting blocks with just one use (the block fill, so no reuse) to bypass the LLC when they are evicted from the previous level caches, and just storing blocks with reuse (at least two LLC accesses), should allow to hold the small fraction of blocks with multiple reuses, increasing the LLC hit rate and improving system performance. Furthermore, Figure 3 shows that most LLC hits are to blocks having multiple reuses, which together with the aforementioned fact that most blocks inserted in the LLC do not experience any reuse, highly justify the idea of a content selector based on reuse detection between private caches and LLC. DASCA scheme In, the authors propose Dead Write Prediction Assisted STT-RAM Cache Architecture (DASCA) to predict and bypass dead writes (writes to data in last level caches not referenced again during the lifetime of corresponding cache blocks) for write energy reduction. In this work dead writes are classified into three categories: dead-on-arrival fills, dead-value fills and closing writes, as a theoretical model for redundant write elimination. On top of that they",
            "title": ""
        },
        {
            "id": "14693222",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "most suitable caching policy in realtime to achieve higher hit ratio than any single policy, but also can improve the robustness against the unexpected popularity changes that a single policy is not designed for. CEC model can be well generalized to different cache scenarios with low computation overheads. It has great potential for deployment in real edge cache networks. (detail in Section VI.) Since video accounts for more than 70% of fixed and mobile Internet traffic today [10], we focus on video-on-demand caching in the following analysis and experiments. But our algorithms can be applied to caching of other content. II. RELATED WORK There is a rich research literature on caching. Many traditional caching algorithms have been proposed in different contexts, such as RANDOM, LRU, First-In First-Out (FIFO), LFU and Greedy-Dual-Size-Frequency (GDSF) [11]- [14]. In the classical caching studies, content requests are assumed to draw from a stationary popularity distribution, the socalled Independent Reference Model (IRM) [15]- [17]. Recent measurement studies have demonstrated that IRM cannot model the intrinsic non-stationarity in online traffic [18], [19]. For better adaptation to the time-varying popularity patterns, forecast-based cache replacement policies have recently been proposed [20], [21]. In [22], authors utilize Matrix Factorization (MF) based user interest forecast to enhance edge caching performance. There are also works on video content caching [23] and mobile edge caching [24], [25]. Deep Reinforcement Learning has also recently been applied to content caching area. For example, [26]- [36] use various critical features to train DRL models in an evolving manner so as to serve",
            "retrieval next text": "requests with popularity fluctuations and bursts. While most existing DRL caching policies generate caching decisions directly, our study focuses on DRL-based policy combination. A. Caching under Dynamic Content Popularity The core idea of caching is to place the most popular items in the limited buffer space to maximally serve user requests. To keep track of content popularity evolution, the cached items are constantly updated in proactive and/or reactive manners. The key to maximize caching gain is to accurately predict and keep track of the future content popularity. From this perspective, the traditional algorithms use simple content request statistics to predict future popularity. For example, the rationale behind LRU is that the more recently accessed content will be more popular in the near future, while LFU works under the assumption that the content accessed the most in the past will remain to be the most popular in future. When the user population handled by a cache server is large, such simple prediction can achieve good performance because of the stable content popularity distributions. In real content delivery systems, both content and users are highly dynamic: new items are constantly added to the catalog, user interests are fast changing, and the active population served by a cache box is time-varying, due to the temporal variations of user activities and user mobility. As a result, the content popularity presents strong and complicated temporal variations. This is particularly evident for edge caching, where the user population served by each cache box is small. A recent study [37], [38] on YouTube",
            "retrieval score": "1.4652123",
            "text": "most suitable caching policy in realtime to achieve higher hit ratio than any single policy, but also can improve the robustness against the unexpected popularity changes that a single policy is not designed for. CEC model can be well generalized to different cache scenarios with low computation overheads. It has great potential for deployment in real edge cache networks. (detail in Section VI.) Since video accounts for more than 70% of fixed and mobile Internet traffic today, we focus on video-on-demand caching in the following analysis and experiments. But our algorithms can be applied to caching of other content. II. RELATED WORK There is a rich research literature on caching. Many traditional caching algorithms have been proposed in different contexts, such as RANDOM, LRU, First-In First-Out (FIFO), LFU and Greedy-Dual-Size-Frequency (GDSF)-. In the classical caching studies, content requests are assumed to draw from a stationary popularity distribution, the socalled Independent Reference Model (IRM)-. Recent measurement studies have demonstrated that IRM cannot model the intrinsic non-stationarity in online traffic,. For better adaptation to the time-varying popularity patterns, forecast-based cache replacement policies have recently been proposed,. In, authors utilize Matrix Factorization (MF) based user interest forecast to enhance edge caching performance. There are also works on video content caching and mobile edge caching,. Deep Reinforcement Learning has also recently been applied to content caching area. For example,- use various critical features to train DRL models in an evolving manner so as to serve",
            "title": ""
        },
        {
            "id": "6208226",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference Large Language Models have excelled in various fields but encounter challenges in memory and time efficiency due to the expanding Key-Value (KV) cache required for long-sequence inference. Recent efforts try to reduce KV cache size to a given memory budget by evicting vast non-critical cache elements during runtime, while preserving generation quality. Our revisiting of current eviction methods reveals that they fundamentally minimize an upper bound of the $L_1$ eviction loss between the pre- and post-eviction outputs of multi-head self-attention mechanisms. Moreover, our analysis indicates that the common practices of uniformly assigning budgets across attention heads harm their post-eviction generation quality. In light of these findings, we propose a simple yet effective adaptive budget allocation algorithm. This algorithm not only optimizes the theoretical loss upper bound but also reduces the $L_1$ eviction loss in practice by aligning with the varied characteristics across different heads. By integrating this algorithm into two state-of-the-art methods, we demonstrate the effectiveness of using adaptive budget allocation to optimize KV cache eviction. Extensive evaluations on 16 datasets and the Needle-in-a-Haystack test confirm significant performance improvements across various tasks.",
            "retrieval next text": "The Leningrad Rock Club in St Petersburg: From Memory to Cultural Heritage Over the last 30 years, the popular musical past has become a subject of interest for researchers in many social sciences and the humanities, from cultural geography and cultural sociology to urban studies and public history. Popular music is regarded not only as a significant part of national and regional cultural history, but also as an important element of the urban cultural heritage. The revision of both the content and the very concept of cultural heritage is associated with large-scale changes in the humanities and social sciences that occurred in the second half of the twentieth century and influenced the transformation of historical politics in many cities, regions, and countries. In Russia, a diverse and rich musical past, related to the history of late Soviet popular and rock music, has only recently begun to be studied and represented in the urban space. The article discusses the ways of authorising the musical heritage associated with the history of the Leningrad Rock Club in contemporary St Petersburg. The research is based on field data collected in 2020\u20132021: in-depth and spontaneous interviews with musicians participating in the Leningrad Rock Club, rock journalists, employees of the Pushkinskaya, 10 Art Centre and the Kamchatka Rock Club-Museum, and field observations during guided tours of rock-related places in St Petersburg and exhibitions dedicated to the history of Leningrad rock. The author proposes to consider the urban musical heritage in St Petersburg self-authorised. The examples studied in the article are initiatives of",
            "retrieval score": "1.4644477",
            "text": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference Large Language Models have excelled in various fields but encounter challenges in memory and time efficiency due to the expanding Key-Value (KV) cache required for long-sequence inference. Recent efforts try to reduce KV cache size to a given memory budget by evicting vast non-critical cache elements during runtime, while preserving generation quality. Our revisiting of current eviction methods reveals that they fundamentally minimize an upper bound of the $L_1$ eviction loss between the pre- and post-eviction outputs of multi-head self-attention mechanisms. Moreover, our analysis indicates that the common practices of uniformly assigning budgets across attention heads harm their post-eviction generation quality. In light of these findings, we propose a simple yet effective adaptive budget allocation algorithm. This algorithm not only optimizes the theoretical loss upper bound but also reduces the $L_1$ eviction loss in practice by aligning with the varied characteristics across different heads. By integrating this algorithm into two state-of-the-art methods, we demonstrate the effectiveness of using adaptive budget allocation to optimize KV cache eviction. Extensive evaluations on 16 datasets and the Needle-in-a-Haystack test confirm significant performance improvements across various tasks.",
            "title": ""
        },
        {
            "id": "6631877",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "fast host-side memories and slow backend storage drives, which causes incredibly long I/O waiting time and a substantial amount of CPU idle wastage. This situation is even exacerbated by the fact that many cloud applications are I/O intensive, such as video streaming, news feeds and social networks (Hoseinzadeh 2019). To alleviate this disparity in latency, caching tiers have been widely employed to reside between main memories and back-end storage drives. The performance of caching systems is usually influenced by three factors: the data allocation policy, the accuracy of data hotness recognition and the data eviction policy. The data allocation policy basically controls the data flow and determines admissions of various data, such as read-only, write-only, or both; the high accuracy of data hotness recognition can prevent cache pollution from unnecessary data, improving cache performance via locality protection; the data eviction policy decides which data block to evict when the cache is full, thus indirectly increasing the effective cache capacity. These three factors, which give attention to three different aspects, are highly relevant. One thing worth mentioning is that conventional caching policies, such as LRU and LFU, are not a one-for-all solution complying with all these factors (Li et al. 2019;Li and Gu 2020;Liu et al. 2020). With these considerations, there arise increasing interests seeking for a unified workaround of optimized caching. In the meantime, both the variety and size of modern workloads are drastically growing, which urges the cache systems to equip with learning capabilities such that they can adjust their behaviors in real time according",
            "retrieval next text": "to versatile working patterns. The recent advancement in reinforcement learning (RL) has paved the way for solving many complicated problems, including but not limited to robotics (Lillicrap et al. 2015;Gu et al. 2017;Hwangbo et al. 2019), video game playing (Jaderberg et al. 2019;Vinyals et al. 2019), autonomous driving (Sallab et al. 2017;Kiran et al. 2020) and neural architecture search (Baker et al. 2017;Tan and Le 2019). Even though there has been a series of investigations applying machine learning for system optimizations (Dean 2017;Lecuyer et al. 2017;Bychkovsky et al. 2018;Song et al. 2020), there is an absence of RL caching systems competitive with existing heuristics, due to two principal unresolved issues: the lack of adequate Markov property in the problem formulation and the prohibitively large training overhead during the online learning process. First, in order to achieve better learning efficiency, the problem formulation (especially the state formulation) should obtain great Markov property. This indicates that the state representation should involve enough information extracted from proper features for the RL agent to make the optimal decisions. Nevertheless, under the cache scenario, increasing information floods into the observation as the data access trace proceeds, making it difficult to distill useful informa-tion for an appropriate state representation. Simultaneously, the state representation has to be compatible with the invariable deep neural network (DNN) input/output structure, which has proved to be powerful function approximators in RL. Second, training overhead is always a concern for online learning based caching systems. To this end, we propose a reuse-aware RL framework for optimized online caching,",
            "retrieval score": "1.4639094",
            "text": "fast host-side memories and slow backend storage drives, which causes incredibly long I/O waiting time and a substantial amount of CPU idle wastage. This situation is even exacerbated by the fact that many cloud applications are I/O intensive, such as video streaming, news feeds and social networks (Hoseinzadeh 2019). To alleviate this disparity in latency, caching tiers have been widely employed to reside between main memories and back-end storage drives. The performance of caching systems is usually influenced by three factors: the data allocation policy, the accuracy of data hotness recognition and the data eviction policy. The data allocation policy basically controls the data flow and determines admissions of various data, such as read-only, write-only, or both; the high accuracy of data hotness recognition can prevent cache pollution from unnecessary data, improving cache performance via locality protection; the data eviction policy decides which data block to evict when the cache is full, thus indirectly increasing the effective cache capacity. These three factors, which give attention to three different aspects, are highly relevant. One thing worth mentioning is that conventional caching policies, such as LRU and LFU, are not a one-for-all solution complying with all these factors (Li et al. 2019;Li and Gu 2020;Liu et al. 2020). With these considerations, there arise increasing interests seeking for a unified workaround of optimized caching. In the meantime, both the variety and size of modern workloads are drastically growing, which urges the cache systems to equip with learning capabilities such that they can adjust their behaviors in real time according",
            "title": ""
        },
        {
            "id": "7950382",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "The results show that the LRU cache eviction policy works better than the LFU eviction policy for small cache size configurations. LRU achieves 79.6% (OOI, 128GB) and 91.2% (GAGE, 32GB) higher throughput than LFU using 1TB and 256GB cache sizes respectively. In contrast, the throughput curve rises steeply when using LFU and achieves higher throughput than LRU with large cache sizes (i.e., 10TB, OOI). This implies that the recency of requests is more relevant than the frequency of request for observatories. Since DTN storage resources are limited and considering the typical scale of observatory data, performing well with a small cache size is important, and as a result, we suggest that using the LRU eviction policy is a better choice. We do not further evaluate advanced recency-based eviction models [49], [50] in this paper and consider it as future work. 2) Analysis of the pre-fetching mechanism: The goal of these experiments is to understand how pre-fetching improves the local data reuse. Figure 13 plots the average percentage of requests that are served from the local cache for the four strategies using the last experiment for the LRU cache configurations. The color in the plots marks the data sources, where blue is the percentage of requests served using cached data, and the yellow is percentage of requests serviced using prefetched data. These plots illustrate that pre-fetching enables users to obtain more data from their local cache. For instance, using the smallest cache sizes (OOI 128GB, GAGE 32GB) and the HPM pre-fetching model, the percentage of local data",
            "retrieval next text": "access is 41 for OOI and GAGE, respectively. As opposed to passively searching cached data, the prefetching mechanism proactively pushes data toward to user. It ensures that users can access more of the their data locally regardless of whether the data is reused from the previous requests. For example, in Figure 13b, GAGE has a smaller number of overlapping requests; however, the pre-fetching mechanism still enables a much higher percentage of local data accesses as compared to the baseline. Furthermore, the pre-fetching mechanism can achieve nearoptimal performance with a small cache size. The best performance is obtained for a 10TB cache size as the entire datasets for both traces fit into the cache. HPM with the smallest cache size can achieve 79.2% and 78.0% of this best performance, for OOI and GAGE, respectively. Based on these experiments, we conclude that our pushbased data delivery framework can enable users to access more data from their local DTN cache by proactively pushing data toward users. 3) Evaluation of the data placement strategy: In this experiment, we use HPM with LRU and the GAGE trace to evaluate the impact of the data placement strategy on the data delivery performance. Table IV summarizes the results. The first row presents the percentage of cached data that has been optimized by the data placement strategy using different cache configurations. We observe that the data placement strategy is more effective for a small cache size because it replicates hot data to the local data hub. We also measure the average throughput for retrieving",
            "retrieval score": "1.4632539",
            "text": "The results show that the LRU cache eviction policy works better than the LFU eviction policy for small cache size configurations. LRU achieves 79.6% (OOI, 128GB) and 91.2% (GAGE, 32GB) higher throughput than LFU using 1TB and 256GB cache sizes respectively. In contrast, the throughput curve rises steeply when using LFU and achieves higher throughput than LRU with large cache sizes (i.e., 10TB, OOI). This implies that the recency of requests is more relevant than the frequency of request for observatories. Since DTN storage resources are limited and considering the typical scale of observatory data, performing well with a small cache size is important, and as a result, we suggest that using the LRU eviction policy is a better choice. We do not further evaluate advanced recency-based eviction models, in this paper and consider it as future work. 2) Analysis of the pre-fetching mechanism: The goal of these experiments is to understand how pre-fetching improves the local data reuse. Figure 13 plots the average percentage of requests that are served from the local cache for the four strategies using the last experiment for the LRU cache configurations. The color in the plots marks the data sources, where blue is the percentage of requests served using cached data, and the yellow is percentage of requests serviced using prefetched data. These plots illustrate that pre-fetching enables users to obtain more data from their local cache. For instance, using the smallest cache sizes (OOI 128GB, GAGE 32GB) and the HPM pre-fetching model, the percentage of local data",
            "title": ""
        },
        {
            "id": "6989990",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "Phase-Aware Cache Partitioning to Target Both Turnaround Time and System Performance The Last Level Cache (LLC) plays a key role in the system performance of current multi-cores by reducing the number of long latency main memory accesses. The inter-application interference at this shared resource, however, can lead the system to undesired situations regarding performance and fairness. Recent approaches have successfully addressed fairness and turnaround time (TT) in commercial processors. Nevertheless, these approaches must face sustaining system performance, which is challenging. This work makes two main contributions. LLC behaviors regarding cache performance, data reuse and cache occupancy, that adversely impact on the final performance are identified. Second, based on these behaviors, we propose the Critical-Phase Aware Partitioning Approach (CPA), which reduces TT while sustaining (and even improving) IPC by making an effective use of the LLC space. Experimental results show that CPA outperforms CA, Dunn and KPart state-of-the-art approaches, and improves TT (over 40 percent in some workloads) over Linux default behavior while sustaining or even improving IPC by more than 3 percent in several mixes.",
            "retrieval next text": "[Cognitive disorders in occupational diseases of after contact period, associated with cardiovascular problems]. Vibration disease patients with chronic uranium intoxication in after contact period demonstrate moderate cognitive disorders of amnestic multifunctional type in 100% of cases. Individuals having occupational diseases due to physical, toxic and radiation hazards, associated with cardiovascular problems, in after contact period manifest mostly with visual-spatial functions, short-term, operative, visual and aural memory disorders. Visual-spatial functions, short-term, operative, visual and aural memory are decreased more in patients with remote period of chronic uranium intoxication.",
            "retrieval score": "1.4630183",
            "text": "Phase-Aware Cache Partitioning to Target Both Turnaround Time and System Performance The Last Level Cache (LLC) plays a key role in the system performance of current multi-cores by reducing the number of long latency main memory accesses. The inter-application interference at this shared resource, however, can lead the system to undesired situations regarding performance and fairness. Recent approaches have successfully addressed fairness and turnaround time (TT) in commercial processors. Nevertheless, these approaches must face sustaining system performance, which is challenging. This work makes two main contributions. LLC behaviors regarding cache performance, data reuse and cache occupancy, that adversely impact on the final performance are identified. Second, based on these behaviors, we propose the Critical-Phase Aware Partitioning Approach (CPA), which reduces TT while sustaining (and even improving) IPC by making an effective use of the LLC space. Experimental results show that CPA outperforms CA, Dunn and KPart state-of-the-art approaches, and improves TT (over 40 percent in some workloads) over Linux default behavior while sustaining or even improving IPC by more than 3 percent in several mixes.",
            "title": ""
        },
        {
            "id": "11029942",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "poorly. We settle this open question by showing that Arc is 4N -competitive and Car is 21Ncompetitive, thus proving that under Sleator and Tarjan's theoretical model, their worstcase behavior is asymptotically optimal, and therefore not much worse than the theoretically optimal Lru. The main challenge in solving these problems is that of carefully designing the potential function for the analysis. After providing relevant background on cache replacement algorithms in Section 2, we discuss the competitive analysis of Arc in Section 3 and the competitive analyses of Clock and Car in Section 4. Concluding remarks can be found in Section 5. Previous Work on Cache Replacement Strategies In practice, since Lru evicts the least recently used entry, it tends to perform well when there are many items that are requested more than once in a relatively short period of time, and perfomrs poorly on \"scans\". Clock implements a circular buffer for the entries, and its replacement strategy involves cycling through the pages in that buffer, treating it like a clock. Instead of the time of reference as in Lru, Clock maintains a reference bit for each item. If an entry is requested, the reference bit is set. Each entry is considered for eviction when the clock hand points to it. It is evicted only if its reference bit is not set, else the reference bit is reset and the clock moves on to the next entry. The DuelingClock algorithm [6] is like Clock but keeps the clock hand at the newest page rather than the oldest one,",
            "retrieval next text": "which allows it to be scan-resistant. More recent algorithms try to improve over Lru by implementating multiple cache levels and leveraging history. In [13] the Lru-K algorithm was introduced. Briefly, the Lru-K algorithm estimates interarrival times from observed requests, and favors retaining pages with shorter interarrival times. Experiments have shown Lru-2 performs better than Lru, and that Lru-K does not show increase in performance over Lru-2 [13], but has a higher implementation overhead. It was also argued that Lru-K is optimal under the independence reference model (IRM) among all algorithms A that have limited knowledge of the K most recent references to a page and no knowledge of the future [13]. In essence, the Lru-K algorithm tries to efficiently approximate Least Frequently Used (Lfu) cache replacement algorithm. As K becomes larger, it gets closer and closer to Lfu. It has been argued that Lfu cannot adapt well to changing workloads because it may replace currently \"hot\" blocks instead of \"cold\" blocks that had been \"hot\" in the past. Lfu is implemented as a heap and takes O(log N ) time per request. Another cache replacement algorithm is Lirs [8]. The Lirs algorithm evicts the page with the largest IRR (inter-reference recency). It attempts to keep a small (\u2248 1%) portion of the cache for HIR (high inter-reference) pages, and a large (\u2248 99%) portion of the cache for LIR (low inter-reference) pages. The Clock-Pro algorithm approximates Lirs efficienctly using Clock [7]. The 2q [9] algorithm is scan-resistant. It keeps a FIFO buffer A 1 of pages",
            "retrieval score": "1.4618967",
            "text": "poorly. We settle this open question by showing that Arc is 4N -competitive and Car is 21Ncompetitive, thus proving that under Sleator and Tarjan's theoretical model, their worstcase behavior is asymptotically optimal, and therefore not much worse than the theoretically optimal Lru. The main challenge in solving these problems is that of carefully designing the potential function for the analysis. After providing relevant background on cache replacement algorithms in Section 2, we discuss the competitive analysis of Arc in Section 3 and the competitive analyses of Clock and Car in Section 4. Concluding remarks can be found in Section 5. Previous Work on Cache Replacement Strategies In practice, since Lru evicts the least recently used entry, it tends to perform well when there are many items that are requested more than once in a relatively short period of time, and perfomrs poorly on \"scans\". Clock implements a circular buffer for the entries, and its replacement strategy involves cycling through the pages in that buffer, treating it like a clock. Instead of the time of reference as in Lru, Clock maintains a reference bit for each item. If an entry is requested, the reference bit is set. Each entry is considered for eviction when the clock hand points to it. It is evicted only if its reference bit is not set, else the reference bit is reset and the clock moves on to the next entry. The DuelingClock algorithm is like Clock but keeps the clock hand at the newest page rather than the oldest one,",
            "title": ""
        },
        {
            "id": "3471572",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving As large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge. Yet using long contexts is challenging, as nothing can be generated until the whole context is processed by the LLM. While the context-processing delay can be reduced by reusing the KV cache of a context across different inputs, fetching the KV cache, which contains large tensors, over the network can cause high extra network delays. CacheGen is a fast context-loading module for LLM systems. First, CacheGen uses a custom tensor encoder, leveraging KV cache's distributional properties to encode a KV cache into more compact bitstream representations with negligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts the compression level of different parts of a KV cache to cope with changes in available bandwidth, in order to maintain low context-loading delay and high generation quality. % When available bandwidth drops, CacheGen may raise the compression level for a part of the context or recompute its KV cache on the fly. We test CacheGen on popular LLMs and datasets. Compared to the recent systems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x and the total delay in fetching and processing contexts by 3.2-3.7x with negligible impact on the LLM response quality. Our code is at: https://github.com/UChi-JCL/CacheGen.",
            "retrieval next text": "Insights into the Na-Alloying Mechanism and Zintl Phase Transition of Lead-Based Anodes Sodium-ion batteries (SIBs) has been received growing attention in the electrical energy storage fields due to their low cost and earth-abundant sodium. [1] However, there has been a lack of new discoveries, growth directions, and real advancement with respect to Na-storage anodes. Despite the chemical similarities between sodium and lithium as alkali elements, the larger Na ion than Li ions (ionic radii of 0.98 \u00c5 and 0.68 \u00c5, respectively) is limited to insertion into host materials and results in different phase transition behavior. [2] Among the available anode candidate materials for SIBs, lead (Pb), which has a large atomic size than other elements (e.g. Si, Sn), provides a big interstitial space to accommodate large Na ions by fast ionic diffusion, enabling reversible Na alloying/dealloying and exhibiting high volumetric capacity. [3] Furthermore, when Pb is used as anode with layered sodium transition metal oxide as cathode, the energy density of the pouch-type cell is estimated to be 549 Wh/L and the cost is lower than 63.5 USD/kWh according to the Argonne BatPac model. [4] Therefore, Pb-based materials have competitive potential as promising anodes and it is crucial to understand the electrochemical process from a fundamental perspective. Here, we investigate a unique Na storage mechanism using a novel Pb-based carbon nanocomposite anode synthesized by a simple high-energy milling method. The electrochemical data show a decent cycle performance with a reversible capacity of 381 mAh/g. Nevertheless, the Na-storage performance of the Pb-based anode was not attractive compared",
            "retrieval score": "1.4607555",
            "text": "CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving As large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge. Yet using long contexts is challenging, as nothing can be generated until the whole context is processed by the LLM. While the context-processing delay can be reduced by reusing the KV cache of a context across different inputs, fetching the KV cache, which contains large tensors, over the network can cause high extra network delays. CacheGen is a fast context-loading module for LLM systems. First, CacheGen uses a custom tensor encoder, leveraging KV cache's distributional properties to encode a KV cache into more compact bitstream representations with negligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts the compression level of different parts of a KV cache to cope with changes in available bandwidth, in order to maintain low context-loading delay and high generation quality. % When available bandwidth drops, CacheGen may raise the compression level for a part of the context or recompute its KV cache on the fly. We test CacheGen on popular LLMs and datasets. Compared to the recent systems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x and the total delay in fetching and processing contexts by 3.2-3.7x with negligible impact on the LLM response quality. Our code is at: https://github.com/UChi-JCL/CacheGen.",
            "title": ""
        },
        {
            "id": "9399669",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "CoT: Decentralized Elastic Caches for Cloud Environments Distributed caches are widely deployed to serve social networks and web applications at billion-user scales. This paper presents Cache-on-Track (CoT), a decentralized, elastic, and predictive caching framework for cloud environments. CoT proposes a new cache replacement policy specifically tailored for small front-end caches that serve skewed workloads. Front-end servers use a heavy hitter tracking algorithm to continuously track the top-k hot keys. CoT dynamically caches the hottest C keys out of the tracked keys. Our experiments show that CoT's replacement policy consistently outperforms the hit-rates of LRU, LFU, and ARC for the same cache size on different skewed workloads. Also, \\algoname slightly outperforms the hit-rate of LRU-2 when both policies are configured with the same tracking (history) size. CoT achieves server size load-balance with 50\\% to 93.75\\% less front-end cache in comparison to other replacement policies. Introduction Social networks, the web, and mobile applications have attracted hundreds of millions of users [3,7]. These users share their relationships and exchange images and videos in timely personalized experiences [13]. To enable this real-time experience, the underlying storage systems have to provide efficient, scalable, and highly available access to big data. Social network users consume several orders of magnitude more data than they produce [10]. In addition, a single page load requires hundreds of object lookups that need to be served in a fraction of a second [13]. Therefore, traditional disk-based storage systems are not suitable to handle requests at this scale due to the high access latency of disks and I/O",
            "retrieval next text": "throughput bounds [50]. To overcome these limitations, distributed caching services have been widely deployed on top of persistent storage in order to efficiently serve user requests at scale [49]. Distributed caching systems such as Memcached [4] and Redis [5] are widely adopted by cloud service providers such as Amazon Elasti-Cache [1] and Azure Redis Cache [2]. These caching services offer significant latency and throughput improvements to systems that directly access the persistent storage layer. Redis and Memcached use consistent hashing [35] to distribute keys among several caching servers. Although consistent hashing ensures a fair distribution of the number of keys assigned to each caching shard, it does not consider the workload per key in the assignment process. Real-world workloads are typically skewed with few keys being significantly hotter than other keys [30]. This skew causes load-imbalance among caching servers. Load imbalance in the caching layer can have significant impact on the overall application performance. In particular, it may cause drastic increases in the latency of operations at the tail end of the access frequency distribution [29]. In addition, the average throughput decreases and the average latency increases when the workload skew increases [15]. This increase in the average and tail latency is amplified for real workloads when operations are executed in chains of dependent data objects [41]. A single Facebook page-load results in retrieving hundreds of objects in multiple rounds of data fetching operations [44,13]. Finally, solutions that equally overprovision the caching layer resources to handle the most loaded caching server suffer from resource underutilization in",
            "retrieval score": "1.4600054",
            "text": "CoT: Decentralized Elastic Caches for Cloud Environments Distributed caches are widely deployed to serve social networks and web applications at billion-user scales. This paper presents Cache-on-Track (CoT), a decentralized, elastic, and predictive caching framework for cloud environments. CoT proposes a new cache replacement policy specifically tailored for small front-end caches that serve skewed workloads. Front-end servers use a heavy hitter tracking algorithm to continuously track the top-k hot keys. CoT dynamically caches the hottest C keys out of the tracked keys. Our experiments show that CoT's replacement policy consistently outperforms the hit-rates of LRU, LFU, and ARC for the same cache size on different skewed workloads. Also, \\algoname slightly outperforms the hit-rate of LRU-2 when both policies are configured with the same tracking (history) size. CoT achieves server size load-balance with 50\\% to 93.75\\% less front-end cache in comparison to other replacement policies. Introduction Social networks, the web, and mobile applications have attracted hundreds of millions of users,7. These users share their relationships and exchange images and videos in timely personalized experiences. To enable this real-time experience, the underlying storage systems have to provide efficient, scalable, and highly available access to big data. Social network users consume several orders of magnitude more data than they produce. In addition, a single page load requires hundreds of object lookups that need to be served in a fraction of a second. Therefore, traditional disk-based storage systems are not suitable to handle requests at this scale due to the high access latency of disks and I/O",
            "title": ""
        },
        {
            "id": "10164463",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "by prior figures in this section. Also, when comparing Figures 32(c) and 32(f) it should be noted that owing to our choice of normalized units for bandwidth, the hit rate is always one when all clients have the same (constant) bandwidth above one (Figure 32(c)) whereas bandwidth variations in the national speedtest dataset (Figure 32(f)) result in significant periods of bandwidth below one even for average values substantially larger than one. Again, in practice, we expect clients sharing the same cache to see bandwidth variation between these two extremes, with operators likely to strive towards increasingly stable network conditions for streaming clients [20]. RELATED WORK Broadly, the related work can be split into works that consider the head movements during viewing of 360\u00b0videos, client-side techniques to provide the best possible QoE (e.g., through adaptive prefetching based on expected viewing directions), and caching of HAS videos. While some recent works have considered optimized cache management policies for 360\u00b0videos [25,29,32], none of these works provide a data-driven characterization of the caching opportunities that would be observed with traditional caching policies that simply cache the requested tiles when the clients apply adaptive prefetching techniques. Head-movement characterization: Some recent works have collected datasets and characterized the 360\u00b0viewer behavior [1,2,9,11,12,26,34]. However, most of these datasets use relatively short video segments and do not capture changes in behavior over time or across classes of videos. The primary exception, and the work most closely related to ours, is the work by Almquist et al. [1], as we use their dataset. In their work,",
            "retrieval next text": "they present a category-based characterization of the head movements over time, and analyze how changes in viewing behavior depend on the time window considered, but do not consider overlapping viewports of users watching the same video or other similarity metrics of users' viewing directions. Coverage and overlap metrics, such as those introduced here, are valuable in identifying potential caching and bandwidth saving opportunities. Client-side techniques: Motivated by HMDs allowing different projection and quality adaptive download techniques [52], various techniques have been proposed to adaptively download different qualities for different viewing directions [2,18,34,39,40,53]. These typically combine user head movement tracking/prediction [2,34,35,48] and bandwidth management [18,31]. For example, Bao et al. [2] show that view-dependent 360\u00b0transmission schemes with motion prediction can reduce bandwidth consumption by 45% at the cost of only a small performance degradation. Similarly, Hosseini and Swaminathan [18] present an adaptive tile-based streaming solution and show that large bandwidth savings (72%) can be achieved with only small quality degradation. Graf et al. [17] studied the impact of projection techniques, quantization parameters, and tile patterns on the playback experience and resource requirements. Others have considered tradeoffs that try to address variations and uncertainties in the user's bandwidth and viewing direction simultaneously [1,34,41,50]. For example, Sun et al. [41] use simulations and experiments that capture the bandwidth variations, Qian et al. [34] have implemented and tested a tile-based solution on a real network, whereas Almquist et al. [1] note that HAS clients typically try to maintain a steady buffer (to protect against stalls) and consider the optimized prefetch-aggressiveness",
            "retrieval score": "1.4591223",
            "text": "by prior figures in this section. Also, when comparing Figures 32(c) and 32(f) it should be noted that owing to our choice of normalized units for bandwidth, the hit rate is always one when all clients have the same (constant) bandwidth above one (Figure 32(c)) whereas bandwidth variations in the national speedtest dataset (Figure 32(f)) result in significant periods of bandwidth below one even for average values substantially larger than one. Again, in practice, we expect clients sharing the same cache to see bandwidth variation between these two extremes, with operators likely to strive towards increasingly stable network conditions for streaming clients. RELATED WORK Broadly, the related work can be split into works that consider the head movements during viewing of 360\u00b0videos, client-side techniques to provide the best possible QoE (e.g., through adaptive prefetching based on expected viewing directions), and caching of HAS videos. While some recent works have considered optimized cache management policies for 360\u00b0videos,29,32, none of these works provide a data-driven characterization of the caching opportunities that would be observed with traditional caching policies that simply cache the requested tiles when the clients apply adaptive prefetching techniques. Head-movement characterization: Some recent works have collected datasets and characterized the 360\u00b0viewer behavior,2,9,11,12,26,34. However, most of these datasets use relatively short video segments and do not capture changes in behavior over time or across classes of videos. The primary exception, and the work most closely related to ours, is the work by Almquist et al., as we use their dataset. In their work,",
            "title": ""
        },
        {
            "id": "5796995",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "cache. They de ne the expected stack distance of a reference with reuse time t to be the average stack distance of all references with reuse time t. The miss-ratio curve is then constructed by computing the expected stack distance of each reuse time weighted by their frequency, this gives us a stack distance distribution. If one were to interpret the Statstack model's relationship between average stack distance vs. time, then we would arrive at the kinetic equation present in the average eviction model (AET). Stackstack performs in the same time bound as the AET model, O(N ) time and O(1) space due to their sampling techniques. OPEN PROBLEMS We have described the history of the miss-ratio curve construction techniques that rely on reuse time and stack distance measurements. In a broader scope, we have the enormous amount of research done in the cache behavior modeling domain. Fixing our limit on constructing practical miss-ratio curves and their applications, we nd that results in the following areas would excite many researchers in this eld. \u2022 To what extent can we relate logical access time to physical clock time? Currently, many models assume logical access time, that is each reference is considered a unique point in time and there are no inactive periods. However, in real systems it is often the case where the number of references per unit of time vary signi cantly. An investigation into how much error is produced when using physical clock time vs. the overhead caused by measuring logical time would give insights",
            "retrieval next text": "on how to better formulate these models for use on real systems. \u2022 How to extend these models to non-LRU caches? While some models could be used for alternative cache policies, there is a large reliance on the nice LRU stack property, this is especially painful since Intel's switch to RRIP policy has made cache modeling signi cantly more di cult. In latest work by Beckmann et. al. [3], they claim a general model, and that policy speci c models will be the subject of future work. \u2022 Despite the above remarks, the theory remains mature and well understood. How can we leverage these new models in day-to-day systems? The implementation of these models leaves much to be desired, rather high overhead has stopped many of these miss-ratio curve guided techniques from making it to practice. What improvements can we expect with miss-ratio curve constructions being done in O(1) space in an online and dynamic workload environment? CONCLUSION Our survey has covered the two main metrics, stack distance and reuse time, and their associated cache models from a miss-ratio curve construction standpoint. In Denning's 1968 work [8], he de ned page residency as how long a page will stay in main memory. In the cache modeling eld, we have called it the stack distance. More recently, it has been called the average eviction time of a cache. You could say that modeling locality has been reused over and over again.",
            "retrieval score": "1.4591122",
            "text": "cache. They de ne the expected stack distance of a reference with reuse time t to be the average stack distance of all references with reuse time t. The miss-ratio curve is then constructed by computing the expected stack distance of each reuse time weighted by their frequency, this gives us a stack distance distribution. If one were to interpret the Statstack model's relationship between average stack distance vs. time, then we would arrive at the kinetic equation present in the average eviction model (AET). Stackstack performs in the same time bound as the AET model, O(N ) time and O(1) space due to their sampling techniques. OPEN PROBLEMS We have described the history of the miss-ratio curve construction techniques that rely on reuse time and stack distance measurements. In a broader scope, we have the enormous amount of research done in the cache behavior modeling domain. Fixing our limit on constructing practical miss-ratio curves and their applications, we nd that results in the following areas would excite many researchers in this eld. \u2022 To what extent can we relate logical access time to physical clock time? Currently, many models assume logical access time, that is each reference is considered a unique point in time and there are no inactive periods. However, in real systems it is often the case where the number of references per unit of time vary signi cantly. An investigation into how much error is produced when using physical clock time vs. the overhead caused by measuring logical time would give insights",
            "title": ""
        },
        {
            "id": "1427792",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "might be selecting in sequence the options women, dress, and then casual, beach or formal. Like these examples, there are in general many frequent patterns when accessing data from a back store. This is also specially relevant in Online Transaction Processing (OLTP) workloads, where many relations between different data entities are exposed in the form of transactions. We claim that by observing and capturing data access patterns to a DKV store it is possible to improve the hit rate of the caches that come coupled with those stores (e.g., the block cache in HBase or the row cache in Cassandra), and thus reduce latency for interactive applications and augmenting throughput for batch computations. In this work, we propose caching items that are predicted to be accessed in a near future according to a history of past observations. Caching and data prefetching are effective techniques to reduce and hide the latency of accessing data on file systems [9,29]. Data prefetching uses data referencing patterns to anticipate cache misses, thereby fetching data in advance from disk to the cache. To be successful, it is nec-essary that prefetches are (i) timely, assuring data is available on cache before it is actually requested; (ii) useful, leading to prefetch hits before being replaced and avoiding cache misses; and (iii) efficient, by not introducing any significant overhead. Also, prefetching can suffer from side effects such as cache pollution, resulting from prefetching data that was inaccurately predicted to be accessed in a near future, and increased I/O bandwidth requirements, which results from creating",
            "retrieval next text": "more I/O concurrent requests through prefetching. To trace data access patterns in DKV stores, we rely on well-known data mining algorithms. Data mining techniques have been used extensively to discover and identify patterns in data accesses to web applications [15], as well as databases [12]. With real reference traces, we are thus able to improve cache hit rate by prefetching data that is predicted to be accessed in a close future with a certain confidence degree. In this paper, we introduce PALPATINE, an in-memory Key-Value cache at the application level, for DKV stores, that is capable of prefetching data that is likely to be accessed in a near future. PALPATINE builds, and updates at runtime, a stochastic graph of frequent sequences of accessed data items. Then, based on cache parameters, like size and current churn rate, we select subgraphs of items to be prefetched in sequential order. PALPATINE can be easily coupled with DKV stores, and is made fully transparent to applications. As a concrete instance, we integrated PALPATINE with the wide-columnar store HBase. Experimental evaluation indicates that PAL-PATINE can improve the latency of HBase in more than an order of magnitude. The key contributions from this paper are: \u2022 We present a solution to leverage the performance of back store caching based on the continuous observation of frequent data access patterns. Our solution is unique in the way real time probabilities of accessed items are used to improve hit rate: we make decisions on the fly to trade-off potentially higher probability of a future cache",
            "retrieval score": "1.4589148",
            "text": "might be selecting in sequence the options women, dress, and then casual, beach or formal. Like these examples, there are in general many frequent patterns when accessing data from a back store. This is also specially relevant in Online Transaction Processing (OLTP) workloads, where many relations between different data entities are exposed in the form of transactions. We claim that by observing and capturing data access patterns to a DKV store it is possible to improve the hit rate of the caches that come coupled with those stores (e.g., the block cache in HBase or the row cache in Cassandra), and thus reduce latency for interactive applications and augmenting throughput for batch computations. In this work, we propose caching items that are predicted to be accessed in a near future according to a history of past observations. Caching and data prefetching are effective techniques to reduce and hide the latency of accessing data on file systems,29. Data prefetching uses data referencing patterns to anticipate cache misses, thereby fetching data in advance from disk to the cache. To be successful, it is nec-essary that prefetches are (i) timely, assuring data is available on cache before it is actually requested; (ii) useful, leading to prefetch hits before being replaced and avoiding cache misses; and (iii) efficient, by not introducing any significant overhead. Also, prefetching can suffer from side effects such as cache pollution, resulting from prefetching data that was inaccurately predicted to be accessed in a near future, and increased I/O bandwidth requirements, which results from creating",
            "title": ""
        },
        {
            "id": "2452779",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "A parallel workload has extreme variability In both high-performance computing (HPC) environments and the public cloud, the duration of time to retrieve or save your results is simultaneously unpredictable and important to your over all resource budget. It is generally accepted (\"Google: Taming the Long Latency Tail - When More Machines Equals Worse Results\", Todd Hoff, highscalability.com 2012), but without a robust explanation, that identical parallel tasks do take different durations to complete -- a phenomena known as variability. This paper advances understanding of this topic. We carefully choose a model from which system-level complexity emerges that can be studied directly. We find that a generalized extreme value (GEV) model for variability naturally emerges. Using the public cloud, we find real-world observations have excellent agreement with our model. Since the GEV distribution is a limit distribution this suggests a universal property of parallel systems gated by the slowest communication element of some sort. Hence, this model is applicable to a variety of processing and IO tasks in parallel environments. These findings have important implications, ranging from characterizing ideal performance for parallel codes to detecting degraded behaviour at extreme scales. INTRODUCTION Where they exist at all, current models for variability of parallel workloads on HPC systems implicitly assume I/O variability follows a normal distribution with the mean and standard deviation the only measure of interest [12,21,32,24,28]. An attempt to fit the tail of task duration to the log-normal distribution has also been made [37] with limited success. [20,16] point out that lowering latency for a given PDSW-DICS",
            "retrieval next text": "'16 WIP, November 14th 2016, Salt Lake City, UT service increases competitiveness of that service. Their work focuses on reducing the tail latency of a parallel task by reducing the latency of the individual tasks that makeup the parallel task. Beyond these studies on parallel workloads, there are an increasing number of phenomena in computer science and beyond that are best modeled by methods of extreme statistics [14,3,15,10,25,11,35,4,2,7,22,30,8,26]. MODEL The modern theory of extreme value distributions can be traced back to the 1920's and two mathematicians: Fisher and Tippett. They considered [13] extreme values of n samples, each of size m drawn from the same underlying population. Provided the population values are independent and identically distributed (i.i.d.), they showed that the distribution of the extreme values (smallest or largest) drawn from sufficiently large sub-samples, which in turn are drawn from a larger sample, tended to one of three possible unique asymptotic forms. For a given underlying distribution e.g. the exponential, the extremal distribution will be one of the three, in this case the Gumbel distribution (the others are Fr\u00e9chet, to which the extremes of power laws are attracted, and the Weibull, also well known in failure rate modeling for example.) The probability density function of the GEV with location \u00b5, scale \u03c3, and shape \u03be is: A detailed description, and physical examples of extreme value theory are presented in [23,11,33]. Next, we choose a common an simple parallel task (a write to a parallel file system) and argue that the i.i.d. assumption needed for GEV behavior",
            "retrieval score": "1.4586585",
            "text": "A parallel workload has extreme variability In both high-performance computing (HPC) environments and the public cloud, the duration of time to retrieve or save your results is simultaneously unpredictable and important to your over all resource budget. It is generally accepted (\"Google: Taming the Long Latency Tail - When More Machines Equals Worse Results\", Todd Hoff, highscalability.com 2012), but without a robust explanation, that identical parallel tasks do take different durations to complete -- a phenomena known as variability. This paper advances understanding of this topic. We carefully choose a model from which system-level complexity emerges that can be studied directly. We find that a generalized extreme value (GEV) model for variability naturally emerges. Using the public cloud, we find real-world observations have excellent agreement with our model. Since the GEV distribution is a limit distribution this suggests a universal property of parallel systems gated by the slowest communication element of some sort. Hence, this model is applicable to a variety of processing and IO tasks in parallel environments. These findings have important implications, ranging from characterizing ideal performance for parallel codes to detecting degraded behaviour at extreme scales. INTRODUCTION Where they exist at all, current models for variability of parallel workloads on HPC systems implicitly assume I/O variability follows a normal distribution with the mean and standard deviation the only measure of interest,21,32,24,28. An attempt to fit the tail of task duration to the log-normal distribution has also been made with limited success.,16 point out that lowering latency for a given PDSW-DICS",
            "title": ""
        },
        {
            "id": "9683239",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "algorithm can at times be over 20% lower when such sub-optimal state transition or a delayed reaction to state change is encountered.Finally, TinyLFU [17] is a CDN cache admission policy that improves performance by keeping the items with the highest frequency within the cache.Presented are various moments where mARC is not performing or deciding as well as it could. The Fear of Missing Out Classic caching solutions are reactive and this impedes their ability to react to workload behavior changes.Many incorporate the notion of eviction history to evaluate the importance of an item [3,5,18].Their post-mortem evaluation of evictions, while valuable, is a belated reaction to the workload's effect on the cache, compared to a direct reaction to the workload itself.Of LRU, ARC, and LIRS, LIRS reacts the most directly to the workload itself through its focus on reuse distance. Non-datapath caching algorithms, owing to the available flexibility of not having to perform cache updates, invariably embody a fear of missing out on responding in a timely fashion to changes in workload behavior.Unfortunately, reacting as a consequence of accesses to items that were evicted requires that items must first enter the cache.Furthermore, if the number of cache hits is not increasing, only having information about the evicted items can obscure the reason for low performance with a limited view of the workload.To respond to workload changes rapidly, observing accesses to newly requested items are crucial.In particular, keeping track of recent accesses that resulted in cache misses allows us to understand what the cache is \"missing out\" on.Here,",
            "retrieval next text": "timely knowledge of workload behavior helps improve the accuracy of the filtering mechanism.We assert that the rate of access to recent cache missed items provides crucial, complementary information about the workload that allows for better understanding of short-term workload behaviors. FOMO is a general non-datapath admission policy that is capable of improving hit rate and write rate consistency by utilizing a simple model that avoids the issues present within the mARC state model.FOMO's Miss-History captures the reuse of recently missed items and does not require the items to be inserted into the cache first.As we shall show in the next section, the design of FOMO focuses on comparing the cache hit rates and reuse rate of recent cache misses to better understand the general workload behavior. Design Non-datapath algorithms such as LARC can be counter-productive to the hit rate as they do not cache an item until it is reused, incurring a compulsory additional cache miss per item.Additionally, when working sets change frequently, the requirement of proof of reuse can significantly impair hit rates.In the case of mARC, its high level of complexity comes not from using three states, but rather its seven, built-in, state transition conditions.However, even with these conditions, mARC still excluded the direct state transitions between the Unique Access and Stable states, thereby imposing unnecessary writes and potentially removing \"soon-to-be-hit\" items from the cache in exchange for \"one-hit wonders\". The following section explains the design of Fear of Missing Out (FOMO) that takes a new approach to storage caching.FOMO learns from our findings",
            "retrieval score": "1.4579697",
            "text": "algorithm can at times be over 20% lower when such sub-optimal state transition or a delayed reaction to state change is encountered.Finally, TinyLFU is a CDN cache admission policy that improves performance by keeping the items with the highest frequency within the cache.Presented are various moments where mARC is not performing or deciding as well as it could. The Fear of Missing Out Classic caching solutions are reactive and this impedes their ability to react to workload behavior changes.Many incorporate the notion of eviction history to evaluate the importance of an item,5,18.Their post-mortem evaluation of evictions, while valuable, is a belated reaction to the workload's effect on the cache, compared to a direct reaction to the workload itself.Of LRU, ARC, and LIRS, LIRS reacts the most directly to the workload itself through its focus on reuse distance. Non-datapath caching algorithms, owing to the available flexibility of not having to perform cache updates, invariably embody a fear of missing out on responding in a timely fashion to changes in workload behavior.Unfortunately, reacting as a consequence of accesses to items that were evicted requires that items must first enter the cache.Furthermore, if the number of cache hits is not increasing, only having information about the evicted items can obscure the reason for low performance with a limited view of the workload.To respond to workload changes rapidly, observing accesses to newly requested items are crucial.In particular, keeping track of recent accesses that resulted in cache misses allows us to understand what the cache is \"missing out\" on.Here,",
            "title": ""
        },
        {
            "id": "8136498",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "with many different choices of which requests to reissue and when. (3) The complex and different workload properties of various interactive services, such as servicetime distributions, arrival patterns, request correlations, and system settings make it difficult to derive general strategies for reducing tail latency. (4) Analytical work using queueing theory is challenging even when making strong assumptions about response-time distributions (e.g. drawn from exponential family), and conclusions draw from such simple models are hard to generalize to more complex systems. Methodology and Key Results. The goal of our work is to find a reissue policy that minimizes a workload's kth percentile tail latency by issuing a fixed percentage (or budget) of redundant requests. We explore the space and devise reissue policies in a principled manner -directed by theoretical analysis to identify the key insights of effective reissue policies, and driven by empirical data from actual systems for wide applicability. We introduce a new family of reissue policies, Single-Time / Random (SingleR), that reissue requests after a delay d with probability q. The use of randomness in SingleR provides an important degree of freedom that allows to bound the reissue budget while also ensuring that reissue requests have sufficient time to respond, exploiting the benefits of both immediate and delayed reissue of prior work. Using a simplified analytical model, we formally prove that Sin-gleR is the optimal trade-off between the immediate and delayed reissue strategies. More precisely, we define the Multiple-Time / Random (MultipleR) policies which reissue requests multiple times with different delays and reissue probabilities. We",
            "retrieval next text": "prove that, surprisingly, the optimal policies in MultipleR and SingleR are equivalent. It is a powerful result, restraining the complexity of reissue policies to one time reissue only while guaranteeing the effectiveness of SingleR. Next, we present how to apply SingleR for interactive services through a data-driven approach to efficiently find the appropriate parameters, reissue time and probability, given sampled response times of the workloads. Our approach takes into account correlations between primary and reissue request response times. It is computationally efficient, finding optimal values of the parameters in close to linear time, with respect to the data size. Moreover, we show how to devise reissue policies for systems which are sensitive to added load by adaptively refining a reissue policy in response to feedback from the system. This method remains oblivious to many system design details, relies on iterative adaptation to discover a system's response-time distributions and its response to added load. This data-driven approach is performed in a principled manner: every refined policy is the solution to a well defined optimization problem based on updated response-time distributions, applicable to a wide range of workloads with varying properties. Empirical evaluation. We illustrate the properties of SingleR using both simulation and system experiments. Through careful simulation, we illustrate two key points: 1) the use of randomization in SingleR is especially important for workloads with correlated service times and queueing delays, 2) the effectiveness of SingleR is robust to varied workload properties and design choices including: utilization, service-time distribution, target latency percentiles, service-time correlations, and load-balancing/request-prioritization strategies.",
            "retrieval score": "1.4578571",
            "text": "with many different choices of which requests to reissue and when. (3) The complex and different workload properties of various interactive services, such as servicetime distributions, arrival patterns, request correlations, and system settings make it difficult to derive general strategies for reducing tail latency. (4) Analytical work using queueing theory is challenging even when making strong assumptions about response-time distributions (e.g. drawn from exponential family), and conclusions draw from such simple models are hard to generalize to more complex systems. Methodology and Key Results. The goal of our work is to find a reissue policy that minimizes a workload's kth percentile tail latency by issuing a fixed percentage (or budget) of redundant requests. We explore the space and devise reissue policies in a principled manner -directed by theoretical analysis to identify the key insights of effective reissue policies, and driven by empirical data from actual systems for wide applicability. We introduce a new family of reissue policies, Single-Time / Random (SingleR), that reissue requests after a delay d with probability q. The use of randomness in SingleR provides an important degree of freedom that allows to bound the reissue budget while also ensuring that reissue requests have sufficient time to respond, exploiting the benefits of both immediate and delayed reissue of prior work. Using a simplified analytical model, we formally prove that Sin-gleR is the optimal trade-off between the immediate and delayed reissue strategies. More precisely, we define the Multiple-Time / Random (MultipleR) policies which reissue requests multiple times with different delays and reissue probabilities. We",
            "title": ""
        },
        {
            "id": "9683253",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "capture many opportunities for hits that go unrecognized by both LARC and mARC, whose hit rate (and mARC's states) can be seen in Figure 9.This is unfortunate for mARC, which places emphasis on identifying workload states.mARC, due to its focus on using cache hit rate to identify workload state instead of something similar to FOMO's Miss-History, cannot see the pattern within this workload, leaving mARC to stay in the Unique Access state.\u20dd FOMO(ARC) is able to recognize a pattern of reuse and is able to promptly respond and have many cache hits that LARC and mARC instead miss. Adversarial Workloads FOMO improves upon the results of the underlying cache replacement algorithm for every workload and cache size, as seen in Figure 7, with the exception of CloudCache.In fact, CloudCache workloads did not appear to have any of the non-datapath caches perform well compared to their datapath caching algorithm counterparts.When investigating the reason for this behavior, several things of note were observed, which will be highlighted with a focused discussion on one of these CloudCache workloads: webserver-2012-11-22-1.blk. Webserver-2012-11-22-1.blk has an instance that shows a large time frame where non-datapath caching algorithms do not perform well compared to the datapath cache algorithms, shown in Figure 11.In particular, the time period between one million and four million requests has this behavior.When observing the block address access pattern plots of this CloudCache workload (Figure 12), it is noticeable that the workloads include several concurrent working sets and patterns (combinations of scans, random accesses, looping and repeated accesses).This mixing of patterns",
            "retrieval next text": "increases the likelihood of both FOMO and mARC observing reuse and discerning patterns in the workload based on them.This is why, as the cache size increases, the difference between the datapath and non-datapath caches begins to decrease. Each of the non-datapath cache algorithms have their own reasons for why these access patterns were problematic.For LARC, which observes reuse on an individual basis, an item being reused often would not be reused again prior to being evicted, leading to many missed opportunities that the datapath cache algorithms can take advantage of.mARC, with its dependence on cache hit rate for decisions, with its use of the Unstable state as a intermediary transition between Stable and Unique Access states, and the long evaluation times, finds itself within the Unstable state more often and gains some level of advantage over both LARC and FOMO due to it.Lastly, FOMO, looking for patterns in the workload, would periodically find a pattern of reuse, change the state to Insert, obtain some cache hits that eventually overtake the reuse found in the workload, change state to Filter and so on repeatedly during such highly overlapping periods.Finally, we note that when particular patterns, or reuse in general, were more significant within these workloads, all of the non-datapath cache algorithms would identify and react to achieve cache hits.blk that highlight the various patterns that could be seen simulataneously during the time period where non-datapath caching algorithms had poorer hit rate compared to their datapath counterparts.Among them we can notice scans, random accesses, and loops all occurring",
            "retrieval score": "1.4576195",
            "text": "capture many opportunities for hits that go unrecognized by both LARC and mARC, whose hit rate (and mARC's states) can be seen in Figure 9.This is unfortunate for mARC, which places emphasis on identifying workload states.mARC, due to its focus on using cache hit rate to identify workload state instead of something similar to FOMO's Miss-History, cannot see the pattern within this workload, leaving mARC to stay in the Unique Access state.\u20dd FOMO(ARC) is able to recognize a pattern of reuse and is able to promptly respond and have many cache hits that LARC and mARC instead miss. Adversarial Workloads FOMO improves upon the results of the underlying cache replacement algorithm for every workload and cache size, as seen in Figure 7, with the exception of CloudCache.In fact, CloudCache workloads did not appear to have any of the non-datapath caches perform well compared to their datapath caching algorithm counterparts.When investigating the reason for this behavior, several things of note were observed, which will be highlighted with a focused discussion on one of these CloudCache workloads: webserver-2012-11-22-1.blk. Webserver-2012-11-22-1.blk has an instance that shows a large time frame where non-datapath caching algorithms do not perform well compared to the datapath cache algorithms, shown in Figure 11.In particular, the time period between one million and four million requests has this behavior.When observing the block address access pattern plots of this CloudCache workload (Figure 12), it is noticeable that the workloads include several concurrent working sets and patterns (combinations of scans, random accesses, looping and repeated accesses).This mixing of patterns",
            "title": ""
        },
        {
            "id": "6631895",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "(Shi et al. 2019a). Though not explicitly mentioned, PARROT (Liu et al. 2020) also mostly focuses on hardware caches, which is an offline imitation learning based policy with LSTM models. These efforts, even though displaying great performance, rely heavily on information from hardware characteristics and it is unclear how to extend to software caches. One state-of-the-art online ML policy is LeCaR (Vietri et al. 2018), making use of recency and frequency information. Different from the work reviewed above, PHOEBE is a reuse-aware RL framework for the optimal online cache replacement, capable to extract adequate information only from a single trace (rather than other hardware based features) and extendable to various cache scenarios. Conclusion In this work, we propose PHOEBE, a reuse-aware RL framework for the optimal online caching via DDPG. Through continuous interactions with the cache environment and varying workloads at runtime, PHOEBE adjusts its policy to approximate the optimal caching policy. All information required by PHOEBE can be distilled from a single cache trace, which would make it be universally applicable in theory and can be generalized for different cache scenarios. In evaluation over a set of Microsoft cloud workloads, PHOEBE is able to close the gap of cache miss rate from LRU and a stateof-the-art online learning based cache policy to the Belady's optimal policy by 70.3% and 52.6%, respectively.",
            "retrieval next text": "GOOD GOVERNANCE IN BANGLADESH: A STUDY OF WORLD GOVERNANCE INDICATORS The inclusive philosophy of good governance is almost indispensable for the progress of state democracy in Bangladesh. However, the Bangladesh government is fighting a malicious virus in the governance structure that is visible in all sectors of the state. Against this backdrop, an attempt has been made to comprehend the general state of good governance in Bangladesh according to World Governance Indicators (WGI) of the World Bank. This study is based on mixed approach containing quantitative data from World Bank website (world economy.com) and qualitative data from existing literatures. The collected data have also been analyzed through table, chart and text. This paper found, the current situation in Bangladesh is appalling and detrimental to the socio-economic development of the country. This situation is crisscrossed by overpopulation, politicization, bureaucratization, corruption, poverty, broken law and order, and the narrow game of politics. This paper then outlines some of the policy guidelines needed to define good governance principles in Bangladesh. The results of this study are expected to be useful for policymakers in devising appropriate strategies to ensure good governance at all levels of government. The inclusive philosophy of good governance is almost indispensable for the development of state democracy in Bangladesh. However, the Bangladesh government is fighting a malicious virus in the governance structure that is visible in all sectors of the state. Against this backdrop, an attempt has been made to comprehend the general state of good governance in Bangladesh according to World Governance Indicators (WGI) of",
            "retrieval score": "1.4572376",
            "text": "(Shi et al. 2019a). Though not explicitly mentioned, PARROT (Liu et al. 2020) also mostly focuses on hardware caches, which is an offline imitation learning based policy with LSTM models. These efforts, even though displaying great performance, rely heavily on information from hardware characteristics and it is unclear how to extend to software caches. One state-of-the-art online ML policy is LeCaR (Vietri et al. 2018), making use of recency and frequency information. Different from the work reviewed above, PHOEBE is a reuse-aware RL framework for the optimal online cache replacement, capable to extract adequate information only from a single trace (rather than other hardware based features) and extendable to various cache scenarios. Conclusion In this work, we propose PHOEBE, a reuse-aware RL framework for the optimal online caching via DDPG. Through continuous interactions with the cache environment and varying workloads at runtime, PHOEBE adjusts its policy to approximate the optimal caching policy. All information required by PHOEBE can be distilled from a single cache trace, which would make it be universally applicable in theory and can be generalized for different cache scenarios. In evaluation over a set of Microsoft cloud workloads, PHOEBE is able to close the gap of cache miss rate from LRU and a stateof-the-art online learning based cache policy to the Belady's optimal policy by 70.3% and 52.6%, respectively.",
            "title": ""
        },
        {
            "id": "11022169",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "to design better caching and routing strategies and can aid network management and maintenance. 1) Caching: A significant amount of effort has been invested in understanding the performance, in particular the net-work hit rate for different cache insertion and eviction policies. Table II succinctly describes the research in determining the hit rate of network caches. In one of the seminal papers, Che et al. derive approximations for the hit rate of LRU caches. This approximation, popularly known as Che's approximation has been shown to be applicable for general content popularity distributions. In recent years, this approximation has been extended to non-stationary requests and to general networks comprising of multiple nodes. Garretto et al. [10] derive expressions for the hit rate for multiple caching insertion and eviction policies such as LRU, p-LRU, k-LRU, FIFO, LFU and RANDOM, and LCE, Leave Copy Down (LCD) and Leave Copy Probabilistically (LCP) respectively. Simulation and trace-based evaluation show that the analytical and simulation results match closely. This study also demonstrates the superiority of the k-LRU policy in comparison to other strategies. Alongside, research effort has also been devoted to analyze the performance of Time To Live (TTL) based caches because in general it is easier to derive exact expressions for uncorrelated and correlated request streams. In [14], [15] the authors analytically study the fundamental limits of caching in wireless networks. For example, the authors in [14] obtain upper bounds on capacity and achievable capacity lower bounds in wireless cache networks. Similarly, in [15], the authors investigate the capacity scaling laws in",
            "retrieval next text": "cache-enabled wireless networks considering the skewness of Approach Summary of Contributions Che et al.'s Approximation Determines the hit rate of LRU caches. [12] Determines the hit rate at LRU caches in cache hierarchies described by a directed acyclic graph. a-Net [13] Iterative algorithm to determine the hit rate of a network of LRU caches. [10] Extends Che's approximation to determine the hit rate of FIFO and RANDOM cache eviction policies. Also determines expressions for LCE, LCD and LCP cache insertion policies. Approximation of TTL caches Determine how to set the parameters of TTL caches to mimic the behavior of other policies such as FIFO and LRU. [16]. 2) Routing: Theoretical analysis has also been conducted to determine the extent to which content search and scoped flooding is beneficial. Analysis and experiments show that the optimal flooding radius is small (less than 3 hops). This means that flooding requests beyond the immediate neighborhood of a requester is likely to incur significant overhead while providing minimal performance improvement. The benefits of opportunistic routing, an important routing paradigm designed for wireless networks that exploits the broadcast nature of the wireless medium to select the best relay to forward a request toward the custodian has been analyzed in [17]. The authors design Markovian models to analyze the performance of opportunistic request routing in wireless cache networks in the presence of multi-path fading. Based on their results, the authors conclude that the benefits of in-network caching are more pronounced when the probability of successful packet transmission is low. This result suggests",
            "retrieval score": "1.4569606",
            "text": "to design better caching and routing strategies and can aid network management and maintenance. 1) Caching: A significant amount of effort has been invested in understanding the performance, in particular the net-work hit rate for different cache insertion and eviction policies. Table II succinctly describes the research in determining the hit rate of network caches. In one of the seminal papers, Che et al. derive approximations for the hit rate of LRU caches. This approximation, popularly known as Che's approximation has been shown to be applicable for general content popularity distributions. In recent years, this approximation has been extended to non-stationary requests and to general networks comprising of multiple nodes. Garretto et al. derive expressions for the hit rate for multiple caching insertion and eviction policies such as LRU, p-LRU, k-LRU, FIFO, LFU and RANDOM, and LCE, Leave Copy Down (LCD) and Leave Copy Probabilistically (LCP) respectively. Simulation and trace-based evaluation show that the analytical and simulation results match closely. This study also demonstrates the superiority of the k-LRU policy in comparison to other strategies. Alongside, research effort has also been devoted to analyze the performance of Time To Live (TTL) based caches because in general it is easier to derive exact expressions for uncorrelated and correlated request streams. In, the authors analytically study the fundamental limits of caching in wireless networks. For example, the authors in obtain upper bounds on capacity and achievable capacity lower bounds in wireless cache networks. Similarly, in, the authors investigate the capacity scaling laws in",
            "title": ""
        },
        {
            "id": "5091263",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O), a KV cache eviction policy that dynamically retains a balance of recent and H$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of H$_2$O with 20% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to 29$\\times$, 29$\\times$,",
            "retrieval next text": "and 3$\\times$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the latency by up to 1.9$\\times$. The code is available at https://github.com/FMInference/H2O.",
            "retrieval score": "1.456145",
            "text": "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O), a KV cache eviction policy that dynamically retains a balance of recent and H$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of H$_2$O with 20% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to 29$\\times$, 29$\\times$,",
            "title": ""
        },
        {
            "id": "11298960",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "by default to support its use in real applications-prefetching and oversubscription.Prefetching is fundamental to allowing UVM applications to achieve performance comparable to programmer-managed memory applications [3].Oversubscription further simplifies programming, allowing applications to work with out-of-core data, but typically at a high performance cost.In this section, we analyze these two features, primarily identifying (1) how costs from the prior section translate into real workloads, (2) how prefetching and oversubscription impact batches qualitatively and quantitatively, and (3) how they differ with system settings. Overall Performance Impacts 6.1.1Oversubscription.Oversubscription causes one major change to the driver workload: once destination GPU memory is filled, the UVM driver must evict some VABlocks before migrating the on-demand pages.Eviction presents an immediate detriment in performance because each eviction is associated with page table updates on both the source and the destination devices and data transfer between them.Eviction is handled solely in the service task component. Figure 11 shows the performance of the UVM driver for Stream and sgemm.Once GPU memory is oversubscribed, batch time increases by more than 3X.Essentially, every batch task has a much higher cost.Oversubscription also causes a shift in the distribution of batch components.Service time is still a dominating factor.Tracker wait time grows to a substantial portion of the batch time (e.g,.for sgemm), indicating that eviction makes the GPU's workload greater than the CPU's workload.By inspecting the source code of UVM, we find that eviction involves a three-step process: try to allocate memory space on the GPU but with a failure, evict victim VABlocks, and re-try to allocate space.While each step",
            "retrieval next text": "incurs overhead, the eviction is especially costly, as detailed in Section 6.2.1.Furthermore, the host memory is the hard-coded destination for eviction.This causes GPU-GPU migration to span three memory locations: host memory, the source GPU's memory, and the destination GPU's memory. The advantage of GPU-GPU disappears for oversubscribed problems, causing the total times across the three platforms to be similar for both benchmarks.This total time is determined by eviction, which is the same among the three platforms.The increase in tracker wait time offsets the decrease in service time using GPU-GPU.In contrast to the significant performance gain for non-oversubscribed problems, GPU-GPU only achieves 1.17X speedup over CPU-GPU for oversubscribed sgemm, and GPU-GPU over NVLink additionally achieves a 1.05X speedup.6.1.2Prefetching.UVM utilizes a runtime prefetching routine as part of the default behavior.Upon a page fault, UVM fetches not only the requested page but also other pages in the same VABlock.Through prefetching, UVM amortizes the transfer latency and obtains a high bandwidth over the interconnect.Prefetching reduces the total number of faults generated during application executions and increases the coverage of pages in a batch.Like eviction, prefetching is handled entirely in the service component of the driver. Compared to the base case, prefetching boosts the performance of the UVM driver by roughly an order of magnitude as shown in Figure 12.Except for tracker wait time, all other tasks' times are smaller with prefetching.This is mainly due to the reduced number of faults and batches: the prefetched pages satisfy the requests in the future and avoid page faults.The same application generates fewer faults",
            "retrieval score": "1.4552112",
            "text": "by default to support its use in real applications-prefetching and oversubscription.Prefetching is fundamental to allowing UVM applications to achieve performance comparable to programmer-managed memory applications.Oversubscription further simplifies programming, allowing applications to work with out-of-core data, but typically at a high performance cost.In this section, we analyze these two features, primarily identifying (1) how costs from the prior section translate into real workloads, (2) how prefetching and oversubscription impact batches qualitatively and quantitatively, and (3) how they differ with system settings. Overall Performance Impacts 6.1.1Oversubscription.Oversubscription causes one major change to the driver workload: once destination GPU memory is filled, the UVM driver must evict some VABlocks before migrating the on-demand pages.Eviction presents an immediate detriment in performance because each eviction is associated with page table updates on both the source and the destination devices and data transfer between them.Eviction is handled solely in the service task component. Figure 11 shows the performance of the UVM driver for Stream and sgemm.Once GPU memory is oversubscribed, batch time increases by more than 3X.Essentially, every batch task has a much higher cost.Oversubscription also causes a shift in the distribution of batch components.Service time is still a dominating factor.Tracker wait time grows to a substantial portion of the batch time (e.g,.for sgemm), indicating that eviction makes the GPU's workload greater than the CPU's workload.By inspecting the source code of UVM, we find that eviction involves a three-step process: try to allocate memory space on the GPU but with a failure, evict victim VABlocks, and re-try to allocate space.While each step",
            "title": ""
        },
        {
            "id": "367989",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "of the same length. Empirically, as prediction errors are dominantly smaller than the actual TTFT, CacheGen can almost always pick the right compression level to meet a given TTFT target. Related work Faster LLM serving: The landscape of LLM systems research is fast evolving. Besides speeding up LLM training (e.g., [86,91]), many recent efforts, including ours, focus on the LLM serving systems. Some try to parallelize the generation of multiple tokens with smaller approximating models [59,74] and accelerate the inference of Mix-of-Expert LLMs on edge devices [111]. Others quantize LLM model weights (e.g., [22]), reduce memory I/O of GPU on-chip SRAM [39], and reduce self-attention computation complexity by k-means [87]. New scheduling strategies are also proposed for LLM services [105,112]. Nonetheless, a common approach to speeding up without modifying the LLMs is by caching KV features of repeatedly used input sequences, such as contexts [38,63,67,74,80,82,89,104,105,118]. However, caching KV features results in a dramatically increased amount of cached data, which requires a bandwidth-efficient KV/context-loading system like CacheGen. Longer LLM input (context): Though the benefits of longer contexts are still under scrutiny [66,87,93], many recent efforts aim at enabling LLMs to accept ever longer contexts. The main challenge is to hold the attention matrices of more tokens without needing a bigger GPU memory. They enable longer input sequences by dynamically offloading less used parts of the attention matrices [89], using external knowledge via KNN [106], retraining the self-attention module to only attend to top-k keys to approximate the full attention [21,26], mapping long inputs into smaller latent space",
            "retrieval next text": "[47], using local windowed self-attention [25], or using dilated attention [42] or sparse attention [113] to scale the input length to even 1 billion tokens. However, the longer inputs (contexts) also inflate the KV cache, and this work complements these efforts by focusing on fast remote loading of KV cache. Context shortening/pruning: More relevant to our work is the emerging literature on shortening and pruning long contexts. These efforts select the most important text segments and prune out the others to reduce context length, based on the similarity between the user query and the potentially relevant documents (e.g., [28]), or drop tokens that are less attended to by the current prompt [67,118]. Different from them, CacheGen compresses the KV cache into bitstreams, rather than shortening the context in its original form (i.e., text or tensor). Furthermore, unlike [28,67,118], CacheGen's KV compression does not need to know the query (prompt). Recently, some work retrains LLM models to directly take in contexts that are rewritten by gisting [76] or autoencoding [45]. Different from them, CacheGen does not need to train a new LLM model and also achieves better compression rate with less information loss by leveraging unique distributional properties of KV features. Finally, we show CacheGen achieves better delay-quality tradeoffs than existing context compressors ( \u00a75.4). and can potentially be combined with them to further improve the delay-quality tradeoffs. Tensor compression: CacheGen's KV encoder can be seen as a special tensor compression technique tailored for KV features produced by LLMs. General tensor compression has been intensively studied, e.g.,",
            "retrieval score": "1.4546101",
            "text": "of the same length. Empirically, as prediction errors are dominantly smaller than the actual TTFT, CacheGen can almost always pick the right compression level to meet a given TTFT target. Related work Faster LLM serving: The landscape of LLM systems research is fast evolving. Besides speeding up LLM training (e.g.,,91), many recent efforts, including ours, focus on the LLM serving systems. Some try to parallelize the generation of multiple tokens with smaller approximating models,74 and accelerate the inference of Mix-of-Expert LLMs on edge devices. Others quantize LLM model weights (e.g.,), reduce memory I/O of GPU on-chip SRAM, and reduce self-attention computation complexity by k-means. New scheduling strategies are also proposed for LLM services,112. Nonetheless, a common approach to speeding up without modifying the LLMs is by caching KV features of repeatedly used input sequences, such as contexts,63,67,74,80,82,89,104,105,118. However, caching KV features results in a dramatically increased amount of cached data, which requires a bandwidth-efficient KV/context-loading system like CacheGen. Longer LLM input (context): Though the benefits of longer contexts are still under scrutiny,87,93, many recent efforts aim at enabling LLMs to accept ever longer contexts. The main challenge is to hold the attention matrices of more tokens without needing a bigger GPU memory. They enable longer input sequences by dynamically offloading less used parts of the attention matrices, using external knowledge via KNN, retraining the self-attention module to only attend to top-k keys to approximate the full attention,26, mapping long inputs into smaller latent space",
            "title": ""
        },
        {
            "id": "1453750",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "Open-AI model Efficient Memory Reduce Management for the Large Language Models (LLMs) Serving with Paged Attention of sharing the KV Cashes Abstract: High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem we proposed a Paged Attention. An alternative algorithm inspired by the classical virtual memory and paging techniques in operating systems. An LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that LLM improves the throughput of popular LLMs by 2-4\u00d7 with the same level of latency compared to the state-of-the-art systems, such as Faster Transformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms",
            "retrieval next text": "Equation of state for boron nitride along the principal Hugoniot to 16 Mbar The thermodynamic properties of boron nitride under extreme pressures and temperatures are of great interest and importance for materials science and inertial confinement fusion physics, but they are poorly understood owing to the challenges of performing experiments and realizing ab initio calculations. Here, we report the first shock Hugoniot data on hexagonal boron nitride at pressures of 5\u201316 Mbar, using hohlraum-driven shock waves at the SGIII-p laser facility in China. Our density functional theory molecular dynamics calculations closely match experimental data, validating the equations of state for modeling the shock response of boron nitride and filling a crucial gap in the knowledge of boron nitride properties in the region of multi-Mbar pressures and eV temperatures. The results presented here provide fundamental insights into boron nitride under the extreme conditions relevant to inertial confinement fusion, hydrogen\u2013boron fusion, and high-energy-density physics.",
            "retrieval score": "1.4543751",
            "text": "Open-AI model Efficient Memory Reduce Management for the Large Language Models (LLMs) Serving with Paged Attention of sharing the KV Cashes Abstract: High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem we proposed a Paged Attention. An alternative algorithm inspired by the classical virtual memory and paging techniques in operating systems. An LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that LLM improves the throughput of popular LLMs by 2-4\u00d7 with the same level of latency compared to the state-of-the-art systems, such as Faster Transformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms",
            "title": ""
        },
        {
            "id": "9258360",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "eCommerce, financial, streaming, and scientific computing. For example, the leading service providers such as Google, Facebook, and Amazon rely on memory caching using Memcached [1] and Redis [2], to accommodate a huge volume of data access transactions in a real-time manner. In a cloud, the in-memory cache system is an important component to preserve the quality of service for individual tenants with different data access characteristics. To estimate the data access performance, cache hit rate has been widely employed since the performance gap between cache memory and secondary storage (e.g., hard drives) is not comparable: the access latency to memory is 15-90ns, while it takes 100 us-15ms to access disks [3]. This considerable discrepancy can result in a significant reduction of the access latency even with a small increase in cache hit rate; for example, over 35% of latency reduction can be expected only with 1% increase in hit rate [4]. In this study, we use cache hit rate to specify the data access performance and requirement. Optimizing system resources is another important concern in a cloud. That is, allocating the minimal cache space meeting the performance requirement should be desirable. Adding extra cache resources due to a non-optimized use of the memory cache is costly and inconvenient due to the possibility of service disruption for the installation and reconfiguration. This is particularly important in a private cloud, in which an organization operates the cloud infrastructure in an isolated manner; one of the important motivations of the private cloud is to maximize the utilization of existing",
            "retrieval next text": "on-premise resources [5]. In enterprise private clouds, the total available resources are relatively limited compared to mega-scale public cloud. In addition, there are typically different tiers of applications with different data access performance requirements. This study targets on a private cloud where it is critical to offer the guaranteed performance, while minimizing the operating cost. Two extreme methods for managing the memory cache are unlimited sharing and complete isolation, both of which would be inefficient with the expensive storage complexity to meet the per-tenant performance requirement. The basic global caching scheme allows unlimited sharing of the cache space, and hence, it would be hard to guarantee the specified performance goal for data access for individual users ( Fig. 1(a)). In contrast, static caching offers an isolated cache space to each tenant for the exclusive use of the allocated cache space ( Fig. 1(b)). While straightforward to manage without sharing, a critical downside of static caching is that it is not an easy task to estimate the required amount of the cache space in advance. In addition, the time-varying property of the workload patterns often requires over-booking of the cache space to keep meeting the performance goal, which may cause the waste of cache resources due to under-utilization. Dynamic caching manages the cache space adaptively by responding to the data access pattern changes [6]- [8]. In other words, the size of cache space for a tenant is adjusted (i.e., grown or shrunk) over time by monitoring the performance measure (e.g., hit rate). It is thus essential to",
            "retrieval score": "1.454299",
            "text": "eCommerce, financial, streaming, and scientific computing. For example, the leading service providers such as Google, Facebook, and Amazon rely on memory caching using Memcached and Redis, to accommodate a huge volume of data access transactions in a real-time manner. In a cloud, the in-memory cache system is an important component to preserve the quality of service for individual tenants with different data access characteristics. To estimate the data access performance, cache hit rate has been widely employed since the performance gap between cache memory and secondary storage (e.g., hard drives) is not comparable: the access latency to memory is 15-90ns, while it takes 100 us-15ms to access disks. This considerable discrepancy can result in a significant reduction of the access latency even with a small increase in cache hit rate; for example, over 35% of latency reduction can be expected only with 1% increase in hit rate. In this study, we use cache hit rate to specify the data access performance and requirement. Optimizing system resources is another important concern in a cloud. That is, allocating the minimal cache space meeting the performance requirement should be desirable. Adding extra cache resources due to a non-optimized use of the memory cache is costly and inconvenient due to the possibility of service disruption for the installation and reconfiguration. This is particularly important in a private cloud, in which an organization operates the cloud infrastructure in an isolated manner; one of the important motivations of the private cloud is to maximize the utilization of existing",
            "title": ""
        },
        {
            "id": "10439009",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "operate in PDP. \u2022 Discussing the impact of design decisions in PDP environments. \u2022 Proposing a new in-network KV platform with major performance and scalability improvements over the stateof-the-art. \u2022 Providing the accessory routing and processing mechanisms to efficiently deliver strong consistency and flexibility. II. KVS DESIGN FOR THE DATA PLANE Research in the workload characteristics of deployed KVS shows that they accommodate read-mostly workloads: the read-write ratio is 380:1 for Google F1 [2], and 500:1 for Facebook TAO [3]. Primary-backup variations, like CR, are specifically designed for read-mostly workloads, making them an appealing candidate for the underlying replication method of data centre KVSs. A. Chain Replication Under CR, the participating nodes/programmable switches form a chain and each has a distinct role: head, tail, or replica. All of the participating nodes hold the same KV pairs. Write queries originate from the head and then propagate across all replica nodes until they reach the tail. The tail issues a response which acts as an acknowledgement for the write. A tail node is also responsible for responding to read queries. Only the tail is considered to be up-to-date with the latest commit for a value and acts as a reference point for the entire chain. In Figure 1a, we show the path of a read query (dashed arrows) and its response (solid arrows). Replicas can replace the head or the tail in case of a failure. Defining the tail as the reference point allows per-key consistency for the entire chain to be achieved. When a write query reaches",
            "retrieval next text": "the tail, it has certainly been processed by all previous chain nodes. Therefore, all chain nodes are updated with its latest version. If the write query is lost before reaching the tail, then all subsequent reads will be replied with the previous version for this object. This ensures consistency in replies. B. In-network KVS 1) IncBricks: The IncBricks [9] paper was among the first to capture the potential of PDP devices because of their location in a query's message path. The message path for a query in a legacy KVS would start from the client, go through network devices, and finally it would reach the servers that host the KVS. IncBricks substituted the need to reach coordination servers for some types of queries and suggested a shorter path for cached values which stays between the client and the network devices. Network programmability was achieved with the use of programmable network processors which offered limited instructions and were tied to proprietary programming tools provided by the manufacturer. Despite this limitation, the realisation of this new role for network devices greatly reduced the query response time and increased the attainable throughput over legacy KVS deployments. 2) NetChain: This new role of switches was further explored by NetChain, which by using P4 [10] managed to deploy an in-network KVS in high-performance ASICs (instead of Network Processor Units (NPUs)), and therefore achieved greater performance than IncBricks. The query response mechanism that was employed relies heavily on the incoming packets that are processed using the match-action pipeline [11]. A custom packet",
            "retrieval score": "1.453346",
            "text": "operate in PDP. \u2022 Discussing the impact of design decisions in PDP environments. \u2022 Proposing a new in-network KV platform with major performance and scalability improvements over the stateof-the-art. \u2022 Providing the accessory routing and processing mechanisms to efficiently deliver strong consistency and flexibility. II. KVS DESIGN FOR THE DATA PLANE Research in the workload characteristics of deployed KVS shows that they accommodate read-mostly workloads: the read-write ratio is 380:1 for Google F1, and 500:1 for Facebook TAO. Primary-backup variations, like CR, are specifically designed for read-mostly workloads, making them an appealing candidate for the underlying replication method of data centre KVSs. A. Chain Replication Under CR, the participating nodes/programmable switches form a chain and each has a distinct role: head, tail, or replica. All of the participating nodes hold the same KV pairs. Write queries originate from the head and then propagate across all replica nodes until they reach the tail. The tail issues a response which acts as an acknowledgement for the write. A tail node is also responsible for responding to read queries. Only the tail is considered to be up-to-date with the latest commit for a value and acts as a reference point for the entire chain. In Figure 1a, we show the path of a read query (dashed arrows) and its response (solid arrows). Replicas can replace the head or the tail in case of a failure. Defining the tail as the reference point allows per-key consistency for the entire chain to be achieved. When a write query reaches",
            "title": ""
        },
        {
            "id": "4856391",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs). However, the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself. Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined. In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss. Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced precision quantization substantially recovers the incurred degradation. On the other hand, we observe that the important KV pairs must be kept at a relatively higher precision to safeguard the generation quality. Motivated by these observations, we propose \\textit{Mixed-precision KV cache}~(MiKV), a reliable cache compression method that simultaneously preserves the context details by retaining the evicted KV pairs in low-precision and ensure generation quality by keeping the important KV pairs in high-precision. Experiments on diverse benchmarks and LLM backbones show that our proposed method offers a state-of-the-art trade-off between compression ratio and performance, compared to other baselines.",
            "retrieval next text": "High-resolution spectrum of xenon ions at 13.4 nm. The spectrum of xenon excited in a low-inductance vacuum spark was photographed at high resolution in the region of 9.5-15.5 nm. The observed transitions were identified as belonging to ions from Xe8+ to Xe13+. In the region of importance for extreme-ultraviolet lithography around 13.4 nm, the strongest lines were identified as 4d8-4d7 5p transitions in Xe10+. The identifications were made by use of energy parameters extrapolated along the isoelectronic sequence.",
            "retrieval score": "1.4533141",
            "text": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs). However, the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself. Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined. In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss. Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced precision quantization substantially recovers the incurred degradation. On the other hand, we observe that the important KV pairs must be kept at a relatively higher precision to safeguard the generation quality. Motivated by these observations, we propose \\textit{Mixed-precision KV cache}~(MiKV), a reliable cache compression method that simultaneously preserves the context details by retaining the evicted KV pairs in low-precision and ensure generation quality by keeping the important KV pairs in high-precision. Experiments on diverse benchmarks and LLM backbones show that our proposed method offers a state-of-the-art trade-off between compression ratio and performance, compared to other baselines.",
            "title": ""
        },
        {
            "id": "6689262",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "in the cache whose next use will occur farthest away in the future. Of course, such a policy cannot be implemented in practice as it would require the OS to know in advance what page(s) will be accessed later. Thus, OSs typically use suboptimal, fixed cache eviction heuristics. For example, assuming that the past is a good predictor of the future, many OSs swap out from the cache pages that are least recently used (LRU), implementing some LRU approximation, for instance, the \"second chance\" clock algorithm (Corbat\u00f3, 1969). Hence, existing cache eviction policies are generally effective with workloads that exhibit temporal/spatial locality. Still, they may fail to capture \"unconventional\" patterns of references, thereby losing the advantage of caching and, ultimately, causing the system to degrade. To overcome this limitation, we propose a policy based on MUlti-STep-Ahead Predictions for CacHe Eviction (MUS-TACHE). This is a new page replacement algorithm whose logic is learned from observed memory access requests rather than fixed as existing methods. More specifically, we formulate the page request prediction problem as a categorical -i.e., discrete-valued -time series forecasting task. Then, our method queries the learned page request forecaster to obtain the next k predicted page memory references to arXiv:2211.02177v1 [cs.OS] 3 Nov 2022 better approximate the optimal OPT strategy. As per how to implement the multi-step-ahead page request forecaster, several techniques can be used. In this work, we only consider deep learning techniques, both standard and specifically designed for handling sequence data in general and time series in particular. Among standard approaches, we use",
            "retrieval next text": "multilayer perceptron (MLP). In addition, we study architectures built upon recurrent neural networks (RNNs), such as \"vanilla\" long short-term memory networks (LSTM (Hochreiter & Schmidhuber, 1997)) and sequence-to-sequence (seq2seq) models like Encoder-Decoder LSTM (Sutskever et al., 2014). To train any of these methods, we first collect a very large dataset that logs the memory traces of the execution of benchmark programs contained in the Princeton Application Repository for Shared-Memory Computers (PARSEC). 1 We evaluate the prediction accuracy of each learned page request forecaster offline using a test set previously held out from the entire dataset above. Thus, we consider the bestperforming forecaster as the building block of our MUS-TACHE policy and integrate it into an open-source cache simulator. 2 Finally, we compare MUSTACHE with existing page cache replacement baselines already implemented in the cache simulator when running the test portions of all the programs in the PARSEC suite. Experiments demonstrate the superiority of our approach, as MUSTACHE improves the cache hit ratio by 1.9% and reduces the number of disk operations (i.e., number of reads/writes) by 18.4% and 10.3% compared to the best heuristic (i.e., exact LRU). To summarize, we provide the following contributions: (i) We collect a very large dataset containing the memory accesses referenced during the execution of PARSEC benchmark programs; (ii) We frame the problem of page request prediction into a multi-step-ahead time series forecasting task; (iii) We train several deep learning models for page request prediction; (iv) We implement our cache replacement strategy (MUS-TACHE) using the best-performing page request predictor learned, and",
            "retrieval score": "1.452381",
            "text": "in the cache whose next use will occur farthest away in the future. Of course, such a policy cannot be implemented in practice as it would require the OS to know in advance what page(s) will be accessed later. Thus, OSs typically use suboptimal, fixed cache eviction heuristics. For example, assuming that the past is a good predictor of the future, many OSs swap out from the cache pages that are least recently used (LRU), implementing some LRU approximation, for instance, the \"second chance\" clock algorithm (Corbat\u00f3, 1969). Hence, existing cache eviction policies are generally effective with workloads that exhibit temporal/spatial locality. Still, they may fail to capture \"unconventional\" patterns of references, thereby losing the advantage of caching and, ultimately, causing the system to degrade. To overcome this limitation, we propose a policy based on MUlti-STep-Ahead Predictions for CacHe Eviction (MUS-TACHE). This is a new page replacement algorithm whose logic is learned from observed memory access requests rather than fixed as existing methods. More specifically, we formulate the page request prediction problem as a categorical -i.e., discrete-valued -time series forecasting task. Then, our method queries the learned page request forecaster to obtain the next k predicted page memory references to arXiv:2211.02177v1 [cs.OS 3 Nov 2022 better approximate the optimal OPT strategy. As per how to implement the multi-step-ahead page request forecaster, several techniques can be used. In this work, we only consider deep learning techniques, both standard and specifically designed for handling sequence data in general and time series in particular. Among standard approaches, we use",
            "title": ""
        },
        {
            "id": "2817757",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "when detecting a change in the workload. The DataSpaces framework [28] exploits memory and SSDs to support dynamic data staging in HSM, driven by user-provided hints about expected data read patterns. Hermes adds memory and NVRAM in the storage hierarchy and proposes three policies that can be manually configured by the user. The cost models and tiering mechanisms used in prior approaches in HSM cannot be directly applied to analytics applications since they are designed to handle block level I/Os (e.g., 4-32 KB) for POSIX-style workloads (e.g., server, database, file systems) [11]. In addition, our main approach for automatically moving data across the storage hierarchy is not based on parameter-driven or user-defined policies but rather on machine learning. Caching Caching is a well-studied problem that appears in various contexts and discussed extensively in several surveys [40,2]. We offer a quick overview of the area and highlight the most closely related work. For CPU caches, virtual memory systems, and database buffer caches, there is extensive work on cache eviction policies (e.g., LRU, LFU, ARC, MQ), which are classified as a) recency-based, b) frequency-based, c) size-based, d) function-based, and e) randomized [40]. Other policies found in main memory databases attempt to identify hot and cold data, also based on access frequencies [33,17,14]. Another recent policy augmented the Marker caching algorithm [15] with a machine learned oracle for improving its competitive ratio [36]. Unlike our approach, these policies operate on fixed-size pages and assume that every accessed page will be inserted into the cache. Many caching policies have been",
            "retrieval next text": "developed for web caches that operate on variable size objects, including SIZE, Hyper-G, Greedy-Dual-Size, and Hybrid [2]. Most of these policies have been designed for improving the hit ratio in web accesses but that does not necessarily improve the performance in storage systems [44]. Machine learning techniques such as logistic regression, artificial neural networks, genetic algorithms, random forests, and others have also been used for developing more intelligent web caching policies [2,50,7,3,47]. However, most of these approaches try to identify and predict relationships between web objects; for example, a visit to web page X is typically followed by a visit to web page Y . Other approaches try to capture associations between file attributes (e.g., owner, creation time, and permissions) and properties (e.g., access pattern, lifespan, and size) [38]. Another recent approach used neural networks to analyze the inter-relationships among web requests for making caching decisions [26]. However, such relationships and associations are not expected to be present in big data analytics workloads and, hence, are not applicable in our setting. Reinforcement learning has also been attempted by using multiple experts to select the best cache eviction policy to use at any given time, but these approaches are known to outperform only the static policies (e.g., LRU, LFU) and are computationally and memory expensive [6,20,51]. Web prefetching predicts the web objects expected to be requested in the near future and brings them into the cache before users actually request them. Existing prefetching approaches can be classified in two categories: (i) contentbased that rely on the analysis",
            "retrieval score": "1.4510965",
            "text": "when detecting a change in the workload. The DataSpaces framework exploits memory and SSDs to support dynamic data staging in HSM, driven by user-provided hints about expected data read patterns. Hermes adds memory and NVRAM in the storage hierarchy and proposes three policies that can be manually configured by the user. The cost models and tiering mechanisms used in prior approaches in HSM cannot be directly applied to analytics applications since they are designed to handle block level I/Os (e.g., 4-32 KB) for POSIX-style workloads (e.g., server, database, file systems). In addition, our main approach for automatically moving data across the storage hierarchy is not based on parameter-driven or user-defined policies but rather on machine learning. Caching Caching is a well-studied problem that appears in various contexts and discussed extensively in several surveys,2. We offer a quick overview of the area and highlight the most closely related work. For CPU caches, virtual memory systems, and database buffer caches, there is extensive work on cache eviction policies (e.g., LRU, LFU, ARC, MQ), which are classified as a) recency-based, b) frequency-based, c) size-based, d) function-based, and e) randomized. Other policies found in main memory databases attempt to identify hot and cold data, also based on access frequencies,17,14. Another recent policy augmented the Marker caching algorithm with a machine learned oracle for improving its competitive ratio. Unlike our approach, these policies operate on fixed-size pages and assume that every accessed page will be inserted into the cache. Many caching policies have been",
            "title": ""
        },
        {
            "id": "13606629",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "workload performance with different file systems is unknown.Thirdly, different types of SSDs have significantly different performance properties such as peak throughput, latency, and read/write interference behavior [23,35].For example, flash-based SSDs have unpredictable performance and read/write interference, but non-flash-based ultra-low latency (UUL) SSDs such as Intel Optane SSDs have stable performance and no read/write interference [44,47]. In conclusion, the lack of understanding of how Kyber's configurations affect the achieved performance with different workloads, file systems, and types of SSDs makes it unclear how to optimize Kyber in practice.Specifically, we investigate the following research questions (RQs) around how Kyber's configurations affect the workloads' performance with different workloads, file systems and types of SSDs: (RQ1) How does Kyber affect the performance of workloads when workloads run concurrently and interfere with each other?We investigate how Kyber affects the performance of different workloads by studying the relation between target latency and the workloads' achieved performance.(RQ2) How to configure Kyber's parameters for diverse types NVMe SSDs and diverse file systems to meet workloads' requirements? The key motivation is to find out if and how our findings on Kyber's configurations performance effects can be generalized to different file systems and types of SSDs. We also provide guidelines on how to configure Kyber to meet the workloads' requirements in practice with diverse software and hardware environments. To address these questions, we conduct a first-of-its-kind systematic study of Linux' Kyber I/O scheduler with various kinds of workloads, file systems, and types of SSDs to establish guidelines on how to configure Kyber in practice.Our key contributions",
            "retrieval next text": "in this work include: \u2022 We extensively study how Kyber with different configurations affects workload performance using different combinations of latency-sensitive and throughput-bound workloads on 2 types of SSDs, resulting in 11 observations.To the best of our knowledge, we are the first to investigate the effect of Kyber's configurations on workloads.Flash-based SSDs are composed of a controller that is connected to an array of flash chips.Each flash chip is organized in a hierarchy of dies, planes, blocks, and pages.SSDs have high internal parallelism as both dies and planes can operate in parallel.The NVMe protocol [10] exposes this parallelism to workloads with a multi-queue interface that allows SSDs to execute multiple I/O requests in parallel.Nevertheless, to fully utilize this parallelism, workloads need to issue multiple concurrent I/O requests to the SSD.A challenge here is that a plane can not execute different types of commands (read or write) in parallel.If a read is issued to a die where a write is already being executed, the read is blocked until the write finishes, leading to a 10-40\u00d7 longer read latency.This performance degradation is called read/write interference [17,45].Moreover, the physical constraints of flash chips do not allow inplace updates or intra-block random writes.Pages in a block can only be written sequentially, and written pages need to be erased before they can be rewritten.Erasures happen at the unit of blocks, not at the unit of pages.To imitate the block interface provided by hard disks, the Flash Translation Layer (FTL) in SSD controllers maps logical addresses provided in the block interface to",
            "retrieval score": "1.450895",
            "text": "workload performance with different file systems is unknown.Thirdly, different types of SSDs have significantly different performance properties such as peak throughput, latency, and read/write interference behavior,35.For example, flash-based SSDs have unpredictable performance and read/write interference, but non-flash-based ultra-low latency (UUL) SSDs such as Intel Optane SSDs have stable performance and no read/write interference,47. In conclusion, the lack of understanding of how Kyber's configurations affect the achieved performance with different workloads, file systems, and types of SSDs makes it unclear how to optimize Kyber in practice.Specifically, we investigate the following research questions (RQs) around how Kyber's configurations affect the workloads' performance with different workloads, file systems and types of SSDs: (RQ1) How does Kyber affect the performance of workloads when workloads run concurrently and interfere with each other?We investigate how Kyber affects the performance of different workloads by studying the relation between target latency and the workloads' achieved performance.(RQ2) How to configure Kyber's parameters for diverse types NVMe SSDs and diverse file systems to meet workloads' requirements? The key motivation is to find out if and how our findings on Kyber's configurations performance effects can be generalized to different file systems and types of SSDs. We also provide guidelines on how to configure Kyber to meet the workloads' requirements in practice with diverse software and hardware environments. To address these questions, we conduct a first-of-its-kind systematic study of Linux' Kyber I/O scheduler with various kinds of workloads, file systems, and types of SSDs to establish guidelines on how to configure Kyber in practice.Our key contributions",
            "title": ""
        },
        {
            "id": "13173327",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "are two main approaches of caching studies currently considered in the literature. In the first approach (conventional) ( [3], [4]), the focus is on considering the eviction policies at individual caches, such as First-In-First-Out (FIFO), Least-Frequently-Used (LFU), Least-Recently-Used (LRU), Time-To-Live (TTL) etc. where maximizing the hit probability in a local cache is the parameter of primary interest. In [5], a hierarchical network based on TTL caches is analyzed. The general cache network is studied in [6]. In [4], the performance of the different caching policies under independent reference model (IRM) traffic and renewal traffic is analyzed. Another group of the works considers caching in a wireless heterogeneous network. The works in [7]- [9] study caches in small cell networks. Some works ( [10], [11]) consider caching in the context of Device-Device (D2D) communication. The second, more recent approach ( [12], [13]) considers static caches sharing a common link with a server. These systems have two different phases: content placement and coded delivery. In the content placement phase caches are populated (usually under low network activity) with files. In the content delivery phase, requests from all the nodes are received by the server and the delivery is performed using coded multicast. This has been shown to reduce the total file transmission rate from the base station substantially as against the above conventional schemes. In [12], [14], an information theoretic approach is taken where the minimum rate required to fulfill requests from all the users is studied. The work in [13] extends similar results to D2D communication. [15]",
            "retrieval next text": "studies coded caching in a system with two layers of caches. In [16], an online coded scheme is presented. These schemes have been widely studied under uniform popularity traffic and have been further extended to general random request case also ([17], [18]). In the above coded caching works, an important aspect of queuing at the server has been ignored. The queuing delay can be the dominant component of the overall delay experienced by a user in a content delivery network. Work in [19] addresses these issues. These authors propose a few queuing models for the cache aided coded multicasting schemes. A queue with multicasting and network coding is also studied in ( [20], [21]) in a different setting where there is no finite library of files to download from and each arriving packet is different and must be received by each receiver. As in [19] we also consider queuing delays at the server. Our major contributions are as follows: 1) We consider a new type of queue called the multicast queue in which new requests for a file are merged with the ones already in the queue. All pending requests for a file get served with one transmission of a file. This exploits the broadcast nature of the wireless channel more effectively than in [19] and reflects the practical scenario more realistically. An immediate impact of this model is that, unlike in [19], our queue at the server is always stable for any request arrival rate from the users for any finite number of files. Furthermore,",
            "retrieval score": "1.4507322",
            "text": "are two main approaches of caching studies currently considered in the literature. In the first approach (conventional) (,), the focus is on considering the eviction policies at individual caches, such as First-In-First-Out (FIFO), Least-Frequently-Used (LFU), Least-Recently-Used (LRU), Time-To-Live (TTL) etc. where maximizing the hit probability in a local cache is the parameter of primary interest. In, a hierarchical network based on TTL caches is analyzed. The general cache network is studied in. In, the performance of the different caching policies under independent reference model (IRM) traffic and renewal traffic is analyzed. Another group of the works considers caching in a wireless heterogeneous network. The works in- study caches in small cell networks. Some works (,) consider caching in the context of Device-Device (D2D) communication. The second, more recent approach (,) considers static caches sharing a common link with a server. These systems have two different phases: content placement and coded delivery. In the content placement phase caches are populated (usually under low network activity) with files. In the content delivery phase, requests from all the nodes are received by the server and the delivery is performed using coded multicast. This has been shown to reduce the total file transmission rate from the base station substantially as against the above conventional schemes. In,, an information theoretic approach is taken where the minimum rate required to fulfill requests from all the users is studied. The work in extends similar results to D2D communication.",
            "title": ""
        },
        {
            "id": "6846385",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "but it does not target the online serving settings.OLLA [48] optimizes the lifetime and location of tensors to reduce fragmentation, but it does not do finegrained block-level management or online serving.FlashAttention [13] applies tiling and kernel optimizations to reduce the peak memory of attention computation and reduce I/O costs.This paper introduces a new idea of block-level memory management in the context of online serving. Conclusion This paper proposes PagedAttention, a new attention algorithm that allows attention keys and values to be stored in non-contiguous paged memory, and presents vLLM, a high-throughput LLM serving system with efficient memory management enabled by PagedAttention.Inspired by operating systems, we demonstrate how established techniques, such as virtual memory and copy-on-write, can be adapted to efficiently manage KV cache and handle various decoding algorithms in LLM serving.Our experiments show that vLLM achieves 2-4\u00d7 throughput improvements over the state-of-the-art systems. Figure 3 . Figure 3. KV cache memory management in existing systems.Three types of memory wastes -reserved, internal fragmentation, and external fragmentation -exist that prevent other requests from fitting into the memory.The token in each memory slot represents its KV cache.Note the same tokens can have different KV cache when at different positions. Figure 9 . Figure 9. Beam search example.sample space.The algorithm relies on the beam width parameter , which determines the number of top candidates retained at every step.During decoding, beam search expands each candidate sequence in the beam by considering all possible tokens, computes their respective probabilities using the LLM, and retains the top- most probable sequences out of \u2022",
            "retrieval next text": "| | candidates, where | | is the vocabulary size.Unlike parallel decoding, beam search facilities sharing not only the initial prompt blocks but also other blocks across different candidates, and the sharing patterns dynamically change as the decoding process advances, similar to the process tree in the OS created by compound forks.Fig.9shows how vLLM manages the KV blocks for a beam search example with = 4. Prior to the iteration illustrated as the dotted line, each candidate sequence has used 4 full logical blocks.All beam candidates share the first block 0 (i.e., prompt).Candidate 3 digresses from others from the second block.Candidates 0-2 share the first 3 blocks and diverge at the fourth block.At subsequent iterations, the top-4 probable candidates all originate from candidates 1 and 2. As the original candidates 0 and 3 are no longer among the top candidates, their logical blocks are freed, and the reference counts of corresponding physical blocks are reduced.vLLM frees all physical blocks whose reference counts reach 0 (blocks 2, 4, 5, 8).Then, vLLM allocates new physical blocks (blocks 9-12) to store the new KV cache from the new candidates.Now, all candidates share blocks 0, 1, 3; candidates 0 and 1 share block 6, and candidates 2 and 3 further share block 7.Previous LLM serving systems require frequent memory copies of the KV cache across the beam candidates.For example, in the case shown in Fig.9, after the dotted line, candidate 3 would need to copy a large portion of candidate 2's KV cache to continue generation.This frequent memory copy overhead",
            "retrieval score": "1.4506543",
            "text": "but it does not target the online serving settings.OLLA optimizes the lifetime and location of tensors to reduce fragmentation, but it does not do finegrained block-level management or online serving.FlashAttention applies tiling and kernel optimizations to reduce the peak memory of attention computation and reduce I/O costs.This paper introduces a new idea of block-level memory management in the context of online serving. Conclusion This paper proposes PagedAttention, a new attention algorithm that allows attention keys and values to be stored in non-contiguous paged memory, and presents vLLM, a high-throughput LLM serving system with efficient memory management enabled by PagedAttention.Inspired by operating systems, we demonstrate how established techniques, such as virtual memory and copy-on-write, can be adapted to efficiently manage KV cache and handle various decoding algorithms in LLM serving.Our experiments show that vLLM achieves 2-4\u00d7 throughput improvements over the state-of-the-art systems. Figure 3 . Figure 3. KV cache memory management in existing systems.Three types of memory wastes -reserved, internal fragmentation, and external fragmentation -exist that prevent other requests from fitting into the memory.The token in each memory slot represents its KV cache.Note the same tokens can have different KV cache when at different positions. Figure 9 . Figure 9. Beam search example.sample space.The algorithm relies on the beam width parameter , which determines the number of top candidates retained at every step.During decoding, beam search expands each candidate sequence in the beam by considering all possible tokens, computes their respective probabilities using the LLM, and retains the top- most probable sequences out of \u2022",
            "title": ""
        },
        {
            "id": "11337438",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "in predicting future resource consumption and detect slow tenants. Additionally, different resource models that either combine resource types in a non-linear function or use the dominant resource fairness approach may improve fairness further. Finally, it appears beneficial to extend Cassandra's gossip protocol to integrate a more robust leader selection algorithm for local weight adjustment. Workload-Aware Resource Reservation for Multi-Tenant NoSQL Stores Resource reservation is a common approach to avoiding performance interference among tenants. The basic idea is to dedicate a portion of a resource to a tenant for its use. Chapter 4 uses throughput regulation to provide fair access among tenants. Such an approach can be viewed as a special case of resource reservation -throughput represents the underlying actual resource consumptions and is treated as a \"resource\" for each tenant. As workloads usually have multiple resources involved e.g. memory for caching, CPU for serialization or deserialization, disk for reading or writing data, a tenant needs to acquire a reservation on each resource. But reservations are not all alike: a workload that has a hotspot access pattern may require more cache than does a workload with a random access pattern. An equal reservation of cache and disk usage for both workloads will not yield the best result. So reservations have to be based on workload characteristics, also called workload-aware reservation. A workload-aware reservation becomes more complicated if a workload bears dynamics i.e. a workload changes its access pattern during the access, which requires the system to be able to adjust accordingly. In addition, the distributed nature of",
            "retrieval next text": "NoSQL stores makes the workload-aware reservation more difficult. For a typical NoSQL store, a request is sent 58 to one node which may contact several other nodes to fetch the data. It is complicated to have a coordination among different resources and nodes. Previous research on preventing performance interference does so by simplifying the scenario, either by considering a single resource [24,59,84,105] (e.g. CPU, cache), or representing multiple resources consumption as a single \"virtual resource\" consumption [95,113]. Similarly, work in Chapter 4 uses throughput to approximate the underlying resource consumption of each tenant and regulates the throughput to provide fair access. Ignoring various resource demands that workloads have could lead to low resource utilization as the system imposes unnecessary constraints to tenants and even failure of preventing interference. Therefore, we propose Argus (the 100-eyed watchman in Greek mythology), a workloadaware resource reservation framework that targets multiple resource reservations and aims to prevent performance interference, in terms of fair throughput violation, in NoSQL stores. Specifically, Argus focuses on cache and disk reservations. It enforces the cache reservation by splitting the cache space among tenants. It approximates the disk usage by the throughput of a distributed file system and uses a request scheduler to enforce throughput reservation. Argus models the workload-aware reservation as a constrained optimization and uses the stochastic hill climbing algorithm to find the proper reservation according to various workloads' resource demands. We applied the idea of Argus to HBase [1], a stateof-art NoSQL data store. In summary, this chapter makes the following contributions: \u2022 Quantitative",
            "retrieval score": "1.4501907",
            "text": "in predicting future resource consumption and detect slow tenants. Additionally, different resource models that either combine resource types in a non-linear function or use the dominant resource fairness approach may improve fairness further. Finally, it appears beneficial to extend Cassandra's gossip protocol to integrate a more robust leader selection algorithm for local weight adjustment. Workload-Aware Resource Reservation for Multi-Tenant NoSQL Stores Resource reservation is a common approach to avoiding performance interference among tenants. The basic idea is to dedicate a portion of a resource to a tenant for its use. Chapter 4 uses throughput regulation to provide fair access among tenants. Such an approach can be viewed as a special case of resource reservation -throughput represents the underlying actual resource consumptions and is treated as a \"resource\" for each tenant. As workloads usually have multiple resources involved e.g. memory for caching, CPU for serialization or deserialization, disk for reading or writing data, a tenant needs to acquire a reservation on each resource. But reservations are not all alike: a workload that has a hotspot access pattern may require more cache than does a workload with a random access pattern. An equal reservation of cache and disk usage for both workloads will not yield the best result. So reservations have to be based on workload characteristics, also called workload-aware reservation. A workload-aware reservation becomes more complicated if a workload bears dynamics i.e. a workload changes its access pattern during the access, which requires the system to be able to adjust accordingly. In addition, the distributed nature of",
            "title": ""
        },
        {
            "id": "10164431",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "for each. The prefetching policy must address a prefetch-aggressiveness tradeoff [1] and balance the use of a larger buffer (to protect against stalls) against making prefetching decisions closer to the time of playback (improving predictions of future viewing directions). To address this problem various head-movement prediction techniques have been proposed and evaluated [2,35,48]. However, prior work has not considered the implications of tiling and associated quality-adaptive prefetching techniques for 360\u00b0video on the performance of content caches. This paper presents new trace-based analysis methods that incorporate users' viewports (the area of the full 360\u00b0view the user actually sees), a first characterization of the cross-user similarities of the users' viewports, and a trace-based analysis of the potential bandwidth savings that cachingbased techniques may offer under different conditions. The paper has three main parts, with the second and third parts building on the prior parts. Furthermore, each part includes both novel methodological contributions and a trace-based characterization or analysis providing insights that can help guide the design of more effective caching policies and other related system solutions. First, we focus on similarities in the viewports among different users when at identical playback points within the same 360\u00b0video. Here, we define basic similarity metrics such as the overlap in viewports for two or more users and study how the overlap between a viewport and the aggregate view cover from prior user views increases with the number of such users and how this differs between different video categories. This analysis provides insight into inherent similarities in viewing behavior, and the metrics",
            "retrieval next text": "are not affected by the details of how video data is delivered. Second, we extend the metrics and analysis to evaluate the impact of chunk granularity on the insights from the first part of the paper. This analysis is important to understand how similarities in viewing direction would impact caching performance. For example, consider the case where two users have significantly different viewing directions at a particular time instant, and yet, over the time duration of a chunk, they have essentially the same viewport cover. Finally, we present simulation results using a novel simulation model. The model captures steady-state performance of many independent sessions, while using only a limited number of traces. We simulate a proxy cache and evaluate the cache hit rates observed when using prefetching algorithms that attempt to adaptively select which tiles to download and the quality of each to optimize the user's quality of experience. The observed cache hit rates (analyzed in Section 5) reflect not only viewing direction similarities, both at identical time instants (analyzed in Section 3) and over the time duration of a chunk (analyzed in Section 4), but also differences in chunk quality selections caused by bandwidth variations and uncertainties in viewing direction. The simulation results provide quantitative example comparisons and deliver insights into how viewing direction similarities and potential caching performance depend on the nature of the 360\u00b0video content. Throughout our analysis we place particular focus on insights regarding differences seen when comparing categories of 360\u00b0videos. For this analysis, we use head-movement traces collected for different 360\u00b0categories",
            "retrieval score": "1.4498613",
            "text": "for each. The prefetching policy must address a prefetch-aggressiveness tradeoff and balance the use of a larger buffer (to protect against stalls) against making prefetching decisions closer to the time of playback (improving predictions of future viewing directions). To address this problem various head-movement prediction techniques have been proposed and evaluated,35,48. However, prior work has not considered the implications of tiling and associated quality-adaptive prefetching techniques for 360\u00b0video on the performance of content caches. This paper presents new trace-based analysis methods that incorporate users' viewports (the area of the full 360\u00b0view the user actually sees), a first characterization of the cross-user similarities of the users' viewports, and a trace-based analysis of the potential bandwidth savings that cachingbased techniques may offer under different conditions. The paper has three main parts, with the second and third parts building on the prior parts. Furthermore, each part includes both novel methodological contributions and a trace-based characterization or analysis providing insights that can help guide the design of more effective caching policies and other related system solutions. First, we focus on similarities in the viewports among different users when at identical playback points within the same 360\u00b0video. Here, we define basic similarity metrics such as the overlap in viewports for two or more users and study how the overlap between a viewport and the aggregate view cover from prior user views increases with the number of such users and how this differs between different video categories. This analysis provides insight into inherent similarities in viewing behavior, and the metrics",
            "title": ""
        },
        {
            "id": "7117726",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "delete, respectively. Finally, the third, using the DQN method, implements two agents in which also the deletions are defined with respect to single files. The two double-agent approaches are schematized in Figure 4, where both the interactions of the Eviction and Addition agents are represented. Related works The most used strategy to manage caches is a \"Write Everything\" approach associated to a Least Recently Used (LRU) or to a Least Frequently Used (LFU) eviction policy [14]. They can be effective in most of the cases, but they cannot deal with content popularity and network topologies changing over the time. Hence, recent efforts have gradually shifted toward developing learning and optimization-based approaches, and several ML techniques have been proposed to improve file caching and, in general, better content management. L. Lei et al. [15] propose to train a DNN in advance to better manage the real-time scheduling of cache content into a heterogeneous network. In [16] a Deep Recurrent Neural Network is applied to predict the cache accesses and to make a better caching decision, but this work has been applied just to cache and synthetic dataset whose sizes are far from the Data Lake volumes. Another example of a prediction approach is presented in [17], where predictions are used to optimize the eviction of a cache with a fixed size. While, in [18], an attempt to automate the caching management of a distributed data cluster using the Gradient Boosting Tree is presented. It is evident that the environment is a critical aspect that has to be",
            "retrieval next text": "taken into account when we talk about caching management and, due to its variability, a more flexible and autonomous solution that can adapt itself is needed. To meet this need, techniques based on RL approach have been recently proposed. In [19] a Deep RL approach is used to cache the highly popular contents across distributed caching entities in the context of CDN. However, even if the system allows an online adaptation, the experiment uses a few files that have to be placed optimally in a hierarchical caching system. There are also Deep RL approaches, like the Wolpertinger architecture [20] used by C. Zhong et al. [21], that try to optimize the cache hit rate. But, in that case, the authors assume that all the files in the cache have the same size, and this is not always the case in High Energy Physics context. Thus, the problems solved by the cited works are not fully comparable in size and needs with respect to the ones that we are targeting in our project, where there are a much larger number of files to manage and a huge amount of requests per day to satisfy. Moreover, the field of application is different and very specific. The High Energy Physics context has a data access pattern that cannot be always directly compared with respect to other use cases as we are dealing with an heterogeneous community of users chaotically producing files of different size and structure. Furthermore, there is a real necessity to meet the future requirements with the",
            "retrieval score": "1.4485297",
            "text": "delete, respectively. Finally, the third, using the DQN method, implements two agents in which also the deletions are defined with respect to single files. The two double-agent approaches are schematized in Figure 4, where both the interactions of the Eviction and Addition agents are represented. Related works The most used strategy to manage caches is a \"Write Everything\" approach associated to a Least Recently Used (LRU) or to a Least Frequently Used (LFU) eviction policy. They can be effective in most of the cases, but they cannot deal with content popularity and network topologies changing over the time. Hence, recent efforts have gradually shifted toward developing learning and optimization-based approaches, and several ML techniques have been proposed to improve file caching and, in general, better content management. L. Lei et al. propose to train a DNN in advance to better manage the real-time scheduling of cache content into a heterogeneous network. In a Deep Recurrent Neural Network is applied to predict the cache accesses and to make a better caching decision, but this work has been applied just to cache and synthetic dataset whose sizes are far from the Data Lake volumes. Another example of a prediction approach is presented in, where predictions are used to optimize the eviction of a cache with a fixed size. While, in, an attempt to automate the caching management of a distributed data cluster using the Gradient Boosting Tree is presented. It is evident that the environment is a critical aspect that has to be",
            "title": ""
        },
        {
            "id": "13291099",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "eviction operations. The looping and short sequential files should be considered carefully. Due to their re-access probabilities, they should be kept in the page cache, not immediately evicted. In addition, the prefetching operation should be considered for data in looping pattern. For data in 'Rand' pattern, they are not suitable for proactive policies as it has no clear repeated data or consecutive data regularities. Note that the len parameter shown in Figure 5 could help Buffer Commander to make rational decisions to make enough room for coming data and then preload predicted length, which is appropriate for sequential or loop files. Compared to conventional posix_fadvise() functions in user space, AOIO conveys hints from Sys_calls in kernel space. As hints are not in real time, the relationship between our hints and the original kernel cache manager is the subordinate relationship. That is to say, all hints will be coordinated by the kernel. Only when the kernel can get around to handling hints, then these hints can be executed. For a cache management policy, how to select a victim file to be deported is an important decision. After making conclusion at the proactive policies above, we can get the eviction sequence in AOIO, 'Seq' \u2265 'Rand' \u2265 'Short-seq' \u2265 'Loop'. However, the eviction process is not only executed by AOIO. Our entire cache workflow of a target application is that AOIO issues hints to the original kernel page cache manager firstly, and then the kernel page cache manager administers page cache according to various hints from AOIO. How",
            "retrieval next text": "to complete the entire cache workflow is described below. Implementation in page cache of a kernel. There are several existing realistic implementations in cache source allocation, such as virtual box [28], cgroups [29], or commands in user space, posix_fadvise() [30,31]. However, existing techniques are complex and nonautomated. For example, the virtual box and cgroups techniques need to be evaluated and set up in advance. Furthermore, their resources cannot be dynamically adjusted during operation. Developers have to understand their applications well before adding the posix_fadvise() function. As the posix_fadvise() function has its corresponding function in kernel, AOIO utilizes the kernel functions directly instead of the posix_fadvise() function in the user-space. The results created by kernel functions or posix_fadvise() function are totally the same. For example, in our code design, we use the 'invalidate_mapping_pages' kernel function to discard pages, which corresponds to the 'POSIX_FADV_DONTNEED' variable of posix_fadvise() function. Note that, the kernel variable like 'POSIX_FADV_DONTNEED' will be discussed later. After deploying AOIO, it can automatically run without knowing details of applications. As a result, our proactive hints in AOIO have been listed in Figure 5. The posix_fadvise() function contains four variables: file descriptor (fd), start page position (offset), predicted length (len), and the future advice (advice). For the Buffer Commander module of AOIO, the fd, offset variables are fixed, and another two parameters, len and advice, should be predicted by rule and line. This function has six advice variables, 'POSIX_FADV_NORMAL', 'POSIX_FADV_SEQUENTIAL', 'POSIX_FADV_RANDOM', 'POSIX_FADV_NOREUSE', 'POSIX_FADV_WILLNEED', and 'POSIX_FADV_DONTNEED'. We can infer the six variables by their definitions. For example, 'POSIX_FADV_NORMAL'",
            "retrieval score": "1.4464598",
            "text": "eviction operations. The looping and short sequential files should be considered carefully. Due to their re-access probabilities, they should be kept in the page cache, not immediately evicted. In addition, the prefetching operation should be considered for data in looping pattern. For data in 'Rand' pattern, they are not suitable for proactive policies as it has no clear repeated data or consecutive data regularities. Note that the len parameter shown in Figure 5 could help Buffer Commander to make rational decisions to make enough room for coming data and then preload predicted length, which is appropriate for sequential or loop files. Compared to conventional posix_fadvise() functions in user space, AOIO conveys hints from Sys_calls in kernel space. As hints are not in real time, the relationship between our hints and the original kernel cache manager is the subordinate relationship. That is to say, all hints will be coordinated by the kernel. Only when the kernel can get around to handling hints, then these hints can be executed. For a cache management policy, how to select a victim file to be deported is an important decision. After making conclusion at the proactive policies above, we can get the eviction sequence in AOIO, 'Seq' \u2265 'Rand' \u2265 'Short-seq' \u2265 'Loop'. However, the eviction process is not only executed by AOIO. Our entire cache workflow of a target application is that AOIO issues hints to the original kernel page cache manager firstly, and then the kernel page cache manager administers page cache according to various hints from AOIO. How",
            "title": ""
        },
        {
            "id": "4381628",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "A Large-scale Analysis of Hundreds of In-memory Key-value Cache Clusters at Twitter Modern web services use in-memory caching extensively to increase throughput and reduce latency. There have been several workload analyses of production systems that have fueled research in improving the effectiveness of in-memory caching systems. However, the coverage is still sparse considering the wide spectrum of industrial cache use cases. In this work, we significantly further the understanding of real-world cache workloads by collecting production traces from 153 in-memory cache clusters at Twitter, sifting through over 80 TB of data, and sometimes interpreting the workloads in the context of the business logic behind them. We perform a comprehensive analysis to characterize cache workloads based on traffic pattern, time-to-live (TTL), popularity distribution, and size distribution. A fine-grained view of different workloads uncover the diversity of use cases: many are far more write-heavy or more skewed than previously shown and some display unique temporal patterns. We also observe that TTL is an important and sometimes defining parameter of cache working sets. Our simulations show that ideal replacement strategy in production caches can be surprising, for example, FIFO works the best for a large number of workloads.",
            "retrieval next text": "Research on Gender Differences in Fertility Intentions: Based on the Theoretical Perspective of \u201cWork-life Balance\u201d Based on the database of the \"Population Fertility and Public Services\" group, firstly investigate the existence of significant differences between male and female fertility intentions through t-tests, and afterwards do the regression analysis by gender through econometric models. At length, the results revealed a significant difference between men's and women's fertility intentions(p<0.01**). With overtime hours significantly and positively correlated with fertility intentions for men, and housework hours and caregiving hours significantly and positively correlated with fertility intentions for women. At the individual level, it will lead to \"work-life conflict\", but at the family level, this gender-based division of labor will lead to \"work-life balance\". In the analysis of career choice, women are more family-oriented in their choice of occupation than men. Nevertheless, this \"work-life balance\" comes at the expense of women's development, which will result in lower fertility intentions among younger women and a significant difference with men's fertility intentions. Therefore, the government should protect the labors\u2019 legitimate rights to have more time to return to the family, especially for men, which will be conducive to improving \u201cwork-life balance\u201d and increasing willingness to have children.",
            "retrieval score": "1.4464285",
            "text": "A Large-scale Analysis of Hundreds of In-memory Key-value Cache Clusters at Twitter Modern web services use in-memory caching extensively to increase throughput and reduce latency. There have been several workload analyses of production systems that have fueled research in improving the effectiveness of in-memory caching systems. However, the coverage is still sparse considering the wide spectrum of industrial cache use cases. In this work, we significantly further the understanding of real-world cache workloads by collecting production traces from 153 in-memory cache clusters at Twitter, sifting through over 80 TB of data, and sometimes interpreting the workloads in the context of the business logic behind them. We perform a comprehensive analysis to characterize cache workloads based on traffic pattern, time-to-live (TTL), popularity distribution, and size distribution. A fine-grained view of different workloads uncover the diversity of use cases: many are far more write-heavy or more skewed than previously shown and some display unique temporal patterns. We also observe that TTL is an important and sometimes defining parameter of cache working sets. Our simulations show that ideal replacement strategy in production caches can be surprising, for example, FIFO works the best for a large number of workloads.",
            "title": ""
        },
        {
            "id": "8391943",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "workloads from Table 2 (this table contains workloads with large working sets, for which caching may be beneficial). Table 1 shows data for experiments with eviction configured to eagerly evict unused blocks and for experiments configured to run without any eviction at all. When the cache is full and new blocks cannot be admitted, eager eviction frees up the space. While admitting recently referenced blocks in favour of those that were less recently accessed should improve the hit rate, this also generates more writes into NVRAM, which may diminish the rate at which we can read cached blocks. Indeed, we see from Table 1 that even though the cache hit rate with eager eviction is higher (as expected), the overall throughput is substantially lower than without any eviction. That is because the amount of cache writes produced with eviction is substantially higher than without it, and the writes slow down the reads. The question we then ask is: how to balance the rate of block admission and removal, which generate writes, with the rate of cache lookups, which produce reads? To address it, we introduce the overhead bypass ratio (OBP): Intuitively, the quantity in the numerator captures the cost of using the cache: the write-generating insertions and removals. The quantity in the denominator captures the benefit: cache lookups. OBP thus expresses the balance between the cost and benefit of using the cache; we experimentally determined that a target ratio of 10% works best, but settings between 5% and 30% were also acceptable. If OBP were to",
            "retrieval next text": "be ported and tuned for different hardware, the thresholds would be adjusted according to the degree to which concurrent writes affect the reads. E.g., on hardware where writes have a smaller effect on the performance of reads, acceptable OBP thresholds would be higher. NVCache continuously updates OBP and abstains from admitting or evicting cache blocks if OBP exceeds its target (10%). The OBP metric proved remarkably stable across workloads and cache sizes. We also found OBP to work better than a simple no-write-allocate policy or OBP used in conjunction with the no-write-allocate policy. The small-bypass+OBP bar in Figure 4 shows that small-bypass and OBP completely eliminate the overhead for the benchmarks that do not stand to benefit from caching. Lesson #3: Only read-dominant workloads benefit While the previous sections focused on the overhead and thus experimented with small-sized workloads that do not stand to benefit from NVCache, here we switch to using largesized workloads, which teach us the third lesson: NVRAM cache benefits only read-dominant workloads. Prior study of a custom NVRAM cache for Facebook's RocksDB came to a similar conclusion [25]. Table 2 shows the large-sized workloads and their characteristics. The rate of operations marked with an asterisk (e.g., insert, update for evict-btree-scan) is kept constant by the workload generator, and so we do not report their throughput, because it is largely insensitive to the system configuration. The data size reported in the second column is the on-disk size of the database reported at the end of the run. The intermediate database size may be",
            "retrieval score": "1.4460592",
            "text": "workloads from Table 2 (this table contains workloads with large working sets, for which caching may be beneficial). Table 1 shows data for experiments with eviction configured to eagerly evict unused blocks and for experiments configured to run without any eviction at all. When the cache is full and new blocks cannot be admitted, eager eviction frees up the space. While admitting recently referenced blocks in favour of those that were less recently accessed should improve the hit rate, this also generates more writes into NVRAM, which may diminish the rate at which we can read cached blocks. Indeed, we see from Table 1 that even though the cache hit rate with eager eviction is higher (as expected), the overall throughput is substantially lower than without any eviction. That is because the amount of cache writes produced with eviction is substantially higher than without it, and the writes slow down the reads. The question we then ask is: how to balance the rate of block admission and removal, which generate writes, with the rate of cache lookups, which produce reads? To address it, we introduce the overhead bypass ratio (OBP): Intuitively, the quantity in the numerator captures the cost of using the cache: the write-generating insertions and removals. The quantity in the denominator captures the benefit: cache lookups. OBP thus expresses the balance between the cost and benefit of using the cache; we experimentally determined that a target ratio of 10% works best, but settings between 5% and 30% were also acceptable. If OBP were to",
            "title": ""
        },
        {
            "id": "4192237",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "explore key system tradeoffs. We find that: \u2022 Dynamic cache instantiation has the potential to provide significant cost reductions, sometimes more than halving the costs of (optimized) baselines that uses a permanent cache, with the cache size selected so as to minimize the cost. \u2022 The cost reductions are strongly dependent on the object popularity skew. When there is high skew, dynamic instantiation can work particularly well since a newly instantiated cache is quickly populated with frequently requested items that will capture a substantial fraction of the requests. \u2022 Selective Cache on k th request cache insertion policies can be even more beneficial in this context than with conventional edge caches, and, when there is high popularity skew, there is likely only modest room for improvement in cost/performance through use of more complex cache insertion and replacement policies. Overall, these results show that dynamic cache instantiation using Cache on k th request is a promising approach for content delivery applications. Finally, it is important to note that there does not exist any analysis (from prior work) that captures the performance of LRU caches under time-varying workloads. Our development of easy-to-compute approximation expressions of the performance of LRU caches under time varying workloads is therefore an important contribution. The reason we use RCW for our analysis (rather than LRU) is in part because it enables both an exact analysis and because it provides a nice approximation for LRU caches, while still capturing the cache performance under timevarying workload volumes. In contrast, exact analysis of large LRU caches",
            "retrieval next text": "is intractable. Of course, in practice, we expect many systems to keep implementing LRU cache replacement policies or some variation thereof. Roadmap: Section 2 describes our workload and system assumptions, the caching policies considered, and the metrics of interest. Section 3 presents our analysis of RCW caches for the baseline case without use of dynamic instantiation. Section 4 provides an analysis of the period of transient behavior as an RCW cache fills. Optimization models and performance results for dynamic instantiation are presented in Sections 5 and 6, respectively. Throughout the paper we derive and present results for both exact and O(1)-approximations. Section 7 describes related work, before Section 8 concludes the paper. SYSTEM DESCRIPTION AND METRICS Workload Assumptions: We focus on a single region within the service area of a content delivery application, or a cache location to which a subset of geographically distributed clients are directed [17]. For this cache location, we consider a time period of duration T (e.g., one day), over which the total (aggregated over all objects) content request rate \u03bb(t) varies. We assume that these variations are predictable (e.g., based on prior days), and so for any desired cache instantiation duration D < T , it would be possible to identify in advance the interval of duration D with the highest average request rate over all intervals of duration D within the time period. Short-term temporal locality, non-stationary object popularities, and high rates of new content creation make dynamic cache instantiation potentially more promising, since they reduce the value of old",
            "retrieval score": "1.4457889",
            "text": "explore key system tradeoffs. We find that: \u2022 Dynamic cache instantiation has the potential to provide significant cost reductions, sometimes more than halving the costs of (optimized) baselines that uses a permanent cache, with the cache size selected so as to minimize the cost. \u2022 The cost reductions are strongly dependent on the object popularity skew. When there is high skew, dynamic instantiation can work particularly well since a newly instantiated cache is quickly populated with frequently requested items that will capture a substantial fraction of the requests. \u2022 Selective Cache on k th request cache insertion policies can be even more beneficial in this context than with conventional edge caches, and, when there is high popularity skew, there is likely only modest room for improvement in cost/performance through use of more complex cache insertion and replacement policies. Overall, these results show that dynamic cache instantiation using Cache on k th request is a promising approach for content delivery applications. Finally, it is important to note that there does not exist any analysis (from prior work) that captures the performance of LRU caches under time-varying workloads. Our development of easy-to-compute approximation expressions of the performance of LRU caches under time varying workloads is therefore an important contribution. The reason we use RCW for our analysis (rather than LRU) is in part because it enables both an exact analysis and because it provides a nice approximation for LRU caches, while still capturing the cache performance under timevarying workload volumes. In contrast, exact analysis of large LRU caches",
            "title": ""
        },
        {
            "id": "5719275",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "the quantity of LLM-annotated data on which the first student model is trained, focusing on the setup with retraining (Table 6).We consider the two more challenging tasks, FEVER and Openbook.We find that QBC performs best overall, and the performance of MS is more sensitive to the initial budget.This observation suggests that determining the decision criteria for transitioning from a frontloading regime to MS poses a relevant question, which we leave for future exploration.and (ii) previous literature in active learning can transfer well to this setup.This is, to our knowledge, the first work that leverages online knowledge distillation, which we believe could play a key role in caching LLMs and saving unnecessary calls to expensive models. In this work, we focused on a stationary (i.i.d.) stream of requests.In practice, the distribution of requests is likely to change over time (Cacciarelli and Kulahci, 2023).As suggested by the online AL literature (Bifet and Gavald\u00e0, 2007), this should further increase the gap between the AL-based approaches and static strategies, e.g., front-loading.In those cases, we would expect improvements in both online and final accuracy.We leave this investigation for future work. Figure 1 : Figure 1: Neural caching (one iteration): A student generates a response to a user request.The policy algorithm determines whether to rely on the student's response or to call an LLM.LLM responses are stored and used to re-train the student as more data becomes available. Figure 3 : Figure 3: Accuracy curve with respect to budgets, in the neural caching problem with student retraining.Error lines indicate variance.We have averaged",
            "retrieval next text": "results across the four datasets. Table 1 : The accuracy of the LLM is similar to training the simple model with 5000 gold labels. Table 2 : Online accuracy (AUC) for neural caching with no student retraining. Table 3 : and per-dataset in Figure4.We observe that both MS and QBC considerably outperform the other methods.The embedding-based strategy Accuracy curve with respect to budgets, in the neural caching problem with student retraining.Error lines indicate variance.Online accuracy (AUC) for neural caching with student retraining. Table 4 : Final accuracy (AUC) of the last student model for neural caching with student retraining. Table 5 : Table5shows the absolute improvements in the online and final accu-Absolute improvements for the online and final accuracy using an oracle that allows us to discard instances with wrong labels from the LLM, averaged across datasets.The improvements are with respect to values from Table3 and 4.",
            "retrieval score": "1.4442198",
            "text": "the quantity of LLM-annotated data on which the first student model is trained, focusing on the setup with retraining (Table 6).We consider the two more challenging tasks, FEVER and Openbook.We find that QBC performs best overall, and the performance of MS is more sensitive to the initial budget.This observation suggests that determining the decision criteria for transitioning from a frontloading regime to MS poses a relevant question, which we leave for future exploration.and (ii) previous literature in active learning can transfer well to this setup.This is, to our knowledge, the first work that leverages online knowledge distillation, which we believe could play a key role in caching LLMs and saving unnecessary calls to expensive models. In this work, we focused on a stationary (i.i.d.) stream of requests.In practice, the distribution of requests is likely to change over time (Cacciarelli and Kulahci, 2023).As suggested by the online AL literature (Bifet and Gavald\u00e0, 2007), this should further increase the gap between the AL-based approaches and static strategies, e.g., front-loading.In those cases, we would expect improvements in both online and final accuracy.We leave this investigation for future work. Figure 1 : Figure 1: Neural caching (one iteration): A student generates a response to a user request.The policy algorithm determines whether to rely on the student's response or to call an LLM.LLM responses are stored and used to re-train the student as more data becomes available. Figure 3 : Figure 3: Accuracy curve with respect to budgets, in the neural caching problem with student retraining.Error lines indicate variance.We have averaged",
            "title": ""
        },
        {
            "id": "7306666",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "cache has a bounded size, CARL leases may cause cache over-allocation or contention , when the number of active leases exceeds the cache size; and cache under-allocation , when the number of active leases is smaller than the cache size. A lease assignment may have the correct average cache size because an overallocated portion of program execution is balanced out by an underallocated portion. Cache over-allocation will lead to contention misses, while under-allocation will result in fewer hits. CLAM is the na\u00efve lease assignment policy and has no mechanism to mitigate these effects. Scope Hooked Eviction Leases . Leases assigned to references based on global reuse interval histograms may target an average cache size. Because these histograms contain no information about when different RIs (and therefore leases) occur, cache allocation may not be balanced in the event that access patterns change significantly throughout execution. If we assume RIs are uniformly distributed throughout execution, then cache usage variance during execution is low, and so contention misses are rare and lease assignments based on average cache size will perform well on a fixed-size cache. However, reuse behavior is not always uniform. Programs may be composed of multiple outer loops, or else alternate between multiple inner loops, each of which may have different reuse behavior. This problem is solved by encoding time information in RI histograms, in a technique we call Scope-Hooked Eviction Leasing (SHEL) . In SHEL, the programmer annotates a set of program scopes. These scopes indicate program phases with possibly different reuse behavior. For simplicity, we",
            "retrieval next text": "assume each reference belongs to a single scope. 2 Thus, by including a scope field in reference RI histogram entries, lease assignment may be done on a per-scope, rather than global, granularity. This allows for leases that are less profitable globally to effectively bypass more profitable leases if they take up space during under-allocated phases. Hence, scope annotation allows for lease assignments that are more balanced throughout program execution, resulting in fewer contention misses. It is possible for the allocation in one phase to spill over into the next phase. SHEL ignores such effects. As a result, SHEL may over-assign leases in a scope if the cache space available to the scope is reduced by the spill-over effect from the previous phase. In programs with coarse-grain phases, cross-phase effects may be negligible, and scopes may be optimized independently. An example is a computation with two steps, and each step computes matrix multiplication. When executed, the second step runs long enough to nullify the lingering effect of any lease assigned in the first step. Intuitively, the spill-over effects can be ignored for a program if all its phases are sufficiently longer than the longest lease. We state this property precisely, as follows: Proposition 1. Let s min be the minimal number of accesses in a phase, l max the longest lease assigned, and c the cache size. If s min l max c, cross-scope RIs can be ignored. Cross-scope RIs can be assumed to be in scope, and the resulting lease is the same. To see why",
            "retrieval score": "1.443748",
            "text": "cache has a bounded size, CARL leases may cause cache over-allocation or contention , when the number of active leases exceeds the cache size; and cache under-allocation , when the number of active leases is smaller than the cache size. A lease assignment may have the correct average cache size because an overallocated portion of program execution is balanced out by an underallocated portion. Cache over-allocation will lead to contention misses, while under-allocation will result in fewer hits. CLAM is the na\u00efve lease assignment policy and has no mechanism to mitigate these effects. Scope Hooked Eviction Leases . Leases assigned to references based on global reuse interval histograms may target an average cache size. Because these histograms contain no information about when different RIs (and therefore leases) occur, cache allocation may not be balanced in the event that access patterns change significantly throughout execution. If we assume RIs are uniformly distributed throughout execution, then cache usage variance during execution is low, and so contention misses are rare and lease assignments based on average cache size will perform well on a fixed-size cache. However, reuse behavior is not always uniform. Programs may be composed of multiple outer loops, or else alternate between multiple inner loops, each of which may have different reuse behavior. This problem is solved by encoding time information in RI histograms, in a technique we call Scope-Hooked Eviction Leasing (SHEL) . In SHEL, the programmer annotates a set of program scopes. These scopes indicate program phases with possibly different reuse behavior. For simplicity, we",
            "title": ""
        },
        {
            "id": "6689261",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "buffer (TLB), which is part of the memory management unit (MMU), stores the recent mappings of virtual to physical memory addresses. Doing so makes the entire address translation process quicker than if the CPU had to access the whole page table stored in the main memory. When implementing virtual memory, the operating system (OS) uses the main memory (RAM) as a cache for secondary storage (disk). In this work, we consider a generic memory cache, possibly managed by the OS kernel, that stores a subset of referenced memory pages. As this memory cache gets populated with pages referenced by active working sets (e.g., via demand paging), it may eventually be filled up. Therefore, if a running task requests * Equal contribution 1 Department of Computer Science, Sapienza University of Rome, Italy 2 IMT School for Advanced Studies, Lucca, Italy. Correspondence to: Gabriele Tolomei <tolomei@di.uniroma1.it>. Preliminary work under review. Copyright 2022 by the author(s). Do not distribute. a page that is not in the cache, the OS must cleverly decide which page to evict from the cache to make room for the newly referenced page while maintaining a high cache hit ratio, i.e., a high percentage of cache hits. Theoretically, the optimal cache replacement algorithm that maximizes the hit ratio (also known as OPT or B\u00e9l\u00e1dy's optimal page replacement policy) is known and works as follows: when a page needs to be swapped in from a slower yet larger memory (e.g., disk) to a quicker yet smaller memory cache (e.g., RAM), the OS swaps out the page",
            "retrieval next text": "in the cache whose next use will occur farthest away in the future. Of course, such a policy cannot be implemented in practice as it would require the OS to know in advance what page(s) will be accessed later. Thus, OSs typically use suboptimal, fixed cache eviction heuristics. For example, assuming that the past is a good predictor of the future, many OSs swap out from the cache pages that are least recently used (LRU), implementing some LRU approximation, for instance, the \"second chance\" clock algorithm (Corbat\u00f3, 1969). Hence, existing cache eviction policies are generally effective with workloads that exhibit temporal/spatial locality. Still, they may fail to capture \"unconventional\" patterns of references, thereby losing the advantage of caching and, ultimately, causing the system to degrade. To overcome this limitation, we propose a policy based on MUlti-STep-Ahead Predictions for CacHe Eviction (MUS-TACHE). This is a new page replacement algorithm whose logic is learned from observed memory access requests rather than fixed as existing methods. More specifically, we formulate the page request prediction problem as a categorical -i.e., discrete-valued -time series forecasting task. Then, our method queries the learned page request forecaster to obtain the next k predicted page memory references to arXiv:2211.02177v1 [cs.OS] 3 Nov 2022 better approximate the optimal OPT strategy. As per how to implement the multi-step-ahead page request forecaster, several techniques can be used. In this work, we only consider deep learning techniques, both standard and specifically designed for handling sequence data in general and time series in particular. Among standard approaches, we use",
            "retrieval score": "1.4432399",
            "text": "buffer (TLB), which is part of the memory management unit (MMU), stores the recent mappings of virtual to physical memory addresses. Doing so makes the entire address translation process quicker than if the CPU had to access the whole page table stored in the main memory. When implementing virtual memory, the operating system (OS) uses the main memory (RAM) as a cache for secondary storage (disk). In this work, we consider a generic memory cache, possibly managed by the OS kernel, that stores a subset of referenced memory pages. As this memory cache gets populated with pages referenced by active working sets (e.g., via demand paging), it may eventually be filled up. Therefore, if a running task requests * Equal contribution 1 Department of Computer Science, Sapienza University of Rome, Italy 2 IMT School for Advanced Studies, Lucca, Italy. Correspondence to: Gabriele Tolomei <tolomei@di.uniroma1.it>. Preliminary work under review. Copyright 2022 by the author(s). Do not distribute. a page that is not in the cache, the OS must cleverly decide which page to evict from the cache to make room for the newly referenced page while maintaining a high cache hit ratio, i.e., a high percentage of cache hits. Theoretically, the optimal cache replacement algorithm that maximizes the hit ratio (also known as OPT or B\u00e9l\u00e1dy's optimal page replacement policy) is known and works as follows: when a page needs to be swapped in from a slower yet larger memory (e.g., disk) to a quicker yet smaller memory cache (e.g., RAM), the OS swaps out the page",
            "title": ""
        },
        {
            "id": "2816822",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "the literature over time. A first phase from 1965-1990 addressed basic caching methods for local CPU and database support. Then, web cache applications boosted work on new strategies from the mid 1990-ies with a broader scope on network wide cost and benefit analysis. Figure 2 indicates high activity of caching analysis studies especially in the last 5 years, although this picture is still incomplete. Some papers are listed twice in different categories of Figure 2, when they contribute to several areas. There are a number of surveys and overview papers on basic caching strategies in the literature [38] [95][101] [102], but their focus is not on analysis methods and they have minor overlap with the main workstreams in this overview. The analysis of caching networks in ICN and clouds is addressed in many surveys [23] [35][64] [80] [97][100] [128]. They investigate optimized placement of caches and content distribution in cache hierarchies, request routing and other network optimization aspects. Caching networks are not in our focus, but their performance analysis often builds on and extends solutions for single caches included in this overview, where each single cache can use network specific measures as input for optimization goals of score-/utility-based strategies. We start the main part in Section 2 with demands for efficient caching. Knapsack bounds of the cache hit and value ratio are derived in Section 3 and extended to arbitrary request pattern in Section 4. Section 5 presents Markov analysis results of caching strategies in steady state and for convergence and mixing times, as well as",
            "retrieval next text": "extensions to multilevel and probabilistic caching. Section 6 addresses the impact of time-to-live conditions on caching performance and summarizes TTL cache analysis methods. Section 7 and the Conclusions compare different types of caching performance results, from which limitations and gaps in the state of the art are identified as prospective future research topics. The entire scale of the hit ratio curve (HRC) is shown over cache sizes from 10 \u2264 M < N = 10 6 . The steady state IRM hit ratio for FIFO and RANDOM caching have been proven to coincide via Markov analysis [52]. LRU hit ratios improve the FIFO results by up to 5%, as is generally proven for IRM [122]. LFU hit ratios are close to the upper bound of Eq. (1) and improve LRU by up to 16%. Small LRU hit ratios below 10% are more than doubled by applying LFU [121]. Evaluations for synthetic request pattern as in Figure 3, and for web request traces are usually performed separately for each cache size. Faster computation of the resulting hit or miss ratio curve (HRC, MRC) is suggested in [123][127] to Figure 3: IRM hit ratio simulation results for unit object size enable online estimation of the working set and caching efficiency. LRU and LFU caching strategies allow to compute the hit ratio for arbitrary cache sizes in one step, because the cache content is obtained from the same stack or ordered list for any cache size. Hence, a request is a hit, if the position of the requested object",
            "retrieval score": "1.441464",
            "text": "the literature over time. A first phase from 1965-1990 addressed basic caching methods for local CPU and database support. Then, web cache applications boosted work on new strategies from the mid 1990-ies with a broader scope on network wide cost and benefit analysis. Figure 2 indicates high activity of caching analysis studies especially in the last 5 years, although this picture is still incomplete. Some papers are listed twice in different categories of Figure 2, when they contribute to several areas. There are a number of surveys and overview papers on basic caching strategies in the literature, but their focus is not on analysis methods and they have minor overlap with the main workstreams in this overview. The analysis of caching networks in ICN and clouds is addressed in many surveys. They investigate optimized placement of caches and content distribution in cache hierarchies, request routing and other network optimization aspects. Caching networks are not in our focus, but their performance analysis often builds on and extends solutions for single caches included in this overview, where each single cache can use network specific measures as input for optimization goals of score-/utility-based strategies. We start the main part in Section 2 with demands for efficient caching. Knapsack bounds of the cache hit and value ratio are derived in Section 3 and extended to arbitrary request pattern in Section 4. Section 5 presents Markov analysis results of caching strategies in steady state and for convergence and mixing times, as well as",
            "title": ""
        },
        {
            "id": "6945399",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "Workload analysis of a large-scale key-value store Key-value stores are a vital component in many scale-out enterprises, including social networks, online retail, and risk analysis. Accordingly, they are receiving increased attention from the research community in an effort to improve their performance, scalability, reliability, cost, and power consumption. To be effective, such efforts require a detailed understanding of realistic key-value workloads. And yet little is known about these workloads outside of the companies that operate them. This paper aims to address this gap. To this end, we have collected detailed traces from Facebook's Memcached deployment, arguably the world's largest. The traces capture over 284 billion requests from five different Memcached use cases over several days. We analyze the workloads from multiple angles, including: request composition, size, and rate; cache efficacy; temporal patterns; and application use cases. We also propose a simple model of the most representative trace to enable the generation of more realistic synthetic workloads by the community. Our analysis details many characteristics of the caching workload. It also reveals a number of surprises: a GET/SET ratio of 30:1 that is higher than assumed in the literature; some applications of Memcached behave more like persistent storage than a cache; strong locality metrics, such as keys accessed many millions of times a day, do not always suffice for a high hit rate; and there is still room for efficiency and hit rate improvements in Memcached's implementation. Toward the last point, we make several suggestions that address the exposed deficiencies.",
            "retrieval next text": "Study on land cover change detection method based on NDVI time series batasets: change detection indexes design The normalized difference vegetation index (NDVI) time-series database, derived from NOAA/AVHRR, SPOT/VEGETATION, TERRA or AQUA/MODIS, is increasingly being recognized as a valuable data source for extracting land cover and its change information at global, continental and large regional scale. However, existing approaches, such as principal component analysis (PCA) and change vector analysis (CVA) present considerable difficulties in taking full advantage the NDVI dataset for land cover change detection. Based on the assumptions that different land cover types have different NDVI temporal profiles and that the NDVI profile curve can be regarded as a spectrum in which an NDVI value for a certain date corresponds to on band value of this spectrum, we analyzed the existing change detection indexes and develop a new land cover change detection method based on Lance distance and a cross correlogram spectral matching (CCSM) technique. The new method was validated in the simulation experiments and a case study area of Beijing. From the results, we have demonstrated that the new method takes the shape and value features of NDVI profile curve into consideration. The relatively better performance of the new method can be attributed to two advantages: (1) the new method can discriminate long-term land cover changes form other changes by excluding false changes caused by vegetation phenology changes, climate events, atmospheric variability and sensor noise; (2) it is similarly sensitive to all kinds of land cover changes no matter where the changes have occurred.",
            "retrieval score": "1.4408456",
            "text": "Workload analysis of a large-scale key-value store Key-value stores are a vital component in many scale-out enterprises, including social networks, online retail, and risk analysis. Accordingly, they are receiving increased attention from the research community in an effort to improve their performance, scalability, reliability, cost, and power consumption. To be effective, such efforts require a detailed understanding of realistic key-value workloads. And yet little is known about these workloads outside of the companies that operate them. This paper aims to address this gap. To this end, we have collected detailed traces from Facebook's Memcached deployment, arguably the world's largest. The traces capture over 284 billion requests from five different Memcached use cases over several days. We analyze the workloads from multiple angles, including: request composition, size, and rate; cache efficacy; temporal patterns; and application use cases. We also propose a simple model of the most representative trace to enable the generation of more realistic synthetic workloads by the community. Our analysis details many characteristics of the caching workload. It also reveals a number of surprises: a GET/SET ratio of 30:1 that is higher than assumed in the literature; some applications of Memcached behave more like persistent storage than a cache; strong locality metrics, such as keys accessed many millions of times a day, do not always suffice for a high hit rate; and there is still room for efficiency and hit rate improvements in Memcached's implementation. Toward the last point, we make several suggestions that address the exposed deficiencies.",
            "title": ""
        },
        {
            "id": "354397",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "i is the cost of fetching an object i from its remote server by the cache, h i is the number of times the valid copy of i is fetched from the cache to serve requesting clients, and r i is the total number of requests to i. CSR also represents byte-hit ratio and delay-saving ratio by substituting the object fetching cost's value for object size and object downloading latency, respectively. The byte-hit ratio should be of interest when bandwidth is scarce whereas the delay-saving ratio should be of interest when access latency is restricted. B. Overall Organization This paper is discrete mathematical in nature instead of empirical. The paper begins with the aforementioned rationale for using the CSR rather than the hit rate as the optimization goal of cache replacement policies for nonuniform object costs. Section II re-defines an optimal offline cache replacement policy for nonuniform objects to support optional eviction to achieve the optimal cost-saving goal. Section III presents related works, offline cache replacement policies that were claimed to be optimal, along with their sub-optimality and the complexity proofs. The time complexity analysis enables the derivation of practical online cache replacement policies. We have also proposed the novel sub-optimal policies of lower or comparable complexities. Finally, findings and our future plan are summarized in order. II. OPTIMAL OFFLINE CACHE REPLACEMENT POLICY WITH OPTIONAL EVICTION FOR NONUNIFORM OBJECTS Hosseini\u2212Khayat's OPT [8] has been proved in terms of CSR to be an optimal offline cache replacement policy with mandatory eviction for nonuniform objects. He has also",
            "retrieval next text": "proposed that the modification of the policy for optional eviction is optimal based on the principle of optimality. Herein, we have re-defined the latter policy variant called Hosseini\u2212Khayat's OPT * algorithm and restated its optimality in Corollary 1. Corollary 1: Hosseini\u2212Khayat's OPT * is an optimal offline cache replacement policy for nonuniform objects. To find the optimal multi-state cache replacement decision of nonuniform objects for the sequence of m requests since the first cache replacement decision occurs, possible cache state transitions is represented as a weighted directed acyclic graph G = (V, E) consists of a set of vertices V representing cache states and a set of edges E denoting state transition due to cache hit or miss. connecting a vertex v' to its adjacent vertex v is assigned a weight c(v', v) representing a nonuniform cost, either 0 (resulting from cache hit or optional eviction) or a missing object fetching cost. Each vertex v is assigned an optimal cost c(v), which is the total weight of the shortest weighted path from a root vertex (representing a cache state where free cache space is found smaller than a missing object size for the first time) and represents the least accumulated cost of fetching missing objects across consecutive cache states from the root to the current one. Let organize G into m layers, excluding a root layer; and let V i , \u03bb(v), \u039b(v), and \u0393(v) denote the set of all vertices in layer i (0 \u2264 i \u2264 m where 0 is the root layer, which",
            "retrieval score": "1.4405084",
            "text": "i is the cost of fetching an object i from its remote server by the cache, h i is the number of times the valid copy of i is fetched from the cache to serve requesting clients, and r i is the total number of requests to i. CSR also represents byte-hit ratio and delay-saving ratio by substituting the object fetching cost's value for object size and object downloading latency, respectively. The byte-hit ratio should be of interest when bandwidth is scarce whereas the delay-saving ratio should be of interest when access latency is restricted. B. Overall Organization This paper is discrete mathematical in nature instead of empirical. The paper begins with the aforementioned rationale for using the CSR rather than the hit rate as the optimization goal of cache replacement policies for nonuniform object costs. Section II re-defines an optimal offline cache replacement policy for nonuniform objects to support optional eviction to achieve the optimal cost-saving goal. Section III presents related works, offline cache replacement policies that were claimed to be optimal, along with their sub-optimality and the complexity proofs. The time complexity analysis enables the derivation of practical online cache replacement policies. We have also proposed the novel sub-optimal policies of lower or comparable complexities. Finally, findings and our future plan are summarized in order. II. OPTIMAL OFFLINE CACHE REPLACEMENT POLICY WITH OPTIONAL EVICTION FOR NONUNIFORM OBJECTS Hosseini\u2212Khayat's OPT has been proved in terms of CSR to be an optimal offline cache replacement policy with mandatory eviction for nonuniform objects. He has also",
            "title": ""
        },
        {
            "id": "8871776",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "provides both CPU-GPU with a definite cache space. On the other hand, the reuse cache is able to achieve within 32% data bus utilization for 4 of the 5 workload mixes. VI. CONCLUSION LLC Management is an important problem in today's heterogeneous processors. The reference stream observed by the shared-LLC (SLLC) from the CPU/GPU exhibits little temporal locality, but instead, it exhibits reuse locality. As a consequence, a high proportion of the SLLC lines is useless because the lines will not be requested again before being evicted, and most hits are expected to come from a small subset of already reused lines. This proportion could be particularly high for the GPU accesses. In this work, we looked to minimize counterproductive insertions and evictions in the SLLC by implementing a reuse cache. We evaluate our proposal by running heterogeneous CPU-GPU workloads on the gem5 AMD APU model. We use static partitioning and GPU LLC bypassing as our baseline comparison. Our major observations are: (i) Static-partitioning performs best when the GPU application working set fits into LLC and the CPU application is not cache-sensitive (ii) For the reuse cache, a performance of within 0.8% (or better when GPU application is cache-sensitive) was achievable for most cases, providing average 40% reduction in area (iii) A tag-to-data cache ratio of 2:1 (where data-array is half of a conventional cache) is a good starting point for reuse-cache design space exploration (iv) Simple LLC bypassing degrades performance for GPU in most cases, though it could improve CPU IPC. Static-partitioning hard-partitions the cache,",
            "retrieval next text": "while GPU LLC-bypassing does not provide space to GPU in LLC. We think reuse cache incorporates properties from both, hence could adapt to a wider range of applications. VII. FUTURE WORK In this paper, we have achieved comparable or slightly better application performance with reuse cache in most cases, at significant area reduction. Further reuse cache performance improvements might be closely linked with data/tag replacement policies and memory-access rate (and thread-awareness) in heterogeneous systems. Potential performance improvement might also be obtained by adding a \"Tag-only\" coherence state in the reuse cache coherence protocol, and increasing the reuse hysteresis. Reuse cache could be combined with cache compression techniques where we have more tags than data blocks, thereby reducing data-array size further.",
            "retrieval score": "1.4404079",
            "text": "provides both CPU-GPU with a definite cache space. On the other hand, the reuse cache is able to achieve within 32% data bus utilization for 4 of the 5 workload mixes. VI. CONCLUSION LLC Management is an important problem in today's heterogeneous processors. The reference stream observed by the shared-LLC (SLLC) from the CPU/GPU exhibits little temporal locality, but instead, it exhibits reuse locality. As a consequence, a high proportion of the SLLC lines is useless because the lines will not be requested again before being evicted, and most hits are expected to come from a small subset of already reused lines. This proportion could be particularly high for the GPU accesses. In this work, we looked to minimize counterproductive insertions and evictions in the SLLC by implementing a reuse cache. We evaluate our proposal by running heterogeneous CPU-GPU workloads on the gem5 AMD APU model. We use static partitioning and GPU LLC bypassing as our baseline comparison. Our major observations are: (i) Static-partitioning performs best when the GPU application working set fits into LLC and the CPU application is not cache-sensitive (ii) For the reuse cache, a performance of within 0.8% (or better when GPU application is cache-sensitive) was achievable for most cases, providing average 40% reduction in area (iii) A tag-to-data cache ratio of 2:1 (where data-array is half of a conventional cache) is a good starting point for reuse-cache design space exploration (iv) Simple LLC bypassing degrades performance for GPU in most cases, though it could improve CPU IPC. Static-partitioning hard-partitions the cache,",
            "title": ""
        },
        {
            "id": "6846367",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "GPU workers.The neighboring logical blocks of both sequences do not need to be contiguous in physical GPU memory and the space of physical blocks can be effectively utilized by both sequences. Application to Other Decoding Scenarios \u00a74.3 shows how PagedAttention and vLLM handle basic decoding algorithms, such as greedy decoding and sampling, that take one user prompt as input and generate a single output sequence.In many successful LLM applications [18,34], an LLM service must offer more complex decoding scenarios that exhibit complex accessing patterns and more opportunities for memory sharing.We show the general applicability of vLLM on them in this section. Parallel sampling.In LLM-based program assistants [6,18], an LLM generates multiple sampled outputs for a single input prompt; users can choose a favorite output from various candidates.So far we have implicitly assumed that a request generates a single sequence.In the remainder of this paper, we assume the more general case in which a request generates multiple sequences.In parallel sampling, one request includes multiple samples sharing the same input prompt, allowing the KV cache of the prompt to be shared as well.Via its PagedAttention and paged memory management, vLLM can realize this sharing easily and save memory.Fig. 8 shows an example of parallel decoding for two outputs.Since both outputs share the same prompt, we only reserve space for one copy of the prompt's state at the prompt phase; the logical blocks for the prompts of both sequences are mapped to the same physical blocks: the logical block 0 and 1 of both sequences are mapped to physical",
            "retrieval next text": "blocks 7 and 1, respectively.Since a single physical block can be mapped to multiple logical blocks, we introduce a reference count for each physical block.In this case, the reference counts for physical blocks 7 and 1 are both 2. At the generation phase, the two outputs sample different output tokens and need separate storage for KV cache.vLLM implements a copy-onwrite mechanism at the block granularity for the physical blocks that need modification by multiple sequences, similar to the copy-on-write technique in OS virtual memory (e.g., when forking a process).Specifically, in Fig. 8, when sample A1 needs to write to its last logical block (logical block 1), vLLM recognizes that the reference count of the corresponding physical block (physical block 1) is greater than 1; it allocates a new physical block (physical block 3), instructs the block engine to copy the information from physical block 1, and decreases the reference count to 1. Next, when sample A2 writes to physical block 1, the reference count is already reduced to 1; thus A2 directly writes its newly generated KV cache to physical block 1. In summary, vLLM enables the sharing of most of the space used to store the prompts' KV cache across multiple output samples, with the exception of the final logical block, which is managed by a copy-on-write mechanism.By sharing physical blocks across multiple samples, memory usage can be greatly reduced, especially for long input prompts. Beam search.In LLM tasks like machine translation [59], the users expect the top- most appropriate translations output by the LLM.Beam",
            "retrieval score": "1.4396117",
            "text": "GPU workers.The neighboring logical blocks of both sequences do not need to be contiguous in physical GPU memory and the space of physical blocks can be effectively utilized by both sequences. Application to Other Decoding Scenarios \u00a74.3 shows how PagedAttention and vLLM handle basic decoding algorithms, such as greedy decoding and sampling, that take one user prompt as input and generate a single output sequence.In many successful LLM applications,34, an LLM service must offer more complex decoding scenarios that exhibit complex accessing patterns and more opportunities for memory sharing.We show the general applicability of vLLM on them in this section. Parallel sampling.In LLM-based program assistants,18, an LLM generates multiple sampled outputs for a single input prompt; users can choose a favorite output from various candidates.So far we have implicitly assumed that a request generates a single sequence.In the remainder of this paper, we assume the more general case in which a request generates multiple sequences.In parallel sampling, one request includes multiple samples sharing the same input prompt, allowing the KV cache of the prompt to be shared as well.Via its PagedAttention and paged memory management, vLLM can realize this sharing easily and save memory.Fig. 8 shows an example of parallel decoding for two outputs.Since both outputs share the same prompt, we only reserve space for one copy of the prompt's state at the prompt phase; the logical blocks for the prompts of both sequences are mapped to the same physical blocks: the logical block 0 and 1 of both sequences are mapped to physical",
            "title": ""
        },
        {
            "id": "10195777",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "testing the blogging feature of the application, is similar to ours: the users search, browse the blog entries, add comments, create new entries in a predefined mix.However, their focus is not the response time perceived by the clients, but the server-side workload, so they mainly analyze the hardware level performance metrics like CPU utilization, cache misses, stalls, etc.They conclude that Web 2.0 workloads cause different server-side behavior opposed to traditional OLTP workloads. Although not dealing with performance analysis, [18] still relates to our work.The authors propose a formal framework for managing the evolution of an application originally designed as a single-tenant system for on-premise usage to a multi-tenant SaaS system.Their approach is illustrated with the Apache Roller application.So if someone plans to run a multi-tenant blog engine as a SaaS, the proposal of [18] can be used to transform Apache Roller, and our paper gives some insight into the potential performance of the blog engine when deploying it on Amazon EC2 Micro or Small instances. More recently, the performance of applications that are deployed in the cloud are of growing interest.A large part of the related research focuses on scientific applications with high performance need.Ostermann et al. [19] conclude that the performance and the reliability of Amazon EC2 is insufficient for scientific computing at large.Similar statements are made in [20], where the authors found that using representative application workloads, Amazon EC2 is six times slower than a typical mid-range Linux cluster, and twenty times slower than a modern High Performance Computing system.They also revealed a strong",
            "retrieval next text": "correlation between the percentage of time an application spends communicating, and its overall performance on EC2.These results are consonant with those published in [21], stating that smaller, too frequent communication between nodes spoil the overall performance of an Amazon EC2 cluster, but less frequent, large data exchanges are acceptable. The authors of [22] found that the performance of Small instances is relatively stable and the mean response time remains within 8% of the long-time average.However, different Small instances, which should provide identical performance, can perform very differently up to a ratio 4.During our experiments, we did not experience such a variance in the performance of the Small instances. A detailed performance and scalability analysis of an n-tier web application deployed on different cloud platforms is done in [23].The authors investigated RUBBoS [24], an n-tier electronic commerce system, on three different cloud platforms: Amazon EC2, Open Cirrus, and Emulab [25].They tested several deployment configurations with respect to the number of web servers, Java web containers, and numbers of SQL and data nodes in the MySQL cluster, thus testing horizontal scalability.In the case of Amazon EC2, vertical scalability was examined as well, by changing the instance types (small, large, extra large, and cluster).The results show a good vertical scaling for Amazon EC2, and good horizontal scalability on Open Cirrus and Emulab.The horizontal scalability of Amazon EC2 was, however, very poor: increasing the number of MySQL data nodes caused a degradation in the throughput, although the CPU utilization of the servers was not saturated.The authors found the cause of",
            "retrieval score": "1.439043",
            "text": "testing the blogging feature of the application, is similar to ours: the users search, browse the blog entries, add comments, create new entries in a predefined mix.However, their focus is not the response time perceived by the clients, but the server-side workload, so they mainly analyze the hardware level performance metrics like CPU utilization, cache misses, stalls, etc.They conclude that Web 2.0 workloads cause different server-side behavior opposed to traditional OLTP workloads. Although not dealing with performance analysis, still relates to our work.The authors propose a formal framework for managing the evolution of an application originally designed as a single-tenant system for on-premise usage to a multi-tenant SaaS system.Their approach is illustrated with the Apache Roller application.So if someone plans to run a multi-tenant blog engine as a SaaS, the proposal of can be used to transform Apache Roller, and our paper gives some insight into the potential performance of the blog engine when deploying it on Amazon EC2 Micro or Small instances. More recently, the performance of applications that are deployed in the cloud are of growing interest.A large part of the related research focuses on scientific applications with high performance need.Ostermann et al. conclude that the performance and the reliability of Amazon EC2 is insufficient for scientific computing at large.Similar statements are made in, where the authors found that using representative application workloads, Amazon EC2 is six times slower than a typical mid-range Linux cluster, and twenty times slower than a modern High Performance Computing system.They also revealed a strong",
            "title": ""
        },
        {
            "id": "8787615",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "related works; Section 3, 4 and 5 discuss \"BME\", \"BPM\", and \"BCPEM\" respectively, Section 6 shows the efficiency of \"BCPEM\" with empirical evidences and Section 7 concludes the paper. Throughout the article, we made the following assumptions: \u2022 One faulty bit makes a cache line completely unusable. ulations are performed to know the number of misses for all the caching capacities (M s) possible in a fully-associative cache and the results are used in regression analysis. The less number of caching capacities are simulated, the less the efficiency of \"PFE\". Therefore, \"PFE\" is not helpful enough to be utilized as a cache miss prediction method to avoid cache simulation in the cache performance evaluation phase in design space exploration. In addition, as there was no known fast method to calculate \u2022 When all the lines wear out in a set, the set as well as the entire the actual values of M * and n * , it was impossible to verify the definition cache dies/wears out. * of M * and n used in \"PFE\" and accuracy of their values found via re- \u2022 Line size is same in all the cache configurations. RELATED WORK Among the available cache simulation techniques (such as system simulation [28], instruction set simulation [16], etc.), application's memory access trace-driven single-pass cache simulation is known to be the fastest and the most resource generous. In a single-pass cache simulator, a trace file that indicates when and which data blocks were accessed by the processor during execution of an application is",
            "retrieval next text": "used as the input. By reading one data block access at a time from the trace file, the single-pass simulator checks whether the requested data block is available in the simulated cache configurations. Cache configurations are represented by an array or a list in single-pass simulation. Therefore, without spending a large amount of time in simulating the exact hardware behavior (unlike system simulation [28] and instruction set simulation [16]), single-pass cache simulators can quickly and accurately estimate the number of cache misses for a particular application on a group of cache configurations. To mimic the hardware behavior minimally and to reduce the need for extensive computing resources, additional mechanisms, such as special data structures [8], trace compression [18,27], running the simulation on parallel hardware [10,25], inclusion properties [24], etc. are applied in single-pass simulation (Point to note, not all of these mechanisms can be used for every cache replacement policy). The state-of-art, single-pass FIFO cache simulator is \"CIPARSim\" [9]. Like the other existing cache gression analysis. Moreover, \"PFE\" does not consider cache set size, and no proposal was made on how to adapt \"PFE\" on modern set-associative processor caches. Therefore, this equation could not help much either in SRAM or in NVM/hybrid cache design space exploration. Several system-level analytical models are available to collect information for architectural studies such as the effect of associativity on the memory fetch time, effect of cache line size on cycle time, etc. CACTI [26] is one such famous system-level model for SRAM caches. For NVM, NVSim [5] is a popular",
            "retrieval score": "1.438912",
            "text": "related works; Section 3, 4 and 5 discuss \"BME\", \"BPM\", and \"BCPEM\" respectively, Section 6 shows the efficiency of \"BCPEM\" with empirical evidences and Section 7 concludes the paper. Throughout the article, we made the following assumptions: \u2022 One faulty bit makes a cache line completely unusable. ulations are performed to know the number of misses for all the caching capacities (M s) possible in a fully-associative cache and the results are used in regression analysis. The less number of caching capacities are simulated, the less the efficiency of \"PFE\". Therefore, \"PFE\" is not helpful enough to be utilized as a cache miss prediction method to avoid cache simulation in the cache performance evaluation phase in design space exploration. In addition, as there was no known fast method to calculate \u2022 When all the lines wear out in a set, the set as well as the entire the actual values of M * and n * , it was impossible to verify the definition cache dies/wears out. * of M * and n used in \"PFE\" and accuracy of their values found via re- \u2022 Line size is same in all the cache configurations. RELATED WORK Among the available cache simulation techniques (such as system simulation, instruction set simulation, etc.), application's memory access trace-driven single-pass cache simulation is known to be the fastest and the most resource generous. In a single-pass cache simulator, a trace file that indicates when and which data blocks were accessed by the processor during execution of an application is",
            "title": ""
        },
        {
            "id": "345689",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "ThinK: Thinner Key Cache by Query-Driven Pruning Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications by leveraging increased model sizes and sequence lengths. However, the associated rise in computational and memory costs poses significant challenges, particularly in managing long sequences due to the quadratic complexity of the transformer attention mechanism. This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence lengths, we uncover that the channel dimension of the KV cache exhibits significant redundancy, characterized by unbalanced magnitude distribution and low-rank structure in attention weights. Based on these observations, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in memory costs by over 20% compared with vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and Mistral models across various long-sequence datasets confirm the efficacy of ThinK, setting a new precedent for efficient LLM deployment without compromising performance. We also outline the potential of extending our method to value cache pruning, demonstrating ThinK's versatility and broad applicability in reducing both memory and computational overheads.",
            "retrieval next text": "Modification of Glass Surface by Atmospheric Pressure Plasma A Diffuse Coplanar Surface Barrier Discharge (DCSBS) operating in air at atmospheric pressure has been used to induce changes in the surface properties of glass. The contact angles of water drop on the surface of glass, which were measured by drop shape analysis, decreased markably with plasma treatment for 1 s. With the increasing of the plasma treatment time, the contact angles of the samples treated by air plasma decreased to lower values, and by using the dynamic treatment mode, the values of contact angles decreased rapidly below unmeasurable limit. We also investigated the durability of the treatment.",
            "retrieval score": "1.4387732",
            "text": "ThinK: Thinner Key Cache by Query-Driven Pruning Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications by leveraging increased model sizes and sequence lengths. However, the associated rise in computational and memory costs poses significant challenges, particularly in managing long sequences due to the quadratic complexity of the transformer attention mechanism. This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence lengths, we uncover that the channel dimension of the KV cache exhibits significant redundancy, characterized by unbalanced magnitude distribution and low-rank structure in attention weights. Based on these observations, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in memory costs by over 20% compared with vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and Mistral models across various long-sequence datasets confirm the efficacy of ThinK, setting a new precedent for efficient LLM deployment without compromising performance. We also outline the potential of extending our method to value cache pruning, demonstrating ThinK's versatility and broad applicability in reducing both memory and computational overheads.",
            "title": ""
        },
        {
            "id": "5617874",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "cliffs by guaranteeing convex performance, and EVA uses planning theory to derive the optimal replacement metric under uncertainty. These policies improve performance and make qualitative contributions: Talus is cheap to predict, and so lets cache partitioning techniques (including virtual caches) work with high-performance cache replacement; and EVA shows that the conventional approach to practical cache replacement is sub-optimal. Designing CMP caches is difficult because architects face many options with many interacting factors. Unlike most prior caching work that employs best-effort heuristics, we reason about the tradeoffs through analytical models. This analytical approach lets us achieve the performance and efficiency of application-specific designs across a broad range of applications, while further providing a coherent theoretical framework to reason about data movement. Compared to a 64-core CMP with a conventional cache design, these techniques improve end-to-end performance by up to 76% and an average of 46%, save 36% of system energy, and reduce \u2026",
            "retrieval next text": "Fast assessment of frequency response of cold load pickup in power system restoration In this paper, we introduce an approach for a fast assessment of the dynamic characteristics of system frequency during the period of cold load pickup following a large-scale blackout. The approach is developed based on a load model and a model of generation control system: the load model considers the unique time-dependent characteristic of inrush power surges during the period of cold load pickup, while the model of generation control system mimics the characteristics of various governors that are crucial to the determination of the key variables including initial rate of frequency change, frequency nadir and settling frequency. With the proposed linearization and estimation methods, the model of generation control system can be quickly constructed to offer a great amount of flexibility and simplicity for the study of the impact of cold load on frequency response. The proposed approach allows system operators to quickly evaluate many different options for system restoration. The application of the proposed approach is demonstrated in a case study based on the IEEE 39-bus system. The differences between the results obtained using the proposed approach and those obtained from complete PSS/E simulation are very small, which suggests that the linearized model of generation control system is fast, and accurate enough to address the major concerns of cold load pickup in system restoration.",
            "retrieval score": "1.4385281",
            "text": "cliffs by guaranteeing convex performance, and EVA uses planning theory to derive the optimal replacement metric under uncertainty. These policies improve performance and make qualitative contributions: Talus is cheap to predict, and so lets cache partitioning techniques (including virtual caches) work with high-performance cache replacement; and EVA shows that the conventional approach to practical cache replacement is sub-optimal. Designing CMP caches is difficult because architects face many options with many interacting factors. Unlike most prior caching work that employs best-effort heuristics, we reason about the tradeoffs through analytical models. This analytical approach lets us achieve the performance and efficiency of application-specific designs across a broad range of applications, while further providing a coherent theoretical framework to reason about data movement. Compared to a 64-core CMP with a conventional cache design, these techniques improve end-to-end performance by up to 76% and an average of 46%, save 36% of system energy, and reduce \u2026",
            "title": ""
        },
        {
            "id": "12564630",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "An evaluation of cache management policies under workloads with malicious requests We study the performance of cache admission and eviction policies under workloads with malicious requests (i.e., with requests that try to poison or pollute the cache with useless information). The eviction policies we have chosen for this study are: LRU, SLRU, and SLRU2. We also evaluated W-TinyLFU, a frequency-based cache admission policy that seeks to increase the effectiveness of the cache, specially under skewed access distributions. We implemented the policies in a caching simulator and evaluated them using Zipfian and real workloads from Yahoo, YouTube, and a feature animation company. In addition, we created modified versions of these workloads with added malicious requests, and use these modified workloads to observe how the malicious requests affect the performance of the policies under study. To the best of our knowledge, this is the first systematic study of the performance of caching policies under workloads with malicious requests. Our results show that W-TinyLFU improves the performance of eviction policies such as LRU and SLRU2 for both real and synthetic workloads, including those that contain malicious requests.",
            "retrieval next text": "Researching sensitive issues in child psychiatric nursing: ethical concerns. TOPIC Research in the field of child psychiatric nursing is greatly needed. The sensitive nature of many potential topics has hindered research in the field. PURPOSE This paper defines sensitive research and explores the ethical issues encountered when designing studies on sensitive issues for the purpose of increasing awareness of the problems involved. SOURCES A synthesis of the nursing, psychiatric, psychological, and sociological literature resulted in identification of potential ethical problems, as well as suggestions for managing these ethical issues. CONCLUSION Well-designed research on sensitive issues that meets the ethical principles of beneficence, respect, and justice will contribute to the knowledge base of child psychiatric nursing.",
            "retrieval score": "1.438524",
            "text": "An evaluation of cache management policies under workloads with malicious requests We study the performance of cache admission and eviction policies under workloads with malicious requests (i.e., with requests that try to poison or pollute the cache with useless information). The eviction policies we have chosen for this study are: LRU, SLRU, and SLRU2. We also evaluated W-TinyLFU, a frequency-based cache admission policy that seeks to increase the effectiveness of the cache, specially under skewed access distributions. We implemented the policies in a caching simulator and evaluated them using Zipfian and real workloads from Yahoo, YouTube, and a feature animation company. In addition, we created modified versions of these workloads with added malicious requests, and use these modified workloads to observe how the malicious requests affect the performance of the policies under study. To the best of our knowledge, this is the first systematic study of the performance of caching policies under workloads with malicious requests. Our results show that W-TinyLFU improves the performance of eviction policies such as LRU and SLRU2 for both real and synthetic workloads, including those that contain malicious requests.",
            "title": ""
        },
        {
            "id": "8212583",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "SnapKV: LLM Knows What You are Looking for Before Generation Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications.",
            "retrieval next text": "Addressing health inequities for children in immigrant families: Psychologists as leaders and links across systems. What can psychologists do to address social determinants of health and promote health equity among America's approximately 20 million children in immigrant families (CIF)? This article identifies gaps in current research and argues for a stronger role for psychologists. Psychologists can advocate for and enact changes in institutional systems that contribute to inequities in social determinants of health and promote resources and services necessary for CIF to flourish. We consider systemic exclusionary and discriminatory barriers faced by CIF, including a heightened anti-immigrant political climate, continued threat of immigration enforcement, restricted access to the social safety net, and the disproportionate health, economic, and educational burden of the COVID-19 pandemic. We highlight the potential role of psychologists in (a) leading prevention that addresses stressors such as poverty and trauma; (b) changing systems to mitigate risk factors for CIF; (c) expanding workforce development across multiple disciplines to better serve their needs; (d) identifying mechanisms, such as racial profiling, that contribute to health inequity, and viewing them as public health harms; and (e) guiding advocacy for resources at local, state, and federal levels, including by linking discriminatory policies or practices with health inequity. A key recommendation to increase psychologists' impact is for academic and professional institutions to strengthen relationships with policymakers to effectively convey these findings in spaces where decisions about policies and practices are made. We conclude that psychologists are well positioned to promote systemic change across multiple societal levels and disciplines to improve",
            "retrieval score": "1.4384995",
            "text": "SnapKV: LLM Knows What You are Looking for Before Generation Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications. We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications.",
            "title": ""
        },
        {
            "id": "4373188",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving Mooncake is the serving platform for Kimi, a leading LLM service provided by Moonshot AI. It features a KVCache-centric disaggregated architecture that separates the prefill and decoding clusters. It also leverages the underutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a disaggregated cache of KVCache. The core of Mooncake is its KVCache-centric scheduler, which balances maximizing overall effective throughput while meeting latency-related Service Level Objectives (SLOs). Unlike traditional studies that assume all requests will be processed, Mooncake faces challenges due to highly overloaded scenarios. To mitigate these, we developed a prediction-based early rejection policy. Experiments show that Mooncake excels in long-context scenarios. Compared to the baseline method, Mooncake can achieve up to a 525% increase in throughput in certain simulated scenarios while adhering to SLOs. Under real workloads, Mooncake's innovative architecture enables Kimi to handle 75% more requests.",
            "retrieval next text": "The Contribution of Domestic, Regional and International Factors to Latin America's Business Cycle This paper quantifies the relative contribution of domestic, regional and international factors to the fluctuation of domestic output in six key Latin American (LA) countries: Argentina, Bolivia, Brazil, Chile, Mexico and Peru. Using quarterly data over the period 1980:1-2003:4, a multi-variate, multi-country time series model was estimated to study the economic interdependence among LA countries and, in addition, between each of them and the three world largest industrial economies: the US, the Euro Area and Japan. Falsifying a common suspicion, it is shown that the proportion of LA countries' domestic output variability explained by industrial countries' factors is modest. By contrast, domestic and regional factors account for the main share of output variability at all simulation horizons. The implications for the choice of the exchange rate regime are also discussed.",
            "retrieval score": "1.4382915",
            "text": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving Mooncake is the serving platform for Kimi, a leading LLM service provided by Moonshot AI. It features a KVCache-centric disaggregated architecture that separates the prefill and decoding clusters. It also leverages the underutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a disaggregated cache of KVCache. The core of Mooncake is its KVCache-centric scheduler, which balances maximizing overall effective throughput while meeting latency-related Service Level Objectives (SLOs). Unlike traditional studies that assume all requests will be processed, Mooncake faces challenges due to highly overloaded scenarios. To mitigate these, we developed a prediction-based early rejection policy. Experiments show that Mooncake excels in long-context scenarios. Compared to the baseline method, Mooncake can achieve up to a 525% increase in throughput in certain simulated scenarios while adhering to SLOs. Under real workloads, Mooncake's innovative architecture enables Kimi to handle 75% more requests.",
            "title": ""
        },
        {
            "id": "8641883",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "the traffic profile at upper-level caches becomes more random (Jin & Bestavros, 1999). That explains why Random policy achieved better performances for intermediate nodes in some of the discussed scenarios. As expected, workloads with temporal locality property have a strong correlation with caching policies (Garetto, Leonardi & Martina, 2016), and variations in the temporal locality patterns directly impact the variations of caching policies performances. Regarding the context attributes explored by the replacement schemes, only two of the works presented evaluations including context features in the eviction logic that helped differentiate the node's position: like the node's number of interfaces (Wang & Bensaou, 2012a) and the node degree as a general rank according to the topology (Li, Simon & Gravey, 2012). However, other works are exploring those, and other context attributes that could be helpful. The context attributes with their respective classification and reference works are: Although the node's location is a context that should be considered when selecting a replacement policy, it is not easy to foresee a straight map between policies and node positions. First, because there are many policies and diversity of topologies with different requirements, but mostly because there are other contextual factors that can also impact the performance of the policies. As we continue to show in the next sections, this SLR was able to pinpoint some of these factors. Table 8 summarizes the characteristics of the corresponding scenarios. In the following, we discuss the reported impacts: According to Sun et al. (2014), the replacement scheme's optimal choice depends on the cache",
            "retrieval next text": "size and the placement policy. The authors combined seven placement policies with five replacement policies: LRU, LFU, FIFO, TTL, Size -and cache size variations of 0.0007%, 0.007%, 0.07%, and 0.7% of the unique contents. The content routers have homogeneous cache sizes for all experiments. We observe that the most significant impact on the replacement scheme choice happens when passing from 0.0007% to 0.007% of cache sizes. That is, for all combinations of placement policies, the best choice of replacement scheme changed when the cache size moved from 0.0007% to 0.007%. Meanwhile, for most combinations of placement policies, the experiments running with 0.007%, 0.07%, and 0.7% of cache sizes presented their highest performance values with the same replacement policy. For example, combined with LCE, LRU and TTL achieved the highest performances for 0.007% of cache size, while LFU stands out for the other sizes. Chao et al. (2013) also show evidence that variations on cache size can lead to variations on the policy with the best performance. This work presents a content-based replacement policy named FCDC that manages the content popularity propertyrequest count-to classify and replace contents according to popularity categories. The evaluation shows comparisons of the proposed scheme against LRU and RUF policies. According to the results, FCDC presents a better cache hit rate than LRU and RUF when the cache memory is less than 5%. Yet, the performance rank changed for cache sizes larger than 10%, and LRU performed slightly better than FCDC. The authors attribute this behavior to each policy's property, in which FCDC",
            "retrieval score": "1.4382035",
            "text": "the traffic profile at upper-level caches becomes more random (Jin & Bestavros, 1999). That explains why Random policy achieved better performances for intermediate nodes in some of the discussed scenarios. As expected, workloads with temporal locality property have a strong correlation with caching policies (Garetto, Leonardi & Martina, 2016), and variations in the temporal locality patterns directly impact the variations of caching policies performances. Regarding the context attributes explored by the replacement schemes, only two of the works presented evaluations including context features in the eviction logic that helped differentiate the node's position: like the node's number of interfaces (Wang & Bensaou, 2012a) and the node degree as a general rank according to the topology (Li, Simon & Gravey, 2012). However, other works are exploring those, and other context attributes that could be helpful. The context attributes with their respective classification and reference works are: Although the node's location is a context that should be considered when selecting a replacement policy, it is not easy to foresee a straight map between policies and node positions. First, because there are many policies and diversity of topologies with different requirements, but mostly because there are other contextual factors that can also impact the performance of the policies. As we continue to show in the next sections, this SLR was able to pinpoint some of these factors. Table 8 summarizes the characteristics of the corresponding scenarios. In the following, we discuss the reported impacts: According to Sun et al. (2014), the replacement scheme's optimal choice depends on the cache",
            "title": ""
        },
        {
            "id": "14062428",
            "source": "pes2o_v3",
            "origin_id": null,
            "retrieval text": "policy to optimize the caching efficiency. However, this approach may be less efficient when the queries do not have high similarities in practical scenarios, which leads to high cache miss rates and evictions. The second direction is to perform optimization at the device level. Gillman et al. [45] proposed to cache the DL models instead of the inference results. It schedules models to be loaded into the limited GPU memory to maximize the probability of servicing an incoming request without swapping the models in and out of the memory, thus accelerating the inference by eliminating the cold start latency with cache hits. The caching and eviction policy considers many runtime aspects of DL inference workloads, including model size, frequency, model accuracy, and speed. This work also discusses some future directions for more dynamic caching mechanisms and policies, like framework-level GPU memory-friendly optimization, proactively loading and evicting, and cluster-level GPU memory allocation. To address the limitation of GPU memory, GSLICE [31] and HiveMind [108] explore the common GPU memory component in different inference models, and propose to save GPU resources via memory sharing. Particularly, GSLICE enables efficient GPU memory sharing by allowing the reuse of model parameters via modifications to the DL framework, exposing the CUDA address to different instances. Therefore, it supports loading the inference model-related parameters only once to the GPUs, resulting in faster module loading. HiveMind extends the shared content and brings more possibilities of shared model weights and layers across different inference workloads, saving the overhead from both model loading and inference evaluation.",
            "retrieval next text": "TrIMS [30] organizes the memory sharing of different models in a more systematic design. The model resource manager in TrIMS offers a multi-tiered cache for DL models to be shared across users' FaaS functions and enables DL model sharing across all levels of the memory hierarchy in the GPU, CPU, local storage, and remote storage in the cloud. TrIMS reconciles the lifecycle of model memory consumption and carefully handles the cache misses and evictions. It also considers multi-node, isolation, and fairness problems during sharing. Extensive evaluations on different models show its general abilities to improve the inference performance by mitigating the model loading overhead. 3) System configuration tuning. Besides the optimization techniques detailed above, there exist some schedulers leveraging end-to-end configuration tuning to improve the system throughput. Morphling [143] formulates the optimal configuration search as a few-shot learning problem. Then it adopts model-agnostic meta-learning (MAML) [40] to combine offline meta-model training for inference serving performance modeling under varied hardware and runtime configurations, and performs online few-shot learning to predict the service performance. Based on the prediction, Morphling auto-tunes the resource provisioning configurations and makes better scheduling decisions. RRL [118] concentrates on optimizing the parallelism configurations from different levels, including request level parallelism and intra-request level (inter-op and intra-op) parallelism, which have strong impacts on the latency of the entire system. RRL utilizes a region-based RL method to tune the parallelism configurations and reduce the inference processing latency, based on the system performance similarity between different configurations within a similar parallelism setting. Resource Consumption Feature Similar to",
            "retrieval score": "1.438105",
            "text": "policy to optimize the caching efficiency. However, this approach may be less efficient when the queries do not have high similarities in practical scenarios, which leads to high cache miss rates and evictions. The second direction is to perform optimization at the device level. Gillman et al. proposed to cache the DL models instead of the inference results. It schedules models to be loaded into the limited GPU memory to maximize the probability of servicing an incoming request without swapping the models in and out of the memory, thus accelerating the inference by eliminating the cold start latency with cache hits. The caching and eviction policy considers many runtime aspects of DL inference workloads, including model size, frequency, model accuracy, and speed. This work also discusses some future directions for more dynamic caching mechanisms and policies, like framework-level GPU memory-friendly optimization, proactively loading and evicting, and cluster-level GPU memory allocation. To address the limitation of GPU memory, GSLICE and HiveMind explore the common GPU memory component in different inference models, and propose to save GPU resources via memory sharing. Particularly, GSLICE enables efficient GPU memory sharing by allowing the reuse of model parameters via modifications to the DL framework, exposing the CUDA address to different instances. Therefore, it supports loading the inference model-related parameters only once to the GPUs, resulting in faster module loading. HiveMind extends the shared content and brings more possibilities of shared model weights and layers across different inference workloads, saving the overhead from both model loading and inference evaluation.",
            "title": ""
        }
    ],
    "ranked_results": {
        "0": -2.8203125,
        "1": -2.341796875,
        "2": -2.03125,
        "3": -2.392578125,
        "4": -2.083984375,
        "5": -1.62890625,
        "6": -2.283203125,
        "7": -1.90234375,
        "8": -2.57421875,
        "9": -2.48828125,
        "10": -2.65234375,
        "11": -2.869140625,
        "12": -2.9765625,
        "13": -2.30859375,
        "14": -1.9931640625,
        "15": -2.623046875,
        "16": -2.54296875,
        "17": -2.650390625,
        "18": -2.33984375,
        "19": -3.33984375,
        "20": -1.0673828125,
        "21": -2.41796875,
        "22": -1.6943359375,
        "23": -1.0283203125,
        "24": -2.419921875,
        "25": -2.673828125,
        "26": -2.15625,
        "27": -2.8984375,
        "28": -2.369140625,
        "29": -2.263671875,
        "30": -1.943359375,
        "31": -2.572265625,
        "32": -3.744140625,
        "33": -2.34765625,
        "34": -1.5068359375,
        "35": -2.8671875,
        "36": -2.435546875,
        "37": -1.31640625,
        "38": -1.837890625,
        "39": -2.76953125,
        "40": -1.8818359375,
        "41": -2.76953125,
        "42": -3.197265625,
        "43": -1.849609375,
        "44": -2.3125,
        "45": -3.056640625,
        "46": -2.71484375,
        "47": -2.6640625,
        "48": -2.62109375,
        "49": -2.830078125,
        "50": -3.087890625,
        "51": -2.5234375,
        "52": -2.400390625,
        "53": -2.818359375,
        "54": -2.5625,
        "55": -2.11328125,
        "56": -1.8330078125,
        "57": -3.37890625,
        "58": -1.8505859375,
        "59": -2.41015625,
        "60": -1.7705078125,
        "61": -2.263671875,
        "62": -2.263671875,
        "63": -2.611328125,
        "64": -3.34375,
        "65": -2.6875,
        "66": -2.1796875,
        "67": -2.41015625,
        "68": -2.66015625,
        "69": -2.443359375,
        "70": -1.962890625,
        "71": -2.001953125,
        "72": -2.80859375,
        "73": -1.3828125,
        "74": -1.9287109375,
        "75": -3.650390625,
        "76": -1.5986328125,
        "77": -2.19921875,
        "78": -3.2421875,
        "79": -2.525390625,
        "80": -2.431640625,
        "81": -2.716796875,
        "82": -1.7998046875,
        "83": -1.0361328125,
        "84": -1.869140625,
        "85": -3.310546875,
        "86": -2.810546875,
        "87": -2.349609375,
        "88": -2.564453125,
        "89": -2.177734375,
        "90": -2.72265625,
        "91": -3.283203125,
        "92": -2.701171875,
        "93": -2.2734375,
        "94": -2.384765625,
        "95": -2.951171875,
        "96": -3.064453125,
        "97": -2.533203125,
        "98": -1.8095703125,
        "99": -2.689453125
    },
    "id_mapping": {
        "0": 23,
        "1": 83,
        "2": 20,
        "3": 37,
        "4": 73,
        "5": 34,
        "6": 76,
        "7": 5,
        "8": 22,
        "9": 60,
        "10": 82,
        "11": 98,
        "12": 56,
        "13": 38,
        "14": 43,
        "15": 58,
        "16": 84,
        "17": 40,
        "18": 7,
        "19": 74,
        "20": 30,
        "21": 70,
        "22": 14,
        "23": 71,
        "24": 2,
        "25": 4,
        "26": 55,
        "27": 26,
        "28": 89,
        "29": 66,
        "30": 77,
        "31": 29,
        "32": 61,
        "33": 62,
        "34": 93,
        "35": 6,
        "36": 13,
        "37": 44,
        "38": 18,
        "39": 1,
        "40": 33,
        "41": 87,
        "42": 28,
        "43": 94,
        "44": 3,
        "45": 52,
        "46": 59,
        "47": 67,
        "48": 21,
        "49": 24,
        "50": 80,
        "51": 36,
        "52": 69,
        "53": 9,
        "54": 51,
        "55": 79,
        "56": 97,
        "57": 16,
        "58": 54,
        "59": 88,
        "60": 31,
        "61": 8,
        "62": 63,
        "63": 48,
        "64": 15,
        "65": 17,
        "66": 10,
        "67": 68,
        "68": 47,
        "69": 25,
        "70": 65,
        "71": 99,
        "72": 92,
        "73": 46,
        "74": 81,
        "75": 90,
        "76": 39,
        "77": 41,
        "78": 72,
        "79": 86,
        "80": 53,
        "81": 0,
        "82": 49,
        "83": 35,
        "84": 11,
        "85": 27,
        "86": 95,
        "87": 12,
        "88": 45,
        "89": 96,
        "90": 50,
        "91": 42,
        "92": 78,
        "93": 91,
        "94": 85,
        "95": 19,
        "96": 64,
        "97": 57,
        "98": 75,
        "99": 32
    },
    "final_passages": "[0] SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \\textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \\textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \\textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \\textit{throttLL'eM} achieves up to 43.8\\% lower energy consumption and an energy efficiency improvement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's Triton server.\n[1] the quantity of LLM-annotated data on which the first student model is trained, focusing on the setup with retraining (Table 6).We consider the two more challenging tasks, FEVER and Openbook.We find that QBC performs best overall, and the performance of MS is more sensitive to the initial budget.This observation suggests that determining the decision criteria for transitioning from a frontloading regime to MS poses a relevant question, which we leave for future exploration.and (ii) previous literature in active learning can transfer well to this setup.This is, to our knowledge, the first work that leverages online knowledge distillation, which we believe could play a key role in caching LLMs and saving unnecessary calls to expensive models. In this work, we focused on a stationary (i.i.d.) stream of requests.In practice, the distribution of requests is likely to change over time (Cacciarelli and Kulahci, 2023).As suggested by the online AL literature (Bifet and Gavald\u00e0, 2007), this should further increase the gap between the AL-based approaches and static strategies, e.g., front-loading.In those cases, we would expect improvements in both online and final accuracy.We leave this investigation for future work. Figure 1 : Figure 1: Neural caching (one iteration): A student generates a response to a user request.The policy algorithm determines whether to rely on the student's response or to call an LLM.LLM responses are stored and used to re-train the student as more data becomes available. Figure 3 : Figure 3: Accuracy curve with respect to budgets, in the neural caching problem with student retraining.Error lines indicate variance.We have averaged\n[2] We have included three more similar recent works (years 2020 and 2021) in the \"Related work\" section. These works show how the latency is today an important problem in NVM architectures due to the mix of the very irregular data access patterns and the high inherent latency of NVM modules. Different hardware prefetching techniques appear in current literature showing the insterest of this topic in the research community. Related work: improve the contrast with current works to highlight novelty of our work We have included current researcho work covering the same topic. We compare the characteristics and limitations of the diferent works against our proposal showing how we deal with some of those limitations by allowing a more complex and complete prefetch solution. The \"Related work\" section should be described in the introduction This section is described in the \"Introduction\". Reviewer #3 Comments Impact of HMM in the production final price. The impact of our prefetch technique in the final production price is an important issue. Sometimes as researchers we are too focused in the design and technical topics and we forget that contributions to the state-of-the-art should also be applied in order to be useful to society. We appreciate the reviewer #3 comments and we have included, in subsection \"Prefetcher Implementation\", an study of the cost of an HMM implementation on an actual processor.\n[3] One Queue Is All You Need: Resolving Head-of-Line Blocking in Large Language Model Serving $ $Large language models (LLMs) have become an increasingly important workload for cloud providers catering to both enterprise and consumer applications. LLM inference requests from these applications have end-to-end latency SLOs that must be adhered to in production settings. However, existing LLM serving systems focus on optimization objectives such as request serving throughput or request execution latency rather than the end-to-end latency SLOs. Achieving end-to-end SLOs for latency-sensitive requests is challenging due to head-of-line (HOL) blocking in the request queue, which results from bursty arrival rates and insufficient resources. To address the above challenge, we propose QLM, a multi-model queue management framework for LLM serving. QLM uses stochastic programming to orchestrate the actions of multiple LLM Serving Operations (LSOs) to reduce HOL blocking and maximize SLO attainment. Specifically, QLM uses the following LSOs: model swapping, request eviction, GPU-CPU state swapping, load balancing, and warm model start. Evaluation on heterogeneous GPU devices and models with real-world LLM serving dataset shows that QLM improves SLO attainment by 40-90% and throughput by 20-400% while maintaining or improving device utilization compared to other state-of-the-art LLM serving systems.\n[4] workload performance with different file systems is unknown.Thirdly, different types of SSDs have significantly different performance properties such as peak throughput, latency, and read/write interference behavior,35.For example, flash-based SSDs have unpredictable performance and read/write interference, but non-flash-based ultra-low latency (UUL) SSDs such as Intel Optane SSDs have stable performance and no read/write interference,47. In conclusion, the lack of understanding of how Kyber's configurations affect the achieved performance with different workloads, file systems, and types of SSDs makes it unclear how to optimize Kyber in practice.Specifically, we investigate the following research questions (RQs) around how Kyber's configurations affect the workloads' performance with different workloads, file systems and types of SSDs: (RQ1) How does Kyber affect the performance of workloads when workloads run concurrently and interfere with each other?We investigate how Kyber affects the performance of different workloads by studying the relation between target latency and the workloads' achieved performance.(RQ2) How to configure Kyber's parameters for diverse types NVMe SSDs and diverse file systems to meet workloads' requirements? The key motivation is to find out if and how our findings on Kyber's configurations performance effects can be generalized to different file systems and types of SSDs. We also provide guidelines on how to configure Kyber to meet the workloads' requirements in practice with diverse software and hardware environments. To address these questions, we conduct a first-of-its-kind systematic study of Linux' Kyber I/O scheduler with various kinds of workloads, file systems, and types of SSDs to establish guidelines on how to configure Kyber in practice.Our key contributions\n[5] either cause premature eviction of a useful cache block, leading to an additional cache miss or cause delay in eviction of a dead block, leading to wastage of cache capacity. This calls for cache management mechanisms and policies that can tolerate variability in the reuse behavior of cache blocks to maximize cache e ciency. Our Proposals Aim of this thesis is: To design robust cache management mechanisms and policies for LLC that minimize cache misses in the face of variability in the reuse behavior of cache blocks, while keeping the cost and complexity of the hardware implementation low. To that end, we propose two cache management techniques, one domain-agnostic and one domain-specialized, that introduce robust mechanisms and policies to address variability in reuse prediction. The rest of the chapter provides a brief overview of both proposals. Domain-Agnostic Cache Management In this part of the thesis, we consider a conventional approach to cache management, namely domain-agnostic cache management, in which the LLC is managed completely in hardware. Such approach is quite attractive in practice as the cache management remains fully transparent to the application software. There has been a rich history of works that proposed various domain-agnostic techniques to improve cache e ciency,18,37,39,40,54,59,63,67,69,71,73,76,78,80,81,82,85,86,87,88,89,97,103,110. The state-of-the-art techniques employ prediction mechanisms that seek to correctly identify as many dead blocks as possible and evict them immediately after their last use to reduce cache thrashing. These predictors all rely on some metric of temporal reuse to make their decisions regarding the end of a given block's useful life. Previous\n[6] in predicting future resource consumption and detect slow tenants. Additionally, different resource models that either combine resource types in a non-linear function or use the dominant resource fairness approach may improve fairness further. Finally, it appears beneficial to extend Cassandra's gossip protocol to integrate a more robust leader selection algorithm for local weight adjustment. Workload-Aware Resource Reservation for Multi-Tenant NoSQL Stores Resource reservation is a common approach to avoiding performance interference among tenants. The basic idea is to dedicate a portion of a resource to a tenant for its use. Chapter 4 uses throughput regulation to provide fair access among tenants. Such an approach can be viewed as a special case of resource reservation -throughput represents the underlying actual resource consumptions and is treated as a \"resource\" for each tenant. As workloads usually have multiple resources involved e.g. memory for caching, CPU for serialization or deserialization, disk for reading or writing data, a tenant needs to acquire a reservation on each resource. But reservations are not all alike: a workload that has a hotspot access pattern may require more cache than does a workload with a random access pattern. An equal reservation of cache and disk usage for both workloads will not yield the best result. So reservations have to be based on workload characteristics, also called workload-aware reservation. A workload-aware reservation becomes more complicated if a workload bears dynamics i.e. a workload changes its access pattern during the access, which requires the system to be able to adjust accordingly. In addition, the distributed nature of\n[7] Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient. Relevant code can be found at https://github.com/hdong920/LESS.\n[8] sequence lengths can vary significantly across different LLM inference requests. Finally, we focused on sequence lengths of up to 3K, and P:D ratio in the range of 1-200. We believe that these are representative of many real-world deployments. However, there has also been an increased interest in supporting very long sequences (e.g., 10s-100s of thousands). Such large sequence lengths may pose new challenges as the cost of attention grows quadratically with the number of tokens. We are actively investigating these challenges. Related Work In this section, we provide a brief summary of related work along two dimensions: systems optimizations and model innovations. Systems Optimizations Memory management: In auto-regressive decoding, the number of tokens that need to be generated for a given request is not known apriori. Therefore, conventional systems pre-allocate memory for the KV cache based on a conservative estimation of the maximum number of tokens. Recently, vLLM showed that this approach is inefficient and proposed a framework -motivated by the virtual memory abstraction -that enables incremental memory allocation for KV caches. This helps improve the batch size, especially when the number of tokens varies significantly across different requests. FlexGen focuses on improving the throughput of offline LLM inference in resource-constrained scenarios e.g., running a large model on a single GPU. Toward this goal, FlexGen employs a judicious combination of memory offloading, quantization, and scheduling. Optimizing (self-)attention: In, the authors propose an algorithm to reduce the memory requirement of self-attention from O(n 2 ) to O(1), with respect to the sequence length.\n[9] with many different choices of which requests to reissue and when. (3) The complex and different workload properties of various interactive services, such as servicetime distributions, arrival patterns, request correlations, and system settings make it difficult to derive general strategies for reducing tail latency. (4) Analytical work using queueing theory is challenging even when making strong assumptions about response-time distributions (e.g. drawn from exponential family), and conclusions draw from such simple models are hard to generalize to more complex systems. Methodology and Key Results. The goal of our work is to find a reissue policy that minimizes a workload's kth percentile tail latency by issuing a fixed percentage (or budget) of redundant requests. We explore the space and devise reissue policies in a principled manner -directed by theoretical analysis to identify the key insights of effective reissue policies, and driven by empirical data from actual systems for wide applicability. We introduce a new family of reissue policies, Single-Time / Random (SingleR), that reissue requests after a delay d with probability q. The use of randomness in SingleR provides an important degree of freedom that allows to bound the reissue budget while also ensuring that reissue requests have sufficient time to respond, exploiting the benefits of both immediate and delayed reissue of prior work. Using a simplified analytical model, we formally prove that Sin-gleR is the optimal trade-off between the immediate and delayed reissue strategies. More precisely, we define the Multiple-Time / Random (MultipleR) policies which reissue requests multiple times with different delays and reissue probabilities. We\n[10] explore key system tradeoffs. We find that: \u2022 Dynamic cache instantiation has the potential to provide significant cost reductions, sometimes more than halving the costs of (optimized) baselines that uses a permanent cache, with the cache size selected so as to minimize the cost. \u2022 The cost reductions are strongly dependent on the object popularity skew. When there is high skew, dynamic instantiation can work particularly well since a newly instantiated cache is quickly populated with frequently requested items that will capture a substantial fraction of the requests. \u2022 Selective Cache on k th request cache insertion policies can be even more beneficial in this context than with conventional edge caches, and, when there is high popularity skew, there is likely only modest room for improvement in cost/performance through use of more complex cache insertion and replacement policies. Overall, these results show that dynamic cache instantiation using Cache on k th request is a promising approach for content delivery applications. Finally, it is important to note that there does not exist any analysis (from prior work) that captures the performance of LRU caches under time-varying workloads. Our development of easy-to-compute approximation expressions of the performance of LRU caches under time varying workloads is therefore an important contribution. The reason we use RCW for our analysis (rather than LRU) is in part because it enables both an exact analysis and because it provides a nice approximation for LRU caches, while still capturing the cache performance under timevarying workload volumes. In contrast, exact analysis of large LRU caches\n[11] the traffic profile at upper-level caches becomes more random (Jin & Bestavros, 1999). That explains why Random policy achieved better performances for intermediate nodes in some of the discussed scenarios. As expected, workloads with temporal locality property have a strong correlation with caching policies (Garetto, Leonardi & Martina, 2016), and variations in the temporal locality patterns directly impact the variations of caching policies performances. Regarding the context attributes explored by the replacement schemes, only two of the works presented evaluations including context features in the eviction logic that helped differentiate the node's position: like the node's number of interfaces (Wang & Bensaou, 2012a) and the node degree as a general rank according to the topology (Li, Simon & Gravey, 2012). However, other works are exploring those, and other context attributes that could be helpful. The context attributes with their respective classification and reference works are: Although the node's location is a context that should be considered when selecting a replacement policy, it is not easy to foresee a straight map between policies and node positions. First, because there are many policies and diversity of topologies with different requirements, but mostly because there are other contextual factors that can also impact the performance of the policies. As we continue to show in the next sections, this SLR was able to pinpoint some of these factors. Table 8 summarizes the characteristics of the corresponding scenarios. In the following, we discuss the reported impacts: According to Sun et al. (2014), the replacement scheme's optimal choice depends on the cache\n[12] cache. They de ne the expected stack distance of a reference with reuse time t to be the average stack distance of all references with reuse time t. The miss-ratio curve is then constructed by computing the expected stack distance of each reuse time weighted by their frequency, this gives us a stack distance distribution. If one were to interpret the Statstack model's relationship between average stack distance vs. time, then we would arrive at the kinetic equation present in the average eviction model (AET). Stackstack performs in the same time bound as the AET model, O(N ) time and O(1) space due to their sampling techniques. OPEN PROBLEMS We have described the history of the miss-ratio curve construction techniques that rely on reuse time and stack distance measurements. In a broader scope, we have the enormous amount of research done in the cache behavior modeling domain. Fixing our limit on constructing practical miss-ratio curves and their applications, we nd that results in the following areas would excite many researchers in this eld. \u2022 To what extent can we relate logical access time to physical clock time? Currently, many models assume logical access time, that is each reference is considered a unique point in time and there are no inactive periods. However, in real systems it is often the case where the number of references per unit of time vary signi cantly. An investigation into how much error is produced when using physical clock time vs. the overhead caused by measuring logical time would give insights\n[13] make good admission decisions, however, most accesses (e.g., to popular objects) do not necessitate exact reuse time prediction; these times vary, but are always very short.Based on these findings, SLAP proposes a segmented reuse time model that predicts a reuse time range for each cache miss rather than an exact value.Specifically, after evaluating multiple options, SLAP selects a Long Short-Term Memory (LSTM) model.Prediction accuracy is monitored during runtime, and retraining is triggered when workloads change.Second, deep learning models (like LSTM) have a high prediction latency.The SLAP LSTM architecture was delicately designed to allow a large portion of computation on recent access features to overlap the miss object fetching.SLAP was able to hide 98% of prediction latency by doing so.Finally, SLAP reuses common prediction computation on concurrent requests to reduce computation overhead, and uses trace sampling to reduce the training time of the LSTM model. We show that SLAP is effective at providing good CDN performance (e.g., hit rate), significantly reducing CDN write traffic (hence lower operational costs), and easily adapting to varying cache sizes.Our experiments with real-world traces show that: SLAP enables consistently better hit rate and significantly smaller write traffic, compared to all state-of-the-art learned and heuristic-based admission policies.For example, applying SLAP with the LRU replacement policy improves the SSDs lifetime by 104% -178% and reduces the cache write traffic by 38% -59%, outperforming all existing approaches.In addition, SLAP improves the hit rate by 3.2% -11.7%.Most importantly, SLAP requires no effort to deploy on cache instances of different size (for the same service) and\n[14] queue up, time out, and retry, creating even more work and exacerbating performance problems. Unfortunately, publicly available information on communi-cation latencies in the cloud is scarce, impeding the decisionmaking process regarding critical timing assumptions.There are a plethora of websites and services that provide some information about average or median latency between cloud regions, but these services often do not provide regionlocal latencies or expose finer statistics on latency distributions to help engineers estimate how often, and by how much the communication latency may deviate from the average or median.As a result, most literature pulls these communication latency timing assumptions out of thin air.For example, the aforementioned Copilots work runs on a dedicated cluster with average inter-node communication latency of 0.25ms, making 1 ms timeout a plausible assumption. In this work, we study the predictability of communication in the cloud to understand and empirically justify the timing assumptions engineers and designers make when working on cloud-native services and applications.To that extent, our contributions are two-fold.First, we present CLT, a simple opensource tool 1 to collect communication latency data across many cloud VMs.Second, we use CLT to study the communication latency patterns between VMs in three large cloud providers: Amazon Web Services (AWS), Google Compute Platform (GCP), and Microsoft Azure. Our tool, Cloud Latency Tester or CLT for short, is a simple echo-like application that can deploy to many VMs in different parts of the cloud, such as different placement groups, availability zones, or regions.The tool runs TCP traffic of configurable payload size and frequency between\n[15] A parallel workload has extreme variability In both high-performance computing (HPC) environments and the public cloud, the duration of time to retrieve or save your results is simultaneously unpredictable and important to your over all resource budget. It is generally accepted (\"Google: Taming the Long Latency Tail - When More Machines Equals Worse Results\", Todd Hoff, highscalability.com 2012), but without a robust explanation, that identical parallel tasks do take different durations to complete -- a phenomena known as variability. This paper advances understanding of this topic. We carefully choose a model from which system-level complexity emerges that can be studied directly. We find that a generalized extreme value (GEV) model for variability naturally emerges. Using the public cloud, we find real-world observations have excellent agreement with our model. Since the GEV distribution is a limit distribution this suggests a universal property of parallel systems gated by the slowest communication element of some sort. Hence, this model is applicable to a variety of processing and IO tasks in parallel environments. These findings have important implications, ranging from characterizing ideal performance for parallel codes to detecting degraded behaviour at extreme scales. INTRODUCTION Where they exist at all, current models for variability of parallel workloads on HPC systems implicitly assume I/O variability follows a normal distribution with the mean and standard deviation the only measure of interest,21,32,24,28. An attempt to fit the tail of task duration to the log-normal distribution has also been made with limited success.,16 point out that lowering latency for a given PDSW-DICS\n[16] cache has a bounded size, CARL leases may cause cache over-allocation or contention , when the number of active leases exceeds the cache size; and cache under-allocation , when the number of active leases is smaller than the cache size. A lease assignment may have the correct average cache size because an overallocated portion of program execution is balanced out by an underallocated portion. Cache over-allocation will lead to contention misses, while under-allocation will result in fewer hits. CLAM is the na\u00efve lease assignment policy and has no mechanism to mitigate these effects. Scope Hooked Eviction Leases . Leases assigned to references based on global reuse interval histograms may target an average cache size. Because these histograms contain no information about when different RIs (and therefore leases) occur, cache allocation may not be balanced in the event that access patterns change significantly throughout execution. If we assume RIs are uniformly distributed throughout execution, then cache usage variance during execution is low, and so contention misses are rare and lease assignments based on average cache size will perform well on a fixed-size cache. However, reuse behavior is not always uniform. Programs may be composed of multiple outer loops, or else alternate between multiple inner loops, each of which may have different reuse behavior. This problem is solved by encoding time information in RI histograms, in a technique we call Scope-Hooked Eviction Leasing (SHEL) . In SHEL, the programmer annotates a set of program scopes. These scopes indicate program phases with possibly different reuse behavior. For simplicity, we\n[17] it will require deploying multiple, smaller caches. Additional help is expected from recommendation systems, whose effect is to shape the demand concentrating it around the most popular content items. Intuitively, having fewer, popular items to serve will improve caching performance. In this context, our paper targets three main questions. Vehicular demand. What is the data demand generated by today's vehicular users? Which apps and services represent the most significant contributions thereto? Caching architectures. Given a target hit ratio, what is the relationship between caching architecture and size of the caches we need to deploy? What is the impact of moving caches from core-level switches to individual base stations, on the total cache size, on the distance data must travel within the core network, and on the load thereof? What changes if a recommendation system is in place? Location-specific content. Content items consumed by future vehicular networks are expected to strongly depend on the location -augmented maps for self-driving vehicles being the most obvious example. What will be the impact of this kind of content on caching? We answer these questions using a set of real-world, large-scale measurement data, coming from users of the WeFi app. Due to its crowd-sourced nature, our dataset includes data for: (i) multiple apps, including video (e.g., YouTube) and maps; (ii) multiple types of users, from pedestrian to vehicular ones; (iii) multiple network technologies, including 3G, LTE, and Wi-Fi; (iv) multiple operators. We describe our dataset, as well as the additional processing we need to perform in order to enhance the\n[18] single GPU workload; they also propose GPU performance and power models for effective workload consolidation on GPUs. Joulemeter that is initially designed as a tool for power usage measurement of virtual machines aims to consolidate multiple workloads on fewer servers for improving resource utilization and power costs. Analyzing the effect of last level processor cache (LLC) on workload consolidation -another topic of interest we investigate in this work-is also covered by several studies. For example, authors of study the behavior of consolidated workloads particularly on sharing caches across a variety of configurations. In authors also study shared resource monitoring to understand resource usage and ways to improve overall throughput as well as quality of service of a data center. A mathematical model has also been proposed to predict the effect of cache contention on the performance of consolidated workloads. After close examination of all these works, we noticed several shortcomings and decided to cover them in this article; thus, we can highlight our contribution in this work through the following items. The first difference of our work with previously reported studies mainly lies in the way we characterize data-intensive applications with two main parameters: file size and request size; such characterization is inspired by well-known filesystem benchmarking tools, namely Iometer, IOzone, TestDFSIO, and Bonnie++. Our second contribution is related to the Hadoop distributed file system that has been never properly covered in previous studies -to the best of our knowledge. We believe this is the first work that\n[19] are two main approaches of caching studies currently considered in the literature. In the first approach (conventional) (,), the focus is on considering the eviction policies at individual caches, such as First-In-First-Out (FIFO), Least-Frequently-Used (LFU), Least-Recently-Used (LRU), Time-To-Live (TTL) etc. where maximizing the hit probability in a local cache is the parameter of primary interest. In, a hierarchical network based on TTL caches is analyzed. The general cache network is studied in. In, the performance of the different caching policies under independent reference model (IRM) traffic and renewal traffic is analyzed. Another group of the works considers caching in a wireless heterogeneous network. The works in- study caches in small cell networks. Some works (,) consider caching in the context of Device-Device (D2D) communication. The second, more recent approach (,) considers static caches sharing a common link with a server. These systems have two different phases: content placement and coded delivery. In the content placement phase caches are populated (usually under low network activity) with files. In the content delivery phase, requests from all the nodes are received by the server and the delivery is performed using coded multicast. This has been shown to reduce the total file transmission rate from the base station substantially as against the above conventional schemes. In,, an information theoretic approach is taken where the minimum rate required to fulfill requests from all the users is studied. The work in extends similar results to D2D communication.\n[20] dominated by tightly-coupled parallel jobs and, more recently, by bag-of-tasks applications. The GWA includes traces collected from grid environments; differently from this work, these traces are dominated by bag-of-tasks applications and by virtual-machine lease-release data. Workload characterization, definition, and modeling: There is much related and relevant work in this area, from which we compare only with the closely related; other characterization work does not focus on comparing traces by domain and does not cover a set of characteristics as diverse as this work, leading to so many findings. Closest to this work, the Google cluster-traces have been analyzed from various points of view, e.g.,,9,34. Amvrosiadis et al.,4 compare the Google cluster traces with three other cluster traces, of 0.3-3 times the size and 3-60 times the duration, and find key differences; our work adds new views and quantitative data on diversity, through both survey and characterization techniques. Bharathi et al. provide a characterization on workflow structures and the effect of workflow input sizes on said structures. Five scientific workflows are used to explain in detail the compositions of their data and computational dependencies. Using the characterization, a workflow generator generator for parameterized workflows is developed. Juve et al. provide a characterization of six scientific workflows using workflow profiling tools that investigate resource consumption and computational characteristics of tasks. The teams of Feitelson and Iosup have provided many characterization and modeling studies for parallel, grid, and hostedbusiness workloads; and Feitelson has written a seminal book on workload modeling. In\n[21] No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization Key-Value (KV) Caching has become an essential technique for accelerating the inference speed and throughput of generative Large Language Models~(LLMs). However, the memory footprint of the KV cache poses a critical bottleneck in LLM deployment as the cache size grows with batch size and sequence length, often surpassing even the size of the model itself. Although recent methods were proposed to select and evict unimportant KV pairs from the cache to reduce memory consumption, the potential ramifications of eviction on the generative process are yet to be thoroughly examined. In this paper, we examine the detrimental impact of cache eviction and observe that unforeseen risks arise as the information contained in the KV pairs is exhaustively discarded, resulting in safety breaches, hallucinations, and context loss. Surprisingly, we find that preserving even a small amount of information contained in the evicted KV pairs via reduced precision quantization substantially recovers the incurred degradation. On the other hand, we observe that the important KV pairs must be kept at a relatively higher precision to safeguard the generation quality. Motivated by these observations, we propose \\textit{Mixed-precision KV cache}~(MiKV), a reliable cache compression method that simultaneously preserves the context details by retaining the evicted KV pairs in low-precision and ensure generation quality by keeping the important KV pairs in high-precision. Experiments on diverse benchmarks and LLM backbones show that our proposed method offers a state-of-the-art trade-off between compression ratio and performance, compared to other baselines.\n[22] It's Time to Revisit LRU vs. FIFO We revisit the question of the effectiveness of the popular LRU cache eviction policy versus the FIFO heuristic which attempts to give an LRU like behavior. Several past works have considered this question and commonly stipulated that while FIFO is much easier to implement, the improved hit ratio of LRU outweighs this. We claim that two main trends call for a reevaluation: new caches such as front-ends to cloud storage have very large scales and this makes managing cache metadata in RAM no longer feasible; and new workloads have emerged that possess different characteristics. We model the overall cost of running LRU and FIFO in a very large scale cache and evaluate this cost using a number of publicly available traces. Our main evaluation workload is a new set of traces that we collected from a large public cloud object storage service and on this new trace FIFO exhibits better overall cost than LRU. We hope that these observations reignite the evaluation of cache eviction policies under new circumstances and that the new traces, that we intend to make public, serve as a testing ground for such work.\n[23] in the cache whose next use will occur farthest away in the future. Of course, such a policy cannot be implemented in practice as it would require the OS to know in advance what page(s) will be accessed later. Thus, OSs typically use suboptimal, fixed cache eviction heuristics. For example, assuming that the past is a good predictor of the future, many OSs swap out from the cache pages that are least recently used (LRU), implementing some LRU approximation, for instance, the \"second chance\" clock algorithm (Corbat\u00f3, 1969). Hence, existing cache eviction policies are generally effective with workloads that exhibit temporal/spatial locality. Still, they may fail to capture \"unconventional\" patterns of references, thereby losing the advantage of caching and, ultimately, causing the system to degrade. To overcome this limitation, we propose a policy based on MUlti-STep-Ahead Predictions for CacHe Eviction (MUS-TACHE). This is a new page replacement algorithm whose logic is learned from observed memory access requests rather than fixed as existing methods. More specifically, we formulate the page request prediction problem as a categorical -i.e., discrete-valued -time series forecasting task. Then, our method queries the learned page request forecaster to obtain the next k predicted page memory references to arXiv:2211.02177v1 [cs.OS 3 Nov 2022 better approximate the optimal OPT strategy. As per how to implement the multi-step-ahead page request forecaster, several techniques can be used. In this work, we only consider deep learning techniques, both standard and specifically designed for handling sequence data in general and time series in particular. Among standard approaches, we use\n[24] this kind of memory architecture. We believe that this work increases the relevance of our proposal and shows how latency is one of the main problems to solve to have NVM architectures fully in use. \uf0b7 Also in section \"Prefetch Based on Hidden Markov Model\" we explain the on-chip memory hierarchy based on virtual address highlighting its advantages and disadvantages. Here we show that LLC analysis that relies on virtual addresses instead of physical addresses reduces drastically access frequency while providing enough information for a good prefetcher to catch algorithmic behavior and produce good prefetch accuracy. We've also added a comparation between the literature proposals and our HMM model highlighting our main differential proposal. Reviewer #1 Comments Latest approaches can be included in the related work \uf0b7 In related work we have included three recent works in hardware prefetching related to different features of LLC, big data and machine learning, irregular workloads and fabric memory integration: \u2022 [Choi21: LLC prefetcher targeted to big data and machine learning processing that adapt granularities entailing cache lines and page granularity. Includes a DRAM buffer with an access history table and several prefetch engines to deal with irregular memory access patterns. \u2022 [Talati21: hardware-software codesign solution to deal with very irregular workloads that uses static information from programs and dynamic from hardware to extract program semantic information used to generate prefetch requests. Our proposal succeeds in capturing irregular data access patterns without the use of compiler information from running programs. \u2022 [Kommareddy20: decoupling of memory from computing is currently under\n[25] workloads. INTRODUCTION Modern large-scale web services rely on caching extensively; inmemory key-value (KV) caches are placed between front-end services and back-end storage systems to achieve high throughput and overcome the long latency gap.In-memory KV caches are widely used and discussed in industry and research communities; Memcached and Redis are two in-memory caching solutions commonly deployed in many production environments.Large web service providers like Facebook and Twitter also developed their general-purpose caching frameworks, Cachelib and Pelikan, respectively, to handle their caching use cases. The performance of these caching systems is largely impacted by its replacement/eviction policy, i.e., the algorithm that decides whether an item should be cached or evicted.The Least Recently Used (LRU) is one of the most commonly known replacement policies, which evicts an item based on the item's access recency.Despite its simplicity, the LRU policy has proven quite effective in many caching systems,9,32,41.There are also many other advanced eviction policies, such as,6,27,33,43, which make eviction decisions based on a combination of item's metadata.The effectiveness of an eviction policy primarily depends on two factors: First, the caching workload; With the rise of cloud and datadriven services, the diversity of in-memory caching workloads has grown drastically compared to the past,4,10,41.Many existing studies have shown that no existing heuristic-based eviction policies can consistently outperform others under every caching use case,33,43.As a result, many special purposes caching frameworks have adopted different replacement policies to accommodate different use cases,26,39,42. Second, the underlying cache's structure and the storage medium also constrain the\n[26] by prior figures in this section. Also, when comparing Figures 32(c) and 32(f) it should be noted that owing to our choice of normalized units for bandwidth, the hit rate is always one when all clients have the same (constant) bandwidth above one (Figure 32(c)) whereas bandwidth variations in the national speedtest dataset (Figure 32(f)) result in significant periods of bandwidth below one even for average values substantially larger than one. Again, in practice, we expect clients sharing the same cache to see bandwidth variation between these two extremes, with operators likely to strive towards increasingly stable network conditions for streaming clients. RELATED WORK Broadly, the related work can be split into works that consider the head movements during viewing of 360\u00b0videos, client-side techniques to provide the best possible QoE (e.g., through adaptive prefetching based on expected viewing directions), and caching of HAS videos. While some recent works have considered optimized cache management policies for 360\u00b0videos,29,32, none of these works provide a data-driven characterization of the caching opportunities that would be observed with traditional caching policies that simply cache the requested tiles when the clients apply adaptive prefetching techniques. Head-movement characterization: Some recent works have collected datasets and characterized the 360\u00b0viewer behavior,2,9,11,12,26,34. However, most of these datasets use relatively short video segments and do not capture changes in behavior over time or across classes of videos. The primary exception, and the work most closely related to ours, is the work by Almquist et al., as we use their dataset. In their work,\n[27] Spects 2005 -decision on Manuscript Id Spects-05-nts-025 Reviewer: 1 Comments to the Author This paper have technical weak points.First the background and references are not adequate!The locality characteristics are found not only with characteristics of 2-tier architecture but also with 3-tier etc. Thus locality characteristics are examined not only in a temporal context but also in client temporal context particularly for web based streams... Secondly, authros do not evaluate through exaustive simulations the correllation bettween locality characteristics and self similar traffic for demand-driven caching schemes. However this paper has a positive point that it hosts many results...But I believe that these results do not figure out any significance... I recommend that authors should exploit circular explanations to concepts reaching readers without the topic's background.\n[28] provides both CPU-GPU with a definite cache space. On the other hand, the reuse cache is able to achieve within 32% data bus utilization for 4 of the 5 workload mixes. VI. CONCLUSION LLC Management is an important problem in today's heterogeneous processors. The reference stream observed by the shared-LLC (SLLC) from the CPU/GPU exhibits little temporal locality, but instead, it exhibits reuse locality. As a consequence, a high proportion of the SLLC lines is useless because the lines will not be requested again before being evicted, and most hits are expected to come from a small subset of already reused lines. This proportion could be particularly high for the GPU accesses. In this work, we looked to minimize counterproductive insertions and evictions in the SLLC by implementing a reuse cache. We evaluate our proposal by running heterogeneous CPU-GPU workloads on the gem5 AMD APU model. We use static partitioning and GPU LLC bypassing as our baseline comparison. Our major observations are: (i) Static-partitioning performs best when the GPU application working set fits into LLC and the CPU application is not cache-sensitive (ii) For the reuse cache, a performance of within 0.8% (or better when GPU application is cache-sensitive) was achievable for most cases, providing average 40% reduction in area (iii) A tag-to-data cache ratio of 2:1 (where data-array is half of a conventional cache) is a good starting point for reuse-cache design space exploration (iv) Simple LLC bypassing degrades performance for GPU in most cases, though it could improve CPU IPC. Static-partitioning hard-partitions the cache,\n[29] of the same length. Empirically, as prediction errors are dominantly smaller than the actual TTFT, CacheGen can almost always pick the right compression level to meet a given TTFT target. Related work Faster LLM serving: The landscape of LLM systems research is fast evolving. Besides speeding up LLM training (e.g.,,91), many recent efforts, including ours, focus on the LLM serving systems. Some try to parallelize the generation of multiple tokens with smaller approximating models,74 and accelerate the inference of Mix-of-Expert LLMs on edge devices. Others quantize LLM model weights (e.g.,), reduce memory I/O of GPU on-chip SRAM, and reduce self-attention computation complexity by k-means. New scheduling strategies are also proposed for LLM services,112. Nonetheless, a common approach to speeding up without modifying the LLMs is by caching KV features of repeatedly used input sequences, such as contexts,63,67,74,80,82,89,104,105,118. However, caching KV features results in a dramatically increased amount of cached data, which requires a bandwidth-efficient KV/context-loading system like CacheGen. Longer LLM input (context): Though the benefits of longer contexts are still under scrutiny,87,93, many recent efforts aim at enabling LLMs to accept ever longer contexts. The main challenge is to hold the attention matrices of more tokens without needing a bigger GPU memory. They enable longer input sequences by dynamically offloading less used parts of the attention matrices, using external knowledge via KNN, retraining the self-attention module to only attend to top-k keys to approximate the full attention,26, mapping long inputs into smaller latent space\n",
    "output": "## Related Works\n\nServing large language models (LLMs) has become increasingly important for cloud providers, with a growing focus on optimizing performance and efficiency. Caching intermediate results, particularly key-value (KV) pairs, has been identified as a crucial technique for improving serving throughput and latency [0][3]. However, the benefits of KV caching are highly dependent on system design decisions, such as cache eviction policies, which are workload-dependent [25].\n\nPrevious studies have explored various aspects of caching in LLM serving. Some have proposed novel cache management techniques, such as leveraging online knowledge distillation [1] or employing machine learning models to predict future KV cache usage [0]. Others have investigated the impact of different cache eviction policies on performance, including the use of least recently used (LRU) and first-in-first-out (FIFO) policies [22][25].\n\nCharacterization of KV workload patterns has also been a subject of research. Studies have analyzed the temporal locality of KV pairs and proposed techniques to improve cache efficiency [28][5]. However, these studies often rely on synthetic workloads or limited datasets, which may not accurately reflect real-world scenarios.\n\nRecent works have focused on optimizing LLM serving systems, including the use of multi-model queue management frameworks [3], dynamic cache instantiation [10], and reliability-aware KV cache compression [21]. These studies demonstrate the importance of understanding workload patterns and optimizing system design decisions to improve performance.\n\nOur work builds on these efforts, presenting a systematic characterization of KV workload patterns from a leading LLM service provider. Our analysis reveals new insights into the reuse patterns of KV pairs, including the importance of reuses between single-turn requests and the predictability of reuse patterns for specific request categories.\n\nNotably, recent studies have demonstrated the effectiveness of retrieval-augmented LMs (RAG) in outperforming fine-tuning approaches in tasks like MMLU and current events [20]. RAG's advantages include its ability to incorporate relevant context without suffering from catastrophic forgetting. Empirical results show that RAG can outperform fine-tuning by a significant margin, with some studies reporting a 30% reduction in hallucinations [2].\n\nFurthermore, efficient LLM serving systems have been proposed, such as ServeGen, which characterizes and generates realistic LLM serving workloads [58]. Other studies have explored the use of novel cache eviction policies, such as TinyLFU [60] and multi-step LRU [85], to improve cache efficiency.\n\nMoreover, recent research has proposed various techniques to optimize KV caching, such as STARC, a sparsity-optimized data mapping scheme for efficient LLM decoding on PIM architectures [88], and LeanKV, a framework that advances KV cache compression by exploiting differences in significance of various components within the KV cache [98]. Additionally, studies have shown that KV cache reusing can save both delay and cloud cost across a range of workloads with long context [94].\n\n## References\n\n[0] SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving \n[1] the quantity of LLM-annotated data on which the first student model is trained, focusing on the setup with retraining \n[3] One Queue Is All You Need: Resolving Head-of-Line Blocking in Large Language Model Serving \n[5] either cause premature eviction of a useful cache block, leading to an additional cache miss \n[10] Dynamic cache instantiation has the potential to provide significant cost reductions \n[20] Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs \n[21] No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization \n[22] It's Time to Revisit LRU vs. FIFO \n[25] in-memory key-value (KV) caches are widely used and discussed in industry and research communities \n[28] LLC Management is an important problem in today's heterogeneous processors\n[31] Engineering Trustworthy Software: A Mission for LLMs \n[32] Large Language Models (LLMs): Deployment, Tokenomics and Sustainability \n[33] Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression \n[34] ServeGen: Workload Characterization and Generation of Large Language Model Serving in Production \n[35] Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs \n[36] Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference \n[37] Activation Approximations Can Incur Safety Vulnerabilities Even in Aligned LLMs: Comprehensive Analysis and Defense \n[38] CEC-Zero: Chinese Error Correction Solution Based on LLM \n[39] Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads on Consumer-Grade Devices \n[40] NanoFlow: Towards Optimal Large Language Model Serving Throughput \n[41] On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference \n[42] In-context KV-Cache Eviction for LLMs via Attention-Gate \n[43] LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization \n[44] The MoE-Empowered Edge LLMs Deployment: Architecture, Challenges, and Opportunities \n[45] ScaleLLM: A Resource-Frugal LLM Serving Framework by Optimizing End-to-End Efficiency \n[58] ServeGen: Workload Characterization and Generation of Large Language Model Serving in Production\n[60] TinyLFU: A Highly Efficient Cache Admission Policy \n[85] Multi-step LRU: SIMD-based Cache Replacement for Lower Overhead and Higher Precision\n[88] Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM \n[94] Towards More Economical Context-Augmented LLM Generation by Reusing Stored KV Cache \n[98] Unifying KV Cache Compression for Large Language Models with LeanKV",
    "initial_result": "## Related Works\n\nServing large language models (LLMs) has become increasingly important for cloud providers, with a growing focus on optimizing performance and efficiency. Caching intermediate results, particularly key-value (KV) pairs, has been identified as a crucial technique for improving serving throughput and latency [0][3]. However, the benefits of KV caching are highly dependent on system design decisions, such as cache eviction policies, which are workload-dependent [25].\n\nPrevious studies have explored various aspects of caching in LLM serving. Some have proposed novel cache management techniques, such as leveraging online knowledge distillation [1] or employing machine learning models to predict future KV cache usage [0]. Others have investigated the impact of different cache eviction policies on performance, including the use of least recently used (LRU) and first-in-first-out (FIFO) policies [22][25].\n\nCharacterization of KV workload patterns has also been a subject of research. Studies have analyzed the temporal locality of KV pairs and proposed techniques to improve cache efficiency [28][5]. However, these studies often rely on synthetic workloads or limited datasets, which may not accurately reflect real-world scenarios.\n\nRecent works have focused on optimizing LLM serving systems, including the use of multi-model queue management frameworks [3], dynamic cache instantiation [10], and reliability-aware KV cache compression [21]. These studies demonstrate the importance of understanding workload patterns and optimizing system design decisions to improve performance.\n\nOur work builds on these efforts, presenting a systematic characterization of KV workload patterns from a leading LLM service provider. Our analysis reveals new insights into the reuse patterns of KV pairs, including the importance of reuses between single-turn requests and the predictability of reuse patterns for specific request categories.\n\n## References\n\n[0] SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving \n[1] the quantity of LLM-annotated data on which the first student model is trained, focusing on the setup with retraining \n[3] One Queue Is All You Need: Resolving Head-of-Line Blocking in Large Language Model Serving \n[5] either cause premature eviction of a useful cache block, leading to an additional cache miss \n[10] Dynamic cache instantiation has the potential to provide significant cost reductions \n[21] No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization \n[22] It's Time to Revisit LRU vs. FIFO \n[25] in-memory key-value (KV) caches are widely used and discussed in industry and research communities \n[28] LLC Management is an important problem in today's heterogeneous processors",
    "feedbacks": [
        [
            "The Related Works section primarily focuses on caching techniques and KV workload patterns but lacks a detailed discussion on the specific challenges of serving large language models (LLMs) and the impact of cache eviction policies on real-world traces.",
            "What are the most significant challenges in serving LLMs, and how do cache eviction policies affect performance in real-world scenarios?"
        ],
        [
            "The answer could benefit from more specific results and comparisons, especially regarding the performance improvements achieved by different cache management techniques and eviction policies in LLM serving.",
            "What are the performance comparisons between different cache eviction policies (e.g., LRU, FIFO) and more advanced techniques in LLM serving?"
        ],
        [
            "The section mentions several related works but does not elaborate on how these works' findings relate to or inform the characterization of KV workload patterns and the proposal of workload-aware cache eviction policies in the paper.",
            "How do the findings from previous studies on KV workload patterns and cache eviction policies support or contrast with the observations and proposals presented in this paper?"
        ],
        [
            "The organization of the Related Works section is clear, but it could be improved by grouping works into categories (e.g., cache management techniques, characterization of KV workload patterns, optimization of LLM serving systems) and providing transitional phrases to enhance flow.",
            ""
        ],
        [
            "While the section mentions several relevant works, it could be strengthened by including more recent studies or those that specifically focus on real-world traces and workload-aware optimizations.",
            "What recent studies (published within the last few years) have investigated workload-aware cache eviction policies or optimizations for LLM serving using real-world traces?"
        ],
        [
            "The section could benefit from a concluding paragraph that summarizes the key findings from related works and highlights the novel contributions of the paper.",
            ""
        ]
    ],
    "edited_answer_0": "## Related Works\n\nServing large language models (LLMs) has become increasingly important for cloud providers, with a growing focus on optimizing performance and efficiency. Caching intermediate results, particularly key-value (KV) pairs, has been identified as a crucial technique for improving serving throughput and latency [0][3]. However, the benefits of KV caching are highly dependent on system design decisions, such as cache eviction policies, which are workload-dependent [25].\n\nPrevious studies have explored various aspects of caching in LLM serving. Some have proposed novel cache management techniques, such as leveraging online knowledge distillation [1] or employing machine learning models to predict future KV cache usage [0]. Others have investigated the impact of different cache eviction policies on performance, including the use of least recently used (LRU) and first-in-first-out (FIFO) policies [22][25].\n\nCharacterization of KV workload patterns has also been a subject of research. Studies have analyzed the temporal locality of KV pairs and proposed techniques to improve cache efficiency [28][5]. However, these studies often rely on synthetic workloads or limited datasets, which may not accurately reflect real-world scenarios.\n\nRecent works have focused on optimizing LLM serving systems, including the use of multi-model queue management frameworks [3], dynamic cache instantiation [10], and reliability-aware KV cache compression [21]. These studies demonstrate the importance of understanding workload patterns and optimizing system design decisions to improve performance.\n\nMoreover, serving LLMs poses significant challenges, including accuracy, scalability, bias, and explainability [31]. The deployment of LLMs also raises concerns about sustainability, environmental impact, and cost [32]. Efficient serving of LLMs requires optimizing end-to-end efficiency, including local inference and communication [45].\n\nOur work builds on these efforts, presenting a systematic characterization of KV workload patterns from a leading LLM service provider. Our analysis reveals new insights into the reuse patterns of KV pairs, including the importance of reuses between single-turn requests and the predictability of reuse patterns for specific request categories.\n\n## References\n\n[0] SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving \n[1] the quantity of LLM-annotated data on which the first student model is trained, focusing on the setup with retraining \n[3] One Queue Is All You Need: Resolving Head-of-Line Blocking in Large Language Model Serving \n[5] either cause premature eviction of a useful cache block, leading to an additional cache miss \n[10] Dynamic cache instantiation has the potential to provide significant cost reductions \n[21] No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization \n[22] It's Time to Revisit LRU vs. FIFO \n[25] in-memory key-value (KV) caches are widely used and discussed in industry and research communities \n[28] LLC Management is an important problem in today's heterogeneous processors\n[31] Engineering Trustworthy Software: A Mission for LLMs \n[32] Large Language Models (LLMs): Deployment, Tokenomics and Sustainability \n[33] Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression \n[34] ServeGen: Workload Characterization and Generation of Large Language Model Serving in Production \n[35] Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs \n[36] Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference \n[37] Activation Approximations Can Incur Safety Vulnerabilities Even in Aligned LLMs: Comprehensive Analysis and Defense \n[38] CEC-Zero: Chinese Error Correction Solution Based on LLM \n[39] Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads on Consumer-Grade Devices \n[40] NanoFlow: Towards Optimal Large Language Model Serving Throughput \n[41] On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference \n[42] In-context KV-Cache Eviction for LLMs via Attention-Gate \n[43] LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization \n[44] The MoE-Empowered Edge LLMs Deployment: Architecture, Challenges, and Opportunities \n[45] ScaleLLM: A Resource-Frugal LLM Serving Framework by Optimizing End-to-End Efficiency \n[46] GenTorrent: Scaling Large Language Model Serving with An Overley Network \n[47] Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities \n[48] BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference \n[49] Learned Best-Effort LLM Serving \n[50] BackCache: Mitigating Contention-Based Cache Timing Attacks by Hiding Cache Line Evictions \n[51] Revisiting SLO and Goodput Metrics in LLM Serving \n[52] Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction \n[53] ENOVA: Autoscaling towards Cost-effective and Stable Serverless LLM Serving \n[54] ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs \n[55] LLM Inference Serving: Survey of Recent Advances and Opportunities \n[56] Time Aware Least Recent Used (TLRU) Cache Management Policy in ICN \n[57] PopNetCod: A Popularity-based Caching Policy for Network Coding enabled Named Data Networking",
    "edited_answer_1": "## Related Works\n\nServing large language models (LLMs) has become increasingly important for cloud providers, with a growing focus on optimizing performance and efficiency. Caching intermediate results, particularly key-value (KV) pairs, has been identified as a crucial technique for improving serving throughput and latency [0][3]. However, the benefits of KV caching are highly dependent on system design decisions, such as cache eviction policies, which are workload-dependent [25].\n\nPrevious studies have explored various aspects of caching in LLM serving. Some have proposed novel cache management techniques, such as leveraging online knowledge distillation [1] or employing machine learning models to predict future KV cache usage [0]. Others have investigated the impact of different cache eviction policies on performance, including the use of least recently used (LRU) and first-in-first-out (FIFO) policies [22][25].\n\nCharacterization of KV workload patterns has also been a subject of research. Studies have analyzed the temporal locality of KV pairs and proposed techniques to improve cache efficiency [28][5]. However, these studies often rely on synthetic workloads or limited datasets, which may not accurately reflect real-world scenarios.\n\nRecent works have focused on optimizing LLM serving systems, including the use of multi-model queue management frameworks [3], dynamic cache instantiation [10], and reliability-aware KV cache compression [21]. These studies demonstrate the importance of understanding workload patterns and optimizing system design decisions to improve performance.\n\nMoreover, serving LLMs poses significant challenges, including accuracy, scalability, bias, and explainability [31]. The deployment of LLMs also raises concerns about sustainability, environmental impact, and cost [32]. Efficient serving of LLMs requires optimizing end-to-end efficiency, including local inference and communication [45].\n\nOur work builds on these efforts, presenting a systematic characterization of KV workload patterns from a leading LLM service provider. Our analysis reveals new insights into the reuse patterns of KV pairs, including the importance of reuses between single-turn requests and the predictability of reuse patterns for specific request categories.\n\nNotably, recent studies have demonstrated the effectiveness of retrieval-augmented LMs (RAG) in outperforming fine-tuning approaches in tasks like MMLU and current events [20]. RAG's advantages include its ability to incorporate relevant context without suffering from catastrophic forgetting. Empirical results show that RAG can outperform fine-tuning by a significant margin, with some studies reporting a 30% reduction in hallucinations [2].\n\nFurthermore, efficient LLM serving systems have been proposed, such as ServeGen, which characterizes and generates realistic LLM serving workloads [58]. Other studies have explored the use of novel cache eviction policies, such as TinyLFU [60] and multi-step LRU [85], to improve cache efficiency.\n\n## References\n\n[0] SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving \n[1] the quantity of LLM-annotated data on which the first student model is trained, focusing on the setup with retraining \n[3] One Queue Is All You Need: Resolving Head-of-Line Blocking in Large Language Model Serving \n[5] either cause premature eviction of a useful cache block, leading to an additional cache miss \n[10] Dynamic cache instantiation has the potential to provide significant cost reductions \n[20] Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs \n[21] No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization \n[22] It's Time to Revisit LRU vs. FIFO \n[25] in-memory key-value (KV) caches are widely used and discussed in industry and research communities \n[28] LLC Management is an important problem in today's heterogeneous processors\n[31] Engineering Trustworthy Software: A Mission for LLMs \n[32] Large Language Models (LLMs): Deployment, Tokenomics and Sustainability \n[33] Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression \n[34] ServeGen: Workload Characterization and Generation of Large Language Model Serving in Production \n[35] Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs \n[36] Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference \n[37] Activation Approximations Can Incur Safety Vulnerabilities Even in Aligned LLMs: Comprehensive Analysis and Defense \n[38] CEC-Zero: Chinese Error Correction Solution Based on LLM \n[39] Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads on Consumer-Grade Devices \n[40] NanoFlow: Towards Optimal Large Language Model Serving Throughput \n[41] On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference \n[42] In-context KV-Cache Eviction for LLMs via Attention-Gate \n[43] LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization \n[44] The MoE-Empowered Edge LLMs Deployment: Architecture, Challenges, and Opportunities \n[45] ScaleLLM: A Resource-Frugal LLM Serving Framework by Optimizing End-to-End Efficiency \n[58] ServeGen: Workload Characterization and Generation of Large Language Model Serving in Production\n[60] TinyLFU: A Highly Efficient Cache Admission Policy \n[85] Multi-step LRU: SIMD-based Cache Replacement for Lower Overhead and Higher Precision",
    "edited_answer_2": "## Related Works\n\nServing large language models (LLMs) has become increasingly important for cloud providers, with a growing focus on optimizing performance and efficiency. Caching intermediate results, particularly key-value (KV) pairs, has been identified as a crucial technique for improving serving throughput and latency [0][3]. However, the benefits of KV caching are highly dependent on system design decisions, such as cache eviction policies, which are workload-dependent [25].\n\nPrevious studies have explored various aspects of caching in LLM serving. Some have proposed novel cache management techniques, such as leveraging online knowledge distillation [1] or employing machine learning models to predict future KV cache usage [0]. Others have investigated the impact of different cache eviction policies on performance, including the use of least recently used (LRU) and first-in-first-out (FIFO) policies [22][25].\n\nCharacterization of KV workload patterns has also been a subject of research. Studies have analyzed the temporal locality of KV pairs and proposed techniques to improve cache efficiency [28][5]. However, these studies often rely on synthetic workloads or limited datasets, which may not accurately reflect real-world scenarios.\n\nRecent works have focused on optimizing LLM serving systems, including the use of multi-model queue management frameworks [3], dynamic cache instantiation [10], and reliability-aware KV cache compression [21]. These studies demonstrate the importance of understanding workload patterns and optimizing system design decisions to improve performance.\n\nOur work builds on these efforts, presenting a systematic characterization of KV workload patterns from a leading LLM service provider. Our analysis reveals new insights into the reuse patterns of KV pairs, including the importance of reuses between single-turn requests and the predictability of reuse patterns for specific request categories.\n\nNotably, recent studies have demonstrated the effectiveness of retrieval-augmented LMs (RAG) in outperforming fine-tuning approaches in tasks like MMLU and current events [20]. RAG's advantages include its ability to incorporate relevant context without suffering from catastrophic forgetting. Empirical results show that RAG can outperform fine-tuning by a significant margin, with some studies reporting a 30% reduction in hallucinations [2].\n\nFurthermore, efficient LLM serving systems have been proposed, such as ServeGen, which characterizes and generates realistic LLM serving workloads [58]. Other studies have explored the use of novel cache eviction policies, such as TinyLFU [60] and multi-step LRU [85], to improve cache efficiency.\n\nMoreover, recent research has proposed various techniques to optimize KV caching, such as STARC, a sparsity-optimized data mapping scheme for efficient LLM decoding on PIM architectures [88], and LeanKV, a framework that advances KV cache compression by exploiting differences in significance of various components within the KV cache [98]. Additionally, studies have shown that KV cache reusing can save both delay and cloud cost across a range of workloads with long context [94].\n\n## References\n\n[0] SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving \n[1] the quantity of LLM-annotated data on which the first student model is trained, focusing on the setup with retraining \n[3] One Queue Is All You Need: Resolving Head-of-Line Blocking in Large Language Model Serving \n[5] either cause premature eviction of a useful cache block, leading to an additional cache miss \n[10] Dynamic cache instantiation has the potential to provide significant cost reductions \n[20] Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs \n[21] No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization \n[22] It's Time to Revisit LRU vs. FIFO \n[25] in-memory key-value (KV) caches are widely used and discussed in industry and research communities \n[28] LLC Management is an important problem in today's heterogeneous processors\n[31] Engineering Trustworthy Software: A Mission for LLMs \n[32] Large Language Models (LLMs): Deployment, Tokenomics and Sustainability \n[33] Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression \n[34] ServeGen: Workload Characterization and Generation of Large Language Model Serving in Production \n[35] Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs \n[36] Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference \n[37] Activation Approximations Can Incur Safety Vulnerabilities Even in Aligned LLMs: Comprehensive Analysis and Defense \n[38] CEC-Zero: Chinese Error Correction Solution Based on LLM \n[39] Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads on Consumer-Grade Devices \n[40] NanoFlow: Towards Optimal Large Language Model Serving Throughput \n[41] On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference \n[42] In-context KV-Cache Eviction for LLMs via Attention-Gate \n[43] LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization \n[44] The MoE-Empowered Edge LLMs Deployment: Architecture, Challenges, and Opportunities \n[45] ScaleLLM: A Resource-Frugal LLM Serving Framework by Optimizing End-to-End Efficiency \n[58] ServeGen: Workload Characterization and Generation of Large Language Model Serving in Production\n[60] TinyLFU: A Highly Efficient Cache Admission Policy \n[85] Multi-step LRU: SIMD-based Cache Replacement for Lower Overhead and Higher Precision\n[88] Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM \n[94] Towards More Economical Context-Augmented LLM Generation by Reusing Stored KV Cache \n[98] Unifying KV Cache Compression for Large Language Models with LeanKV",
    "cost": {
        "virtual cost": 0.0,
        "virtual tokens": 760525,
        "physical cost": 0.0,
        "physical tokens": 760525,
        "cache hits": 0,
        "operator cache hits": 0
    }
}