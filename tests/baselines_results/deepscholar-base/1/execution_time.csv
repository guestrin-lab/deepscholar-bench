e27ee484-dbe1-497c-9fa6-c80b67849c64, generate_queries, 0.4415712356567383, {'generate_queries': 2}
e27ee484-dbe1-497c-9fa6-c80b67849c64, lotus_search_async, 22.563138723373413, {'multiquery_search': 91}
e27ee484-dbe1-497c-9fa6-c80b67849c64, multiquery_web_search, 23.011110305786133, {'step': 'Generated web queries and got web search results', 'extra_info': {'source': 'web', 'num_queries': 2, 'num_docs': 91}, 'message': 'Generated 2 queries and got 91 results'}
e27ee484-dbe1-497c-9fa6-c80b67849c64, generate_background, 10.799803018569946, {'step': 'Generated summary of web search results', 'extra_info': {'num_papers_to_summarize': 91}, 'message': 'Generated summary of 91 results'}
e27ee484-dbe1-497c-9fa6-c80b67849c64, generate_queries, 0.5089402198791504, {'generate_queries': 2}
e27ee484-dbe1-497c-9fa6-c80b67849c64, lotus_search_async, 22.48970079421997, {'multiquery_search': 91}
e27ee484-dbe1-497c-9fa6-c80b67849c64, multiquery_arxiv_search, 23.004240036010742, {'step': 'Generated arxiv queries and got arxiv search results', 'extra_info': {'source': ['arxiv'], 'num_queries': 2, 'num_docs': 91}, 'message': 'Generated 2 queries and got 91 results'}
e27ee484-dbe1-497c-9fa6-c80b67849c64, sem_filter, 49.40459132194519, {'papers_after_sem_filter': 45}
e27ee484-dbe1-497c-9fa6-c80b67849c64, sem_topk, 216.30864572525024, {'papers_after_sem_topk': 30}
e27ee484-dbe1-497c-9fa6-c80b67849c64, filter_docs, 265.7402093410492, {'step': 'Filtered arxiv search results', 'extra_info': {'num_filtered_papers': 30}, 'message': 'Filtered search results to 30 documents'}
e27ee484-dbe1-497c-9fa6-c80b67849c64, generate_insights, 7.596837520599365, {'step': 'Generated insights from papers', 'extra_info': {'num_papers_for_insight': 30}, 'message': 'Derived insights from 30 results'}
e27ee484-dbe1-497c-9fa6-c80b67849c64, cluster_df, 0.5530836582183838, {'cluster_sizes': {5: 4, 4: 4, 3: 4, 2: 3, 0: 3, 8: 3, 9: 2, 1: 2, 6: 2, 7: 1, 11: 1, 10: 1}}
e27ee484-dbe1-497c-9fa6-c80b67849c64, select_representatives, 0.02633500099182129, {'num_representatives': [1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1]}
e27ee484-dbe1-497c-9fa6-c80b67849c64, create_categories, 1.5535266399383545, {'num_categories': [10]}
e27ee484-dbe1-497c-9fa6-c80b67849c64, generate_categories, 2.1428232192993164, {'step': 'Generated categories', 'extra_info': {'num_categories': 10}, 'message': 'Identified 10 categories'}
e27ee484-dbe1-497c-9fa6-c80b67849c64, map_papers_to_categories, 1.1740736961364746, {'step': 'Mapped papers to categories', 'extra_info': {'all_categories': 7, 'num_categories': 7, 'num_docs': 30}, 'message': 'Mapped 30 papers to 7 categories'}
e27ee484-dbe1-497c-9fa6-c80b67849c64, generate_category_summaries, 6.143033742904663, {'step': 'Generated category summaries', 'extra_info': {'num_categories': 7}, 'message': 'Generated summaries for 7 categories'}
e27ee484-dbe1-497c-9fa6-c80b67849c64, gen_background, 14.678983211517334, {'step': 'Generated background section', 'extra_info': {}, 'message': 'Generated background section'}
e27ee484-dbe1-497c-9fa6-c80b67849c64, generate_final_report, 0.19444012641906738, {'step': 'Generated final report', 'extra_info': {}, 'message': 'Generated final report'}
e27ee484-dbe1-497c-9fa6-c80b67849c64, v1_pipeline, 355.2126362323761, {'step': 'v1 Pipeline Executed', 'extra_info': {'id': 'e27ee484-dbe1-497c-9fa6-c80b67849c64', 'topic': "\nYour task is to write a Related Works section for an academic paper given the paper's abstract. Your response should provide the Related Works section and references. Only include references that are published before 2025-06-03T08:51:38+00:00. Mention them in a separate reference list at the end and cite them properly in the Related Works section. Here is the paper abstract:\nServing large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity..\n\nNote that today's date is 06/09/25.", 'date': '06/09/25', 'intro_section': '## Related Works\n\n**Efficient serving of large lan', 'num_papers': 30, 'num_categories': 7, 'query': "\nYour task is to write a Related Works section for an academic paper given the paper's abstract. Your response should provide the Related Works section and references. Only include references that are published before 2025-06-03T08:51:38+00:00. Mention them in a separate reference list at the end and cite them properly in the Related Works section. Here is the paper abstract:\nServing large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity..\n"}, 'message': "Generated report from 30 papers for \nYour task is to write a Related Works section for an academic paper given the paper's abstract. Your response should provide the Related Works section and references. Only include references that are published before 2025-06-03T08:51:38+00:00. Mention them in a separate reference list at the end and cite them properly in the Related Works section. Here is the paper abstract:\nServing large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity..\n"}
