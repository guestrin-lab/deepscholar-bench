{
    "type": "final",
    "id": "e27ee484-dbe1-497c-9fa6-c80b67849c64",
    "title": "Related Works",
    "query": "\nYour task is to write a Related Works section for an academic paper given the paper's abstract. Your response should provide the Related Works section and references. Only include references that are published before 2025-06-03T08:51:38+00:00. Mention them in a separate reference list at the end and cite them properly in the Related Works section. Here is the paper abstract:\nServing large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity..\n",
    "description": "Efficient serving of large language models (LLMs) relies heavily on effective key-value (KV) caching mechanisms",
    "introduction": "## Related Works\n\n**Efficient serving of large language models (LLMs) relies heavily on effective key-value (KV) caching mechanisms**. Caching intermediate results after processing each request substantially improves serving throughput and latency \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. However, the benefits of KV caching are highly dependent on system design decisions, such as cache eviction policies, which are workload-dependent \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\].\n\nSeveral studies have proposed various KV cache management strategies, including token-level, model-level, and system-level optimizations \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. Model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\].\n\nSome notable works include PagedAttention \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\], which achieves near-zero waste in KV cache memory and flexible sharing of KV cache within and across requests. vLLM, a system built on top of PagedAttention, improves the throughput of popular LLMs by 2-4\u00d7 with the same level of latency compared to state-of-the-art systems \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Other works, such as Mixed-precision KV cache (MiKV) \\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\\] and QAQ \\[[Shichen Dong' 2024-03-07](http://arxiv.org/abs/2403.04643v2)\\], propose reliable cache compression methods that preserve context details and ensure generation quality.\n\nRecent studies have also explored the reuse of KV caches across different requests and conversations \\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\]\\[[Bin Gao' 2024-03-23](http://arxiv.org/abs/2403.19708v3)\\]. CachedAttention \\[[Bin Gao' 2024-03-23](http://arxiv.org/abs/2403.19708v3)\\] enables reuse of KV caches across multi-turn conversations, significantly reducing repetitive computation overheads. Similarly, EFIM \\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\] proposes a transformed prompt format to unleash the performance potential of KV cache reuse in infilling tasks.\n\nCharacterization of KV workload patterns has also been studied \\[[Hanshi Sun' 2024-10-28](http://arxiv.org/abs/2410.21465v3)\\]\\[[Cunchen Hu' 2024-06-25](http://arxiv.org/abs/2406.17565v3)\\]. MemServe \\[[Cunchen Hu' 2024-06-25](http://arxiv.org/abs/2406.17565v3)\\] presents a unified system that integrates inter-request and intra-request optimizations, introducing an elastic memory pool managing distributed memory and KV caches.\n\nOur work builds upon these studies, providing a systematic characterization of KV workload patterns from a leading LLM service provider. We draw observations that can inform the design of workload-aware cache eviction policies, which improve serving performance under real-world traces, especially with limited cache capacity.\n\n## References\n\n\\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\] Efficient Memory Management for Large Language Model Serving with PagedAttention\n\\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\] A Survey on Large Language Model Acceleration based on KV Cache Management\n\\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\] No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization\n\\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\\] Towards More Economical Context-Augmented LLM Generation by Reusing Stored KV Cache\n\\[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\\] Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing\n\\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\] Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV Cache Management\n\\[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\\] KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction\n\\[[Yihua Cheng' 2024-09-16](http://arxiv.org/abs/2409.13761v2)\\] Do Large Language Models Need a Content Delivery Network?\n\\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\] EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse\n\\[[Shichen Dong' 2024-03-07](http://arxiv.org/abs/2403.04643v2)\\] QAQ: Quality Adaptive Quantization for LLM KV Cache\n\\[[Yixuan Wang' 2025-05-24](http://arxiv.org/abs/2505.20334v1)\\] Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query\n\\[[Jingbo Yang' 2025-02-21](http://arxiv.org/abs/2502.16002v2)\\] KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse\n\\[[Ke Hong' 2025-04-28](http://arxiv.org/abs/2504.19867v1)\\] semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage\n\\[[Shiwei Gao' 2024-10-07](http://arxiv.org/abs/2410.05004v1)\\] Fast State Restoration in LLM Serving with HCache\n\\[[Peilin Chen' 2025-05-23](http://arxiv.org/abs/2505.17787v1)\\] Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM Acceleration\n\\[[Shihong Gao' 2025-04-10](http://arxiv.org/abs/2504.07494v1)\\] Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving\n\\[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\\] On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference\n\\[[Ahmed Burak Gulhan' 2025-02-18](http://arxiv.org/abs/2502.13176v2)\\] BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference\n\\[[Hang Zhang' 2025-04-19](http://arxiv.org/abs/2505.03756v1)\\] Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management\n\\[[Hanshi Sun' 2024-10-28](http://arxiv.org/abs/2410.21465v3)\\] ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference\n\\[[Jincheng Dai' 2024-04-24](http://arxiv.org/abs/2404.15949v2)\\] CORM: Cache Optimization with Recent Message for Large Language Model Inference\n\\[[Amir Zandieh' 2024-06-05](http://arxiv.org/abs/2406.03482v2)\\] QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead\n\\[[Jing Xiong' 2024-10-04](http://arxiv.org/abs/2410.03090v1)\\] UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference\n\\[[Jiayi Yao' 2024-05-26](http://arxiv.org/abs/2405.16444v3)\\] CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion\n\\[[Shibo Jie' 2025-03-20](http://arxiv.org/abs/2503.16163v1)\\] SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs\n\\[[Bin Gao' 2024-03-23](http://arxiv.org/abs/2403.19708v3)\\] Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention\n\\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\] Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference\n\\[[Cunchen Hu' 2024-06-25](http://arxiv.org/abs/2406.17565v3)\\] MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool\n\\[[Yilong Chen' 2024-08-07](http://arxiv.org/abs/2408.03675v2)\\] NACL: A General and Effective KV Cache Eviction Framework for LLMs at Inference Time\n",
    "overview": ". Caching intermediate results after processing each request substantially improves serving throughput and latency \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. However, the benefits of KV caching are highly dependent on system design decisions, such as cache eviction policies, which are workload-dependent \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\].\n\nSeveral studies have proposed various KV cache management strategies, including token-level, model-level, and system-level optimizations \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. Model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\].\n\nSome notable works include PagedAttention \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\], which achieves near-zero waste in KV cache memory and flexible sharing of KV cache within and across requests. vLLM, a system built on top of PagedAttention, improves the throughput of popular LLMs by 2-4\u00d7 with the same level of latency compared to state-of-the-art systems \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Other works, such as Mixed-precision KV cache (MiKV) \\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\\] and QAQ \\[[Shichen Dong' 2024-03-07](http://arxiv.org/abs/2403.04643v2)\\], propose reliable cache compression methods that preserve context details and ensure generation quality.\n\nRecent studies have also explored the reuse of KV caches across different requests and conversations \\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\]\\[[Bin Gao' 2024-03-23](http://arxiv.org/abs/2403.19708v3)\\]. CachedAttention \\[[Bin Gao' 2024-03-23](http://arxiv.org/abs/2403.19708v3)\\] enables reuse of KV caches across multi-turn conversations, significantly reducing repetitive computation overheads. Similarly, EFIM \\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\] proposes a transformed prompt format to unleash the performance potential of KV cache reuse in infilling tasks.\n\nCharacterization of KV workload patterns has also been studied \\[[Hanshi Sun' 2024-10-28](http://arxiv.org/abs/2410.21465v3)\\]\\[[Cunchen Hu' 2024-06-25](http://arxiv.org/abs/2406.17565v3)\\]. MemServe \\[[Cunchen Hu' 2024-06-25](http://arxiv.org/abs/2406.17565v3)\\] presents a unified system that integrates inter-request and intra-request optimizations, introducing an elastic memory pool managing distributed memory and KV caches.\n\nOur work builds upon these studies, providing a systematic characterization of KV workload patterns from a leading LLM service provider. We draw observations that can inform the design of workload-aware cache eviction policies, which improve serving performance under real-world traces, especially with limited cache capacity.\n\n## References\n\n\\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\] Efficient Memory Management for Large Language Model Serving with PagedAttention\n\\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\] A Survey on Large Language Model Acceleration based on KV Cache Management\n\\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\] No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization\n\\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\\] Towards More Economical Context-Augmented LLM Generation by Reusing Stored KV Cache\n\\[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\\] Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing\n\\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\] Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV Cache Management\n\\[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\\] KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction\n\\[[Yihua Cheng' 2024-09-16](http://arxiv.org/abs/2409.13761v2)\\] Do Large Language Models Need a Content Delivery Network?\n\\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\] EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse\n\\[[Shichen Dong' 2024-03-07](http://arxiv.org/abs/2403.04643v2)\\] QAQ: Quality Adaptive Quantization for LLM KV Cache\n\\[[Yixuan Wang' 2025-05-24](http://arxiv.org/abs/2505.20334v1)\\] Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query\n\\[[Jingbo Yang' 2025-02-21](http://arxiv.org/abs/2502.16002v2)\\] KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse\n\\[[Ke Hong' 2025-04-28](http://arxiv.org/abs/2504.19867v1)\\] semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage\n\\[[Shiwei Gao' 2024-10-07](http://arxiv.org/abs/2410.05004v1)\\] Fast State Restoration in LLM Serving with HCache\n\\[[Peilin Chen' 2025-05-23](http://arxiv.org/abs/2505.17787v1)\\] Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM Acceleration\n\\[[Shihong Gao' 2025-04-10](http://arxiv.org/abs/2504.07494v1)\\] Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving\n\\[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\\] On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference\n\\[[Ahmed Burak Gulhan' 2025-02-18](http://arxiv.org/abs/2502.13176v2)\\] BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference\n\\[[Hang Zhang' 2025-04-19](http://arxiv.org/abs/2505.03756v1)\\] Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management\n\\[[Hanshi Sun' 2024-10-28](http://arxiv.org/abs/2410.21465v3)\\] ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference\n\\[[Jincheng Dai' 2024-04-24](http://arxiv.org/abs/2404.15949v2)\\] CORM: Cache Optimization with Recent Message for Large Language Model Inference\n\\[[Amir Zandieh' 2024-06-05](http://arxiv.org/abs/2406.03482v2)\\] QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead\n\\[[Jing Xiong' 2024-10-04](http://arxiv.org/abs/2410.03090v1)\\] UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference\n\\[[Jiayi Yao' 2024-05-26](http://arxiv.org/abs/2405.16444v3)\\] CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion\n\\[[Shibo Jie' 2025-03-20](http://arxiv.org/abs/2503.16163v1)\\] SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs\n\\[[Bin Gao' 2024-03-23](http://arxiv.org/abs/2403.19708v3)\\] Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention\n\\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\] Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference\n\\[[Cunchen Hu' 2024-06-25](http://arxiv.org/abs/2406.17565v3)\\] MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool\n\\[[Yilong Chen' 2024-08-07](http://arxiv.org/abs/2408.03675v2)\\] NACL: A General and Effective KV Cache Eviction Framework for LLMs at Inference Time\n",
    "researchGroups": [
        {
            "title": "Large Language Model Serving and Inference Optimization",
            "count": 18,
            "summary": "Large language model (LLM) serving and inference optimization have gained significant attention in recent years. Efficient serving of LLMs requires optimizing key-value (KV) cache memory, which can be huge and dynamic, leading to waste and fragmentation \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Various approaches have been proposed to address this issue, including PagedAttention, which inspires an attention algorithm from virtual memory and paging techniques \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Other works, such as vLLM, propose near-zero waste in KV cache memory and flexible sharing within and across requests \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Additionally, techniques like KV cache reusing \\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\\], MorphServe \\[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\\], MELL \\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\], and Knowledge Delivery Network (KDN) \\[[Yihua Cheng' 2024-09-16](http://arxiv.org/abs/2409.13761v2)\\] have been proposed to optimize LLM serving. These include optimizing cache eviction policies \\[[Xuanfan Ni' 2025-02-24](http://arxiv.org/abs/2502.16886v1)\\], attention mechanisms \\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\], and memory management \\[[Shiwei Gao' 2024-10-07](http://arxiv.org/abs/2410.05004v1)\\]. Some works also focus on multi-GPU KV cache management \\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\], infilling tasks \\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\], and long-context LLMs \\[[Hanshi Sun' 2024-10-28](http://arxiv.org/abs/2410.21465v3)\\]. Furthermore, methods like CacheBlend \\[[Jiayi Yao' 2024-05-26](http://arxiv.org/abs/2405.16444v3)\\] and SpeCache \\[[Shibo Jie' 2025-03-20](http://arxiv.org/abs/2503.16163v1)\\] aim to improve cache efficiency. These studies demonstrate the importance of optimizing KV cache in LLM serving, with improvements in throughput, latency, and memory usage \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]\\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\\]\\[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\\]\\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\]\\[[Yihua Cheng' 2024-09-16](http://arxiv.org/abs/2409.13761v2)\\]\\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\].",
            "description": "Large language model (LLM) serving and inference optimization have gained significant attention in recent years. Efficient serving of LLMs requires optimizing key-value (KV) cache memory, which can be huge and dynamic, leading to waste and fragmentation \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Various approaches have been proposed to address this issue, including PagedAttention, which inspires an attention algorithm from virtual memory and paging techniques \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Other works, such as vLLM, propose near-zero waste in KV cache memory and flexible sharing within and across requests \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Additionally, techniques like KV cache reusing \\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\\], MorphServe \\[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\\], MELL \\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\], and Knowledge Delivery Network (KDN) \\[[Yihua Cheng' 2024-09-16](http://arxiv.org/abs/2409.13761v2)\\] have been proposed to optimize LLM serving. These include optimizing cache eviction policies \\[[Xuanfan Ni' 2025-02-24](http://arxiv.org/abs/2502.16886v1)\\], attention mechanisms \\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\], and memory management \\[[Shiwei Gao' 2024-10-07](http://arxiv.org/abs/2410.05004v1)\\]. Some works also focus on multi-GPU KV cache management \\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\], infilling tasks \\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\], and long-context LLMs \\[[Hanshi Sun' 2024-10-28](http://arxiv.org/abs/2410.21465v3)\\]. Furthermore, methods like CacheBlend \\[[Jiayi Yao' 2024-05-26](http://arxiv.org/abs/2405.16444v3)\\] and SpeCache \\[[Shibo Jie' 2025-03-20](http://arxiv.org/abs/2503.16163v1)\\] aim to improve cache efficiency. These studies demonstrate the importance of optimizing KV cache in LLM serving, with improvements in throughput, latency, and memory usage \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]\\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\\]\\[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\\]\\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\]\\[[Yihua Cheng' 2024-09-16](http://arxiv.org/abs/2409.13761v2)\\]\\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\].",
            "papers": [
                {
                    "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
                    "authors": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica",
                    "date": "2023-09-12 12:50:04+00:00",
                    "url": "http://arxiv.org/abs/2309.06180v1",
                    "source": "arXiV",
                    "keyIdea": "PagedAttention, an attention algorithm inspired by virtual memory and paging techniques, is proposed to efficiently manage the key-value cache memory for large language models.",
                    "mainResult": "vLLM, a serving system built on PagedAttention, improves the throughput of popular LLMs by 2-4 times with the same level of latency compared to state-of-the-art systems."
                },
                {
                    "title": "Towards More Economical Context-Augmented LLM Generation by Reusing Stored KV Cache",
                    "authors": "Hanchen Li, Yuhan Liu, Yihua Cheng, Kuntai Du, Junchen Jiang",
                    "date": "2025-03-18 18:52:03+00:00",
                    "url": "http://arxiv.org/abs/2503.14647v1",
                    "source": "arXiV",
                    "keyIdea": "The paper explores the economic feasibility of reusing KV caches in large language model applications to save prefill delays and cloud costs.",
                    "mainResult": "Preliminary results show that KV cache reusing is able to save both delay and cloud cost across a range of workloads with long context."
                },
                {
                    "title": "Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing",
                    "authors": "Zhaoyuan Su, Tingfeng Lan, Zirui Wang, Juncheng Yang, Yue Cheng",
                    "date": "2025-05-24 06:12:31+00:00",
                    "url": "http://arxiv.org/abs/2506.02006v1",
                    "source": "arXiV",
                    "keyIdea": "A dynamic, workload-aware LLM serving framework called MorphServe is proposed to efficiently serve large language models under dynamic and bursty workloads.",
                    "mainResult": "MorphServe reduces average SLO violations by 92.45 percent and improves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving, without compromising generation quality."
                },
                {
                    "title": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV Cache Management",
                    "authors": "Liu Qianli, Hong Zicong, Chen Fahao, Li Peng, Guo Song",
                    "date": "2025-01-12 04:29:39+00:00",
                    "url": "http://arxiv.org/abs/2501.06709v1",
                    "source": "arXiV",
                    "keyIdea": "This paper proposes MELL, a memory-efficient LLM serving system via multi-GPU KV cache management, to reduce the number of GPUs needed by balancing the dynamic KV cache load and managing costly request migration.",
                    "mainResult": "MELL reduces the number of GPUs by 31% and increases the GPU utilization by 43% at most compared to existing LLM serving systems."
                },
                {
                    "title": "Do Large Language Models Need a Content Delivery Network?",
                    "authors": "Yihua Cheng, Kuntai Du, Jiayi Yao, Junchen Jiang",
                    "date": "2024-09-16 18:46:24+00:00",
                    "url": "http://arxiv.org/abs/2409.13761v2",
                    "source": "arXiV",
                    "keyIdea": "The paper proposes using KV caches as a medium for knowledge injection in large language models (LLMs) to enable more modular management and efficient serving.",
                    "mainResult": "The paper argues that a Knowledge Delivery Network (KDN) can dynamically optimize the storage, transfer, and composition of KV cache across LLM engines and other resources."
                },
                {
                    "title": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse",
                    "authors": "Tianyu Guo, Hande Dong, Yichong Leng, Feng Liu, Cheater Lin, Nong Xiao, Xianwei Zhang",
                    "date": "2025-05-28 02:07:03+00:00",
                    "url": "http://arxiv.org/abs/2505.21889v2",
                    "source": "arXiV",
                    "keyIdea": "The paper proposes EFIM, a transformed prompt format and a fragment tokenization training method to improve the efficiency of cross-request KV cache reuse in infilling tasks for large language models.",
                    "mainResult": "EFIM can lower the latency by 52% and improve the throughput by 98% while maintaining the original infilling capability for large language models."
                },
                {
                    "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage",
                    "authors": "Ke Hong, Lufang Chen, Zhong Wang, Xiuhong Li, Qiuli Mao, Jianping Ma, Chao Xiong, Guanyu Wu, Buhe Han, Guohao Dai, Yun Liang, Yu Wang",
                    "date": "2025-04-28 15:00:03+00:00",
                    "url": "http://arxiv.org/abs/2504.19867v1",
                    "source": "arXiV",
                    "keyIdea": "The paper proposes a novel LLM serving system, semi-PD, which combines disaggregated computation with unified storage to address storage inefficiencies in existing systems.",
                    "mainResult": "semi-PD maintains lower latency at higher request rates, reducing the average end-to-end latency per request by 1.27-2.58x on DeepSeek series models, and serves 1.55-1.72x more requests adhering to latency constraints on Llama series models."
                },
                {
                    "title": "Fast State Restoration in LLM Serving with HCache",
                    "authors": "Shiwei Gao, Youmin Chen, Jiwu Shu",
                    "date": "2024-10-07 13:03:45+00:00",
                    "url": "http://arxiv.org/abs/2410.05004v1",
                    "source": "arXiV",
                    "keyIdea": "The key idea is to restore LLM states from intermediate activations and thus utilize computational and I/O resources with low overhead.",
                    "mainResult": "HCache reduces the TTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less storage space; compared to token recomputation, HCache achieves up to 5.73X reduction in TTFT."
                },
                {
                    "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving",
                    "authors": "Shihong Gao, Xin Zhang, Yanyan Shen, Lei Chen",
                    "date": "2025-04-10 06:51:23+00:00",
                    "url": "http://arxiv.org/abs/2504.07494v1",
                    "source": "arXiV",
                    "keyIdea": "Apt-Serve is a scalable framework designed to enhance effective throughput in LLM inference serving by introducing a hybrid cache scheme and an adaptive runtime scheduling mechanism.",
                    "mainResult": "Apt-Serve achieves up to 8.8x improvement in effective throughput compared to the state-of-the-art inference serving systems."
                },
                {
                    "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
                    "authors": "Ahmed Burak Gulhan, Krishna Teja Chitty-Venkata, Murali Emani, Mahmut Kandemir, Venkatram Vishwanath",
                    "date": "2025-02-18 04:08:29+00:00",
                    "url": "http://arxiv.org/abs/2502.13176v2",
                    "source": "arXiV",
                    "keyIdea": "We introduce BaKlaVa, a method to allocate optimal memory for individual KV-caches across the model by estimating the importance of each KV-cache.",
                    "mainResult": "BaKlaVa achieves up to a 70% compression ratio while keeping baseline performance and delivering up to an order-of-magnitude accuracy improvement at higher compression levels."
                },
                {
                    "title": "Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management",
                    "authors": "Hang Zhang, Jiuchen Shi, Yixiao Wang, Quan Chen, Yizhou Shan, Minyi Guo",
                    "date": "2025-04-19 13:17:34+00:00",
                    "url": "http://arxiv.org/abs/2505.03756v1",
                    "source": "arXiV",
                    "keyIdea": "FASTLIBRA is proposed as a Multi-LoRA caching system to optimize the serving performance of task-specific Large Language Model applications.",
                    "mainResult": "FASTLIBRA reduces the Time-To-First-Token (TTFT) by 63.4% on average compared to state-of-the-art works."
                },
                {
                    "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference",
                    "authors": "Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, Beidi Chen",
                    "date": "2024-10-28 19:08:12+00:00",
                    "url": "http://arxiv.org/abs/2410.21465v3",
                    "source": "arXiV",
                    "keyIdea": "The paper presents ShadowKV, a high-throughput long-context LLM inference system that reduces memory footprint and decoding latency.",
                    "mainResult": "ShadowKV supports up to 6 times larger batch sizes and boosts throughput by up to 3.04 times on an A100 GPU without sacrificing accuracy."
                },
                {
                    "title": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference",
                    "authors": "Jing Xiong, Jianghan Shen, Fanghua Ye, Chaofan Tao, Zhongwei Wan, Jianqiao Lu, Xun Wu, Chuanyang Zheng, Zhijiang Guo, Lingpeng Kong, Ngai Wong",
                    "date": "2024-10-04 02:32:36+00:00",
                    "url": "http://arxiv.org/abs/2410.03090v1",
                    "source": "arXiV",
                    "keyIdea": "We propose UNComp, an uncertainty-aware compression scheme that leverages matrix entropy to estimate model uncertainty across layers and heads at the token sequence level.",
                    "mainResult": "Our method achieves a 1.6x speedup in the prefilling stage and reduces the KV cache to 4.74% of its original size, resulting in a 6.4x increase in throughput and a 1.4x speedup in inference with only a 1.41% performance loss."
                },
                {
                    "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion",
                    "authors": "Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, Junchen Jiang",
                    "date": "2024-05-26 06:00:17+00:00",
                    "url": "http://arxiv.org/abs/2405.16444v3",
                    "source": "arXiV",
                    "keyIdea": "The paper presents CacheBlend, a scheme that reuses precomputed KV caches of text chunks and selectively recomputes KV values to quickly combine them for achieving the same generation quality as full prefill in large language models.",
                    "mainResult": "CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3x and increases the inference throughput by 2.8-5x from full KV recompute without compromising generation quality."
                },
                {
                    "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
                    "authors": "Shibo Jie, Yehui Tang, Kai Han, Zhi-Hong Deng, Jing Han",
                    "date": "2025-03-20 14:01:56+00:00",
                    "url": "http://arxiv.org/abs/2503.16163v1",
                    "source": "arXiV",
                    "keyIdea": "The paper proposes SpeCache, a method that offloads the complete KV cache to CPU memory and dynamically fetches KV pairs back in each decoding step based on their importance, to address the bottleneck of limited GPU memory for long-text tasks.",
                    "mainResult": "SpeCache effectively reduces VRAM usage while avoiding information forgetting for long sequences without re-training, even with a 10x high KV cache compression ratio."
                },
                {
                    "title": "Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention",
                    "authors": "Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic, Junbo Deng, Xingkun Yang, Zhou Yu, Pengfei Zuo",
                    "date": "2024-03-23 10:42:49+00:00",
                    "url": "http://arxiv.org/abs/2403.19708v3",
                    "source": "arXiV",
                    "keyIdea": "The paper proposes CachedAttention, a new attention mechanism that enables reuse of key-value (KV) caches across multi-turn conversations, reducing repetitive computation overheads.",
                    "mainResult": "CachedAttention significantly decreases the time to the first token (TTFT) by up to 87%, improves the prompt prefilling throughput by up to 7.8$\times$ for multi-turn conversations, and reduces the end-to-end inference cost by up to 70%."
                },
                {
                    "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool",
                    "authors": "Cunchen Hu, Heyang Huang, Junhao Hu, Jiang Xu, Xusheng Chen, Tao Xie, Chenxi Wang, Sa Wang, Yungang Bao, Ninghui Sun, Yizhou Shan",
                    "date": "2024-06-25 14:02:08+00:00",
                    "url": "http://arxiv.org/abs/2406.17565v3",
                    "source": "arXiV",
                    "keyIdea": "MemServe is a unified system that integrates inter-request and intra-request optimizations for large language model serving, introducing an elastic memory pool called MemPool.",
                    "mainResult": "MemServe significantly improves job completion time and time-to-first-time through its optimizations."
                },
                {
                    "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance",
                    "authors": "Xuanfan Ni, Liyan Xu, Chenyang Lyu, Longyue Wang, Mo Yu, Lemao Liu, Fandong Meng, Jie Zhou, Piji Li",
                    "date": "2025-02-24 06:33:39+00:00",
                    "url": "http://arxiv.org/abs/2502.16886v1",
                    "source": "arXiV",
                    "keyIdea": "We propose a new KV cache compression objective and introduce a novel method, DBudgetKV, which features an attention-based metric to signal when the remaining KV cache is unlikely to match the full-cache performance.",
                    "mainResult": "Our method achieves lossless KV pruning effectively and robustly, exceeding 25% compression ratio on average, and shows reduced inference time compared to existing methods."
                }
            ]
        },
        {
            "title": "Surveys and Overviews of KV Cache Management",
            "count": 1,
            "summary": "The category of KV cache management has gained significant attention in recent years due to its potential to accelerate Large Language Model (LLM) inference by reducing redundant computations and improving memory utilization \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. Various strategies have been proposed to optimize KV cache management, including token-level, model-level, and system-level optimizations \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. Token-level strategies, such as KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, aim to improve cache efficiency \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. Model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. A comprehensive survey of these strategies provides valuable insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\].",
            "description": "The category of KV cache management has gained significant attention in recent years due to its potential to accelerate Large Language Model (LLM) inference by reducing redundant computations and improving memory utilization \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. Various strategies have been proposed to optimize KV cache management, including token-level, model-level, and system-level optimizations \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. Token-level strategies, such as KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, aim to improve cache efficiency \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. Model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. A comprehensive survey of these strategies provides valuable insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\].",
            "papers": [
                {
                    "title": "A Survey on Large Language Model Acceleration based on KV Cache Management",
                    "authors": "Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, Lei Chen",
                    "date": "2024-12-27 04:17:57+00:00",
                    "url": "http://arxiv.org/abs/2412.19442v2",
                    "source": "arXiV",
                    "keyIdea": "This survey provides a comprehensive overview of Key-Value (KV) cache management strategies for Large Language Model (LLM) acceleration, categorizing them into token-level, model-level, and system-level optimizations.",
                    "mainResult": "The survey presents detailed taxonomies and comparative analyses of KV cache management strategies, aiming to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques."
                }
            ]
        },
        {
            "title": "KV Cache Compression and Quantization Methods",
            "count": 3,
            "summary": "KV cache compression and quantization methods have been explored to address the memory footprint bottleneck in large language model (LLM) deployment. Some approaches focus on selecting and evicting unimportant KV pairs from the cache to reduce memory consumption, but this may lead to safety breaches, hallucinations, and context loss \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. Others propose quantization schemes, such as Quality Adaptive Quantization (QAQ), which achieves up to 10x compression ratio with negligible impact on model performance \\[[Shichen Dong' 2024-03-07](http://arxiv.org/abs/2403.04643v2)\\]. Additionally, methods like Mixed-precision KV cache (MiKV) and QJL have been proposed to balance cache compression and generation quality, with MiKV offering a state-of-the-art trade-off between compression ratio and performance \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\], and QJL demonstrating a more than fivefold reduction in KV cache memory usage without compromising accuracy \\[[Amir Zandieh' 2024-06-05](http://arxiv.org/abs/2406.03482v2)\\]. These methods aim to optimize cache utilization, reduce memory overhead, and improve model serving performance.",
            "description": "KV cache compression and quantization methods have been explored to address the memory footprint bottleneck in large language model (LLM) deployment. Some approaches focus on selecting and evicting unimportant KV pairs from the cache to reduce memory consumption, but this may lead to safety breaches, hallucinations, and context loss \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. Others propose quantization schemes, such as Quality Adaptive Quantization (QAQ), which achieves up to 10x compression ratio with negligible impact on model performance \\[[Shichen Dong' 2024-03-07](http://arxiv.org/abs/2403.04643v2)\\]. Additionally, methods like Mixed-precision KV cache (MiKV) and QJL have been proposed to balance cache compression and generation quality, with MiKV offering a state-of-the-art trade-off between compression ratio and performance \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\], and QJL demonstrating a more than fivefold reduction in KV cache memory usage without compromising accuracy \\[[Amir Zandieh' 2024-06-05](http://arxiv.org/abs/2406.03482v2)\\]. These methods aim to optimize cache utilization, reduce memory overhead, and improve model serving performance.",
            "papers": [
                {
                    "title": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization",
                    "authors": "June Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon, Gunho Park, Eunho Yang, Se Jung Kwon, Dongsoo Lee",
                    "date": "2024-02-28 06:34:54+00:00",
                    "url": "http://arxiv.org/abs/2402.18096v1",
                    "source": "arXiV",
                    "keyIdea": "The paper examines the impact of KV cache eviction on generative Large Language Models and proposes a mixed-precision KV cache method to balance cache compression and generation quality.",
                    "mainResult": "The proposed mixed-precision KV cache method, MiKV, offers a state-of-the-art trade-off between compression ratio and performance compared to other baselines."
                },
                {
                    "title": "QAQ: Quality Adaptive Quantization for LLM KV Cache",
                    "authors": "Shichen Dong, Wen Cheng, Jiayu Qin, Wei Wang",
                    "date": "2024-03-07 16:42:37+00:00",
                    "url": "http://arxiv.org/abs/2403.04643v2",
                    "source": "arXiV",
                    "keyIdea": "We propose QAQ, a Quality Adaptive Quantization scheme for the KV cache to address the bottleneck in model deployment due to the linear expansion of the KV cache with context length.",
                    "mainResult": "QAQ achieves up to 10x the compression ratio of the KV cache size with a neglectable impact on model performance."
                },
                {
                    "title": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead",
                    "authors": "Amir Zandieh, Majid Daliri, Insu Han",
                    "date": "2024-06-05 17:42:05+00:00",
                    "url": "http://arxiv.org/abs/2406.03482v2",
                    "source": "arXiV",
                    "keyIdea": "The paper introduces QJL, a new quantization approach that eliminates memory overheads by removing the need for storing quantization constants, to compress KV cache in LLMs.",
                    "mainResult": "QJL demonstrates a more than fivefold reduction in KV cache memory usage without compromising accuracy, all while achieving faster runtime, when applied across various LLMs and NLP tasks to quantize the KV cache to only 3 bits."
                }
            ]
        },
        {
            "title": "Cache Optimization Techniques for Large Language Models",
            "count": 1,
            "summary": "Cache optimization techniques for large language models have gained significant attention due to the substantial memory overhead and increased attention latency associated with growing context lengths. One approach to address this issue is through the use of KV cache eviction methods. For instance, KVzip, a query-agnostic KV cache eviction method, enables effective reuse of compressed KV caches across diverse queries, reducing KV cache size by 3-4$\\times$ and FlashAttention decoding latency by approximately 2$\\times$, with negligible performance loss in various tasks \\[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\\]. This method quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. The effectiveness of KVzip and other cache optimization techniques highlights the importance of efficient cache management in serving large language models, which is crucial for cloud providers to improve serving throughput and latency \\[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\\].",
            "description": "Cache optimization techniques for large language models have gained significant attention due to the substantial memory overhead and increased attention latency associated with growing context lengths. One approach to address this issue is through the use of KV cache eviction methods. For instance, KVzip, a query-agnostic KV cache eviction method, enables effective reuse of compressed KV caches across diverse queries, reducing KV cache size by 3-4$\\times$ and FlashAttention decoding latency by approximately 2$\\times$, with negligible performance loss in various tasks \\[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\\]. This method quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. The effectiveness of KVzip and other cache optimization techniques highlights the importance of efficient cache management in serving large language models, which is crucial for cloud providers to improve serving throughput and latency \\[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\\].",
            "papers": [
                {
                    "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
                    "authors": "Jang-Hyun Kim, Jinuk Kim, Sangwoo Kwon, Jae W. Lee, Sangdoo Yun, Hyun Oh Song",
                    "date": "2025-05-29 13:05:47+00:00",
                    "url": "http://arxiv.org/abs/2505.23416v1",
                    "source": "arXiV",
                    "keyIdea": "The paper introduces KVzip, a query-agnostic KV cache eviction method that enables effective reuse of compressed KV caches across diverse queries.",
                    "mainResult": "KVzip reduces KV cache size by 3-4$\times$ and FlashAttention decoding latency by approximately 2$\times$, with negligible performance loss in various tasks."
                }
            ]
        },
        {
            "title": "KV Cache Eviction Policies",
            "count": 4,
            "summary": "Several studies have explored cache eviction policies for large language models (LLMs) to address the challenges of memory usage and computational demands. Lookahead Q-Cache (LAQ) \\[[Yixuan Wang' 2025-05-24](http://arxiv.org/abs/2505.20334v1)\\] generates pseudo lookahead queries to approximate true decoding-stage queries, achieving a 1-4 point improvement on LongBench under limited cache budgets. RoCo \\[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\\] uses temporal attention scores and robustness measures to outperform existing eviction policies. CORM \\[[Jincheng Dai' 2024-04-24](http://arxiv.org/abs/2404.15949v2)\\] minimizes memory footprint by dynamically retaining essential key-value pairs, reducing inference memory usage by up to 70% with negligible performance degradation. NACL \\[[Yilong Chen' 2024-08-07](http://arxiv.org/abs/2408.03675v2)\\] proposes a general framework for long-context KV cache eviction, achieving optimal eviction and reducing KV Cache by up to 50% with over 95% performance maintenance. These studies demonstrate the importance of effective cache eviction policies in improving LLM serving performance, particularly under limited cache capacity \\[[Yixuan Wang' 2025-05-24](http://arxiv.org/abs/2505.20334v1)\\]\\[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\\]\\[[Jincheng Dai' 2024-04-24](http://arxiv.org/abs/2404.15949v2)\\]\\[[Yilong Chen' 2024-08-07](http://arxiv.org/abs/2408.03675v2)\\].",
            "description": "Several studies have explored cache eviction policies for large language models (LLMs) to address the challenges of memory usage and computational demands. Lookahead Q-Cache (LAQ) \\[[Yixuan Wang' 2025-05-24](http://arxiv.org/abs/2505.20334v1)\\] generates pseudo lookahead queries to approximate true decoding-stage queries, achieving a 1-4 point improvement on LongBench under limited cache budgets. RoCo \\[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\\] uses temporal attention scores and robustness measures to outperform existing eviction policies. CORM \\[[Jincheng Dai' 2024-04-24](http://arxiv.org/abs/2404.15949v2)\\] minimizes memory footprint by dynamically retaining essential key-value pairs, reducing inference memory usage by up to 70% with negligible performance degradation. NACL \\[[Yilong Chen' 2024-08-07](http://arxiv.org/abs/2408.03675v2)\\] proposes a general framework for long-context KV cache eviction, achieving optimal eviction and reducing KV Cache by up to 50% with over 95% performance maintenance. These studies demonstrate the importance of effective cache eviction policies in improving LLM serving performance, particularly under limited cache capacity \\[[Yixuan Wang' 2025-05-24](http://arxiv.org/abs/2505.20334v1)\\]\\[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\\]\\[[Jincheng Dai' 2024-04-24](http://arxiv.org/abs/2404.15949v2)\\]\\[[Yilong Chen' 2024-08-07](http://arxiv.org/abs/2408.03675v2)\\].",
            "papers": [
                {
                    "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query",
                    "authors": "Yixuan Wang, Shiyu Ji, Yijun Liu, Yuzhuang Xu, Yang Xu, Qingfu Zhu, Wanxiang Che",
                    "date": "2025-05-24 10:34:38+00:00",
                    "url": "http://arxiv.org/abs/2505.20334v1",
                    "source": "arXiV",
                    "keyIdea": "The paper proposes Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries for efficient KV cache eviction in large language models.",
                    "mainResult": "Experimental results show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\to$ 4 point improvement on LongBench under limited cache budget."
                },
                {
                    "title": "On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference",
                    "authors": "Siyu Ren, Kenny Q. Zhu",
                    "date": "2024-02-09 09:20:59+00:00",
                    "url": "http://arxiv.org/abs/2402.06262v2",
                    "source": "arXiV",
                    "keyIdea": "Large Language Models (LLMs) are cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands, and this paper proposes a robust cache omission policy called RoCo to address this issue.",
                    "mainResult": "The proposed RoCo policy, based on temporal attention scores and robustness measures, outperforms existing eviction policies in maintaining the overhead of key-value cache under a given budget, as validated by extensive experimentation."
                },
                {
                    "title": "CORM: Cache Optimization with Recent Message for Large Language Model Inference",
                    "authors": "Jincheng Dai, Zhuowei Huang, Haiyun Jiang, Chen Chen, Deng Cai, Wei Bi, Shuming Shi",
                    "date": "2024-04-24 16:11:54+00:00",
                    "url": "http://arxiv.org/abs/2404.15949v2",
                    "source": "arXiV",
                    "keyIdea": "The paper introduces a method for optimizing the KV cache in Large Language Models, which considerably minimizes its memory footprint.",
                    "mainResult": "CORM reduces the inference memory usage of KV cache by up to 70% with negligible performance degradation across six tasks in LongBench."
                },
                {
                    "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at Inference Time",
                    "authors": "Yilong Chen, Guoxia Wang, Junyuan Shang, Shiyao Cui, Zhenyu Zhang, Tingwen Liu, Shuohuan Wang, Yu Sun, Dianhai Yu, Hua Wu",
                    "date": "2024-08-07 10:31:07+00:00",
                    "url": "http://arxiv.org/abs/2408.03675v2",
                    "source": "arXiV",
                    "keyIdea": "The paper proposes NACL, a general framework for long-context KV cache eviction that achieves more optimal and efficient eviction in a single operation during the encoding phase.",
                    "mainResult": "The method significantly improves the performance on short- and long-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50% with over 95% performance maintenance."
                }
            ]
        },
        {
            "title": "LLM Acceleration using KV Cache Management",
            "count": 2,
            "summary": "Efficient key-value (KV) cache management is crucial for accelerating large language models (LLMs) in various applications. KVLink, a approach for efficient KV cache reuse, precomputes the KV cache of each document independently and concatenates them during inference, achieving an average improvement of 4% in question answering accuracy and reducing time-to-first-token by up to 96% compared to standard LLM inference \\[[Jingbo Yang' 2025-02-21](http://arxiv.org/abs/2502.16002v2)\\]. Another approach, Titanus, proposes a software-hardware co-design to compress the KV cache on-the-fly, achieving 159.9x and 34.8x energy efficiency and throughput compared to Nvidia A100 GPU and FlightLLM, respectively \\[[Peilin Chen' 2025-05-23](http://arxiv.org/abs/2505.17787v1)\\]. These works demonstrate the significance of optimizing KV cache management for LLMs, highlighting the need for efficient cache reuse and compression techniques to improve performance and reduce latency.",
            "description": "Efficient key-value (KV) cache management is crucial for accelerating large language models (LLMs) in various applications. KVLink, a approach for efficient KV cache reuse, precomputes the KV cache of each document independently and concatenates them during inference, achieving an average improvement of 4% in question answering accuracy and reducing time-to-first-token by up to 96% compared to standard LLM inference \\[[Jingbo Yang' 2025-02-21](http://arxiv.org/abs/2502.16002v2)\\]. Another approach, Titanus, proposes a software-hardware co-design to compress the KV cache on-the-fly, achieving 159.9x and 34.8x energy efficiency and throughput compared to Nvidia A100 GPU and FlightLLM, respectively \\[[Peilin Chen' 2025-05-23](http://arxiv.org/abs/2505.17787v1)\\]. These works demonstrate the significance of optimizing KV cache management for LLMs, highlighting the need for efficient cache reuse and compression techniques to improve performance and reduce latency.",
            "papers": [
                {
                    "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                    "authors": "Jingbo Yang, Bairu Hou, Wei Wei, Yujia Bao, Shiyu Chang",
                    "date": "2025-02-21 23:34:29+00:00",
                    "url": "http://arxiv.org/abs/2502.16002v2",
                    "source": "arXiV",
                    "keyIdea": "KVLink is an approach for efficient key-value (KV) cache reuse in large language models (LLMs) by precomputing the KV cache of each document independently and concatenating them during inference.",
                    "mainResult": "KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods and reduces time-to-first-token by up to 96% compared to standard LLM inference."
                },
                {
                    "title": "Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM Acceleration",
                    "authors": "Peilin Chen, Xiaoxuan Yang",
                    "date": "2025-05-23 12:00:09+00:00",
                    "url": "http://arxiv.org/abs/2505.17787v1",
                    "source": "arXiV",
                    "keyIdea": "The paper proposes Titanus, a software-hardware co-design to efficiently compress the key-value (KV) cache on-the-fly for large language models.",
                    "mainResult": "Titanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency (throughput) compared to Nvidia A100 GPU and FlightLLM respectively."
                }
            ]
        },
        {
            "title": "Adaptive KV Cache Eviction and Budget Allocation",
            "count": 1,
            "summary": "Efficient caching of intermediate results, particularly Key-Value (KV) cache, is crucial for serving large language models (LLMs) effectively. Recent studies have focused on optimizing KV cache eviction and budget allocation to improve serving throughput and latency. One approach involves evicting non-critical cache elements during runtime while preserving generation quality \\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\]. However, uniform allocation of compression budgets across all attention heads can be suboptimal, as it neglects unique attention patterns of each head. A head-wise adaptive budget allocation strategy, called Ada-KV, has been proposed to address this limitation, offering plug-and-play benefits and improving generation quality \\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\]. This approach has demonstrated substantial quality improvements over existing methods through extensive evaluations on 29 datasets. By adaptively allocating budgets, Ada-KV provides a promising solution for efficient KV cache management in LLMs \\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\].",
            "description": "Efficient caching of intermediate results, particularly Key-Value (KV) cache, is crucial for serving large language models (LLMs) effectively. Recent studies have focused on optimizing KV cache eviction and budget allocation to improve serving throughput and latency. One approach involves evicting non-critical cache elements during runtime while preserving generation quality \\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\]. However, uniform allocation of compression budgets across all attention heads can be suboptimal, as it neglects unique attention patterns of each head. A head-wise adaptive budget allocation strategy, called Ada-KV, has been proposed to address this limitation, offering plug-and-play benefits and improving generation quality \\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\]. This approach has demonstrated substantial quality improvements over existing methods through extensive evaluations on 29 datasets. By adaptively allocating budgets, Ada-KV provides a promising solution for efficient KV cache management in LLMs \\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\].",
            "papers": [
                {
                    "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference",
                    "authors": "Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou",
                    "date": "2024-07-16 09:53:32+00:00",
                    "url": "http://arxiv.org/abs/2407.11550v4",
                    "source": "arXiV",
                    "keyIdea": "We propose a head-wise adaptive budget allocation strategy, Ada-KV, for efficient KV cache eviction in large language models, which offers plug-and-play benefits and improves generation quality.",
                    "mainResult": "Extensive evaluations on 29 datasets demonstrate substantial quality improvements over existing methods."
                }
            ]
        }
    ],
    "paperGroups": [
        {
            "title": "Large Language Model Serving and Inference Optimization",
            "count": 18,
            "summary": "Large language model (LLM) serving and inference optimization have gained significant attention in recent years. Efficient serving of LLMs requires optimizing key-value (KV) cache memory, which can be huge and dynamic, leading to waste and fragmentation \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Various approaches have been proposed to address this issue, including PagedAttention, which inspires an attention algorithm from virtual memory and paging techniques \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Other works, such as vLLM, propose near-zero waste in KV cache memory and flexible sharing within and across requests \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Additionally, techniques like KV cache reusing \\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\\], MorphServe \\[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\\], MELL \\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\], and Knowledge Delivery Network (KDN) \\[[Yihua Cheng' 2024-09-16](http://arxiv.org/abs/2409.13761v2)\\] have been proposed to optimize LLM serving. These include optimizing cache eviction policies \\[[Xuanfan Ni' 2025-02-24](http://arxiv.org/abs/2502.16886v1)\\], attention mechanisms \\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\], and memory management \\[[Shiwei Gao' 2024-10-07](http://arxiv.org/abs/2410.05004v1)\\]. Some works also focus on multi-GPU KV cache management \\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\], infilling tasks \\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\], and long-context LLMs \\[[Hanshi Sun' 2024-10-28](http://arxiv.org/abs/2410.21465v3)\\]. Furthermore, methods like CacheBlend \\[[Jiayi Yao' 2024-05-26](http://arxiv.org/abs/2405.16444v3)\\] and SpeCache \\[[Shibo Jie' 2025-03-20](http://arxiv.org/abs/2503.16163v1)\\] aim to improve cache efficiency. These studies demonstrate the importance of optimizing KV cache in LLM serving, with improvements in throughput, latency, and memory usage \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]\\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\\]\\[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\\]\\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\]\\[[Yihua Cheng' 2024-09-16](http://arxiv.org/abs/2409.13761v2)\\]\\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\].",
            "description": "Large language model (LLM) serving and inference optimization have gained significant attention in recent years. Efficient serving of LLMs requires optimizing key-value (KV) cache memory, which can be huge and dynamic, leading to waste and fragmentation \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Various approaches have been proposed to address this issue, including PagedAttention, which inspires an attention algorithm from virtual memory and paging techniques \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Other works, such as vLLM, propose near-zero waste in KV cache memory and flexible sharing within and across requests \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Additionally, techniques like KV cache reusing \\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\\], MorphServe \\[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\\], MELL \\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\], and Knowledge Delivery Network (KDN) \\[[Yihua Cheng' 2024-09-16](http://arxiv.org/abs/2409.13761v2)\\] have been proposed to optimize LLM serving. These include optimizing cache eviction policies \\[[Xuanfan Ni' 2025-02-24](http://arxiv.org/abs/2502.16886v1)\\], attention mechanisms \\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\], and memory management \\[[Shiwei Gao' 2024-10-07](http://arxiv.org/abs/2410.05004v1)\\]. Some works also focus on multi-GPU KV cache management \\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\], infilling tasks \\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\], and long-context LLMs \\[[Hanshi Sun' 2024-10-28](http://arxiv.org/abs/2410.21465v3)\\]. Furthermore, methods like CacheBlend \\[[Jiayi Yao' 2024-05-26](http://arxiv.org/abs/2405.16444v3)\\] and SpeCache \\[[Shibo Jie' 2025-03-20](http://arxiv.org/abs/2503.16163v1)\\] aim to improve cache efficiency. These studies demonstrate the importance of optimizing KV cache in LLM serving, with improvements in throughput, latency, and memory usage \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]\\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\\]\\[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\\]\\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\]\\[[Yihua Cheng' 2024-09-16](http://arxiv.org/abs/2409.13761v2)\\]\\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\].",
            "papers": [
                {
                    "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
                    "authors": "Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica",
                    "date": "2023-09-12 12:50:04+00:00",
                    "url": "http://arxiv.org/abs/2309.06180v1",
                    "source": "arXiV",
                    "keyIdea": "PagedAttention, an attention algorithm inspired by virtual memory and paging techniques, is proposed to efficiently manage the key-value cache memory for large language models.",
                    "mainResult": "vLLM, a serving system built on PagedAttention, improves the throughput of popular LLMs by 2-4 times with the same level of latency compared to state-of-the-art systems."
                },
                {
                    "title": "Towards More Economical Context-Augmented LLM Generation by Reusing Stored KV Cache",
                    "authors": "Hanchen Li, Yuhan Liu, Yihua Cheng, Kuntai Du, Junchen Jiang",
                    "date": "2025-03-18 18:52:03+00:00",
                    "url": "http://arxiv.org/abs/2503.14647v1",
                    "source": "arXiV",
                    "keyIdea": "The paper explores the economic feasibility of reusing KV caches in large language model applications to save prefill delays and cloud costs.",
                    "mainResult": "Preliminary results show that KV cache reusing is able to save both delay and cloud cost across a range of workloads with long context."
                },
                {
                    "title": "Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing",
                    "authors": "Zhaoyuan Su, Tingfeng Lan, Zirui Wang, Juncheng Yang, Yue Cheng",
                    "date": "2025-05-24 06:12:31+00:00",
                    "url": "http://arxiv.org/abs/2506.02006v1",
                    "source": "arXiV",
                    "keyIdea": "A dynamic, workload-aware LLM serving framework called MorphServe is proposed to efficiently serve large language models under dynamic and bursty workloads.",
                    "mainResult": "MorphServe reduces average SLO violations by 92.45 percent and improves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving, without compromising generation quality."
                },
                {
                    "title": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV Cache Management",
                    "authors": "Liu Qianli, Hong Zicong, Chen Fahao, Li Peng, Guo Song",
                    "date": "2025-01-12 04:29:39+00:00",
                    "url": "http://arxiv.org/abs/2501.06709v1",
                    "source": "arXiV",
                    "keyIdea": "This paper proposes MELL, a memory-efficient LLM serving system via multi-GPU KV cache management, to reduce the number of GPUs needed by balancing the dynamic KV cache load and managing costly request migration.",
                    "mainResult": "MELL reduces the number of GPUs by 31% and increases the GPU utilization by 43% at most compared to existing LLM serving systems."
                },
                {
                    "title": "Do Large Language Models Need a Content Delivery Network?",
                    "authors": "Yihua Cheng, Kuntai Du, Jiayi Yao, Junchen Jiang",
                    "date": "2024-09-16 18:46:24+00:00",
                    "url": "http://arxiv.org/abs/2409.13761v2",
                    "source": "arXiV",
                    "keyIdea": "The paper proposes using KV caches as a medium for knowledge injection in large language models (LLMs) to enable more modular management and efficient serving.",
                    "mainResult": "The paper argues that a Knowledge Delivery Network (KDN) can dynamically optimize the storage, transfer, and composition of KV cache across LLM engines and other resources."
                },
                {
                    "title": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse",
                    "authors": "Tianyu Guo, Hande Dong, Yichong Leng, Feng Liu, Cheater Lin, Nong Xiao, Xianwei Zhang",
                    "date": "2025-05-28 02:07:03+00:00",
                    "url": "http://arxiv.org/abs/2505.21889v2",
                    "source": "arXiV",
                    "keyIdea": "The paper proposes EFIM, a transformed prompt format and a fragment tokenization training method to improve the efficiency of cross-request KV cache reuse in infilling tasks for large language models.",
                    "mainResult": "EFIM can lower the latency by 52% and improve the throughput by 98% while maintaining the original infilling capability for large language models."
                },
                {
                    "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage",
                    "authors": "Ke Hong, Lufang Chen, Zhong Wang, Xiuhong Li, Qiuli Mao, Jianping Ma, Chao Xiong, Guanyu Wu, Buhe Han, Guohao Dai, Yun Liang, Yu Wang",
                    "date": "2025-04-28 15:00:03+00:00",
                    "url": "http://arxiv.org/abs/2504.19867v1",
                    "source": "arXiV",
                    "keyIdea": "The paper proposes a novel LLM serving system, semi-PD, which combines disaggregated computation with unified storage to address storage inefficiencies in existing systems.",
                    "mainResult": "semi-PD maintains lower latency at higher request rates, reducing the average end-to-end latency per request by 1.27-2.58x on DeepSeek series models, and serves 1.55-1.72x more requests adhering to latency constraints on Llama series models."
                },
                {
                    "title": "Fast State Restoration in LLM Serving with HCache",
                    "authors": "Shiwei Gao, Youmin Chen, Jiwu Shu",
                    "date": "2024-10-07 13:03:45+00:00",
                    "url": "http://arxiv.org/abs/2410.05004v1",
                    "source": "arXiV",
                    "keyIdea": "The key idea is to restore LLM states from intermediate activations and thus utilize computational and I/O resources with low overhead.",
                    "mainResult": "HCache reduces the TTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less storage space; compared to token recomputation, HCache achieves up to 5.73X reduction in TTFT."
                },
                {
                    "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving",
                    "authors": "Shihong Gao, Xin Zhang, Yanyan Shen, Lei Chen",
                    "date": "2025-04-10 06:51:23+00:00",
                    "url": "http://arxiv.org/abs/2504.07494v1",
                    "source": "arXiV",
                    "keyIdea": "Apt-Serve is a scalable framework designed to enhance effective throughput in LLM inference serving by introducing a hybrid cache scheme and an adaptive runtime scheduling mechanism.",
                    "mainResult": "Apt-Serve achieves up to 8.8x improvement in effective throughput compared to the state-of-the-art inference serving systems."
                },
                {
                    "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
                    "authors": "Ahmed Burak Gulhan, Krishna Teja Chitty-Venkata, Murali Emani, Mahmut Kandemir, Venkatram Vishwanath",
                    "date": "2025-02-18 04:08:29+00:00",
                    "url": "http://arxiv.org/abs/2502.13176v2",
                    "source": "arXiV",
                    "keyIdea": "We introduce BaKlaVa, a method to allocate optimal memory for individual KV-caches across the model by estimating the importance of each KV-cache.",
                    "mainResult": "BaKlaVa achieves up to a 70% compression ratio while keeping baseline performance and delivering up to an order-of-magnitude accuracy improvement at higher compression levels."
                },
                {
                    "title": "Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management",
                    "authors": "Hang Zhang, Jiuchen Shi, Yixiao Wang, Quan Chen, Yizhou Shan, Minyi Guo",
                    "date": "2025-04-19 13:17:34+00:00",
                    "url": "http://arxiv.org/abs/2505.03756v1",
                    "source": "arXiV",
                    "keyIdea": "FASTLIBRA is proposed as a Multi-LoRA caching system to optimize the serving performance of task-specific Large Language Model applications.",
                    "mainResult": "FASTLIBRA reduces the Time-To-First-Token (TTFT) by 63.4% on average compared to state-of-the-art works."
                },
                {
                    "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference",
                    "authors": "Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, Beidi Chen",
                    "date": "2024-10-28 19:08:12+00:00",
                    "url": "http://arxiv.org/abs/2410.21465v3",
                    "source": "arXiV",
                    "keyIdea": "The paper presents ShadowKV, a high-throughput long-context LLM inference system that reduces memory footprint and decoding latency.",
                    "mainResult": "ShadowKV supports up to 6 times larger batch sizes and boosts throughput by up to 3.04 times on an A100 GPU without sacrificing accuracy."
                },
                {
                    "title": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference",
                    "authors": "Jing Xiong, Jianghan Shen, Fanghua Ye, Chaofan Tao, Zhongwei Wan, Jianqiao Lu, Xun Wu, Chuanyang Zheng, Zhijiang Guo, Lingpeng Kong, Ngai Wong",
                    "date": "2024-10-04 02:32:36+00:00",
                    "url": "http://arxiv.org/abs/2410.03090v1",
                    "source": "arXiV",
                    "keyIdea": "We propose UNComp, an uncertainty-aware compression scheme that leverages matrix entropy to estimate model uncertainty across layers and heads at the token sequence level.",
                    "mainResult": "Our method achieves a 1.6x speedup in the prefilling stage and reduces the KV cache to 4.74% of its original size, resulting in a 6.4x increase in throughput and a 1.4x speedup in inference with only a 1.41% performance loss."
                },
                {
                    "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion",
                    "authors": "Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, Junchen Jiang",
                    "date": "2024-05-26 06:00:17+00:00",
                    "url": "http://arxiv.org/abs/2405.16444v3",
                    "source": "arXiV",
                    "keyIdea": "The paper presents CacheBlend, a scheme that reuses precomputed KV caches of text chunks and selectively recomputes KV values to quickly combine them for achieving the same generation quality as full prefill in large language models.",
                    "mainResult": "CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3x and increases the inference throughput by 2.8-5x from full KV recompute without compromising generation quality."
                },
                {
                    "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
                    "authors": "Shibo Jie, Yehui Tang, Kai Han, Zhi-Hong Deng, Jing Han",
                    "date": "2025-03-20 14:01:56+00:00",
                    "url": "http://arxiv.org/abs/2503.16163v1",
                    "source": "arXiV",
                    "keyIdea": "The paper proposes SpeCache, a method that offloads the complete KV cache to CPU memory and dynamically fetches KV pairs back in each decoding step based on their importance, to address the bottleneck of limited GPU memory for long-text tasks.",
                    "mainResult": "SpeCache effectively reduces VRAM usage while avoiding information forgetting for long sequences without re-training, even with a 10x high KV cache compression ratio."
                },
                {
                    "title": "Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention",
                    "authors": "Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic, Junbo Deng, Xingkun Yang, Zhou Yu, Pengfei Zuo",
                    "date": "2024-03-23 10:42:49+00:00",
                    "url": "http://arxiv.org/abs/2403.19708v3",
                    "source": "arXiV",
                    "keyIdea": "The paper proposes CachedAttention, a new attention mechanism that enables reuse of key-value (KV) caches across multi-turn conversations, reducing repetitive computation overheads.",
                    "mainResult": "CachedAttention significantly decreases the time to the first token (TTFT) by up to 87%, improves the prompt prefilling throughput by up to 7.8$\times$ for multi-turn conversations, and reduces the end-to-end inference cost by up to 70%."
                },
                {
                    "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool",
                    "authors": "Cunchen Hu, Heyang Huang, Junhao Hu, Jiang Xu, Xusheng Chen, Tao Xie, Chenxi Wang, Sa Wang, Yungang Bao, Ninghui Sun, Yizhou Shan",
                    "date": "2024-06-25 14:02:08+00:00",
                    "url": "http://arxiv.org/abs/2406.17565v3",
                    "source": "arXiV",
                    "keyIdea": "MemServe is a unified system that integrates inter-request and intra-request optimizations for large language model serving, introducing an elastic memory pool called MemPool.",
                    "mainResult": "MemServe significantly improves job completion time and time-to-first-time through its optimizations."
                },
                {
                    "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance",
                    "authors": "Xuanfan Ni, Liyan Xu, Chenyang Lyu, Longyue Wang, Mo Yu, Lemao Liu, Fandong Meng, Jie Zhou, Piji Li",
                    "date": "2025-02-24 06:33:39+00:00",
                    "url": "http://arxiv.org/abs/2502.16886v1",
                    "source": "arXiV",
                    "keyIdea": "We propose a new KV cache compression objective and introduce a novel method, DBudgetKV, which features an attention-based metric to signal when the remaining KV cache is unlikely to match the full-cache performance.",
                    "mainResult": "Our method achieves lossless KV pruning effectively and robustly, exceeding 25% compression ratio on average, and shows reduced inference time compared to existing methods."
                }
            ]
        },
        {
            "title": "Surveys and Overviews of KV Cache Management",
            "count": 1,
            "summary": "The category of KV cache management has gained significant attention in recent years due to its potential to accelerate Large Language Model (LLM) inference by reducing redundant computations and improving memory utilization \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. Various strategies have been proposed to optimize KV cache management, including token-level, model-level, and system-level optimizations \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. Token-level strategies, such as KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, aim to improve cache efficiency \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. Model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. A comprehensive survey of these strategies provides valuable insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\].",
            "description": "The category of KV cache management has gained significant attention in recent years due to its potential to accelerate Large Language Model (LLM) inference by reducing redundant computations and improving memory utilization \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. Various strategies have been proposed to optimize KV cache management, including token-level, model-level, and system-level optimizations \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. Token-level strategies, such as KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, aim to improve cache efficiency \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. Model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. A comprehensive survey of these strategies provides valuable insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\].",
            "papers": [
                {
                    "title": "A Survey on Large Language Model Acceleration based on KV Cache Management",
                    "authors": "Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, Lei Chen",
                    "date": "2024-12-27 04:17:57+00:00",
                    "url": "http://arxiv.org/abs/2412.19442v2",
                    "source": "arXiV",
                    "keyIdea": "This survey provides a comprehensive overview of Key-Value (KV) cache management strategies for Large Language Model (LLM) acceleration, categorizing them into token-level, model-level, and system-level optimizations.",
                    "mainResult": "The survey presents detailed taxonomies and comparative analyses of KV cache management strategies, aiming to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques."
                }
            ]
        },
        {
            "title": "KV Cache Compression and Quantization Methods",
            "count": 3,
            "summary": "KV cache compression and quantization methods have been explored to address the memory footprint bottleneck in large language model (LLM) deployment. Some approaches focus on selecting and evicting unimportant KV pairs from the cache to reduce memory consumption, but this may lead to safety breaches, hallucinations, and context loss \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. Others propose quantization schemes, such as Quality Adaptive Quantization (QAQ), which achieves up to 10x compression ratio with negligible impact on model performance \\[[Shichen Dong' 2024-03-07](http://arxiv.org/abs/2403.04643v2)\\]. Additionally, methods like Mixed-precision KV cache (MiKV) and QJL have been proposed to balance cache compression and generation quality, with MiKV offering a state-of-the-art trade-off between compression ratio and performance \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\], and QJL demonstrating a more than fivefold reduction in KV cache memory usage without compromising accuracy \\[[Amir Zandieh' 2024-06-05](http://arxiv.org/abs/2406.03482v2)\\]. These methods aim to optimize cache utilization, reduce memory overhead, and improve model serving performance.",
            "description": "KV cache compression and quantization methods have been explored to address the memory footprint bottleneck in large language model (LLM) deployment. Some approaches focus on selecting and evicting unimportant KV pairs from the cache to reduce memory consumption, but this may lead to safety breaches, hallucinations, and context loss \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. Others propose quantization schemes, such as Quality Adaptive Quantization (QAQ), which achieves up to 10x compression ratio with negligible impact on model performance \\[[Shichen Dong' 2024-03-07](http://arxiv.org/abs/2403.04643v2)\\]. Additionally, methods like Mixed-precision KV cache (MiKV) and QJL have been proposed to balance cache compression and generation quality, with MiKV offering a state-of-the-art trade-off between compression ratio and performance \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\], and QJL demonstrating a more than fivefold reduction in KV cache memory usage without compromising accuracy \\[[Amir Zandieh' 2024-06-05](http://arxiv.org/abs/2406.03482v2)\\]. These methods aim to optimize cache utilization, reduce memory overhead, and improve model serving performance.",
            "papers": [
                {
                    "title": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization",
                    "authors": "June Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon, Gunho Park, Eunho Yang, Se Jung Kwon, Dongsoo Lee",
                    "date": "2024-02-28 06:34:54+00:00",
                    "url": "http://arxiv.org/abs/2402.18096v1",
                    "source": "arXiV",
                    "keyIdea": "The paper examines the impact of KV cache eviction on generative Large Language Models and proposes a mixed-precision KV cache method to balance cache compression and generation quality.",
                    "mainResult": "The proposed mixed-precision KV cache method, MiKV, offers a state-of-the-art trade-off between compression ratio and performance compared to other baselines."
                },
                {
                    "title": "QAQ: Quality Adaptive Quantization for LLM KV Cache",
                    "authors": "Shichen Dong, Wen Cheng, Jiayu Qin, Wei Wang",
                    "date": "2024-03-07 16:42:37+00:00",
                    "url": "http://arxiv.org/abs/2403.04643v2",
                    "source": "arXiV",
                    "keyIdea": "We propose QAQ, a Quality Adaptive Quantization scheme for the KV cache to address the bottleneck in model deployment due to the linear expansion of the KV cache with context length.",
                    "mainResult": "QAQ achieves up to 10x the compression ratio of the KV cache size with a neglectable impact on model performance."
                },
                {
                    "title": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead",
                    "authors": "Amir Zandieh, Majid Daliri, Insu Han",
                    "date": "2024-06-05 17:42:05+00:00",
                    "url": "http://arxiv.org/abs/2406.03482v2",
                    "source": "arXiV",
                    "keyIdea": "The paper introduces QJL, a new quantization approach that eliminates memory overheads by removing the need for storing quantization constants, to compress KV cache in LLMs.",
                    "mainResult": "QJL demonstrates a more than fivefold reduction in KV cache memory usage without compromising accuracy, all while achieving faster runtime, when applied across various LLMs and NLP tasks to quantize the KV cache to only 3 bits."
                }
            ]
        },
        {
            "title": "Cache Optimization Techniques for Large Language Models",
            "count": 1,
            "summary": "Cache optimization techniques for large language models have gained significant attention due to the substantial memory overhead and increased attention latency associated with growing context lengths. One approach to address this issue is through the use of KV cache eviction methods. For instance, KVzip, a query-agnostic KV cache eviction method, enables effective reuse of compressed KV caches across diverse queries, reducing KV cache size by 3-4$\\times$ and FlashAttention decoding latency by approximately 2$\\times$, with negligible performance loss in various tasks \\[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\\]. This method quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. The effectiveness of KVzip and other cache optimization techniques highlights the importance of efficient cache management in serving large language models, which is crucial for cloud providers to improve serving throughput and latency \\[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\\].",
            "description": "Cache optimization techniques for large language models have gained significant attention due to the substantial memory overhead and increased attention latency associated with growing context lengths. One approach to address this issue is through the use of KV cache eviction methods. For instance, KVzip, a query-agnostic KV cache eviction method, enables effective reuse of compressed KV caches across diverse queries, reducing KV cache size by 3-4$\\times$ and FlashAttention decoding latency by approximately 2$\\times$, with negligible performance loss in various tasks \\[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\\]. This method quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. The effectiveness of KVzip and other cache optimization techniques highlights the importance of efficient cache management in serving large language models, which is crucial for cloud providers to improve serving throughput and latency \\[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\\].",
            "papers": [
                {
                    "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
                    "authors": "Jang-Hyun Kim, Jinuk Kim, Sangwoo Kwon, Jae W. Lee, Sangdoo Yun, Hyun Oh Song",
                    "date": "2025-05-29 13:05:47+00:00",
                    "url": "http://arxiv.org/abs/2505.23416v1",
                    "source": "arXiV",
                    "keyIdea": "The paper introduces KVzip, a query-agnostic KV cache eviction method that enables effective reuse of compressed KV caches across diverse queries.",
                    "mainResult": "KVzip reduces KV cache size by 3-4$\times$ and FlashAttention decoding latency by approximately 2$\times$, with negligible performance loss in various tasks."
                }
            ]
        },
        {
            "title": "KV Cache Eviction Policies",
            "count": 4,
            "summary": "Several studies have explored cache eviction policies for large language models (LLMs) to address the challenges of memory usage and computational demands. Lookahead Q-Cache (LAQ) \\[[Yixuan Wang' 2025-05-24](http://arxiv.org/abs/2505.20334v1)\\] generates pseudo lookahead queries to approximate true decoding-stage queries, achieving a 1-4 point improvement on LongBench under limited cache budgets. RoCo \\[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\\] uses temporal attention scores and robustness measures to outperform existing eviction policies. CORM \\[[Jincheng Dai' 2024-04-24](http://arxiv.org/abs/2404.15949v2)\\] minimizes memory footprint by dynamically retaining essential key-value pairs, reducing inference memory usage by up to 70% with negligible performance degradation. NACL \\[[Yilong Chen' 2024-08-07](http://arxiv.org/abs/2408.03675v2)\\] proposes a general framework for long-context KV cache eviction, achieving optimal eviction and reducing KV Cache by up to 50% with over 95% performance maintenance. These studies demonstrate the importance of effective cache eviction policies in improving LLM serving performance, particularly under limited cache capacity \\[[Yixuan Wang' 2025-05-24](http://arxiv.org/abs/2505.20334v1)\\]\\[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\\]\\[[Jincheng Dai' 2024-04-24](http://arxiv.org/abs/2404.15949v2)\\]\\[[Yilong Chen' 2024-08-07](http://arxiv.org/abs/2408.03675v2)\\].",
            "description": "Several studies have explored cache eviction policies for large language models (LLMs) to address the challenges of memory usage and computational demands. Lookahead Q-Cache (LAQ) \\[[Yixuan Wang' 2025-05-24](http://arxiv.org/abs/2505.20334v1)\\] generates pseudo lookahead queries to approximate true decoding-stage queries, achieving a 1-4 point improvement on LongBench under limited cache budgets. RoCo \\[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\\] uses temporal attention scores and robustness measures to outperform existing eviction policies. CORM \\[[Jincheng Dai' 2024-04-24](http://arxiv.org/abs/2404.15949v2)\\] minimizes memory footprint by dynamically retaining essential key-value pairs, reducing inference memory usage by up to 70% with negligible performance degradation. NACL \\[[Yilong Chen' 2024-08-07](http://arxiv.org/abs/2408.03675v2)\\] proposes a general framework for long-context KV cache eviction, achieving optimal eviction and reducing KV Cache by up to 50% with over 95% performance maintenance. These studies demonstrate the importance of effective cache eviction policies in improving LLM serving performance, particularly under limited cache capacity \\[[Yixuan Wang' 2025-05-24](http://arxiv.org/abs/2505.20334v1)\\]\\[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\\]\\[[Jincheng Dai' 2024-04-24](http://arxiv.org/abs/2404.15949v2)\\]\\[[Yilong Chen' 2024-08-07](http://arxiv.org/abs/2408.03675v2)\\].",
            "papers": [
                {
                    "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query",
                    "authors": "Yixuan Wang, Shiyu Ji, Yijun Liu, Yuzhuang Xu, Yang Xu, Qingfu Zhu, Wanxiang Che",
                    "date": "2025-05-24 10:34:38+00:00",
                    "url": "http://arxiv.org/abs/2505.20334v1",
                    "source": "arXiV",
                    "keyIdea": "The paper proposes Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries for efficient KV cache eviction in large language models.",
                    "mainResult": "Experimental results show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\to$ 4 point improvement on LongBench under limited cache budget."
                },
                {
                    "title": "On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference",
                    "authors": "Siyu Ren, Kenny Q. Zhu",
                    "date": "2024-02-09 09:20:59+00:00",
                    "url": "http://arxiv.org/abs/2402.06262v2",
                    "source": "arXiV",
                    "keyIdea": "Large Language Models (LLMs) are cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands, and this paper proposes a robust cache omission policy called RoCo to address this issue.",
                    "mainResult": "The proposed RoCo policy, based on temporal attention scores and robustness measures, outperforms existing eviction policies in maintaining the overhead of key-value cache under a given budget, as validated by extensive experimentation."
                },
                {
                    "title": "CORM: Cache Optimization with Recent Message for Large Language Model Inference",
                    "authors": "Jincheng Dai, Zhuowei Huang, Haiyun Jiang, Chen Chen, Deng Cai, Wei Bi, Shuming Shi",
                    "date": "2024-04-24 16:11:54+00:00",
                    "url": "http://arxiv.org/abs/2404.15949v2",
                    "source": "arXiV",
                    "keyIdea": "The paper introduces a method for optimizing the KV cache in Large Language Models, which considerably minimizes its memory footprint.",
                    "mainResult": "CORM reduces the inference memory usage of KV cache by up to 70% with negligible performance degradation across six tasks in LongBench."
                },
                {
                    "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at Inference Time",
                    "authors": "Yilong Chen, Guoxia Wang, Junyuan Shang, Shiyao Cui, Zhenyu Zhang, Tingwen Liu, Shuohuan Wang, Yu Sun, Dianhai Yu, Hua Wu",
                    "date": "2024-08-07 10:31:07+00:00",
                    "url": "http://arxiv.org/abs/2408.03675v2",
                    "source": "arXiV",
                    "keyIdea": "The paper proposes NACL, a general framework for long-context KV cache eviction that achieves more optimal and efficient eviction in a single operation during the encoding phase.",
                    "mainResult": "The method significantly improves the performance on short- and long-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50% with over 95% performance maintenance."
                }
            ]
        },
        {
            "title": "LLM Acceleration using KV Cache Management",
            "count": 2,
            "summary": "Efficient key-value (KV) cache management is crucial for accelerating large language models (LLMs) in various applications. KVLink, a approach for efficient KV cache reuse, precomputes the KV cache of each document independently and concatenates them during inference, achieving an average improvement of 4% in question answering accuracy and reducing time-to-first-token by up to 96% compared to standard LLM inference \\[[Jingbo Yang' 2025-02-21](http://arxiv.org/abs/2502.16002v2)\\]. Another approach, Titanus, proposes a software-hardware co-design to compress the KV cache on-the-fly, achieving 159.9x and 34.8x energy efficiency and throughput compared to Nvidia A100 GPU and FlightLLM, respectively \\[[Peilin Chen' 2025-05-23](http://arxiv.org/abs/2505.17787v1)\\]. These works demonstrate the significance of optimizing KV cache management for LLMs, highlighting the need for efficient cache reuse and compression techniques to improve performance and reduce latency.",
            "description": "Efficient key-value (KV) cache management is crucial for accelerating large language models (LLMs) in various applications. KVLink, a approach for efficient KV cache reuse, precomputes the KV cache of each document independently and concatenates them during inference, achieving an average improvement of 4% in question answering accuracy and reducing time-to-first-token by up to 96% compared to standard LLM inference \\[[Jingbo Yang' 2025-02-21](http://arxiv.org/abs/2502.16002v2)\\]. Another approach, Titanus, proposes a software-hardware co-design to compress the KV cache on-the-fly, achieving 159.9x and 34.8x energy efficiency and throughput compared to Nvidia A100 GPU and FlightLLM, respectively \\[[Peilin Chen' 2025-05-23](http://arxiv.org/abs/2505.17787v1)\\]. These works demonstrate the significance of optimizing KV cache management for LLMs, highlighting the need for efficient cache reuse and compression techniques to improve performance and reduce latency.",
            "papers": [
                {
                    "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                    "authors": "Jingbo Yang, Bairu Hou, Wei Wei, Yujia Bao, Shiyu Chang",
                    "date": "2025-02-21 23:34:29+00:00",
                    "url": "http://arxiv.org/abs/2502.16002v2",
                    "source": "arXiV",
                    "keyIdea": "KVLink is an approach for efficient key-value (KV) cache reuse in large language models (LLMs) by precomputing the KV cache of each document independently and concatenating them during inference.",
                    "mainResult": "KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods and reduces time-to-first-token by up to 96% compared to standard LLM inference."
                },
                {
                    "title": "Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM Acceleration",
                    "authors": "Peilin Chen, Xiaoxuan Yang",
                    "date": "2025-05-23 12:00:09+00:00",
                    "url": "http://arxiv.org/abs/2505.17787v1",
                    "source": "arXiV",
                    "keyIdea": "The paper proposes Titanus, a software-hardware co-design to efficiently compress the key-value (KV) cache on-the-fly for large language models.",
                    "mainResult": "Titanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency (throughput) compared to Nvidia A100 GPU and FlightLLM respectively."
                }
            ]
        },
        {
            "title": "Adaptive KV Cache Eviction and Budget Allocation",
            "count": 1,
            "summary": "Efficient caching of intermediate results, particularly Key-Value (KV) cache, is crucial for serving large language models (LLMs) effectively. Recent studies have focused on optimizing KV cache eviction and budget allocation to improve serving throughput and latency. One approach involves evicting non-critical cache elements during runtime while preserving generation quality \\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\]. However, uniform allocation of compression budgets across all attention heads can be suboptimal, as it neglects unique attention patterns of each head. A head-wise adaptive budget allocation strategy, called Ada-KV, has been proposed to address this limitation, offering plug-and-play benefits and improving generation quality \\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\]. This approach has demonstrated substantial quality improvements over existing methods through extensive evaluations on 29 datasets. By adaptively allocating budgets, Ada-KV provides a promising solution for efficient KV cache management in LLMs \\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\].",
            "description": "Efficient caching of intermediate results, particularly Key-Value (KV) cache, is crucial for serving large language models (LLMs) effectively. Recent studies have focused on optimizing KV cache eviction and budget allocation to improve serving throughput and latency. One approach involves evicting non-critical cache elements during runtime while preserving generation quality \\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\]. However, uniform allocation of compression budgets across all attention heads can be suboptimal, as it neglects unique attention patterns of each head. A head-wise adaptive budget allocation strategy, called Ada-KV, has been proposed to address this limitation, offering plug-and-play benefits and improving generation quality \\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\]. This approach has demonstrated substantial quality improvements over existing methods through extensive evaluations on 29 datasets. By adaptively allocating budgets, Ada-KV provides a promising solution for efficient KV cache management in LLMs \\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\].",
            "papers": [
                {
                    "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference",
                    "authors": "Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou",
                    "date": "2024-07-16 09:53:32+00:00",
                    "url": "http://arxiv.org/abs/2407.11550v4",
                    "source": "arXiV",
                    "keyIdea": "We propose a head-wise adaptive budget allocation strategy, Ada-KV, for efficient KV cache eviction in large language models, which offers plug-and-play benefits and improves generation quality.",
                    "mainResult": "Extensive evaluations on 29 datasets demonstrate substantial quality improvements over existing methods."
                }
            ]
        }
    ],
    "sources": {
        "arxiv": 30
    },
    "full_report": "## Related Works\n\n**Efficient serving of large language models (LLMs) relies heavily on effective key-value (KV) caching mechanisms**. Caching intermediate results after processing each request substantially improves serving throughput and latency \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. However, the benefits of KV caching are highly dependent on system design decisions, such as cache eviction policies, which are workload-dependent \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\].\n\nSeveral studies have proposed various KV cache management strategies, including token-level, model-level, and system-level optimizations \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. Model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\].\n\nSome notable works include PagedAttention \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\], which achieves near-zero waste in KV cache memory and flexible sharing of KV cache within and across requests. vLLM, a system built on top of PagedAttention, improves the throughput of popular LLMs by 2-4\u00d7 with the same level of latency compared to state-of-the-art systems \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Other works, such as Mixed-precision KV cache (MiKV) \\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\\] and QAQ \\[[Shichen Dong' 2024-03-07](http://arxiv.org/abs/2403.04643v2)\\], propose reliable cache compression methods that preserve context details and ensure generation quality.\n\nRecent studies have also explored the reuse of KV caches across different requests and conversations \\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\]\\[[Bin Gao' 2024-03-23](http://arxiv.org/abs/2403.19708v3)\\]. CachedAttention \\[[Bin Gao' 2024-03-23](http://arxiv.org/abs/2403.19708v3)\\] enables reuse of KV caches across multi-turn conversations, significantly reducing repetitive computation overheads. Similarly, EFIM \\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\] proposes a transformed prompt format to unleash the performance potential of KV cache reuse in infilling tasks.\n\nCharacterization of KV workload patterns has also been studied \\[[Hanshi Sun' 2024-10-28](http://arxiv.org/abs/2410.21465v3)\\]\\[[Cunchen Hu' 2024-06-25](http://arxiv.org/abs/2406.17565v3)\\]. MemServe \\[[Cunchen Hu' 2024-06-25](http://arxiv.org/abs/2406.17565v3)\\] presents a unified system that integrates inter-request and intra-request optimizations, introducing an elastic memory pool managing distributed memory and KV caches.\n\nOur work builds upon these studies, providing a systematic characterization of KV workload patterns from a leading LLM service provider. We draw observations that can inform the design of workload-aware cache eviction policies, which improve serving performance under real-world traces, especially with limited cache capacity.\n\n## References\n\n\\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\] Efficient Memory Management for Large Language Model Serving with PagedAttention\n\\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\] A Survey on Large Language Model Acceleration based on KV Cache Management\n\\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\] No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization\n\\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\\] Towards More Economical Context-Augmented LLM Generation by Reusing Stored KV Cache\n\\[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\\] Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing\n\\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\] Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV Cache Management\n\\[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\\] KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction\n\\[[Yihua Cheng' 2024-09-16](http://arxiv.org/abs/2409.13761v2)\\] Do Large Language Models Need a Content Delivery Network?\n\\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\] EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse\n\\[[Shichen Dong' 2024-03-07](http://arxiv.org/abs/2403.04643v2)\\] QAQ: Quality Adaptive Quantization for LLM KV Cache\n\\[[Yixuan Wang' 2025-05-24](http://arxiv.org/abs/2505.20334v1)\\] Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query\n\\[[Jingbo Yang' 2025-02-21](http://arxiv.org/abs/2502.16002v2)\\] KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse\n\\[[Ke Hong' 2025-04-28](http://arxiv.org/abs/2504.19867v1)\\] semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage\n\\[[Shiwei Gao' 2024-10-07](http://arxiv.org/abs/2410.05004v1)\\] Fast State Restoration in LLM Serving with HCache\n\\[[Peilin Chen' 2025-05-23](http://arxiv.org/abs/2505.17787v1)\\] Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM Acceleration\n\\[[Shihong Gao' 2025-04-10](http://arxiv.org/abs/2504.07494v1)\\] Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving\n\\[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\\] On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference\n\\[[Ahmed Burak Gulhan' 2025-02-18](http://arxiv.org/abs/2502.13176v2)\\] BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference\n\\[[Hang Zhang' 2025-04-19](http://arxiv.org/abs/2505.03756v1)\\] Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management\n\\[[Hanshi Sun' 2024-10-28](http://arxiv.org/abs/2410.21465v3)\\] ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference\n\\[[Jincheng Dai' 2024-04-24](http://arxiv.org/abs/2404.15949v2)\\] CORM: Cache Optimization with Recent Message for Large Language Model Inference\n\\[[Amir Zandieh' 2024-06-05](http://arxiv.org/abs/2406.03482v2)\\] QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead\n\\[[Jing Xiong' 2024-10-04](http://arxiv.org/abs/2410.03090v1)\\] UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference\n\\[[Jiayi Yao' 2024-05-26](http://arxiv.org/abs/2405.16444v3)\\] CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion\n\\[[Shibo Jie' 2025-03-20](http://arxiv.org/abs/2503.16163v1)\\] SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs\n\\[[Bin Gao' 2024-03-23](http://arxiv.org/abs/2403.19708v3)\\] Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention\n\\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\] Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference\n\\[[Cunchen Hu' 2024-06-25](http://arxiv.org/abs/2406.17565v3)\\] MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool\n\\[[Yilong Chen' 2024-08-07](http://arxiv.org/abs/2408.03675v2)\\] NACL: A General and Effective KV Cache Eviction Framework for LLMs at Inference Time\n\n## Research papers\n### Outline\n- [Large Language Model Serving and Inference Optimization [18 papers]](#category-large-language-model-serving-and-inference-optimization)\n- [Surveys and Overviews of KV Cache Management [1 papers]](#category-surveys-and-overviews-of-kv-cache-management)\n- [KV Cache Compression and Quantization Methods [3 papers]](#category-kv-cache-compression-and-quantization-methods)\n- [Cache Optimization Techniques for Large Language Models [1 papers]](#category-cache-optimization-techniques-for-large-language-models)\n- [KV Cache Eviction Policies [4 papers]](#category-kv-cache-eviction-policies)\n- [LLM Acceleration using KV Cache Management [2 papers]](#category-llm-acceleration-using-kv-cache-management)\n- [Adaptive KV Cache Eviction and Budget Allocation [1 papers]](#category-adaptive-kv-cache-eviction-and-budget-allocation)\n\n### <a id='category-large-language-model-serving-and-inference-optimization'></a>Large Language Model Serving and Inference Optimization: 18 papers found.\nLarge language model (LLM) serving and inference optimization have gained significant attention in recent years. Efficient serving of LLMs requires optimizing key-value (KV) cache memory, which can be huge and dynamic, leading to waste and fragmentation \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Various approaches have been proposed to address this issue, including PagedAttention, which inspires an attention algorithm from virtual memory and paging techniques \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Other works, such as vLLM, propose near-zero waste in KV cache memory and flexible sharing within and across requests \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]. Additionally, techniques like KV cache reusing \\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\\], MorphServe \\[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\\], MELL \\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\], and Knowledge Delivery Network (KDN) \\[[Yihua Cheng' 2024-09-16](http://arxiv.org/abs/2409.13761v2)\\] have been proposed to optimize LLM serving. These include optimizing cache eviction policies \\[[Xuanfan Ni' 2025-02-24](http://arxiv.org/abs/2502.16886v1)\\], attention mechanisms \\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\], and memory management \\[[Shiwei Gao' 2024-10-07](http://arxiv.org/abs/2410.05004v1)\\]. Some works also focus on multi-GPU KV cache management \\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\], infilling tasks \\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\], and long-context LLMs \\[[Hanshi Sun' 2024-10-28](http://arxiv.org/abs/2410.21465v3)\\]. Furthermore, methods like CacheBlend \\[[Jiayi Yao' 2024-05-26](http://arxiv.org/abs/2405.16444v3)\\] and SpeCache \\[[Shibo Jie' 2025-03-20](http://arxiv.org/abs/2503.16163v1)\\] aim to improve cache efficiency. These studies demonstrate the importance of optimizing KV cache in LLM serving, with improvements in throughput, latency, and memory usage \\[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\\]\\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\\]\\[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\\]\\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\\]\\[[Yihua Cheng' 2024-09-16](http://arxiv.org/abs/2409.13761v2)\\]\\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\\].\n|    | paper                                                                                                                                                 | date     | key idea                                                                                                                                                                                                                                            | main result                                                                                                                                                                                                                                       |\n|----|-------------------------------------------------------------------------------------------------------------------------------------------------------|----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|  0 | [Efficient Memory Management for Large Language Model Serving with PagedAttention](http://arxiv.org/abs/2309.06180v1)                                 | 09/12/23 | PagedAttention, an attention algorithm inspired by virtual memory and paging techniques, is proposed to efficiently manage the key-value cache memory for large language models.                                                                    | vLLM, a serving system built on PagedAttention, improves the throughput of popular LLMs by 2-4 times with the same level of latency compared to state-of-the-art systems.                                                                         |\n|  1 | [Towards More Economical Context-Augmented LLM Generation by Reusing Stored KV Cache](http://arxiv.org/abs/2503.14647v1)                              | 03/18/25 | The paper explores the economic feasibility of reusing KV caches in large language model applications to save prefill delays and cloud costs.                                                                                                       | Preliminary results show that KV cache reusing is able to save both delay and cloud cost across a range of workloads with long context.                                                                                                           |\n|  2 | [Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing](http://arxiv.org/abs/2506.02006v1)                        | 05/24/25 | A dynamic, workload-aware LLM serving framework called MorphServe is proposed to efficiently serve large language models under dynamic and bursty workloads.                                                                                        | MorphServe reduces average SLO violations by 92.45 percent and improves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving, without compromising generation quality.                                                            |\n|  3 | [Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV Cache Management](http://arxiv.org/abs/2501.06709v1)                            | 01/12/25 | This paper proposes MELL, a memory-efficient LLM serving system via multi-GPU KV cache management, to reduce the number of GPUs needed by balancing the dynamic KV cache load and managing costly request migration.                                | MELL reduces the number of GPUs by 31% and increases the GPU utilization by 43% at most compared to existing LLM serving systems.                                                                                                                 |\n|  4 | [Do Large Language Models Need a Content Delivery Network?](http://arxiv.org/abs/2409.13761v2)                                                        | 09/16/24 | The paper proposes using KV caches as a medium for knowledge injection in large language models (LLMs) to enable more modular management and efficient serving.                                                                                     | The paper argues that a Knowledge Delivery Network (KDN) can dynamically optimize the storage, transfer, and composition of KV cache across LLM engines and other resources.                                                                      |\n|  5 | [EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse](http://arxiv.org/abs/2505.21889v2)                                 | 05/28/25 | The paper proposes EFIM, a transformed prompt format and a fragment tokenization training method to improve the efficiency of cross-request KV cache reuse in infilling tasks for large language models.                                            | EFIM can lower the latency by 52% and improve the throughput by 98% while maintaining the original infilling capability for large language models.                                                                                                |\n|  6 | [semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage](http://arxiv.org/abs/2504.19867v1)              | 04/28/25 | The paper proposes a novel LLM serving system, semi-PD, which combines disaggregated computation with unified storage to address storage inefficiencies in existing systems.                                                                        | semi-PD maintains lower latency at higher request rates, reducing the average end-to-end latency per request by 1.27-2.58x on DeepSeek series models, and serves 1.55-1.72x more requests adhering to latency constraints on Llama series models. |\n|  7 | [Fast State Restoration in LLM Serving with HCache](http://arxiv.org/abs/2410.05004v1)                                                                | 10/07/24 | The key idea is to restore LLM states from intermediate activations and thus utilize computational and I/O resources with low overhead.                                                                                                             | HCache reduces the TTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less storage space; compared to token recomputation, HCache achieves up to 5.73X reduction in TTFT.                                                      |\n|  8 | [Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving](http://arxiv.org/abs/2504.07494v1)                        | 04/10/25 | Apt-Serve is a scalable framework designed to enhance effective throughput in LLM inference serving by introducing a hybrid cache scheme and an adaptive runtime scheduling mechanism.                                                              | Apt-Serve achieves up to 8.8x improvement in effective throughput compared to the state-of-the-art inference serving systems.                                                                                                                     |\n|  9 | [BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference](http://arxiv.org/abs/2502.13176v2)                                            | 02/18/25 | We introduce BaKlaVa, a method to allocate optimal memory for individual KV-caches across the model by estimating the importance of each KV-cache.                                                                                                  | BaKlaVa achieves up to a 70% compression ratio while keeping baseline performance and delivering up to an order-of-magnitude accuracy improvement at higher compression levels.                                                                   |\n| 10 | [Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management](http://arxiv.org/abs/2505.03756v1) | 04/19/25 | FASTLIBRA is proposed as a Multi-LoRA caching system to optimize the serving performance of task-specific Large Language Model applications.                                                                                                        | FASTLIBRA reduces the Time-To-First-Token (TTFT) by 63.4% on average compared to state-of-the-art works.                                                                                                                                          |\n| 11 | [ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference](http://arxiv.org/abs/2410.21465v3)                                     | 10/28/24 | The paper presents ShadowKV, a high-throughput long-context LLM inference system that reduces memory footprint and decoding latency.                                                                                                                | ShadowKV supports up to 6 times larger batch sizes and boosts throughput by up to 3.04 times on an A100 GPU without sacrificing accuracy.                                                                                                         |\n| 12 | [UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference](http://arxiv.org/abs/2410.03090v1)                   | 10/04/24 | We propose UNComp, an uncertainty-aware compression scheme that leverages matrix entropy to estimate model uncertainty across layers and heads at the token sequence level.                                                                         | Our method achieves a 1.6x speedup in the prefilling stage and reduces the KV cache to 4.74% of its original size, resulting in a 6.4x increase in throughput and a 1.4x speedup in inference with only a 1.41% performance loss.                 |\n| 13 | [CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion](http://arxiv.org/abs/2405.16444v3)                               | 05/26/24 | The paper presents CacheBlend, a scheme that reuses precomputed KV caches of text chunks and selectively recomputes KV values to quickly combine them for achieving the same generation quality as full prefill in large language models.           | CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3x and increases the inference throughput by 2.8-5x from full KV recompute without compromising generation quality.                                                                        |\n| 14 | [SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs](http://arxiv.org/abs/2503.16163v1)                                         | 03/20/25 | The paper proposes SpeCache, a method that offloads the complete KV cache to CPU memory and dynamically fetches KV pairs back in each decoding step based on their importance, to address the bottleneck of limited GPU memory for long-text tasks. | SpeCache effectively reduces VRAM usage while avoiding information forgetting for long sequences without re-training, even with a 10x high KV cache compression ratio.                                                                            |\n| 15 | [Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention](http://arxiv.org/abs/2403.19708v3)                    | 03/23/24 | The paper proposes CachedAttention, a new attention mechanism that enables reuse of key-value (KV) caches across multi-turn conversations, reducing repetitive computation overheads.                                                               | CachedAttention significantly decreases the time to the first token (TTFT) by up to 87%, improves the prompt prefilling throughput by up to 7.8$\times$ for multi-turn conversations, and reduces the end-to-end inference cost by up to 70%.      |\n| 16 | [MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool](http://arxiv.org/abs/2406.17565v3)                                 | 06/25/24 | MemServe is a unified system that integrates inter-request and intra-request optimizations for large language model serving, introducing an elastic memory pool called MemPool.                                                                     | MemServe significantly improves job completion time and time-to-first-time through its optimizations.                                                                                                                                             |\n| 17 | [DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance](http://arxiv.org/abs/2502.16886v1)                               | 02/24/25 | We propose a new KV cache compression objective and introduce a novel method, DBudgetKV, which features an attention-based metric to signal when the remaining KV cache is unlikely to match the full-cache performance.                            | Our method achieves lossless KV pruning effectively and robustly, exceeding 25% compression ratio on average, and shows reduced inference time compared to existing methods.                                                                      |\n### <a id='category-surveys-and-overviews-of-kv-cache-management'></a>Surveys and Overviews of KV Cache Management: 1 papers found.\nThe category of KV cache management has gained significant attention in recent years due to its potential to accelerate Large Language Model (LLM) inference by reducing redundant computations and improving memory utilization \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. Various strategies have been proposed to optimize KV cache management, including token-level, model-level, and system-level optimizations \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. Token-level strategies, such as KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, aim to improve cache efficiency \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. Model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\]. A comprehensive survey of these strategies provides valuable insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques \\[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\\].\n|    | paper                                                                                                           | date     | key idea                                                                                                                                                                                                                  | main result                                                                                                                                                                                                                                                |\n|----|-----------------------------------------------------------------------------------------------------------------|----------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|  0 | [A Survey on Large Language Model Acceleration based on KV Cache Management](http://arxiv.org/abs/2412.19442v2) | 12/27/24 | This survey provides a comprehensive overview of Key-Value (KV) cache management strategies for Large Language Model (LLM) acceleration, categorizing them into token-level, model-level, and system-level optimizations. | The survey presents detailed taxonomies and comparative analyses of KV cache management strategies, aiming to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques. |\n### <a id='category-kv-cache-compression-and-quantization-methods'></a>KV Cache Compression and Quantization Methods: 3 papers found.\nKV cache compression and quantization methods have been explored to address the memory footprint bottleneck in large language model (LLM) deployment. Some approaches focus on selecting and evicting unimportant KV pairs from the cache to reduce memory consumption, but this may lead to safety breaches, hallucinations, and context loss \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\]. Others propose quantization schemes, such as Quality Adaptive Quantization (QAQ), which achieves up to 10x compression ratio with negligible impact on model performance \\[[Shichen Dong' 2024-03-07](http://arxiv.org/abs/2403.04643v2)\\]. Additionally, methods like Mixed-precision KV cache (MiKV) and QJL have been proposed to balance cache compression and generation quality, with MiKV offering a state-of-the-art trade-off between compression ratio and performance \\[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\\], and QJL demonstrating a more than fivefold reduction in KV cache memory usage without compromising accuracy \\[[Amir Zandieh' 2024-06-05](http://arxiv.org/abs/2406.03482v2)\\]. These methods aim to optimize cache utilization, reduce memory overhead, and improve model serving performance.\n|    | paper                                                                                                                                      | date     | key idea                                                                                                                                                                                   | main result                                                                                                                                                                                                                         |\n|----|--------------------------------------------------------------------------------------------------------------------------------------------|----------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|  0 | [No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization](http://arxiv.org/abs/2402.18096v1) | 02/28/24 | The paper examines the impact of KV cache eviction on generative Large Language Models and proposes a mixed-precision KV cache method to balance cache compression and generation quality. | The proposed mixed-precision KV cache method, MiKV, offers a state-of-the-art trade-off between compression ratio and performance compared to other baselines.                                                                      |\n|  1 | [QAQ: Quality Adaptive Quantization for LLM KV Cache](http://arxiv.org/abs/2403.04643v2)                                                   | 03/07/24 | We propose QAQ, a Quality Adaptive Quantization scheme for the KV cache to address the bottleneck in model deployment due to the linear expansion of the KV cache with context length.     | QAQ achieves up to 10x the compression ratio of the KV cache size with a neglectable impact on model performance.                                                                                                                   |\n|  2 | [QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead](http://arxiv.org/abs/2406.03482v2)                        | 06/05/24 | The paper introduces QJL, a new quantization approach that eliminates memory overheads by removing the need for storing quantization constants, to compress KV cache in LLMs.              | QJL demonstrates a more than fivefold reduction in KV cache memory usage without compromising accuracy, all while achieving faster runtime, when applied across various LLMs and NLP tasks to quantize the KV cache to only 3 bits. |\n### <a id='category-cache-optimization-techniques-for-large-language-models'></a>Cache Optimization Techniques for Large Language Models: 1 papers found.\nCache optimization techniques for large language models have gained significant attention due to the substantial memory overhead and increased attention latency associated with growing context lengths. One approach to address this issue is through the use of KV cache eviction methods. For instance, KVzip, a query-agnostic KV cache eviction method, enables effective reuse of compressed KV caches across diverse queries, reducing KV cache size by 3-4$\\times$ and FlashAttention decoding latency by approximately 2$\\times$, with negligible performance loss in various tasks \\[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\\]. This method quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. The effectiveness of KVzip and other cache optimization techniques highlights the importance of efficient cache management in serving large language models, which is crucial for cloud providers to improve serving throughput and latency \\[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\\].\n|    | paper                                                                                                       | date     | key idea                                                                                                                                           | main result                                                                                                                                                 |\n|----|-------------------------------------------------------------------------------------------------------------|----------|----------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|  0 | [KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction](http://arxiv.org/abs/2505.23416v1) | 05/29/25 | The paper introduces KVzip, a query-agnostic KV cache eviction method that enables effective reuse of compressed KV caches across diverse queries. | KVzip reduces KV cache size by 3-4$\times$ and FlashAttention decoding latency by approximately 2$\times$, with negligible performance loss in various tasks. |\n### <a id='category-kv-cache-eviction-policies'></a>KV Cache Eviction Policies: 4 papers found.\nSeveral studies have explored cache eviction policies for large language models (LLMs) to address the challenges of memory usage and computational demands. Lookahead Q-Cache (LAQ) \\[[Yixuan Wang' 2025-05-24](http://arxiv.org/abs/2505.20334v1)\\] generates pseudo lookahead queries to approximate true decoding-stage queries, achieving a 1-4 point improvement on LongBench under limited cache budgets. RoCo \\[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\\] uses temporal attention scores and robustness measures to outperform existing eviction policies. CORM \\[[Jincheng Dai' 2024-04-24](http://arxiv.org/abs/2404.15949v2)\\] minimizes memory footprint by dynamically retaining essential key-value pairs, reducing inference memory usage by up to 70% with negligible performance degradation. NACL \\[[Yilong Chen' 2024-08-07](http://arxiv.org/abs/2408.03675v2)\\] proposes a general framework for long-context KV cache eviction, achieving optimal eviction and reducing KV Cache by up to 50% with over 95% performance maintenance. These studies demonstrate the importance of effective cache eviction policies in improving LLM serving performance, particularly under limited cache capacity \\[[Yixuan Wang' 2025-05-24](http://arxiv.org/abs/2505.20334v1)\\]\\[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\\]\\[[Jincheng Dai' 2024-04-24](http://arxiv.org/abs/2404.15949v2)\\]\\[[Yilong Chen' 2024-08-07](http://arxiv.org/abs/2408.03675v2)\\].\n|    | paper                                                                                                                                 | date     | key idea                                                                                                                                                                                                                                            | main result                                                                                                                                                                                                                                  |\n|----|---------------------------------------------------------------------------------------------------------------------------------------|----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|  0 | [Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query](http://arxiv.org/abs/2505.20334v1)                  | 05/24/25 | The paper proposes Lookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost pseudo lookahead queries to better approximate the true decoding-stage queries for efficient KV cache eviction in large language models.             | Experimental results show that LAQ outperforms existing methods across various budget levels, achieving a 1 $\to$ 4 point improvement on LongBench under limited cache budget.                                                                |\n|  1 | [On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference](http://arxiv.org/abs/2402.06262v2) | 02/09/24 | Large Language Models (LLMs) are cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands, and this paper proposes a robust cache omission policy called RoCo to address this issue. | The proposed RoCo policy, based on temporal attention scores and robustness measures, outperforms existing eviction policies in maintaining the overhead of key-value cache under a given budget, as validated by extensive experimentation. |\n|  2 | [CORM: Cache Optimization with Recent Message for Large Language Model Inference](http://arxiv.org/abs/2404.15949v2)                  | 04/24/24 | The paper introduces a method for optimizing the KV cache in Large Language Models, which considerably minimizes its memory footprint.                                                                                                              | CORM reduces the inference memory usage of KV cache by up to 70% with negligible performance degradation across six tasks in LongBench.                                                                                                      |\n|  3 | [NACL: A General and Effective KV Cache Eviction Framework for LLMs at Inference Time](http://arxiv.org/abs/2408.03675v2)             | 08/07/24 | The paper proposes NACL, a general framework for long-context KV cache eviction that achieves more optimal and efficient eviction in a single operation during the encoding phase.                                                                  | The method significantly improves the performance on short- and long-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50% with over 95% performance maintenance.                                                           |\n### <a id='category-llm-acceleration-using-kv-cache-management'></a>LLM Acceleration using KV Cache Management: 2 papers found.\nEfficient key-value (KV) cache management is crucial for accelerating large language models (LLMs) in various applications. KVLink, a approach for efficient KV cache reuse, precomputes the KV cache of each document independently and concatenates them during inference, achieving an average improvement of 4% in question answering accuracy and reducing time-to-first-token by up to 96% compared to standard LLM inference \\[[Jingbo Yang' 2025-02-21](http://arxiv.org/abs/2502.16002v2)\\]. Another approach, Titanus, proposes a software-hardware co-design to compress the KV cache on-the-fly, achieving 159.9x and 34.8x energy efficiency and throughput compared to Nvidia A100 GPU and FlightLLM, respectively \\[[Peilin Chen' 2025-05-23](http://arxiv.org/abs/2505.17787v1)\\]. These works demonstrate the significance of optimizing KV cache management for LLMs, highlighting the need for efficient cache reuse and compression techniques to improve performance and reduce latency.\n|    | paper                                                                                                                    | date     | key idea                                                                                                                                                                                            | main result                                                                                                                                                                    |\n|----|--------------------------------------------------------------------------------------------------------------------------|----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|  0 | [KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse](http://arxiv.org/abs/2502.16002v2)             | 02/21/25 | KVLink is an approach for efficient key-value (KV) cache reuse in large language models (LLMs) by precomputing the KV cache of each document independently and concatenating them during inference. | KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods and reduces time-to-first-token by up to 96% compared to standard LLM inference. |\n|  1 | [Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM Acceleration](http://arxiv.org/abs/2505.17787v1) | 05/23/25 | The paper proposes Titanus, a software-hardware co-design to efficiently compress the key-value (KV) cache on-the-fly for large language models.                                                    | Titanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency (throughput) compared to Nvidia A100 GPU and FlightLLM respectively.                                       |\n### <a id='category-adaptive-kv-cache-eviction-and-budget-allocation'></a>Adaptive KV Cache Eviction and Budget Allocation: 1 papers found.\nEfficient caching of intermediate results, particularly Key-Value (KV) cache, is crucial for serving large language models (LLMs) effectively. Recent studies have focused on optimizing KV cache eviction and budget allocation to improve serving throughput and latency. One approach involves evicting non-critical cache elements during runtime while preserving generation quality \\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\]. However, uniform allocation of compression budgets across all attention heads can be suboptimal, as it neglects unique attention patterns of each head. A head-wise adaptive budget allocation strategy, called Ada-KV, has been proposed to address this limitation, offering plug-and-play benefits and improving generation quality \\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\]. This approach has demonstrated substantial quality improvements over existing methods through extensive evaluations on 29 datasets. By adaptively allocating budgets, Ada-KV provides a promising solution for efficient KV cache management in LLMs \\[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\\].\n|    | paper                                                                                                                               | date     | key idea                                                                                                                                                                                           | main result                                                                                              |\n|----|-------------------------------------------------------------------------------------------------------------------------------------|----------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|\n|  0 | [Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference](http://arxiv.org/abs/2407.11550v4) | 07/16/24 | We propose a head-wise adaptive budget allocation strategy, Ada-KV, for efficient KV cache eviction in large language models, which offers plug-and-play benefits and improves generation quality. | Extensive evaluations on 29 datasets demonstrate substantial quality improvements over existing methods. |\n"
}