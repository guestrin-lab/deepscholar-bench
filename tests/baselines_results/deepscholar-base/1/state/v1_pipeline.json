{
    "id": "e27ee484-dbe1-497c-9fa6-c80b67849c64",
    "topic": "\nYour task is to write a Related Works section for an academic paper given the paper's abstract. Your response should provide the Related Works section and references. Only include references that are published before 2025-06-03T08:51:38+00:00. Mention them in a separate reference list at the end and cite them properly in the Related Works section. Here is the paper abstract:\nServing large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity..\n\nNote that today's date is 06/09/25.",
    "date": "06/09/25",
    "intro_section": "## Related Works\n\n**Efficient serving of large lan",
    "num_papers": 30,
    "num_categories": 7
}