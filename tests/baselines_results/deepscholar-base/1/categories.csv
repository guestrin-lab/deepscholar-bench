summary,category
"Efficient caching of intermediate results, particularly Key-Value (KV) cache, is crucial for serving large language models (LLMs) effectively. Recent studies have focused on optimizing KV cache eviction and budget allocation to improve serving throughput and latency. One approach involves evicting non-critical cache elements during runtime while preserving generation quality \[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\]. However, uniform allocation of compression budgets across all attention heads can be suboptimal, as it neglects unique attention patterns of each head. A head-wise adaptive budget allocation strategy, called Ada-KV, has been proposed to address this limitation, offering plug-and-play benefits and improving generation quality \[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\]. This approach has demonstrated substantial quality improvements over existing methods through extensive evaluations on 29 datasets. By adaptively allocating budgets, Ada-KV provides a promising solution for efficient KV cache management in LLMs \[[Yuan Feng' 2024-07-16](http://arxiv.org/abs/2407.11550v4)\].",Adaptive KV Cache Eviction and Budget Allocation
"Cache optimization techniques for large language models have gained significant attention due to the substantial memory overhead and increased attention latency associated with growing context lengths. One approach to address this issue is through the use of KV cache eviction methods. For instance, KVzip, a query-agnostic KV cache eviction method, enables effective reuse of compressed KV caches across diverse queries, reducing KV cache size by 3-4$\times$ and FlashAttention decoding latency by approximately 2$\times$, with negligible performance loss in various tasks \[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\]. This method quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. The effectiveness of KVzip and other cache optimization techniques highlights the importance of efficient cache management in serving large language models, which is crucial for cloud providers to improve serving throughput and latency \[[Jang-Hyun Kim' 2025-05-29](http://arxiv.org/abs/2505.23416v1)\].",Cache Optimization Techniques for Large Language Models
"KV cache compression and quantization methods have been explored to address the memory footprint bottleneck in large language model (LLM) deployment. Some approaches focus on selecting and evicting unimportant KV pairs from the cache to reduce memory consumption, but this may lead to safety breaches, hallucinations, and context loss \[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\]. Others propose quantization schemes, such as Quality Adaptive Quantization (QAQ), which achieves up to 10x compression ratio with negligible impact on model performance \[[Shichen Dong' 2024-03-07](http://arxiv.org/abs/2403.04643v2)\]. Additionally, methods like Mixed-precision KV cache (MiKV) and QJL have been proposed to balance cache compression and generation quality, with MiKV offering a state-of-the-art trade-off between compression ratio and performance \[[June Yong Yang' 2024-02-28](http://arxiv.org/abs/2402.18096v1)\], and QJL demonstrating a more than fivefold reduction in KV cache memory usage without compromising accuracy \[[Amir Zandieh' 2024-06-05](http://arxiv.org/abs/2406.03482v2)\]. These methods aim to optimize cache utilization, reduce memory overhead, and improve model serving performance.",KV Cache Compression and Quantization Methods
"Several studies have explored cache eviction policies for large language models (LLMs) to address the challenges of memory usage and computational demands. Lookahead Q-Cache (LAQ) \[[Yixuan Wang' 2025-05-24](http://arxiv.org/abs/2505.20334v1)\] generates pseudo lookahead queries to approximate true decoding-stage queries, achieving a 1-4 point improvement on LongBench under limited cache budgets. RoCo \[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\] uses temporal attention scores and robustness measures to outperform existing eviction policies. CORM \[[Jincheng Dai' 2024-04-24](http://arxiv.org/abs/2404.15949v2)\] minimizes memory footprint by dynamically retaining essential key-value pairs, reducing inference memory usage by up to 70% with negligible performance degradation. NACL \[[Yilong Chen' 2024-08-07](http://arxiv.org/abs/2408.03675v2)\] proposes a general framework for long-context KV cache eviction, achieving optimal eviction and reducing KV Cache by up to 50% with over 95% performance maintenance. These studies demonstrate the importance of effective cache eviction policies in improving LLM serving performance, particularly under limited cache capacity \[[Yixuan Wang' 2025-05-24](http://arxiv.org/abs/2505.20334v1)\]\[[Siyu Ren' 2024-02-09](http://arxiv.org/abs/2402.06262v2)\]\[[Jincheng Dai' 2024-04-24](http://arxiv.org/abs/2404.15949v2)\]\[[Yilong Chen' 2024-08-07](http://arxiv.org/abs/2408.03675v2)\].",KV Cache Eviction Policies
"Efficient key-value (KV) cache management is crucial for accelerating large language models (LLMs) in various applications. KVLink, a approach for efficient KV cache reuse, precomputes the KV cache of each document independently and concatenates them during inference, achieving an average improvement of 4% in question answering accuracy and reducing time-to-first-token by up to 96% compared to standard LLM inference \[[Jingbo Yang' 2025-02-21](http://arxiv.org/abs/2502.16002v2)\]. Another approach, Titanus, proposes a software-hardware co-design to compress the KV cache on-the-fly, achieving 159.9x and 34.8x energy efficiency and throughput compared to Nvidia A100 GPU and FlightLLM, respectively \[[Peilin Chen' 2025-05-23](http://arxiv.org/abs/2505.17787v1)\]. These works demonstrate the significance of optimizing KV cache management for LLMs, highlighting the need for efficient cache reuse and compression techniques to improve performance and reduce latency.",LLM Acceleration using KV Cache Management
"Large language model (LLM) serving and inference optimization have gained significant attention in recent years. Efficient serving of LLMs requires optimizing key-value (KV) cache memory, which can be huge and dynamic, leading to waste and fragmentation \[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\]. Various approaches have been proposed to address this issue, including PagedAttention, which inspires an attention algorithm from virtual memory and paging techniques \[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\]. Other works, such as vLLM, propose near-zero waste in KV cache memory and flexible sharing within and across requests \[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\]. Additionally, techniques like KV cache reusing \[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\], MorphServe \[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\], MELL \[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\], and Knowledge Delivery Network (KDN) \[[Yihua Cheng' 2024-09-16](http://arxiv.org/abs/2409.13761v2)\] have been proposed to optimize LLM serving. These include optimizing cache eviction policies \[[Xuanfan Ni' 2025-02-24](http://arxiv.org/abs/2502.16886v1)\], attention mechanisms \[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\], and memory management \[[Shiwei Gao' 2024-10-07](http://arxiv.org/abs/2410.05004v1)\]. Some works also focus on multi-GPU KV cache management \[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\], infilling tasks \[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\], and long-context LLMs \[[Hanshi Sun' 2024-10-28](http://arxiv.org/abs/2410.21465v3)\]. Furthermore, methods like CacheBlend \[[Jiayi Yao' 2024-05-26](http://arxiv.org/abs/2405.16444v3)\] and SpeCache \[[Shibo Jie' 2025-03-20](http://arxiv.org/abs/2503.16163v1)\] aim to improve cache efficiency. These studies demonstrate the importance of optimizing KV cache in LLM serving, with improvements in throughput, latency, and memory usage \[[Woosuk Kwon' 2023-09-12](http://arxiv.org/abs/2309.06180v1)\]\[[Hanchen Li' 2025-03-18](http://arxiv.org/abs/2503.14647v1)\]\[[Zhaoyuan Su' 2025-05-24](http://arxiv.org/abs/2506.02006v1)\]\[[Liu Qianli' 2025-01-12](http://arxiv.org/abs/2501.06709v1)\]\[[Yihua Cheng' 2024-09-16](http://arxiv.org/abs/2409.13761v2)\]\[[Tianyu Guo' 2025-05-28](http://arxiv.org/abs/2505.21889v2)\].",Large Language Model Serving and Inference Optimization
"The category of KV cache management has gained significant attention in recent years due to its potential to accelerate Large Language Model (LLM) inference by reducing redundant computations and improving memory utilization \[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\]. Various strategies have been proposed to optimize KV cache management, including token-level, model-level, and system-level optimizations \[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\]. Token-level strategies, such as KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, aim to improve cache efficiency \[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\]. Model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse \[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\]. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments \[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\]. A comprehensive survey of these strategies provides valuable insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques \[[Haoyang Li' 2024-12-27](http://arxiv.org/abs/2412.19442v2)\].",Surveys and Overviews of KV Cache Management
