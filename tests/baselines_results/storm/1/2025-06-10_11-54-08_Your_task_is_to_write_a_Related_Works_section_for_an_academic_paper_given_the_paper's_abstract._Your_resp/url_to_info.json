{
    "url_to_unified_index": {
        "http://arxiv.org/abs/2504.06261v3": 2,
        "http://arxiv.org/abs/2406.16441v1": 6,
        "http://arxiv.org/abs/2005.13013v2": 5,
        "http://arxiv.org/abs/2404.02523v1": 7,
        "http://arxiv.org/abs/2504.03651v1": 1,
        "http://arxiv.org/abs/2502.00022v1": 8,
        "http://arxiv.org/abs/2409.12299v1": 4,
        "http://arxiv.org/abs/2505.14398v1": 3
    },
    "url_to_info": {
        "http://arxiv.org/abs/2504.06261v3": {
            "url": "http://arxiv.org/abs/2504.06261v3",
            "description": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the LLM instances to come up with their own collaboration\nstrategy for the problem at hand, all the while \"seeing\" each other's memory in\nthe concurrent KV cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\nmemory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)\nto avoid recomputation while improving parallel hardware utilization. We find\nthat modern reasoning-capable LLMs can perform inference with shared Key-Value\ncache out of the box, without additional fine-tuning.",
            "snippets": [
                "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the LLM instances to come up with their own collaboration\nstrategy for the problem at hand, all the while \"seeing\" each other's memory in\nthe concurrent KV cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\nmemory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)\nto avoid recomputation while improving parallel hardware utilization. We find\nthat modern reasoning-capable LLMs can perform inference with shared Key-Value\ncache out of the box, without additional fine-tuning."
            ],
            "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
            "meta": {
                "query": "LLM caching strategies for natural language processing tasks"
            },
            "citation_uuid": -1
        },
        "http://arxiv.org/abs/2406.16441v1": {
            "url": "http://arxiv.org/abs/2406.16441v1",
            "description": "Intermediate reasoning or acting steps have successfully improved large\nlanguage models (LLMs) for handling various downstream natural language\nprocessing (NLP) tasks. When applying LLMs for code generation, recent works\nmainly focus on directing the models to articulate intermediate\nnatural-language reasoning steps, as in chain-of-thought (CoT) prompting, and\nthen output code with the natural language or other structured intermediate\nsteps. However, such output is not suitable for code translation or generation\ntasks since the standard CoT has different logical structures and forms of\nexpression with the code. In this work, we introduce the universal code\n(UniCode) as the intermediate representation. It is a description of algorithm\nsteps using a mix of conventions of programming languages, such as assignment\noperator, conditional operator, and loop. Hence, we collect an instruction\ndataset UniCoder-Instruct to train our model UniCoder on multi-task learning\nobjectives. UniCoder-Instruct comprises natural-language questions, code\nsolutions, and the corresponding universal code. The alignment between the\nintermediate universal code representation and the final code solution\nsignificantly improves the quality of the generated code. The experimental\nresults demonstrate that UniCoder with the universal code significantly\noutperforms the previous prompting methods by a large margin, showcasing the\neffectiveness of the structural clues in pseudo-code.",
            "snippets": [
                "Intermediate reasoning or acting steps have successfully improved large\nlanguage models (LLMs) for handling various downstream natural language\nprocessing (NLP) tasks. When applying LLMs for code generation, recent works\nmainly focus on directing the models to articulate intermediate\nnatural-language reasoning steps, as in chain-of-thought (CoT) prompting, and\nthen output code with the natural language or other structured intermediate\nsteps. However, such output is not suitable for code translation or generation\ntasks since the standard CoT has different logical structures and forms of\nexpression with the code. In this work, we introduce the universal code\n(UniCode) as the intermediate representation. It is a description of algorithm\nsteps using a mix of conventions of programming languages, such as assignment\noperator, conditional operator, and loop. Hence, we collect an instruction\ndataset UniCoder-Instruct to train our model UniCoder on multi-task learning\nobjectives. UniCoder-Instruct comprises natural-language questions, code\nsolutions, and the corresponding universal code. The alignment between the\nintermediate universal code representation and the final code solution\nsignificantly improves the quality of the generated code. The experimental\nresults demonstrate that UniCoder with the universal code significantly\noutperforms the previous prompting methods by a large margin, showcasing the\neffectiveness of the structural clues in pseudo-code."
            ],
            "title": "UniCoder: Scaling Code Large Language Model via Universal Code",
            "meta": {
                "query": "caching intermediate results in large language models for natural language processing tasks"
            },
            "citation_uuid": -1
        },
        "http://arxiv.org/abs/2005.13013v2": {
            "url": "http://arxiv.org/abs/2005.13013v2",
            "description": "Intermediate-task training---fine-tuning a pretrained model on an\nintermediate task before fine-tuning again on the target task---often improves\nmodel performance substantially on language understanding tasks in monolingual\nEnglish settings. We investigate whether English intermediate-task training is\nstill helpful on non-English target tasks. Using nine intermediate\nlanguage-understanding tasks, we evaluate intermediate-task transfer in a\nzero-shot cross-lingual setting on the XTREME benchmark. We see large\nimprovements from intermediate training on the BUCC and Tatoeba sentence\nretrieval tasks and moderate improvements on question-answering target tasks.\nMNLI, SQuAD and HellaSwag achieve the best overall results as intermediate\ntasks, while multi-task intermediate offers small additional improvements.\nUsing our best intermediate-task models for each target task, we obtain a 5.4\npoint improvement over XLM-R Large on the XTREME benchmark, setting the state\nof the art as of June 2020. We also investigate continuing multilingual MLM\nduring intermediate-task training and using machine-translated\nintermediate-task data, but neither consistently outperforms simply performing\nEnglish intermediate-task training.",
            "snippets": [
                "Intermediate-task training---fine-tuning a pretrained model on an\nintermediate task before fine-tuning again on the target task---often improves\nmodel performance substantially on language understanding tasks in monolingual\nEnglish settings. We investigate whether English intermediate-task training is\nstill helpful on non-English target tasks. Using nine intermediate\nlanguage-understanding tasks, we evaluate intermediate-task transfer in a\nzero-shot cross-lingual setting on the XTREME benchmark. We see large\nimprovements from intermediate training on the BUCC and Tatoeba sentence\nretrieval tasks and moderate improvements on question-answering target tasks.\nMNLI, SQuAD and HellaSwag achieve the best overall results as intermediate\ntasks, while multi-task intermediate offers small additional improvements.\nUsing our best intermediate-task models for each target task, we obtain a 5.4\npoint improvement over XLM-R Large on the XTREME benchmark, setting the state\nof the art as of June 2020. We also investigate continuing multilingual MLM\nduring intermediate-task training and using machine-translated\nintermediate-task data, but neither consistently outperforms simply performing\nEnglish intermediate-task training."
            ],
            "title": "English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too",
            "meta": {
                "query": "caching intermediate results in large language models for natural language processing tasks"
            },
            "citation_uuid": -1
        },
        "http://arxiv.org/abs/2404.02523v1": {
            "url": "http://arxiv.org/abs/2404.02523v1",
            "description": "Visual affordance learning is a key component for robots to understand how to\ninteract with objects. Conventional approaches in this field rely on\npre-defined objects and actions, falling short of capturing diverse\ninteractions in realworld scenarios. The key idea of our approach is employing\ntextual instruction, targeting various affordances for a wide range of objects.\nThis approach covers both hand-object and tool-object interactions. We\nintroduce text-driven affordance learning, aiming to learn contact points and\nmanipulation trajectories from an egocentric view following textual\ninstruction. In our task, contact points are represented as heatmaps, and the\nmanipulation trajectory as sequences of coordinates that incorporate both\nlinear and rotational movements for various manipulations. However, when we\ngather data for this task, manual annotations of these diverse interactions are\ncostly. To this end, we propose a pseudo dataset creation pipeline and build a\nlarge pseudo-training dataset: TextAFF80K, consisting of over 80K instances of\nthe contact points, trajectories, images, and text tuples. We extend existing\nreferring expression comprehension models for our task, and experimental\nresults show that our approach robustly handles multiple affordances, serving\nas a new standard for affordance learning in real-world scenarios.",
            "snippets": [
                "Visual affordance learning is a key component for robots to understand how to\ninteract with objects. Conventional approaches in this field rely on\npre-defined objects and actions, falling short of capturing diverse\ninteractions in realworld scenarios. The key idea of our approach is employing\ntextual instruction, targeting various affordances for a wide range of objects.\nThis approach covers both hand-object and tool-object interactions. We\nintroduce text-driven affordance learning, aiming to learn contact points and\nmanipulation trajectories from an egocentric view following textual\ninstruction. In our task, contact points are represented as heatmaps, and the\nmanipulation trajectory as sequences of coordinates that incorporate both\nlinear and rotational movements for various manipulations. However, when we\ngather data for this task, manual annotations of these diverse interactions are\ncostly. To this end, we propose a pseudo dataset creation pipeline and build a\nlarge pseudo-training dataset: TextAFF80K, consisting of over 80K instances of\nthe contact points, trajectories, images, and text tuples. We extend existing\nreferring expression comprehension models for our task, and experimental\nresults show that our approach robustly handles multiple affordances, serving\nas a new standard for affordance learning in real-world scenarios."
            ],
            "title": "Text-driven Affordance Learning from Egocentric Vision",
            "meta": {
                "query": "KVcache workload patterns in realworld LLM serving scenarios"
            },
            "citation_uuid": -1
        },
        "http://arxiv.org/abs/2504.03651v1": {
            "url": "http://arxiv.org/abs/2504.03651v1",
            "description": "Large language models have been widely deployed in various applications,\nencompassing both interactive online tasks and batched offline tasks. Given the\nburstiness and latency sensitivity of online tasks, over-provisioning resources\nis common practice. This allows for the integration of latency-insensitive\noffline tasks during periods of low online load, enhancing resource\nutilization. However, strategically serving online and offline tasks through a\npreemption mechanism fails to fully leverage the flexibility of offline tasks\nand suffers from KV cache recomputation and irregular workloads.\n  In this paper, we introduce Echo, a collaborative online-offline task serving\nsystem, including a scheduler, a KV cache manager, and estimation toolkits. The\nscheduler and KV cache manager work tightly to maximize the throughput of\noffline tasks, while the estimator further predicts execution time to ensure\nonline task SLOs. The scheduler leverages the batch information of last\niteration to reduce the search space for finding the optimal schedule. The KV\ncache manager sets the priority of the KV cache based on the type of tasks and\nthe opportunity of prefix sharing to reduce the recomputation. Finally, the\nestimation toolkits predict the execution time, future memory consumption, and\nthe throughput of offline tasks to guide the scheduler, KV cache manager, and\nthe system deployer. Evaluation based on real-world workloads demonstrates that\nEcho can increase offline task throughput by up to $3.3\\times$, while\nsatisfying online task SLOs.",
            "snippets": [
                "Large language models have been widely deployed in various applications,\nencompassing both interactive online tasks and batched offline tasks. Given the\nburstiness and latency sensitivity of online tasks, over-provisioning resources\nis common practice. This allows for the integration of latency-insensitive\noffline tasks during periods of low online load, enhancing resource\nutilization. However, strategically serving online and offline tasks through a\npreemption mechanism fails to fully leverage the flexibility of offline tasks\nand suffers from KV cache recomputation and irregular workloads.\n  In this paper, we introduce Echo, a collaborative online-offline task serving\nsystem, including a scheduler, a KV cache manager, and estimation toolkits. The\nscheduler and KV cache manager work tightly to maximize the throughput of\noffline tasks, while the estimator further predicts execution time to ensure\nonline task SLOs. The scheduler leverages the batch information of last\niteration to reduce the search space for finding the optimal schedule. The KV\ncache manager sets the priority of the KV cache based on the type of tasks and\nthe opportunity of prefix sharing to reduce the recomputation. Finally, the\nestimation toolkits predict the execution time, future memory consumption, and\nthe throughput of offline tasks to guide the scheduler, KV cache manager, and\nthe system deployer. Evaluation based on real-world workloads demonstrates that\nEcho can increase offline task throughput by up to $3.3\\times$, while\nsatisfying online task SLOs."
            ],
            "title": "Echo: Efficient Co-Scheduling of Hybrid Online-Offline Tasks for Large Language Model Serving",
            "meta": {
                "query": "realworld KV workload patterns in large language models"
            },
            "citation_uuid": -1
        },
        "http://arxiv.org/abs/2502.00022v1": {
            "url": "http://arxiv.org/abs/2502.00022v1",
            "description": "HRA (Human Reliability Analysis) data is crucial for advancing HRA\nmethodologies. however, existing data collection methods lack the necessary\ngranularity, and most approaches fail to capture dynamic features.\nAdditionally, many methods require expert knowledge as input, making them\ntime-consuming and labor-intensive. To address these challenges, we propose a\nnew paradigm for the automated collection of HRA data. Our approach focuses on\nkey indicators behind human error, specifically measuring workload in\ncollaborative settings. This study introduces a novel, scenario-driven method\nfor workload estimation, leveraging fine-tuned large language models (LLMs). By\ntraining LLMs on real-world operational data from high-temperature gas-cooled\nreactors (HTGRs), we simulate human behavior and cognitive load in real time\nacross various collaborative scenarios. The method dynamically adapts to\nchanges in operator workload, providing more accurate, flexible, and scalable\nworkload estimates. The results demonstrate that the proposed WELLA (Workload\nEstimation with LLMs and Agents) outperforms existing commercial LLM-based\nmethods in terms of prediction accuracy.",
            "snippets": [
                "HRA (Human Reliability Analysis) data is crucial for advancing HRA\nmethodologies. however, existing data collection methods lack the necessary\ngranularity, and most approaches fail to capture dynamic features.\nAdditionally, many methods require expert knowledge as input, making them\ntime-consuming and labor-intensive. To address these challenges, we propose a\nnew paradigm for the automated collection of HRA data. Our approach focuses on\nkey indicators behind human error, specifically measuring workload in\ncollaborative settings. This study introduces a novel, scenario-driven method\nfor workload estimation, leveraging fine-tuned large language models (LLMs). By\ntraining LLMs on real-world operational data from high-temperature gas-cooled\nreactors (HTGRs), we simulate human behavior and cognitive load in real time\nacross various collaborative scenarios. The method dynamically adapts to\nchanges in operator workload, providing more accurate, flexible, and scalable\nworkload estimates. The results demonstrate that the proposed WELLA (Workload\nEstimation with LLMs and Agents) outperforms existing commercial LLM-based\nmethods in terms of prediction accuracy."
            ],
            "title": "A Dynamic and High-Precision Method for Scenario-Based HRA Synthetic Data Collection in Multi-Agent Collaborative Environments Driven by LLMs",
            "meta": {
                "query": "KVcache workload patterns in realworld LLM serving scenarios"
            },
            "citation_uuid": -1
        },
        "http://arxiv.org/abs/2409.12299v1": {
            "url": "http://arxiv.org/abs/2409.12299v1",
            "description": "Web applications, accessible via web browsers over the Internet, facilitate\ncomplex functionalities without local software installation. In the context of\nweb applications, a workload refers to the number of user requests sent by\nusers or applications to the underlying system. Existing studies have leveraged\nweb application workloads to achieve various objectives, such as workload\nprediction and auto-scaling. However, these studies are conducted in an ad hoc\nmanner, lacking a systematic understanding of the characteristics of web\napplication workloads. In this study, we first conduct a systematic literature\nreview to identify and analyze existing studies leveraging web application\nworkloads. Our analysis sheds light on their workload utilization, analysis\ntechniques, and high-level objectives. We further systematically analyze the\ncharacteristics of the web application workloads identified in the literature\nreview. Our analysis centers on characterizing these workloads at two distinct\ntemporal granularities: daily and weekly. We successfully identify and\ncategorize three daily and three weekly patterns within the workloads. By\nproviding a statistical characterization of these workload patterns, our study\nhighlights the uniqueness of each pattern, paving the way for the development\nof realistic workload generation and resource provisioning techniques that can\nbenefit a range of applications and research areas.",
            "snippets": [
                "Web applications, accessible via web browsers over the Internet, facilitate\ncomplex functionalities without local software installation. In the context of\nweb applications, a workload refers to the number of user requests sent by\nusers or applications to the underlying system. Existing studies have leveraged\nweb application workloads to achieve various objectives, such as workload\nprediction and auto-scaling. However, these studies are conducted in an ad hoc\nmanner, lacking a systematic understanding of the characteristics of web\napplication workloads. In this study, we first conduct a systematic literature\nreview to identify and analyze existing studies leveraging web application\nworkloads. Our analysis sheds light on their workload utilization, analysis\ntechniques, and high-level objectives. We further systematically analyze the\ncharacteristics of the web application workloads identified in the literature\nreview. Our analysis centers on characterizing these workloads at two distinct\ntemporal granularities: daily and weekly. We successfully identify and\ncategorize three daily and three weekly patterns within the workloads. By\nproviding a statistical characterization of these workload patterns, our study\nhighlights the uniqueness of each pattern, paving the way for the development\nof realistic workload generation and resource provisioning techniques that can\nbenefit a range of applications and research areas."
            ],
            "title": "Understanding Web Application Workloads and Their Applications: Systematic Literature Review and Characterization",
            "meta": {
                "query": "Recent studies on characterizing KVcache workload patterns"
            },
            "citation_uuid": -1
        },
        "http://arxiv.org/abs/2505.14398v1": {
            "url": "http://arxiv.org/abs/2505.14398v1",
            "description": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.",
            "snippets": [
                "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques."
            ],
            "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation",
            "meta": {
                "query": "KV reuse characteristics in LLMs"
            },
            "citation_uuid": -1
        }
    }
}