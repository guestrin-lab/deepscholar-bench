{
    "http://arxiv.org/abs/2405.16444v3": {
        "url": "http://arxiv.org/abs/2405.16444v3",
        "description": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache.",
        "snippets": [
            "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache."
        ],
        "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2406.10247v1": {
        "url": "http://arxiv.org/abs/2406.10247v1",
        "description": "Excessive memory requirements of key and value features (KV-cache) present\nsignificant challenges in the autoregressive inference of large language models\n(LLMs), restricting both the speed and length of text generation. Approaches\nsuch as Multi-Query Attention (MQA) and Grouped Query Attention (GQA) mitigate\nthese challenges by grouping query heads and consequently reducing the number\nof corresponding key and value heads. However, MQA and GQA decrease the\nKV-cache size requirements at the expense of LLM accuracy (quality of text\ngeneration). These methods do not ensure an optimal tradeoff between KV-cache\nsize and text generation quality due to the absence of quality-aware grouping\nof query heads. To address this issue, we propose Quality and Capacity-Aware\nGrouped Query Attention (QCQA), which identifies optimal query head groupings\nusing an evolutionary algorithm with a computationally efficient and\ninexpensive fitness function. We demonstrate that QCQA achieves a significantly\nbetter tradeoff between KV-cache capacity and LLM accuracy compared to GQA. For\nthe Llama2 $7\\,$B model, QCQA achieves $\\mathbf{20}$\\% higher accuracy than GQA\nwith similar KV-cache size requirements in the absence of fine-tuning. After\nfine-tuning both QCQA and GQA, for a similar KV-cache size, QCQA provides\n$\\mathbf{10.55}\\,$\\% higher accuracy than GQA. Furthermore, QCQA requires\n$40\\,$\\% less KV-cache size than GQA to attain similar accuracy. The proposed\nquality and capacity-aware grouping of query heads can serve as a new paradigm\nfor KV-cache optimization in autoregressive LLM inference.",
        "snippets": [
            "Excessive memory requirements of key and value features (KV-cache) present\nsignificant challenges in the autoregressive inference of large language models\n(LLMs), restricting both the speed and length of text generation. Approaches\nsuch as Multi-Query Attention (MQA) and Grouped Query Attention (GQA) mitigate\nthese challenges by grouping query heads and consequently reducing the number\nof corresponding key and value heads. However, MQA and GQA decrease the\nKV-cache size requirements at the expense of LLM accuracy (quality of text\ngeneration). These methods do not ensure an optimal tradeoff between KV-cache\nsize and text generation quality due to the absence of quality-aware grouping\nof query heads. To address this issue, we propose Quality and Capacity-Aware\nGrouped Query Attention (QCQA), which identifies optimal query head groupings\nusing an evolutionary algorithm with a computationally efficient and\ninexpensive fitness function. We demonstrate that QCQA achieves a significantly\nbetter tradeoff between KV-cache capacity and LLM accuracy compared to GQA. For\nthe Llama2 $7\\,$B model, QCQA achieves $\\mathbf{20}$\\% higher accuracy than GQA\nwith similar KV-cache size requirements in the absence of fine-tuning. After\nfine-tuning both QCQA and GQA, for a similar KV-cache size, QCQA provides\n$\\mathbf{10.55}\\,$\\% higher accuracy than GQA. Furthermore, QCQA requires\n$40\\,$\\% less KV-cache size than GQA to attain similar accuracy. The proposed\nquality and capacity-aware grouping of query heads can serve as a new paradigm\nfor KV-cache optimization in autoregressive LLM inference."
        ],
        "title": "QCQA: Quality and Capacity-aware grouped Query Attention",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2411.18077v3": {
        "url": "http://arxiv.org/abs/2411.18077v3",
        "description": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
        "snippets": [
            "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
        ],
        "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2406.19707v1": {
        "url": "http://arxiv.org/abs/2406.19707v1",
        "description": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance across various natural language processing tasks. Serving LLM\ninference for generating long contents, however, poses a challenge due to the\nenormous memory footprint of the transient state, known as the key-value (KV)\ncache, which scales with the sequence length and batch size. In this paper, we\npresent InfiniGen, a novel KV cache management framework tailored for long-text\ngeneration, which synergistically works with modern offloading-based inference\nsystems. InfiniGen leverages the key insight that a few important tokens that\nare essential for computing the subsequent attention layer in the Transformer\ncan be speculated by performing a minimal rehearsal with the inputs of the\ncurrent layer and part of the query weight and key cache of the subsequent\nlayer. This allows us to prefetch only the essential KV cache entries (without\nfetching them all), thereby mitigating the fetch overhead from the host memory\nin offloading-based LLM serving systems. Our evaluation on several\nrepresentative LLMs shows that InfiniGen improves the overall performance of a\nmodern offloading-based system by up to 3.00x compared to prior KV cache\nmanagement methods while offering substantially better model accuracy.",
        "snippets": [
            "Transformer-based large language models (LLMs) demonstrate impressive\nperformance across various natural language processing tasks. Serving LLM\ninference for generating long contents, however, poses a challenge due to the\nenormous memory footprint of the transient state, known as the key-value (KV)\ncache, which scales with the sequence length and batch size. In this paper, we\npresent InfiniGen, a novel KV cache management framework tailored for long-text\ngeneration, which synergistically works with modern offloading-based inference\nsystems. InfiniGen leverages the key insight that a few important tokens that\nare essential for computing the subsequent attention layer in the Transformer\ncan be speculated by performing a minimal rehearsal with the inputs of the\ncurrent layer and part of the query weight and key cache of the subsequent\nlayer. This allows us to prefetch only the essential KV cache entries (without\nfetching them all), thereby mitigating the fetch overhead from the host memory\nin offloading-based LLM serving systems. Our evaluation on several\nrepresentative LLMs shows that InfiniGen improves the overall performance of a\nmodern offloading-based system by up to 3.00x compared to prior KV cache\nmanagement methods while offering substantially better model accuracy."
        ],
        "title": "InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.18773v1": {
        "url": "http://arxiv.org/abs/2503.18773v1",
        "description": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding.",
        "snippets": [
            "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding."
        ],
        "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with Low-Bit KV Cache",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.03756v1": {
        "url": "http://arxiv.org/abs/2505.03756v1",
        "description": "Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for\ntask-specific Large Language Model (LLM) applications. For multi-LoRA serving,\ncaching hot KV caches and LoRA adapters in high bandwidth memory of\naccelerations can improve inference performance. However, existing Multi-LoRA\ninference systems fail to optimize serving performance like Time-To-First-Toke\n(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore\npropose FASTLIBRA, a Multi-LoRA caching system to optimize the serving\nperformance. FASTLIBRA comprises a dependency-aware cache manager and a\nperformance-driven cache swapper. The cache manager maintains the usage\ndependencies between LoRAs and KV caches during the inference with a unified\ncaching pool. The cache swapper determines the swap-in or out of LoRAs and KV\ncaches based on a unified cost model, when the HBM is idle or busy,\nrespectively. Experimental results show that ELORA reduces the TTFT by 63.4% on\naverage, compared to state-of-the-art works.",
        "snippets": [
            "Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for\ntask-specific Large Language Model (LLM) applications. For multi-LoRA serving,\ncaching hot KV caches and LoRA adapters in high bandwidth memory of\naccelerations can improve inference performance. However, existing Multi-LoRA\ninference systems fail to optimize serving performance like Time-To-First-Toke\n(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore\npropose FASTLIBRA, a Multi-LoRA caching system to optimize the serving\nperformance. FASTLIBRA comprises a dependency-aware cache manager and a\nperformance-driven cache swapper. The cache manager maintains the usage\ndependencies between LoRAs and KV caches during the inference with a unified\ncaching pool. The cache swapper determines the swap-in or out of LoRAs and KV\ncaches based on a unified cost model, when the HBM is idle or busy,\nrespectively. Experimental results show that ELORA reduces the TTFT by 63.4% on\naverage, compared to state-of-the-art works."
        ],
        "title": "Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.00161v2": {
        "url": "http://arxiv.org/abs/2410.00161v2",
        "description": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.",
        "snippets": [
            "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches."
        ],
        "title": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates per Attention Head",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2501.06709v1": {
        "url": "http://arxiv.org/abs/2501.06709v1",
        "description": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems.",
        "snippets": [
            "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems."
        ],
        "title": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV Cache Management",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.08461v1": {
        "url": "http://arxiv.org/abs/2503.08461v1",
        "description": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression.",
        "snippets": [
            "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression."
        ],
        "title": "FastCache: Optimizing Multimodal LLM Serving through Lightweight KV-Cache Compression Framework",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2402.02750v2": {
        "url": "http://arxiv.org/abs/2402.02750v2",
        "description": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
        "snippets": [
            "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
        ],
        "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2310.07240v6": {
        "url": "http://arxiv.org/abs/2310.07240v6",
        "description": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen.",
        "snippets": [
            "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen."
        ],
        "title": "CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.16525v2": {
        "url": "http://arxiv.org/abs/2503.16525v2",
        "description": "Recent advances in long-text understanding have pushed the context length of\nlarge language models (LLMs) up to one million tokens. It boosts LLMs's\naccuracy and reasoning capacity but causes exorbitant computational costs and\nunsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the\nexact same KV cache of prefixes and templates or shares similar ones but with\nextra selective recomputation, offers a promising way to tackle this issue.\nHowever, prior studies overlook the cross-request KV reuse and the attention\ndeviations introduced by new tokens during the decoding stage. In this paper,\nwe present a KV cache management module that shares the KV cache across\nrequests under multi-tenant scenarios without sacrificing model accuracy. Our\nsystem, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage\nHigh Deviation algorithm (DHD) that conditionally selects a small portion of KV\ncache to be recomputed during both prefill and decode phases, and 2) a\ncache-aware scheduler that prioritizes requests based on their KV cache hit\nrates and orchestrates continuous batching to achieve enhanced system\nefficiency and faster TTFT. Multi-task experiments conducted on models such as\nQwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up\nto 9.39x and increases 1.2x of the throughput compared to the full KV\nrecompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy\ncompared to SOTA methods.",
        "snippets": [
            "Recent advances in long-text understanding have pushed the context length of\nlarge language models (LLMs) up to one million tokens. It boosts LLMs's\naccuracy and reasoning capacity but causes exorbitant computational costs and\nunsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the\nexact same KV cache of prefixes and templates or shares similar ones but with\nextra selective recomputation, offers a promising way to tackle this issue.\nHowever, prior studies overlook the cross-request KV reuse and the attention\ndeviations introduced by new tokens during the decoding stage. In this paper,\nwe present a KV cache management module that shares the KV cache across\nrequests under multi-tenant scenarios without sacrificing model accuracy. Our\nsystem, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage\nHigh Deviation algorithm (DHD) that conditionally selects a small portion of KV\ncache to be recomputed during both prefill and decode phases, and 2) a\ncache-aware scheduler that prioritizes requests based on their KV cache hit\nrates and orchestrates continuous batching to achieve enhanced system\nefficiency and faster TTFT. Multi-task experiments conducted on models such as\nQwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up\nto 9.39x and increases 1.2x of the throughput compared to the full KV\nrecompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy\ncompared to SOTA methods."
        ],
        "title": "KVShare: An LLM Service System with Efficient and Effective Multi-Tenant KV Cache Reuse",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.23970v1": {
        "url": "http://arxiv.org/abs/2505.23970v1",
        "description": "As large language models (LLMs) become widely used, their environmental\nimpact$\\unicode{x2014}$especially carbon emissions$\\unicode{x2014}$has\nattracted more attention. Prior studies focus on compute-related carbon\nemissions. In this paper, we find that storage is another key contributor. LLM\ncaching, which saves and reuses KV caches for repeated context, reduces\noperational carbon by avoiding redundant computation. However, this benefit\ncomes at the cost of embodied carbon from high-capacity, high-speed SSDs. As\nLLMs scale, the embodied carbon of storage grows significantly.\n  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching\nframework that selects the optimal cache size for LLM serving. EmbAdvisor\nprofiles different LLM tasks and uses an Integer Linear Programming (ILP)\nsolver to select cache sizes that meet SLOs while minimizing total carbon\nemissions. Overall, EmbAdvisor reduces the average carbon emissions of a\nLlama-3 70B model by 9.5% under various carbon intensities compared to a\nnon-adaptive cache scenario, and can save up to 31.2% when the carbon intensity\nis low.",
        "snippets": [
            "As large language models (LLMs) become widely used, their environmental\nimpact$\\unicode{x2014}$especially carbon emissions$\\unicode{x2014}$has\nattracted more attention. Prior studies focus on compute-related carbon\nemissions. In this paper, we find that storage is another key contributor. LLM\ncaching, which saves and reuses KV caches for repeated context, reduces\noperational carbon by avoiding redundant computation. However, this benefit\ncomes at the cost of embodied carbon from high-capacity, high-speed SSDs. As\nLLMs scale, the embodied carbon of storage grows significantly.\n  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching\nframework that selects the optimal cache size for LLM serving. EmbAdvisor\nprofiles different LLM tasks and uses an Integer Linear Programming (ILP)\nsolver to select cache sizes that meet SLOs while minimizing total carbon\nemissions. Overall, EmbAdvisor reduces the average carbon emissions of a\nLlama-3 70B model by 9.5% under various carbon intensities compared to a\nnon-adaptive cache scenario, and can save up to 31.2% when the carbon intensity\nis low."
        ],
        "title": "EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2412.19442v2": {
        "url": "http://arxiv.org/abs/2412.19442v2",
        "description": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
        "snippets": [
            "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
        ],
        "title": "A Survey on Large Language Model Acceleration based on KV Cache Management",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2412.11741v1": {
        "url": "http://arxiv.org/abs/2412.11741v1",
        "description": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
        "snippets": [
            "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
        ],
        "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2412.10319v2": {
        "url": "http://arxiv.org/abs/2412.10319v2",
        "description": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
        "snippets": [
            "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
        ],
        "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2403.19708v3": {
        "url": "http://arxiv.org/abs/2403.19708v3",
        "description": "Interacting with humans through multi-turn conversations is a fundamental\nfeature of large language models (LLMs). However, existing LLM serving engines\nexecuting multi-turn conversations are inefficient due to the need to\nrepeatedly compute the key-value (KV) caches of historical tokens, incurring\nhigh serving costs. To address the problem, this paper proposes\nCachedAttention, a new attention mechanism that enables reuse of KV caches\nacross multi-turn conversations, significantly reducing the repetitive\ncomputation overheads. CachedAttention maintains a hierarchical KV caching\nsystem that leverages cost-effective memory/storage mediums to save KV caches\nfor all requests. To reduce KV cache access overheads from slow mediums,\nCachedAttention employs layer-wise pre-loading and asynchronous saving schemes\nto overlap the KV cache access with the GPU computation. To ensure that the KV\ncaches to be accessed are placed in the fastest hierarchy, CachedAttention\nemploys scheduler-aware fetching and eviction schemes to consciously place the\nKV caches in different layers based on the hints from the inference job\nscheduler. To avoid the invalidation of the saved KV caches incurred by context\nwindow overflow, CachedAttention enables the saved KV caches to remain valid\nvia decoupling the positional encoding and effectively truncating the KV\ncaches. Extensive experimental results demonstrate that CachedAttention\nsignificantly decreases the time to the first token (TTFT) by up to 87%,\nimproves the prompt prefilling throughput by up to 7.8$\\times$ for multi-turn\nconversations, and reduces the end-to-end inference cost by up to 70%.",
        "snippets": [
            "Interacting with humans through multi-turn conversations is a fundamental\nfeature of large language models (LLMs). However, existing LLM serving engines\nexecuting multi-turn conversations are inefficient due to the need to\nrepeatedly compute the key-value (KV) caches of historical tokens, incurring\nhigh serving costs. To address the problem, this paper proposes\nCachedAttention, a new attention mechanism that enables reuse of KV caches\nacross multi-turn conversations, significantly reducing the repetitive\ncomputation overheads. CachedAttention maintains a hierarchical KV caching\nsystem that leverages cost-effective memory/storage mediums to save KV caches\nfor all requests. To reduce KV cache access overheads from slow mediums,\nCachedAttention employs layer-wise pre-loading and asynchronous saving schemes\nto overlap the KV cache access with the GPU computation. To ensure that the KV\ncaches to be accessed are placed in the fastest hierarchy, CachedAttention\nemploys scheduler-aware fetching and eviction schemes to consciously place the\nKV caches in different layers based on the hints from the inference job\nscheduler. To avoid the invalidation of the saved KV caches incurred by context\nwindow overflow, CachedAttention enables the saved KV caches to remain valid\nvia decoupling the positional encoding and effectively truncating the KV\ncaches. Extensive experimental results demonstrate that CachedAttention\nsignificantly decreases the time to the first token (TTFT) by up to 87%,\nimproves the prompt prefilling throughput by up to 7.8$\\times$ for multi-turn\nconversations, and reduces the end-to-end inference cost by up to 70%."
        ],
        "title": "Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.22913v1": {
        "url": "http://arxiv.org/abs/2505.22913v1",
        "description": "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar.",
        "snippets": [
            "We demonstrate that unstructured sparsity significantly improves KV cache\ncompression for LLMs, enabling sparsity levels up to 70% without compromising\naccuracy or requiring fine-tuning. We conduct a systematic exploration of\npruning strategies and find per-token magnitude-based pruning as highly\neffective for both Key and Value caches under unstructured sparsity, surpassing\nprior structured pruning schemes. The Key cache benefits from prominent outlier\nelements, while the Value cache surprisingly benefits from a simple\nmagnitude-based pruning despite its uniform distribution. KV cache size is the\nmajor bottleneck in decode performance due to high memory overhead for large\ncontext lengths. To address this, we use a bitmap-based sparse format and a\ncustom attention kernel capable of compressing and directly computing over\ncompressed caches pruned to arbitrary sparsity patterns, significantly\naccelerating memory-bound operations in decode computations and thereby\ncompensating for the overhead of runtime pruning and compression. Our custom\nattention kernel coupled with the bitmap-based format delivers substantial\ncompression of KV cache upto 45% of dense inference and thereby enables longer\ncontext length and increased tokens/sec throughput of upto 2.23x compared to\ndense inference. Our pruning mechanism and sparse attention kernel is available\nat https://github.com/dhjoo98/mustafar."
        ],
        "title": "Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2405.05329v2": {
        "url": "http://arxiv.org/abs/2405.05329v2",
        "description": "Large Language Model or LLM inference has two phases, the prompt (or prefill)\nphase to output the first token and the extension (or decoding) phase to the\ngenerate subsequent tokens. In this work, we propose an efficient\nparallelization scheme, KV-Runahead to accelerate the prompt phase. The key\nobservation is that the extension phase generates tokens faster than the prompt\nphase because of key-value cache (KV-cache). Hence, KV-Runahead parallelizes\nthe prompt phase by orchestrating multiple processes to populate the KV-cache\nand minimizes the time-to-first-token (TTFT). Dual-purposing the KV-cache\nscheme has two main benefits. First, since KV-cache is designed to leverage the\ncausal attention map, we minimize computation and computation automatically.\nSecond, since it already exists for the extension phase, KV-Runahead is easy to\nimplement. We further propose context-level load-balancing to handle uneven\nKV-cache generation (due to the causal attention) and to optimize TTFT.\nCompared with an existing parallelization scheme such as tensor or sequential\nparallelization where keys and values are locally generated and exchanged via\nall-gather collectives, our experimental results demonstrate that KV-Runahead\ncan offer over 1.4x and 1.6x speedups for Llama 7B and Falcon 7B respectively.",
        "snippets": [
            "Large Language Model or LLM inference has two phases, the prompt (or prefill)\nphase to output the first token and the extension (or decoding) phase to the\ngenerate subsequent tokens. In this work, we propose an efficient\nparallelization scheme, KV-Runahead to accelerate the prompt phase. The key\nobservation is that the extension phase generates tokens faster than the prompt\nphase because of key-value cache (KV-cache). Hence, KV-Runahead parallelizes\nthe prompt phase by orchestrating multiple processes to populate the KV-cache\nand minimizes the time-to-first-token (TTFT). Dual-purposing the KV-cache\nscheme has two main benefits. First, since KV-cache is designed to leverage the\ncausal attention map, we minimize computation and computation automatically.\nSecond, since it already exists for the extension phase, KV-Runahead is easy to\nimplement. We further propose context-level load-balancing to handle uneven\nKV-cache generation (due to the causal attention) and to optimize TTFT.\nCompared with an existing parallelization scheme such as tensor or sequential\nparallelization where keys and values are locally generated and exchanged via\nall-gather collectives, our experimental results demonstrate that KV-Runahead\ncan offer over 1.4x and 1.6x speedups for Llama 7B and Falcon 7B respectively."
        ],
        "title": "KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2407.11550v4": {
        "url": "http://arxiv.org/abs/2407.11550v4",
        "description": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods.",
        "snippets": [
            "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods."
        ],
        "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2504.02921v1": {
        "url": "http://arxiv.org/abs/2504.02921v1",
        "description": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service.",
        "snippets": [
            "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service."
        ],
        "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented Generation with Reranker KV-Cache Reuse",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.03065v2": {
        "url": "http://arxiv.org/abs/2410.03065v2",
        "description": "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments.",
        "snippets": [
            "Large Language Models (LLMs) are increasingly deployed in large-scale online\nservices, enabling sophisticated applications. However, the computational\noverhead of generating key-value (KV) caches in the prefill stage presents a\nmajor bottleneck, particularly for long-context inputs. Prefix caching\nmitigates this issue by storing KV caches for reuse, reducing redundant\ncomputation. Despite its advantages, prefix caching suffers from high latency\ndue to the limited I/O bandwidth of storage devices, constraining inference\nefficiency. To address this challenge, we introduce Cake, a novel KV cache\nloading system that optimally utilizes both computational and I/O resources in\nparallel. Cake employs a bidirectional scheduling strategy that dynamically\nbalances KV cache computation and loading, ensuring efficient resource\nutilization. Additionally, Cake incorporates an adaptive scheduling mechanism\nthat seamlessly integrates with non-prefix caching requests, improving system\nthroughput and adapting to fluctuating resource availabilty. Through extensive\nevaluations across various hardware configurations, datasets, and storage\nconditions, Cake achieves on average 2.6x reduction in Time to First Token\n(TTFT) compared to compute-only and I/O-only methods. Our findings highlight\nCake as an effective and practical solution for optimizing long-context LLM\ninference, bridging the gap between computation and I/O efficiency in\nlarge-scale AI deployments."
        ],
        "title": "Compute Or Load KV Cache? Why Not Both?",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.14051v1": {
        "url": "http://arxiv.org/abs/2502.14051v1",
        "description": "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks.",
        "snippets": [
            "Transformer-based Large Language Models rely critically on KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy designed\nspecifically to reduce both memory bandwidth and capacity demand of KV cache\nduring the decode phase. RocketKV contains two consecutive stages. In the first\nstage, it performs coarse-grain KV cache eviction on the input sequence tokens\nwith SnapKV++, a method improved upon SnapKV by introducing adaptive pooling\nsize and full compatibility with grouped-query attention. In the second stage,\nit adopts a hybrid attention method to conduct fine-grain top-k sparse\nattention, approximating the attention scores by leveraging both head and\nsequence dimensional reductions. Combining these two stages, RocketKV achieves\nsignificant KV cache fetching bandwidth and storage savings while maintaining\ncomparable accuracy to full KV cache attention. We show that RocketKV provides\nend-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to\n31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache\nbaseline, while achieving negligible accuracy loss on a variety of long-context\ntasks."
        ],
        "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.00392v1": {
        "url": "http://arxiv.org/abs/2503.00392v1",
        "description": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively.",
        "snippets": [
            "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively."
        ],
        "title": "Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2409.13761v2": {
        "url": "http://arxiv.org/abs/2409.13761v2",
        "description": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
        "snippets": [
            "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
        ],
        "title": "Do Large Language Models Need a Content Delivery Network?",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2406.18139v1": {
        "url": "http://arxiv.org/abs/2406.18139v1",
        "description": "Long-context Multimodal Large Language Models (MLLMs) demand substantial\ncomputational resources for inference as the growth of their multimodal\nKey-Value (KV) cache, in response to increasing input lengths, challenges\nmemory and time efficiency. Unlike single-modality LLMs that manage only\ntextual contexts, the KV cache of long-context MLLMs includes representations\nfrom multiple images with temporal and spatial relationships and related\ntextual contexts. The predominance of image tokens means traditional\noptimizations for LLMs' KV caches are unsuitable for multimodal long-context\nsettings, and no prior works have addressed this challenge. In this work, we\nintroduce LOOK-M, a pioneering, fine-tuning-free approach that efficiently\nreduces the multimodal KV cache size while maintaining performance comparable\nto a full cache. We observe that during prompt prefill, the model prioritizes\nmore textual attention over image features, and based on the multimodal\ninteraction observation, a new proposed text-prior method is explored to\ncompress the KV cache. Furthermore, to mitigate the degradation of image\ncontextual information, we propose several compensatory strategies using KV\npairs merging. LOOK-M demonstrates that with a significant reduction in KV\nCache memory usage, such as reducing it by 80% in some cases, it not only\nachieves up to 1.5x faster decoding but also maintains or even enhances\nperformance across a variety of long context multimodal tasks.",
        "snippets": [
            "Long-context Multimodal Large Language Models (MLLMs) demand substantial\ncomputational resources for inference as the growth of their multimodal\nKey-Value (KV) cache, in response to increasing input lengths, challenges\nmemory and time efficiency. Unlike single-modality LLMs that manage only\ntextual contexts, the KV cache of long-context MLLMs includes representations\nfrom multiple images with temporal and spatial relationships and related\ntextual contexts. The predominance of image tokens means traditional\noptimizations for LLMs' KV caches are unsuitable for multimodal long-context\nsettings, and no prior works have addressed this challenge. In this work, we\nintroduce LOOK-M, a pioneering, fine-tuning-free approach that efficiently\nreduces the multimodal KV cache size while maintaining performance comparable\nto a full cache. We observe that during prompt prefill, the model prioritizes\nmore textual attention over image features, and based on the multimodal\ninteraction observation, a new proposed text-prior method is explored to\ncompress the KV cache. Furthermore, to mitigate the degradation of image\ncontextual information, we propose several compensatory strategies using KV\npairs merging. LOOK-M demonstrates that with a significant reduction in KV\nCache memory usage, such as reducing it by 80% in some cases, it not only\nachieves up to 1.5x faster decoding but also maintains or even enhances\nperformance across a variety of long context multimodal tasks."
        ],
        "title": "LOOK-M: Look-Once Optimization in KV Cache for Efficient Multimodal Long-Context Inference",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2405.12532v2": {
        "url": "http://arxiv.org/abs/2405.12532v2",
        "description": "Large Language Models (LLMs) have shown remarkable comprehension abilities\nbut face challenges in GPU memory usage during inference, hindering their\nscalability for real-time applications like chatbots. To accelerate inference,\nwe store computed keys and values (KV cache) in the GPU memory. Existing\nmethods study the KV cache compression to reduce memory by pruning the\npre-computed KV cache. However, they neglect the inter-layer dependency between\nlayers and huge memory consumption in pre-computation. To explore these\ndeficiencies, we find that the number of crucial keys and values that influence\nfuture generations decreases layer by layer and we can extract them by the\nconsistency in attention weights. Based on the findings, we propose\nPyramidInfer, a method that compresses the KV cache by layer-wise retaining\ncrucial context. PyramidInfer saves significant memory by computing fewer keys\nand values without sacrificing performance. Experimental results show\nPyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU\nmemory reduction in KV cache.",
        "snippets": [
            "Large Language Models (LLMs) have shown remarkable comprehension abilities\nbut face challenges in GPU memory usage during inference, hindering their\nscalability for real-time applications like chatbots. To accelerate inference,\nwe store computed keys and values (KV cache) in the GPU memory. Existing\nmethods study the KV cache compression to reduce memory by pruning the\npre-computed KV cache. However, they neglect the inter-layer dependency between\nlayers and huge memory consumption in pre-computation. To explore these\ndeficiencies, we find that the number of crucial keys and values that influence\nfuture generations decreases layer by layer and we can extract them by the\nconsistency in attention weights. Based on the findings, we propose\nPyramidInfer, a method that compresses the KV cache by layer-wise retaining\ncrucial context. PyramidInfer saves significant memory by computing fewer keys\nand values without sacrificing performance. Experimental results show\nPyramidInfer improves 2.2x throughput compared to Accelerate with over 54% GPU\nmemory reduction in KV cache."
        ],
        "title": "PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.10424v1": {
        "url": "http://arxiv.org/abs/2502.10424v1",
        "description": "Large Language Models (LLMs) are increasingly being deployed on edge devices\nfor long-context settings, creating a growing need for fast and efficient\nlong-context inference. In these scenarios, the Key-Value (KV) cache is the\nprimary bottleneck in terms of both GPU memory and latency, as the full KV\ncache must be loaded for each decoding step. While speculative decoding is a\nwidely accepted technique to accelerate autoregressive decoding, existing\nmethods often struggle to achieve significant speedups due to inefficient KV\ncache optimization strategies and result in low acceptance rates. To address\nthese challenges, we propose a novel self-speculative decoding framework,\nQuantSpec, where the draft model shares the architecture of the target model\nbut employs a hierarchical 4-bit quantized KV cache and 4-bit quantized weights\nfor acceleration. QuantSpec maintains high acceptance rates ($>$90%) and\nreliably provides consistent end-to-end speedups upto $\\sim2.5\\times$,\noutperforming other self-speculative decoding methods that use sparse KV cache\nfor long-context LLM inference. QuantSpec also reduces the memory requirements\nby $\\sim 1.3\\times$ compared to these alternatives.",
        "snippets": [
            "Large Language Models (LLMs) are increasingly being deployed on edge devices\nfor long-context settings, creating a growing need for fast and efficient\nlong-context inference. In these scenarios, the Key-Value (KV) cache is the\nprimary bottleneck in terms of both GPU memory and latency, as the full KV\ncache must be loaded for each decoding step. While speculative decoding is a\nwidely accepted technique to accelerate autoregressive decoding, existing\nmethods often struggle to achieve significant speedups due to inefficient KV\ncache optimization strategies and result in low acceptance rates. To address\nthese challenges, we propose a novel self-speculative decoding framework,\nQuantSpec, where the draft model shares the architecture of the target model\nbut employs a hierarchical 4-bit quantized KV cache and 4-bit quantized weights\nfor acceleration. QuantSpec maintains high acceptance rates ($>$90%) and\nreliably provides consistent end-to-end speedups upto $\\sim2.5\\times$,\noutperforming other self-speculative decoding methods that use sparse KV cache\nfor long-context LLM inference. QuantSpec also reduces the memory requirements\nby $\\sim 1.3\\times$ compared to these alternatives."
        ],
        "title": "QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2412.03131v2": {
        "url": "http://arxiv.org/abs/2412.03131v2",
        "description": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
        "snippets": [
            "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$."
        ],
        "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
        "meta": {
            "query": "benefits and challenges of KV cache in LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2402.06262v2": {
        "url": "http://arxiv.org/abs/2402.06262v2",
        "description": "Despite the recent success associated with Large Language Models (LLMs), they\nare notably cost-prohibitive to deploy in resource-constrained environments due\nto their excessive memory and computational demands. In addition to model\nparameters, the key-value cache is also stored in GPU memory, growing linearly\nwith batch size and sequence length. As a remedy, recent works have proposed\nvarious eviction policies for maintaining the overhead of key-value cache under\na given budget. This paper embarks on the efficacy of existing eviction\npolicies in terms of importance score calculation and eviction scope\nconstruction. We identify the deficiency of prior policies in these two aspects\nand introduce RoCo, a robust cache omission policy based on temporal attention\nscores and robustness measures. Extensive experimentation spanning prefilling\nand auto-regressive decoding stages validates the superiority of RoCo. Finally,\nwe release EasyKV, a versatile software package dedicated to user-friendly\nkey-value constrained generative inference. Code available at\nhttps://github.com/DRSY/EasyKV.",
        "snippets": [
            "Despite the recent success associated with Large Language Models (LLMs), they\nare notably cost-prohibitive to deploy in resource-constrained environments due\nto their excessive memory and computational demands. In addition to model\nparameters, the key-value cache is also stored in GPU memory, growing linearly\nwith batch size and sequence length. As a remedy, recent works have proposed\nvarious eviction policies for maintaining the overhead of key-value cache under\na given budget. This paper embarks on the efficacy of existing eviction\npolicies in terms of importance score calculation and eviction scope\nconstruction. We identify the deficiency of prior policies in these two aspects\nand introduce RoCo, a robust cache omission policy based on temporal attention\nscores and robustness measures. Extensive experimentation spanning prefilling\nand auto-regressive decoding stages validates the superiority of RoCo. Finally,\nwe release EasyKV, a versatile software package dedicated to user-friendly\nkey-value constrained generative inference. Code available at\nhttps://github.com/DRSY/EasyKV."
        ],
        "title": "On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.12876v3": {
        "url": "http://arxiv.org/abs/2410.12876v3",
        "description": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance.",
        "snippets": [
            "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system. This paper enables a novel dynamic\nKV-Cache eviction policy by injecting a lightweight module called\nAttention-Gate to the model. It accepts the global context as input and yields\neviction flags for each token. The self-attention modules in the model proceed\naccording to the flags and cache only a subset of the KV states for next token\nprediction. The Attention-Gates can yield various flags for different heads and\nlayers and be easily tuned on top of a pre-trained LLM via continual\npre-training or supervised fine-tuning. The computational and memory overhead\nintroduced by Attention-Gates can be minimal. We empirically evaluate the\nproposed approach across multiple scenarios, showing that effective eviction of\nredundant tokens can not only improve efficiency but also enhance performance."
        ],
        "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.01805v2": {
        "url": "http://arxiv.org/abs/2410.01805v2",
        "description": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.",
        "snippets": [
            "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training."
        ],
        "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads on Consumer-Grade Devices",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.20334v1": {
        "url": "http://arxiv.org/abs/2505.20334v1",
        "description": "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements.",
        "snippets": [
            "Large language models (LLMs) rely on key-value cache (KV cache) to accelerate\ndecoding by reducing redundant computations. However, the KV cache memory usage\ngrows substantially with longer text sequences, posing challenges for efficient\ndeployment. Existing KV cache eviction methods prune tokens using\nprefilling-stage attention scores, causing inconsistency with actual inference\nqueries, especially under tight memory budgets. In this paper, we propose\nLookahead Q-Cache (LAQ), a novel eviction framework that generates low-cost\npseudo lookahead queries to better approximate the true decoding-stage queries.\nBy using these lookahead queries as the observation window for importance\nestimation, LAQ achieves more consistent and accurate KV cache eviction aligned\nwith real inference scenarios. Experimental results on LongBench and\nNeedle-in-a-Haystack benchmarks show that LAQ outperforms existing methods\nacross various budget levels, achieving a 1 $\\sim$ 4 point improvement on\nLongBench under limited cache budget. Moreover, LAQ is complementary to\nexisting approaches and can be flexibly combined to yield further improvements."
        ],
        "title": "Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2408.03675v2": {
        "url": "http://arxiv.org/abs/2408.03675v2",
        "description": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
        "snippets": [
            "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
        ],
        "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at Inference Time",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2504.14051v2": {
        "url": "http://arxiv.org/abs/2504.14051v2",
        "description": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
        "snippets": [
            "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
        ],
        "title": "CAOTE: KV Caching through Attention Output Error based Token Eviction",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.13176v2": {
        "url": "http://arxiv.org/abs/2502.13176v2",
        "description": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
        "snippets": [
            "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels."
        ],
        "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.12491v1": {
        "url": "http://arxiv.org/abs/2503.12491v1",
        "description": "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv.",
        "snippets": [
            "Large language models (LLMs) excel at processing long sequences, boosting\ndemand for key-value (KV) caching. While recent efforts to evict KV cache have\nalleviated the inference burden, they often fail to allocate resources\nrationally across layers with different attention patterns. In this paper, we\nintroduce Cascading and Adaptive KV cache Eviction (CAKE), a novel approach\nthat frames KV cache eviction as a \"cake-slicing problem.\" CAKE assesses\nlayer-specific preferences by considering attention dynamics in both spatial\nand temporal dimensions, allocates rational cache size for layers accordingly,\nand manages memory constraints in a cascading manner. This approach enables a\nglobal view of cache allocation, adaptively distributing resources across\ndiverse attention mechanisms while maintaining memory budgets. CAKE also\nemploys a new eviction indicator that considers the shifting importance of\ntokens over time, addressing limitations in existing methods that overlook\ntemporal dynamics. Comprehensive experiments on LongBench and NeedleBench show\nthat CAKE maintains model performance with only 3.2% of the KV cache and\nconsistently outperforms current baselines across various models and memory\nconstraints, particularly in low-memory settings. Additionally, CAKE achieves\nover 10x speedup in decoding latency compared to full cache when processing\ncontexts of 128K tokens with FlashAttention-2. Our code is available at\nhttps://github.com/antgroup/cakekv."
        ],
        "title": "CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2501.06807v1": {
        "url": "http://arxiv.org/abs/2501.06807v1",
        "description": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively.",
        "snippets": [
            "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively."
        ],
        "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large Language Model Inference",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2009.09206v1": {
        "url": "http://arxiv.org/abs/2009.09206v1",
        "description": "Recent approaches for learning policies to improve caching, target just one\nout of the prefetching, admission and eviction processes. In contrast, we\npropose an end to end pipeline to learn all three policies using machine\nlearning. We also take inspiration from the success of pretraining on large\ncorpora to learn specialized embeddings for the task. We model prefetching as a\nsequence prediction task based on past misses. Following previous works\nsuggesting that frequency and recency are the two orthogonal fundamental\nattributes for caching, we use an online reinforcement learning technique to\nlearn the optimal policy distribution between two orthogonal eviction\nstrategies based on them. While previous approaches used the past as an\nindicator of the future, we instead explicitly model the future frequency and\nrecency in a multi-task fashion with prefetching, leveraging the abilities of\ndeep networks to capture futuristic trends and use them for learning eviction\nand admission. We also model the distribution of the data in an online fashion\nusing Kernel Density Estimation in our approach, to deal with the problem of\ncaching non-stationary data. We present our approach as a \"proof of concept\" of\nlearning all three components of cache strategies using machine learning and\nleave improving practical deployment for future work.",
        "snippets": [
            "Recent approaches for learning policies to improve caching, target just one\nout of the prefetching, admission and eviction processes. In contrast, we\npropose an end to end pipeline to learn all three policies using machine\nlearning. We also take inspiration from the success of pretraining on large\ncorpora to learn specialized embeddings for the task. We model prefetching as a\nsequence prediction task based on past misses. Following previous works\nsuggesting that frequency and recency are the two orthogonal fundamental\nattributes for caching, we use an online reinforcement learning technique to\nlearn the optimal policy distribution between two orthogonal eviction\nstrategies based on them. While previous approaches used the past as an\nindicator of the future, we instead explicitly model the future frequency and\nrecency in a multi-task fashion with prefetching, leveraging the abilities of\ndeep networks to capture futuristic trends and use them for learning eviction\nand admission. We also model the distribution of the data in an online fashion\nusing Kernel Density Estimation in our approach, to deal with the problem of\ncaching non-stationary data. We present our approach as a \"proof of concept\" of\nlearning all three components of cache strategies using machine learning and\nleave improving practical deployment for future work."
        ],
        "title": "DEAP Cache: Deep Eviction Admission and Prefetching for Cache",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2402.18096v1": {
        "url": "http://arxiv.org/abs/2402.18096v1",
        "description": "Key-Value (KV) Caching has become an essential technique for accelerating the\ninference speed and throughput of generative Large Language Models~(LLMs).\nHowever, the memory footprint of the KV cache poses a critical bottleneck in\nLLM deployment as the cache size grows with batch size and sequence length,\noften surpassing even the size of the model itself. Although recent methods\nwere proposed to select and evict unimportant KV pairs from the cache to reduce\nmemory consumption, the potential ramifications of eviction on the generative\nprocess are yet to be thoroughly examined. In this paper, we examine the\ndetrimental impact of cache eviction and observe that unforeseen risks arise as\nthe information contained in the KV pairs is exhaustively discarded, resulting\nin safety breaches, hallucinations, and context loss. Surprisingly, we find\nthat preserving even a small amount of information contained in the evicted KV\npairs via reduced precision quantization substantially recovers the incurred\ndegradation. On the other hand, we observe that the important KV pairs must be\nkept at a relatively higher precision to safeguard the generation quality.\nMotivated by these observations, we propose \\textit{Mixed-precision KV\ncache}~(MiKV), a reliable cache compression method that simultaneously\npreserves the context details by retaining the evicted KV pairs in\nlow-precision and ensure generation quality by keeping the important KV pairs\nin high-precision. Experiments on diverse benchmarks and LLM backbones show\nthat our proposed method offers a state-of-the-art trade-off between\ncompression ratio and performance, compared to other baselines.",
        "snippets": [
            "Key-Value (KV) Caching has become an essential technique for accelerating the\ninference speed and throughput of generative Large Language Models~(LLMs).\nHowever, the memory footprint of the KV cache poses a critical bottleneck in\nLLM deployment as the cache size grows with batch size and sequence length,\noften surpassing even the size of the model itself. Although recent methods\nwere proposed to select and evict unimportant KV pairs from the cache to reduce\nmemory consumption, the potential ramifications of eviction on the generative\nprocess are yet to be thoroughly examined. In this paper, we examine the\ndetrimental impact of cache eviction and observe that unforeseen risks arise as\nthe information contained in the KV pairs is exhaustively discarded, resulting\nin safety breaches, hallucinations, and context loss. Surprisingly, we find\nthat preserving even a small amount of information contained in the evicted KV\npairs via reduced precision quantization substantially recovers the incurred\ndegradation. On the other hand, we observe that the important KV pairs must be\nkept at a relatively higher precision to safeguard the generation quality.\nMotivated by these observations, we propose \\textit{Mixed-precision KV\ncache}~(MiKV), a reliable cache compression method that simultaneously\npreserves the context details by retaining the evicted KV pairs in\nlow-precision and ensure generation quality by keeping the important KV pairs\nin high-precision. Experiments on diverse benchmarks and LLM backbones show\nthat our proposed method offers a state-of-the-art trade-off between\ncompression ratio and performance, compared to other baselines."
        ],
        "title": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2404.15949v2": {
        "url": "http://arxiv.org/abs/2404.15949v2",
        "description": "Large Language Models (LLMs), despite their remarkable performance across a\nwide range of tasks, necessitate substantial GPU memory and consume significant\ncomputational resources. Beyond the memory taken up by model weights, the\nmemory used by the KV cache rises linearly with sequence length, becoming a\nprimary bottleneck for inference. In this paper, we introduce an innovative\nmethod for optimizing the KV cache, which considerably minimizes its memory\nfootprint. Upon thorough investigation, we discover that in most Transformer\nmodels, (i) there is a striking similarity between adjacent tokens' query\nvectors, and (ii) the attention calculation of the current query can rely\nexclusively on the attention information of a small fraction of preceding\nqueries. Based on these observations, we present CORM, a KV cache eviction\npolicy that dynamically retains essential key-value pairs for inference without\nthe need for model fine-tuning. Our validation shows that CORM reduces the\ninference memory usage of KV cache by up to 70\\% with negligible performance\ndegradation across six tasks in LongBench. Furthermore, we demonstrate that\nCORM is compatible with GQA for further compression rate.",
        "snippets": [
            "Large Language Models (LLMs), despite their remarkable performance across a\nwide range of tasks, necessitate substantial GPU memory and consume significant\ncomputational resources. Beyond the memory taken up by model weights, the\nmemory used by the KV cache rises linearly with sequence length, becoming a\nprimary bottleneck for inference. In this paper, we introduce an innovative\nmethod for optimizing the KV cache, which considerably minimizes its memory\nfootprint. Upon thorough investigation, we discover that in most Transformer\nmodels, (i) there is a striking similarity between adjacent tokens' query\nvectors, and (ii) the attention calculation of the current query can rely\nexclusively on the attention information of a small fraction of preceding\nqueries. Based on these observations, we present CORM, a KV cache eviction\npolicy that dynamically retains essential key-value pairs for inference without\nthe need for model fine-tuning. Our validation shows that CORM reduces the\ninference memory usage of KV cache by up to 70\\% with negligible performance\ndegradation across six tasks in LongBench. Furthermore, we demonstrate that\nCORM is compatible with GQA for further compression rate."
        ],
        "title": "CORM: Cache Optimization with Recent Message for Large Language Model Inference",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.08879v1": {
        "url": "http://arxiv.org/abs/2503.08879v1",
        "description": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest.",
        "snippets": [
            "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest."
        ],
        "title": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for Efficient Long-Context Inference",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2411.19379v3": {
        "url": "http://arxiv.org/abs/2411.19379v3",
        "description": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
        "snippets": [
            "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
        ],
        "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2412.05693v1": {
        "url": "http://arxiv.org/abs/2412.05693v1",
        "description": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
        "snippets": [
            "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
        ],
        "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1801.00390v1": {
        "url": "http://arxiv.org/abs/1801.00390v1",
        "description": "The information centric networks (ICN) can be viewed as a network of caches.\nConversely, ICN type of cache networks has distinctive features e.g, contents\npopularity, usability time of content and other factors inflicts some diverse\nrequirements for cache eviction policies. In this paper we defined four\nimportant characteristics of a suitable eviction policy for ICN. We analysed\nwell known eviction policies in view of defined characteristics. Based upon\nanalysis we propose a new eviction scheme which is well suitable for ICN type\nof cache networks.",
        "snippets": [
            "The information centric networks (ICN) can be viewed as a network of caches.\nConversely, ICN type of cache networks has distinctive features e.g, contents\npopularity, usability time of content and other factors inflicts some diverse\nrequirements for cache eviction policies. In this paper we defined four\nimportant characteristics of a suitable eviction policy for ICN. We analysed\nwell known eviction policies in view of defined characteristics. Based upon\nanalysis we propose a new eviction scheme which is well suitable for ICN type\nof cache networks."
        ],
        "title": "Time Aware Least Recent Used (TLRU) Cache Management Policy in ICN",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2402.09398v2": {
        "url": "http://arxiv.org/abs/2402.09398v2",
        "description": "Many computational factors limit broader deployment of large language models.\nIn this paper, we focus on a memory bottleneck imposed by the key-value (KV)\ncache, a computational shortcut that requires storing previous KV pairs during\ndecoding. While existing KV cache methods approach this problem by pruning or\nevicting large swaths of relatively less important KV pairs to dramatically\nreduce the memory footprint of the cache, they can have limited success in\ntasks that require recollecting a majority of previous tokens. To alleviate\nthis issue, we propose LESS, a simple integration of a (nearly free) constant\nsized cache with eviction-based cache methods, such that all tokens can be\nqueried at later decoding steps. Its ability to retain information throughout\ntime shows merit on a variety of tasks where we demonstrate LESS can help\nreduce the performance gap from caching everything, sometimes even matching it,\nall while being efficient. Relevant code can be found at\nhttps://github.com/hdong920/LESS.",
        "snippets": [
            "Many computational factors limit broader deployment of large language models.\nIn this paper, we focus on a memory bottleneck imposed by the key-value (KV)\ncache, a computational shortcut that requires storing previous KV pairs during\ndecoding. While existing KV cache methods approach this problem by pruning or\nevicting large swaths of relatively less important KV pairs to dramatically\nreduce the memory footprint of the cache, they can have limited success in\ntasks that require recollecting a majority of previous tokens. To alleviate\nthis issue, we propose LESS, a simple integration of a (nearly free) constant\nsized cache with eviction-based cache methods, such that all tokens can be\nqueried at later decoding steps. Its ability to retain information throughout\ntime shows merit on a variety of tasks where we demonstrate LESS can help\nreduce the performance gap from caching everything, sometimes even matching it,\nall while being efficient. Relevant code can be found at\nhttps://github.com/hdong920/LESS."
        ],
        "title": "Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2406.00025v1": {
        "url": "http://arxiv.org/abs/2406.00025v1",
        "description": "Large Language Models (LLMs) have become increasingly popular, transforming a\nwide range of applications across various domains. However, the real-world\neffectiveness of their query cache systems has not been thoroughly\ninvestigated. In this work, we for the first time conducted an analysis on\nreal-world human-to-LLM interaction data, identifying key challenges in\nexisting caching solutions for LLM-based chat services. Our findings reveal\nthat current caching methods fail to leverage semantic connections, leading to\ninefficient cache performance and extra token costs. To address these issues,\nwe propose SCALM, a new cache architecture that emphasizes semantic analysis\nand identifies significant cache entries and patterns. We also detail the\nimplementations of the corresponding cache storage and eviction strategies. Our\nevaluations show that SCALM increases cache hit ratios and reduces operational\ncosts for LLMChat services. Compared with other state-of-the-art solutions in\nGPTCache, SCALM shows, on average, a relative increase of 63% in cache hit\nratio and a relative improvement of 77% in tokens savings.",
        "snippets": [
            "Large Language Models (LLMs) have become increasingly popular, transforming a\nwide range of applications across various domains. However, the real-world\neffectiveness of their query cache systems has not been thoroughly\ninvestigated. In this work, we for the first time conducted an analysis on\nreal-world human-to-LLM interaction data, identifying key challenges in\nexisting caching solutions for LLM-based chat services. Our findings reveal\nthat current caching methods fail to leverage semantic connections, leading to\ninefficient cache performance and extra token costs. To address these issues,\nwe propose SCALM, a new cache architecture that emphasizes semantic analysis\nand identifies significant cache entries and patterns. We also detail the\nimplementations of the corresponding cache storage and eviction strategies. Our\nevaluations show that SCALM increases cache hit ratios and reduces operational\ncosts for LLMChat services. Compared with other state-of-the-art solutions in\nGPTCache, SCALM shows, on average, a relative increase of 63% in cache hit\nratio and a relative improvement of 77% in tokens savings."
        ],
        "title": "SCALM: Towards Semantic Caching for Automated Chat Services with Large Language Models",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1402.5987v1": {
        "url": "http://arxiv.org/abs/1402.5987v1",
        "description": "TTL caching models have recently regained significant research interest,\nlargely due to their ability to fit popular caching policies such as LRU. This\npaper advances the state-of-the-art analysis of TTL-based cache networks by\ndeveloping two exact methods with orthogonal generality and computational\ncomplexity. The first method generalizes existing results for line networks\nunder renewal requests to the broad class of caching policies whereby evictions\nare driven by stopping times. The obtained results are further generalized,\nusing the second method, to feedforward networks with Markov arrival processes\n(MAP) requests. MAPs are particularly suitable for non-line networks because\nthey are closed not only under superposition and splitting, as known, but also\nunder input-output caching operations as proven herein for phase-type TTL\ndistributions. The crucial benefit of the two closure properties is that they\njointly enable the first exact analysis of feedforward networks of TTL caches\nin great generality.",
        "snippets": [
            "TTL caching models have recently regained significant research interest,\nlargely due to their ability to fit popular caching policies such as LRU. This\npaper advances the state-of-the-art analysis of TTL-based cache networks by\ndeveloping two exact methods with orthogonal generality and computational\ncomplexity. The first method generalizes existing results for line networks\nunder renewal requests to the broad class of caching policies whereby evictions\nare driven by stopping times. The obtained results are further generalized,\nusing the second method, to feedforward networks with Markov arrival processes\n(MAP) requests. MAPs are particularly suitable for non-line networks because\nthey are closed not only under superposition and splitting, as known, but also\nunder input-output caching operations as proven herein for phase-type TTL\ndistributions. The crucial benefit of the two closure properties is that they\njointly enable the first exact analysis of feedforward networks of TTL caches\nin great generality."
        ],
        "title": "Exact Analysis of TTL Cache Networks: The Case of Caching Policies driven by Stopping Times",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2306.14048v3": {
        "url": "http://arxiv.org/abs/2306.14048v3",
        "description": "Large Language Models (LLMs), despite their recent impressive\naccomplishments, are notably cost-prohibitive to deploy, particularly for\napplications involving long-content generation, such as dialogue systems and\nstory writing. Often, a large amount of transient state information, referred\nto as the KV cache, is stored in GPU memory in addition to model parameters,\nscaling linearly with the sequence length and batch size. In this paper, we\nintroduce a novel approach for implementing the KV cache which significantly\nreduces its memory footprint. Our approach is based on the noteworthy\nobservation that a small portion of tokens contributes most of the value when\ncomputing attention scores. We call these tokens Heavy Hitters (H$_2$). Through\na comprehensive investigation, we find that (i) the emergence of H$_2$ is\nnatural and strongly correlates with the frequent co-occurrence of tokens in\nthe text, and (ii) removing them results in significant performance\ndegradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O),\na KV cache eviction policy that dynamically retains a balance of recent and\nH$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular\nproblem and prove (under mild assumptions) a theoretical guarantee for our\nnovel eviction algorithm which could help guide future work. We validate the\naccuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of\ntasks. Our implementation of H$_2$O with 20% heavy hitters improves the\nthroughput over three leading inference systems DeepSpeed Zero-Inference,\nHugging Face Accelerate, and FlexGen by up to 29$\\times$, 29$\\times$, and\n3$\\times$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the\nlatency by up to 1.9$\\times$. The code is available at\nhttps://github.com/FMInference/H2O.",
        "snippets": [
            "Large Language Models (LLMs), despite their recent impressive\naccomplishments, are notably cost-prohibitive to deploy, particularly for\napplications involving long-content generation, such as dialogue systems and\nstory writing. Often, a large amount of transient state information, referred\nto as the KV cache, is stored in GPU memory in addition to model parameters,\nscaling linearly with the sequence length and batch size. In this paper, we\nintroduce a novel approach for implementing the KV cache which significantly\nreduces its memory footprint. Our approach is based on the noteworthy\nobservation that a small portion of tokens contributes most of the value when\ncomputing attention scores. We call these tokens Heavy Hitters (H$_2$). Through\na comprehensive investigation, we find that (i) the emergence of H$_2$ is\nnatural and strongly correlates with the frequent co-occurrence of tokens in\nthe text, and (ii) removing them results in significant performance\ndegradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O),\na KV cache eviction policy that dynamically retains a balance of recent and\nH$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular\nproblem and prove (under mild assumptions) a theoretical guarantee for our\nnovel eviction algorithm which could help guide future work. We validate the\naccuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of\ntasks. Our implementation of H$_2$O with 20% heavy hitters improves the\nthroughput over three leading inference systems DeepSpeed Zero-Inference,\nHugging Face Accelerate, and FlexGen by up to 29$\\times$, 29$\\times$, and\n3$\\times$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the\nlatency by up to 1.9$\\times$. The code is available at\nhttps://github.com/FMInference/H2O."
        ],
        "title": "H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2406.12018v2": {
        "url": "http://arxiv.org/abs/2406.12018v2",
        "description": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS.",
        "snippets": [
            "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS."
        ],
        "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.23416v1": {
        "url": "http://arxiv.org/abs/2505.23416v1",
        "description": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.",
        "snippets": [
            "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4$\\times$ and FlashAttention decoding latency by approximately\n2$\\times$, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios."
        ],
        "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2009.09090v3": {
        "url": "http://arxiv.org/abs/2009.09090v3",
        "description": "Shared processor caches are vulnerable to conflict-based side-channel\nattacks, where an attacker can monitor access patterns of a victim by evicting\nvictim cache lines using cache-set conflicts. Recent mitigations propose\nrandomized mapping of addresses to cache lines to obfuscate the locations of\nset-conflicts. However, these are vulnerable to new attacks that discover\nconflicting sets of addresses despite such mitigations, because these designs\nselect eviction-candidates from a small set of conflicting lines.\n  This paper presents Mirage, a practical design for a fully associative cache,\nwherein eviction candidates are selected randomly from all lines resident in\nthe cache, to be immune to set-conflicts. A key challenge for enabling such\ndesigns in large shared caches (containing tens of thousands of cache lines) is\nthe complexity of cache-lookup, as a naive design can require searching through\nall the resident lines. Mirage achieves full-associativity while retaining\npractical set-associative lookups by decoupling placement and replacement,\nusing pointer-based indirection from tag-store to data-store to allow a newly\ninstalled address to globally evict the data of any random resident line. To\neliminate set-conflicts, Mirage provisions extra invalid tags in a\nskewed-associative tag-store design where lines can be installed without\nset-conflict, along with a load-aware skew-selection policy that guarantees the\navailability of sets with invalid tags. Our analysis shows Mirage provides the\nglobal eviction property of a fully-associative cache throughout system\nlifetime (violations of full-associativity, i.e. set-conflicts, occur less than\nonce in 10^4 to 10^17 years), thus offering a principled defense against any\neviction-set discovery and any potential conflict based attacks. Mirage incurs\nlimited slowdown (2%) and 17-20% extra storage compared to a non-secure cache.",
        "snippets": [
            "Shared processor caches are vulnerable to conflict-based side-channel\nattacks, where an attacker can monitor access patterns of a victim by evicting\nvictim cache lines using cache-set conflicts. Recent mitigations propose\nrandomized mapping of addresses to cache lines to obfuscate the locations of\nset-conflicts. However, these are vulnerable to new attacks that discover\nconflicting sets of addresses despite such mitigations, because these designs\nselect eviction-candidates from a small set of conflicting lines.\n  This paper presents Mirage, a practical design for a fully associative cache,\nwherein eviction candidates are selected randomly from all lines resident in\nthe cache, to be immune to set-conflicts. A key challenge for enabling such\ndesigns in large shared caches (containing tens of thousands of cache lines) is\nthe complexity of cache-lookup, as a naive design can require searching through\nall the resident lines. Mirage achieves full-associativity while retaining\npractical set-associative lookups by decoupling placement and replacement,\nusing pointer-based indirection from tag-store to data-store to allow a newly\ninstalled address to globally evict the data of any random resident line. To\neliminate set-conflicts, Mirage provisions extra invalid tags in a\nskewed-associative tag-store design where lines can be installed without\nset-conflict, along with a load-aware skew-selection policy that guarantees the\navailability of sets with invalid tags. Our analysis shows Mirage provides the\nglobal eviction property of a fully-associative cache throughout system\nlifetime (violations of full-associativity, i.e. set-conflicts, occur less than\nonce in 10^4 to 10^17 years), thus offering a principled defense against any\neviction-set discovery and any potential conflict based attacks. Mirage incurs\nlimited slowdown (2%) and 17-20% extra storage compared to a non-secure cache."
        ],
        "title": "MIRAGE: Mitigating Conflict-Based Cache Attacks with a Practical Fully-Associative Design",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.03111v1": {
        "url": "http://arxiv.org/abs/2410.03111v1",
        "description": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance.",
        "snippets": [
            "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance."
        ],
        "title": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive Compression Strategy",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.03090v1": {
        "url": "http://arxiv.org/abs/2410.03090v1",
        "description": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes.",
        "snippets": [
            "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes."
        ],
        "title": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1704.04849v8": {
        "url": "http://arxiv.org/abs/1704.04849v8",
        "description": "Many different caching mechanisms have been previously proposed, exploring\ndifferent insertion and eviction policies and their performance individually\nand as part of caching networks. We obtain a novel closed-form stationary\ninvariant distribution for a generalization of LRU and MRU caching nodes under\na reference Markov model. Numerical comparisons are made with an \"Incremental\nRank Progress\" (IRP a.k.a. CLIMB) and random eviction (a.k.a. random\nreplacement) methods under a steady-state Zipf popularity distribution. The\nrange of cache hit probabilities is smaller under MRU and larger under IRP\ncompared to LRU. We conclude with the invariant distribution for a special case\nof a random-eviction caching tree-network and associated discussion.",
        "snippets": [
            "Many different caching mechanisms have been previously proposed, exploring\ndifferent insertion and eviction policies and their performance individually\nand as part of caching networks. We obtain a novel closed-form stationary\ninvariant distribution for a generalization of LRU and MRU caching nodes under\na reference Markov model. Numerical comparisons are made with an \"Incremental\nRank Progress\" (IRP a.k.a. CLIMB) and random eviction (a.k.a. random\nreplacement) methods under a steady-state Zipf popularity distribution. The\nrange of cache hit probabilities is smaller under MRU and larger under IRP\ncompared to LRU. We conclude with the invariant distribution for a special case\nof a random-eviction caching tree-network and associated discussion."
        ],
        "title": "Stationary Distribution of a Generalized LRU-MRU Content Cache",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2407.18121v1": {
        "url": "http://arxiv.org/abs/2407.18121v1",
        "description": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
        "snippets": [
            "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
        ],
        "title": "Efficient Inference of Vision Instruction-Following Models with Elastic Cache",
        "meta": {
            "query": "cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2404.12457v2": {
        "url": "http://arxiv.org/abs/2404.12457v2",
        "description": "Retrieval-Augmented Generation (RAG) has shown significant improvements in\nvarious natural language processing tasks by integrating the strengths of large\nlanguage models (LLMs) and external knowledge databases. However, RAG\nintroduces long sequence generation and leads to high computation and memory\ncosts. We propose RAGCache, a novel multilevel dynamic caching system tailored\nfor RAG. Our analysis benchmarks current RAG systems, pinpointing the\nperformance bottleneck (i.e., long sequence due to knowledge injection) and\noptimization opportunities (i.e., caching knowledge's intermediate states).\nBased on these insights, we design RAGCache, which organizes the intermediate\nstates of retrieved knowledge in a knowledge tree and caches them in the GPU\nand host memory hierarchy. RAGCache proposes a replacement policy that is aware\nof LLM inference characteristics and RAG retrieval patterns. It also\ndynamically overlaps the retrieval and inference steps to minimize the\nend-to-end latency. We implement RAGCache and evaluate it on vLLM, a\nstate-of-the-art LLM inference system and Faiss, a state-of-the-art vector\ndatabase. The experimental results show that RAGCache reduces the time to first\ntoken (TTFT) by up to 4x and improves the throughput by up to 2.1x compared to\nvLLM integrated with Faiss.",
        "snippets": [
            "Retrieval-Augmented Generation (RAG) has shown significant improvements in\nvarious natural language processing tasks by integrating the strengths of large\nlanguage models (LLMs) and external knowledge databases. However, RAG\nintroduces long sequence generation and leads to high computation and memory\ncosts. We propose RAGCache, a novel multilevel dynamic caching system tailored\nfor RAG. Our analysis benchmarks current RAG systems, pinpointing the\nperformance bottleneck (i.e., long sequence due to knowledge injection) and\noptimization opportunities (i.e., caching knowledge's intermediate states).\nBased on these insights, we design RAGCache, which organizes the intermediate\nstates of retrieved knowledge in a knowledge tree and caches them in the GPU\nand host memory hierarchy. RAGCache proposes a replacement policy that is aware\nof LLM inference characteristics and RAG retrieval patterns. It also\ndynamically overlaps the retrieval and inference steps to minimize the\nend-to-end latency. We implement RAGCache and evaluate it on vLLM, a\nstate-of-the-art LLM inference system and Faiss, a state-of-the-art vector\ndatabase. The experimental results show that RAGCache reduces the time to first\ntoken (TTFT) by up to 4x and improves the throughput by up to 2.1x compared to\nvLLM integrated with Faiss."
        ],
        "title": "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2406.11430v4": {
        "url": "http://arxiv.org/abs/2406.11430v4",
        "description": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
        "snippets": [
            "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
        ],
        "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache Compression",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2408.05646v2": {
        "url": "http://arxiv.org/abs/2408.05646v2",
        "description": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
        "snippets": [
            "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
        ],
        "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2412.16434v1": {
        "url": "http://arxiv.org/abs/2412.16434v1",
        "description": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
        "snippets": [
            "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
        ],
        "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.07776v1": {
        "url": "http://arxiv.org/abs/2502.07776v1",
        "description": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
        "snippets": [
            "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
        ],
        "title": "Auditing Prompt Caching in Language Model APIs",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2407.15360v1": {
        "url": "http://arxiv.org/abs/2407.15360v1",
        "description": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications.",
        "snippets": [
            "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications."
        ],
        "title": "Dissecting Multiplication in Transformers: Insights into LLMs",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.23079v1": {
        "url": "http://arxiv.org/abs/2410.23079v1",
        "description": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
        "snippets": [
            "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
        ],
        "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters for Efficient LLM Inference",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2404.06954v1": {
        "url": "http://arxiv.org/abs/2404.06954v1",
        "description": "Recently, dynamic computation methods have shown notable acceleration for\nLarge Language Models (LLMs) by skipping several layers of computations through\nelaborate heuristics or additional predictors. However, in the decoding process\nof existing approaches, different samples are assigned different computational\nbudgets, which cannot guarantee a stable and precise acceleration effect.\nFurthermore, existing approaches generally skip multiple contiguous layers at\nthe bottom or top of the layers, leading to a drastic change in the model's\nlayer-wise representations, and thus a consequent performance degeneration.\nTherefore, we propose a Unified Layer Skipping strategy, which selects the\nnumber of layers to skip computation based solely on the target speedup ratio,\nand then skips the corresponding number of intermediate layer computations in a\nbalanced manner. Since the Unified Layer Skipping strategy is independent of\ninput samples, it naturally supports popular acceleration techniques such as\nbatch decoding and KV caching, thus demonstrating more practicality for\nreal-world applications. Experimental results on two common tasks, i.e.,\nmachine translation and text summarization, indicate that given a target\nspeedup ratio, the Unified Layer Skipping strategy significantly enhances both\nthe inference performance and the actual model throughput over existing dynamic\napproaches.",
        "snippets": [
            "Recently, dynamic computation methods have shown notable acceleration for\nLarge Language Models (LLMs) by skipping several layers of computations through\nelaborate heuristics or additional predictors. However, in the decoding process\nof existing approaches, different samples are assigned different computational\nbudgets, which cannot guarantee a stable and precise acceleration effect.\nFurthermore, existing approaches generally skip multiple contiguous layers at\nthe bottom or top of the layers, leading to a drastic change in the model's\nlayer-wise representations, and thus a consequent performance degeneration.\nTherefore, we propose a Unified Layer Skipping strategy, which selects the\nnumber of layers to skip computation based solely on the target speedup ratio,\nand then skips the corresponding number of intermediate layer computations in a\nbalanced manner. Since the Unified Layer Skipping strategy is independent of\ninput samples, it naturally supports popular acceleration techniques such as\nbatch decoding and KV caching, thus demonstrating more practicality for\nreal-world applications. Experimental results on two common tasks, i.e.,\nmachine translation and text summarization, indicate that given a target\nspeedup ratio, the Unified Layer Skipping strategy significantly enhances both\nthe inference performance and the actual model throughput over existing dynamic\napproaches."
        ],
        "title": "Accelerating Inference in Large Language Models with a Unified Layer Skipping Strategy",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2406.16441v1": {
        "url": "http://arxiv.org/abs/2406.16441v1",
        "description": "Intermediate reasoning or acting steps have successfully improved large\nlanguage models (LLMs) for handling various downstream natural language\nprocessing (NLP) tasks. When applying LLMs for code generation, recent works\nmainly focus on directing the models to articulate intermediate\nnatural-language reasoning steps, as in chain-of-thought (CoT) prompting, and\nthen output code with the natural language or other structured intermediate\nsteps. However, such output is not suitable for code translation or generation\ntasks since the standard CoT has different logical structures and forms of\nexpression with the code. In this work, we introduce the universal code\n(UniCode) as the intermediate representation. It is a description of algorithm\nsteps using a mix of conventions of programming languages, such as assignment\noperator, conditional operator, and loop. Hence, we collect an instruction\ndataset UniCoder-Instruct to train our model UniCoder on multi-task learning\nobjectives. UniCoder-Instruct comprises natural-language questions, code\nsolutions, and the corresponding universal code. The alignment between the\nintermediate universal code representation and the final code solution\nsignificantly improves the quality of the generated code. The experimental\nresults demonstrate that UniCoder with the universal code significantly\noutperforms the previous prompting methods by a large margin, showcasing the\neffectiveness of the structural clues in pseudo-code.",
        "snippets": [
            "Intermediate reasoning or acting steps have successfully improved large\nlanguage models (LLMs) for handling various downstream natural language\nprocessing (NLP) tasks. When applying LLMs for code generation, recent works\nmainly focus on directing the models to articulate intermediate\nnatural-language reasoning steps, as in chain-of-thought (CoT) prompting, and\nthen output code with the natural language or other structured intermediate\nsteps. However, such output is not suitable for code translation or generation\ntasks since the standard CoT has different logical structures and forms of\nexpression with the code. In this work, we introduce the universal code\n(UniCode) as the intermediate representation. It is a description of algorithm\nsteps using a mix of conventions of programming languages, such as assignment\noperator, conditional operator, and loop. Hence, we collect an instruction\ndataset UniCoder-Instruct to train our model UniCoder on multi-task learning\nobjectives. UniCoder-Instruct comprises natural-language questions, code\nsolutions, and the corresponding universal code. The alignment between the\nintermediate universal code representation and the final code solution\nsignificantly improves the quality of the generated code. The experimental\nresults demonstrate that UniCoder with the universal code significantly\noutperforms the previous prompting methods by a large margin, showcasing the\neffectiveness of the structural clues in pseudo-code."
        ],
        "title": "UniCoder: Scaling Code Large Language Model via Universal Code",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.23317v1": {
        "url": "http://arxiv.org/abs/2410.23317v1",
        "description": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
        "snippets": [
            "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
        ],
        "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2109.07048v2": {
        "url": "http://arxiv.org/abs/2109.07048v2",
        "description": "Adversarial regularization can improve model generalization in many natural\nlanguage processing tasks. However, conventional approaches are computationally\nexpensive since they need to generate a perturbation for each sample in each\nepoch. We propose a new adversarial regularization method ARCH (adversarial\nregularization with caching), where perturbations are generated and cached once\nevery several epochs. As caching all the perturbations imposes memory usage\nconcerns, we adopt a K-nearest neighbors-based strategy to tackle this issue.\nThe strategy only requires caching a small amount of perturbations, without\nintroducing additional training time. We evaluate our proposed method on a set\nof neural machine translation and natural language understanding tasks. We\nobserve that ARCH significantly eases the computational burden (saves up to 70%\nof computational time in comparison with conventional approaches). More\nsurprisingly, by reducing the variance of stochastic gradients, ARCH produces a\nnotably better (in most of the tasks) or comparable model generalization. Our\ncode is available at https://github.com/SimiaoZuo/Caching-Adv.",
        "snippets": [
            "Adversarial regularization can improve model generalization in many natural\nlanguage processing tasks. However, conventional approaches are computationally\nexpensive since they need to generate a perturbation for each sample in each\nepoch. We propose a new adversarial regularization method ARCH (adversarial\nregularization with caching), where perturbations are generated and cached once\nevery several epochs. As caching all the perturbations imposes memory usage\nconcerns, we adopt a K-nearest neighbors-based strategy to tackle this issue.\nThe strategy only requires caching a small amount of perturbations, without\nintroducing additional training time. We evaluate our proposed method on a set\nof neural machine translation and natural language understanding tasks. We\nobserve that ARCH significantly eases the computational burden (saves up to 70%\nof computational time in comparison with conventional approaches). More\nsurprisingly, by reducing the variance of stochastic gradients, ARCH produces a\nnotably better (in most of the tasks) or comparable model generalization. Our\ncode is available at https://github.com/SimiaoZuo/Caching-Adv."
        ],
        "title": "ARCH: Efficient Adversarial Regularized Training with Caching",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1809.08826v1": {
        "url": "http://arxiv.org/abs/1809.08826v1",
        "description": "Neural cache language models (LMs) extend the idea of regular cache language\nmodels by making the cache probability dependent on the similarity between the\ncurrent context and the context of the words in the cache. We make an extensive\ncomparison of 'regular' cache models with neural cache models, both in terms of\nperplexity and WER after rescoring first-pass ASR results. Furthermore, we\npropose two extensions to this neural cache model that make use of the content\nvalue/information weight of the word: firstly, combining the cache probability\nand LM probability with an information-weighted interpolation and secondly,\nselectively adding only content words to the cache. We obtain a 29.9%/32.1%\n(validation/test set) relative improvement in perplexity with respect to a\nbaseline LSTM LM on the WikiText-2 dataset, outperforming previous work on\nneural cache LMs. Additionally, we observe significant WER reductions with\nrespect to the baseline model on the WSJ ASR task.",
        "snippets": [
            "Neural cache language models (LMs) extend the idea of regular cache language\nmodels by making the cache probability dependent on the similarity between the\ncurrent context and the context of the words in the cache. We make an extensive\ncomparison of 'regular' cache models with neural cache models, both in terms of\nperplexity and WER after rescoring first-pass ASR results. Furthermore, we\npropose two extensions to this neural cache model that make use of the content\nvalue/information weight of the word: firstly, combining the cache probability\nand LM probability with an information-weighted interpolation and secondly,\nselectively adding only content words to the cache. We obtain a 29.9%/32.1%\n(validation/test set) relative improvement in perplexity with respect to a\nbaseline LSTM LM on the WikiText-2 dataset, outperforming previous work on\nneural cache LMs. Additionally, we observe significant WER reductions with\nrespect to the baseline model on the WSJ ASR task."
        ],
        "title": "Information-Weighted Neural Cache Language Models for ASR",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.21035v2": {
        "url": "http://arxiv.org/abs/2410.21035v2",
        "description": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters.",
        "snippets": [
            "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters."
        ],
        "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2402.01173v1": {
        "url": "http://arxiv.org/abs/2402.01173v1",
        "description": "Large language models (LLMs) have achieved huge success in numerous natural\nlanguage process (NLP) tasks. However, it faces the challenge of significant\nresource consumption during inference. In this paper, we aim to improve the\ninference efficiency of LLMs by prompt caching, i.e., if the current prompt can\nbe answered by the same response of a previous prompt, one can directly utilize\nthat previous response without calling the LLM. Specifically, we focus on the\nprediction accuracy of prompt caching for single-round question-answering tasks\nvia embedding similarity. The existing embeddings of prompts mostly focus on\nwhether two prompts are semantically similar, which is not necessarily\nequivalent to whether the same response can answer them. Therefore, we propose\na distillation-based method to fine-tune the existing embeddings for better\ncaching prediction. Theoretically, we provide finite-sample guarantees for the\nconvergence of our method under different types of loss functions. Empirically,\nwe carefully construct a hard dataset based on Kwiatkowski et al. (2019) where\nthe existing embedding model (Wang et al., 2022) only achieves an AUC of 0.51.\nWe then fine-tune the above embedding model, which significantly improves the\nAUC of caching prediction from 0.51 to 0.81. We also conduct simulations\ndemonstrating that our trained models achieve better caching efficiency than\nthe previous embedding model.",
        "snippets": [
            "Large language models (LLMs) have achieved huge success in numerous natural\nlanguage process (NLP) tasks. However, it faces the challenge of significant\nresource consumption during inference. In this paper, we aim to improve the\ninference efficiency of LLMs by prompt caching, i.e., if the current prompt can\nbe answered by the same response of a previous prompt, one can directly utilize\nthat previous response without calling the LLM. Specifically, we focus on the\nprediction accuracy of prompt caching for single-round question-answering tasks\nvia embedding similarity. The existing embeddings of prompts mostly focus on\nwhether two prompts are semantically similar, which is not necessarily\nequivalent to whether the same response can answer them. Therefore, we propose\na distillation-based method to fine-tune the existing embeddings for better\ncaching prediction. Theoretically, we provide finite-sample guarantees for the\nconvergence of our method under different types of loss functions. Empirically,\nwe carefully construct a hard dataset based on Kwiatkowski et al. (2019) where\nthe existing embedding model (Wang et al., 2022) only achieves an AUC of 0.51.\nWe then fine-tune the above embedding model, which significantly improves the\nAUC of caching prediction from 0.51 to 0.81. We also conduct simulations\ndemonstrating that our trained models achieve better caching efficiency than\nthe previous embedding model."
        ],
        "title": "Efficient Prompt Caching via Embedding Similarity",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2403.02694v4": {
        "url": "http://arxiv.org/abs/2403.02694v4",
        "description": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
        "snippets": [
            "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
        ],
        "title": "MeanCache: User-Centric Semantic Caching for LLM Web Services",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.16886v2": {
        "url": "http://arxiv.org/abs/2502.16886v2",
        "description": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods.",
        "snippets": [
            "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods."
        ],
        "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2409.10593v3": {
        "url": "http://arxiv.org/abs/2409.10593v3",
        "description": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
        "snippets": [
            "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV."
        ],
        "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context Scenarios",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.15781v1": {
        "url": "http://arxiv.org/abs/2505.15781v1",
        "description": "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs.",
        "snippets": [
            "Diffusion Language Models (DLMs) have been seen as a promising competitor for\nautoregressive language models. However, diffusion language models have long\nbeen constrained by slow inference. A core challenge is that their\nnon-autoregressive architecture and bidirectional attention preclude the\nkey-value cache that accelerates decoding. We address this bottleneck by\nproposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising\nprocess of DLMs. Our approach is motivated by the observation that different\ntokens have distinct representation dynamics throughout the diffusion process.\nAccordingly, we propose a delayed and conditioned caching strategy for key and\nvalue states. We design two complementary variants to cache key and value\nstep-by-step: (1) dKV-Cache-Decode, which provides almost lossless\nacceleration, and even improves performance on long sequences, suggesting that\nexisting DLMs may under-utilise contextual information during inference. (2)\ndKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving\nhigher speed-ups with quadratic time complexity at the cost of some performance\ndegradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,\nlargely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on\nseveral benchmarks, delivering acceleration across general language\nunderstanding, mathematical, and code-generation benchmarks. Experiments\ndemonstrate that cache can also be used in DLMs, even in a training-free manner\nfrom current DLMs."
        ],
        "title": "dKV-Cache: The Cache for Diffusion Language Models",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.21889v2": {
        "url": "http://arxiv.org/abs/2505.21889v2",
        "description": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability. EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM.",
        "snippets": [
            "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability. EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM."
        ],
        "title": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.02435v1": {
        "url": "http://arxiv.org/abs/2503.02435v1",
        "description": "As the demand for querying databases in all areas of life continues to grow,\nresearchers have devoted significant attention to the natural language\ninterface for databases (NLIDB). This paper presents a comprehensive survey of\nrecently proposed NLIDBs. We begin with a brief introduction to natural\nlanguage processing techniques, executable database languages and the\nintermediate representation between natural language and executable language,\nand then provide an overview of the translation process from natural language\nto executable database language. The translation process is divided into three\nstages: (i) natural language preprocessing, (ii) natural language\nunderstanding, and (iii) natural language translation. Traditional and\ndata-driven methods are utilized in the preprocessing stage. Traditional\napproaches rely on predefined rules and grammars, and involve techniques such\nas regular expressions, dependency parsing and named entity recognition.\nData-driven approaches depend on large-scale data and machine learning models,\nusing techniques including word embedding and pattern linking. Natural language\nunderstanding methods are classified into three categories: (i) rule-based,\n(ii) machine learning-based, and (iii) hybrid. We then describe a general\nconstruction process for executable languages over relational and\nspatio-temporal databases. Subsequently, common benchmarks and evaluation\nmetrics for transforming natural language into executable language are\npresented, and methods for generating new benchmarks are explored. Finally, we\nsummarize the classification, development, and enhancement of NLIDB systems,\nand discuss deep language understanding and database interaction techniques\nrelated to NLIDB, including (i) using LLM for Text2SQL tasks, (ii) generating\nnatural language interpretations from SQL, and (iii) transforming speech\nqueries into SQL.",
        "snippets": [
            "As the demand for querying databases in all areas of life continues to grow,\nresearchers have devoted significant attention to the natural language\ninterface for databases (NLIDB). This paper presents a comprehensive survey of\nrecently proposed NLIDBs. We begin with a brief introduction to natural\nlanguage processing techniques, executable database languages and the\nintermediate representation between natural language and executable language,\nand then provide an overview of the translation process from natural language\nto executable database language. The translation process is divided into three\nstages: (i) natural language preprocessing, (ii) natural language\nunderstanding, and (iii) natural language translation. Traditional and\ndata-driven methods are utilized in the preprocessing stage. Traditional\napproaches rely on predefined rules and grammars, and involve techniques such\nas regular expressions, dependency parsing and named entity recognition.\nData-driven approaches depend on large-scale data and machine learning models,\nusing techniques including word embedding and pattern linking. Natural language\nunderstanding methods are classified into three categories: (i) rule-based,\n(ii) machine learning-based, and (iii) hybrid. We then describe a general\nconstruction process for executable languages over relational and\nspatio-temporal databases. Subsequently, common benchmarks and evaluation\nmetrics for transforming natural language into executable language are\npresented, and methods for generating new benchmarks are explored. Finally, we\nsummarize the classification, development, and enhancement of NLIDB systems,\nand discuss deep language understanding and database interaction techniques\nrelated to NLIDB, including (i) using LLM for Text2SQL tasks, (ii) generating\nnatural language interpretations from SQL, and (iii) transforming speech\nqueries into SQL."
        ],
        "title": "NLI4DB: A Systematic Review of Natural Language Interfaces for Databases",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.02175v1": {
        "url": "http://arxiv.org/abs/2502.02175v1",
        "description": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate.",
        "snippets": [
            "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate."
        ],
        "title": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2403.17312v1": {
        "url": "http://arxiv.org/abs/2403.17312v1",
        "description": "The Transformer architecture has significantly advanced natural language\nprocessing (NLP) and has been foundational in developing large language models\n(LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP\ntasks. Despite their superior accuracy, LLMs present unique challenges in\npractical inference, concerning the compute and memory-intensive nature. Thanks\nto the autoregressive characteristic of LLM inference, KV caching for the\nattention layers in Transformers can effectively accelerate LLM inference by\nsubstituting quadratic-complexity computation with linear-complexity memory\naccesses. Yet, this approach requires increasing memory as demand grows for\nprocessing longer sequences. The overhead leads to reduced throughput due to\nI/O bottlenecks and even out-of-memory errors, particularly on\nresource-constrained systems like a single commodity GPU. In this paper, we\npropose ALISA, a novel algorithm-system co-design solution to address the\nchallenges imposed by KV caching. On the algorithm level, ALISA prioritizes\ntokens that are most important in generating a new token via a Sparse Window\nAttention (SWA) algorithm. SWA introduces high sparsity in attention layers and\nreduces the memory footprint of KV caching at negligible accuracy loss. On the\nsystem level, ALISA employs three-phase token-level dynamical scheduling and\noptimizes the trade-off between caching and recomputation, thus maximizing the\noverall performance in resource-constrained systems. In a single GPU-CPU\nsystem, we demonstrate that under varying workloads, ALISA improves the\nthroughput of baseline systems such as FlexGen and vLLM by up to 3X and 1.9X,\nrespectively.",
        "snippets": [
            "The Transformer architecture has significantly advanced natural language\nprocessing (NLP) and has been foundational in developing large language models\n(LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP\ntasks. Despite their superior accuracy, LLMs present unique challenges in\npractical inference, concerning the compute and memory-intensive nature. Thanks\nto the autoregressive characteristic of LLM inference, KV caching for the\nattention layers in Transformers can effectively accelerate LLM inference by\nsubstituting quadratic-complexity computation with linear-complexity memory\naccesses. Yet, this approach requires increasing memory as demand grows for\nprocessing longer sequences. The overhead leads to reduced throughput due to\nI/O bottlenecks and even out-of-memory errors, particularly on\nresource-constrained systems like a single commodity GPU. In this paper, we\npropose ALISA, a novel algorithm-system co-design solution to address the\nchallenges imposed by KV caching. On the algorithm level, ALISA prioritizes\ntokens that are most important in generating a new token via a Sparse Window\nAttention (SWA) algorithm. SWA introduces high sparsity in attention layers and\nreduces the memory footprint of KV caching at negligible accuracy loss. On the\nsystem level, ALISA employs three-phase token-level dynamical scheduling and\noptimizes the trade-off between caching and recomputation, thus maximizing the\noverall performance in resource-constrained systems. In a single GPU-CPU\nsystem, we demonstrate that under varying workloads, ALISA improves the\nthroughput of baseline systems such as FlexGen and vLLM by up to 3X and 1.9X,\nrespectively."
        ],
        "title": "ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2005.13013v2": {
        "url": "http://arxiv.org/abs/2005.13013v2",
        "description": "Intermediate-task training---fine-tuning a pretrained model on an\nintermediate task before fine-tuning again on the target task---often improves\nmodel performance substantially on language understanding tasks in monolingual\nEnglish settings. We investigate whether English intermediate-task training is\nstill helpful on non-English target tasks. Using nine intermediate\nlanguage-understanding tasks, we evaluate intermediate-task transfer in a\nzero-shot cross-lingual setting on the XTREME benchmark. We see large\nimprovements from intermediate training on the BUCC and Tatoeba sentence\nretrieval tasks and moderate improvements on question-answering target tasks.\nMNLI, SQuAD and HellaSwag achieve the best overall results as intermediate\ntasks, while multi-task intermediate offers small additional improvements.\nUsing our best intermediate-task models for each target task, we obtain a 5.4\npoint improvement over XLM-R Large on the XTREME benchmark, setting the state\nof the art as of June 2020. We also investigate continuing multilingual MLM\nduring intermediate-task training and using machine-translated\nintermediate-task data, but neither consistently outperforms simply performing\nEnglish intermediate-task training.",
        "snippets": [
            "Intermediate-task training---fine-tuning a pretrained model on an\nintermediate task before fine-tuning again on the target task---often improves\nmodel performance substantially on language understanding tasks in monolingual\nEnglish settings. We investigate whether English intermediate-task training is\nstill helpful on non-English target tasks. Using nine intermediate\nlanguage-understanding tasks, we evaluate intermediate-task transfer in a\nzero-shot cross-lingual setting on the XTREME benchmark. We see large\nimprovements from intermediate training on the BUCC and Tatoeba sentence\nretrieval tasks and moderate improvements on question-answering target tasks.\nMNLI, SQuAD and HellaSwag achieve the best overall results as intermediate\ntasks, while multi-task intermediate offers small additional improvements.\nUsing our best intermediate-task models for each target task, we obtain a 5.4\npoint improvement over XLM-R Large on the XTREME benchmark, setting the state\nof the art as of June 2020. We also investigate continuing multilingual MLM\nduring intermediate-task training and using machine-translated\nintermediate-task data, but neither consistently outperforms simply performing\nEnglish intermediate-task training."
        ],
        "title": "English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2112.01705v1": {
        "url": "http://arxiv.org/abs/2112.01705v1",
        "description": "As the fourth largest language family in the world, the Dravidian languages\nhave become a research hotspot in natural language processing (NLP). Although\nthe Dravidian languages contain a large number of languages, there are\nrelatively few public available resources. Besides, text classification task,\nas a basic task of natural language processing, how to combine it to multiple\nlanguages in the Dravidian languages, is still a major difficulty in Dravidian\nNatural Language Processing. Hence, to address these problems, we proposed a\nmultilingual text classification framework for the Dravidian languages. On the\none hand, the framework used the LaBSE pre-trained model as the base model.\nAiming at the problem of text information bias in multi-task learning, we\npropose to use the MLM strategy to select language-specific words, and used\nadversarial training to perturb them. On the other hand, in view of the problem\nthat the model cannot well recognize and utilize the correlation among\nlanguages, we further proposed a language-specific representation module to\nenrich semantic information for the model. The experimental results\ndemonstrated that the framework we proposed has a significant performance in\nmultilingual text classification tasks with each strategy achieving certain\nimprovements.",
        "snippets": [
            "As the fourth largest language family in the world, the Dravidian languages\nhave become a research hotspot in natural language processing (NLP). Although\nthe Dravidian languages contain a large number of languages, there are\nrelatively few public available resources. Besides, text classification task,\nas a basic task of natural language processing, how to combine it to multiple\nlanguages in the Dravidian languages, is still a major difficulty in Dravidian\nNatural Language Processing. Hence, to address these problems, we proposed a\nmultilingual text classification framework for the Dravidian languages. On the\none hand, the framework used the LaBSE pre-trained model as the base model.\nAiming at the problem of text information bias in multi-task learning, we\npropose to use the MLM strategy to select language-specific words, and used\nadversarial training to perturb them. On the other hand, in view of the problem\nthat the model cannot well recognize and utilize the correlation among\nlanguages, we further proposed a language-specific representation module to\nenrich semantic information for the model. The experimental results\ndemonstrated that the framework we proposed has a significant performance in\nmultilingual text classification tasks with each strategy achieving certain\nimprovements."
        ],
        "title": "Multilingual Text Classification for Dravidian Languages",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2407.05411v1": {
        "url": "http://arxiv.org/abs/2407.05411v1",
        "description": "Intermediate step methodologies like chain of thoughts (COT) have\ndemonstrated effectiveness in enhancing the performance of Large Language\nModels (LLMs) on code generation. This study explores the utilization of\nintermediate languages, including various programming languages, natural\nlanguage solutions, and pseudo-code, and systematically evaluates their impact\non the performance of LLMs in code generation tasks. Our experiments encompass\neleven models across the CodeLlama, GPT, and Mistral families, as well as newly\nreleased smaller models. Our findings reveal that intermediate languages\ngenerally exhibit greater efficacy in larger models that have not yet achieved\nstate-of-the-art performance. Natural language consistently emerges as the most\neffective intermediate representation across all target languages. However, we\nobserve no universally effective intermediate formal language across different\nmodels and target languages. Furthermore, we uncover a weak correlation between\nthe correctness of intermediate solutions and final generation, suggesting that\nimprovements may stem from the chain-of-thought effect rather than\nlanguage-specific transfer. Interestingly, we discover that for GPT family\nmodels, prompting multiple times without explicit self-correction instructions\nyields performance gains across the studied languages.",
        "snippets": [
            "Intermediate step methodologies like chain of thoughts (COT) have\ndemonstrated effectiveness in enhancing the performance of Large Language\nModels (LLMs) on code generation. This study explores the utilization of\nintermediate languages, including various programming languages, natural\nlanguage solutions, and pseudo-code, and systematically evaluates their impact\non the performance of LLMs in code generation tasks. Our experiments encompass\neleven models across the CodeLlama, GPT, and Mistral families, as well as newly\nreleased smaller models. Our findings reveal that intermediate languages\ngenerally exhibit greater efficacy in larger models that have not yet achieved\nstate-of-the-art performance. Natural language consistently emerges as the most\neffective intermediate representation across all target languages. However, we\nobserve no universally effective intermediate formal language across different\nmodels and target languages. Furthermore, we uncover a weak correlation between\nthe correctness of intermediate solutions and final generation, suggesting that\nimprovements may stem from the chain-of-thought effect rather than\nlanguage-specific transfer. Interestingly, we discover that for GPT family\nmodels, prompting multiple times without explicit self-correction instructions\nyields performance gains across the studied languages."
        ],
        "title": "Assessing Code Generation with Intermediate Languages",
        "meta": {
            "query": "caching intermediate results in large language models for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2406.07056v1": {
        "url": "http://arxiv.org/abs/2406.07056v1",
        "description": "The advent of pre-trained large language models (LLMs) has revolutionized\nvarious natural language processing tasks. These models predominantly employ an\nauto-regressive decoding mechanism that utilizes Key-Value (KV) caches to\neliminate redundant calculations for previous tokens. Nevertheless, as context\nlengths and batch sizes increase, the linear expansion in memory footprint of\nKV caches becomes a key bottleneck of LLM deployment, which decreases\ngeneration speeds significantly. To mitigate this issue, previous techniques\nlike multi-query attention (MQA) and grouped-query attention (GQA) have been\ndeveloped, in order to reduce KV heads to accelerate inference with comparable\naccuracy to multi-head attention (MHA). Despite their effectiveness, existing\nstrategies for compressing MHA often overlook the intrinsic properties of the\nKV caches. In this work, we explore the low-rank characteristics of the KV\ncaches and propose a novel approach for compressing KV heads. In particular, we\ncarefully optimize the MHA-to-GQA transformation to minimize compression error,\nand to remain compatible with rotary position embeddings (RoPE), we also\nintroduce specialized strategies for key caches with RoPE. We demonstrate that\nour method can compress half or even three-quarters of KV heads while\nmaintaining performance comparable to the original LLMs, which presents a\npromising direction for more efficient LLM deployment in resource-constrained\nenvironments.",
        "snippets": [
            "The advent of pre-trained large language models (LLMs) has revolutionized\nvarious natural language processing tasks. These models predominantly employ an\nauto-regressive decoding mechanism that utilizes Key-Value (KV) caches to\neliminate redundant calculations for previous tokens. Nevertheless, as context\nlengths and batch sizes increase, the linear expansion in memory footprint of\nKV caches becomes a key bottleneck of LLM deployment, which decreases\ngeneration speeds significantly. To mitigate this issue, previous techniques\nlike multi-query attention (MQA) and grouped-query attention (GQA) have been\ndeveloped, in order to reduce KV heads to accelerate inference with comparable\naccuracy to multi-head attention (MHA). Despite their effectiveness, existing\nstrategies for compressing MHA often overlook the intrinsic properties of the\nKV caches. In this work, we explore the low-rank characteristics of the KV\ncaches and propose a novel approach for compressing KV heads. In particular, we\ncarefully optimize the MHA-to-GQA transformation to minimize compression error,\nand to remain compatible with rotary position embeddings (RoPE), we also\nintroduce specialized strategies for key caches with RoPE. We demonstrate that\nour method can compress half or even three-quarters of KV heads while\nmaintaining performance comparable to the original LLMs, which presents a\npromising direction for more efficient LLM deployment in resource-constrained\nenvironments."
        ],
        "title": "Effectively Compress KV Heads for LLM",
        "meta": {
            "query": "LLM caching strategies for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2402.03408v3": {
        "url": "http://arxiv.org/abs/2402.03408v3",
        "description": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
        "snippets": [
            "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
        ],
        "title": "A Framework for Effective Invocation Methods of Various LLM Services",
        "meta": {
            "query": "LLM caching strategies for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2403.05826v2": {
        "url": "http://arxiv.org/abs/2403.05826v2",
        "description": "Edge intelligence in space-air-ground integrated networks (SAGINs) can enable\nworldwide network coverage beyond geographical limitations for users to access\nubiquitous and low-latency intelligence services. Facing global coverage and\ncomplex environments in SAGINs, edge intelligence can provision approximate\nlarge language models (LLMs) agents for users via edge servers at ground base\nstations (BSs) or cloud data centers relayed by satellites. As LLMs with\nbillions of parameters are pre-trained on vast datasets, LLM agents have\nfew-shot learning capabilities, e.g., chain-of-thought (CoT) prompting for\ncomplex tasks, which raises a new trade-off between resource consumption and\nperformance in SAGINs. In this paper, we propose a joint caching and inference\nframework for edge intelligence to provision sustainable and ubiquitous LLM\nagents in SAGINs. We introduce \"cached model-as-a-resource\" for offering LLMs\nwith limited context windows and propose a novel optimization framework, i.e.,\njoint model caching and inference, to utilize cached model resources for\nprovisioning LLM agent services along with communication, computing, and\nstorage resources. We design \"age of thought\" (AoT) considering the CoT\nprompting of LLMs, and propose a least AoT cached model replacement algorithm\nfor optimizing the provisioning cost. We propose a deep Q-network-based\nmodified second-bid (DQMSB) auction to incentivize network operators, which can\nenhance allocation efficiency by 23% while guaranteeing strategy-proofness and\nfree from adverse selection.",
        "snippets": [
            "Edge intelligence in space-air-ground integrated networks (SAGINs) can enable\nworldwide network coverage beyond geographical limitations for users to access\nubiquitous and low-latency intelligence services. Facing global coverage and\ncomplex environments in SAGINs, edge intelligence can provision approximate\nlarge language models (LLMs) agents for users via edge servers at ground base\nstations (BSs) or cloud data centers relayed by satellites. As LLMs with\nbillions of parameters are pre-trained on vast datasets, LLM agents have\nfew-shot learning capabilities, e.g., chain-of-thought (CoT) prompting for\ncomplex tasks, which raises a new trade-off between resource consumption and\nperformance in SAGINs. In this paper, we propose a joint caching and inference\nframework for edge intelligence to provision sustainable and ubiquitous LLM\nagents in SAGINs. We introduce \"cached model-as-a-resource\" for offering LLMs\nwith limited context windows and propose a novel optimization framework, i.e.,\njoint model caching and inference, to utilize cached model resources for\nprovisioning LLM agent services along with communication, computing, and\nstorage resources. We design \"age of thought\" (AoT) considering the CoT\nprompting of LLMs, and propose a least AoT cached model replacement algorithm\nfor optimizing the provisioning cost. We propose a deep Q-network-based\nmodified second-bid (DQMSB) auction to incentivize network operators, which can\nenhance allocation efficiency by 23% while guaranteeing strategy-proofness and\nfree from adverse selection."
        ],
        "title": "Cached Model-as-a-Resource: Provisioning Large Language Model Agents for Edge Intelligence in Space-air-ground Integrated Networks",
        "meta": {
            "query": "LLM caching strategies for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2405.06219v3": {
        "url": "http://arxiv.org/abs/2405.06219v3",
        "description": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding.",
        "snippets": [
            "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding."
        ],
        "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models",
        "meta": {
            "query": "LLM caching strategies for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.11101v1": {
        "url": "http://arxiv.org/abs/2502.11101v1",
        "description": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation.",
        "snippets": [
            "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation."
        ],
        "title": "CacheFocus: Dynamic Cache Re-Positioning for Efficient Retrieval-Augmented Generation",
        "meta": {
            "query": "LLM caching strategies for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.15075v2": {
        "url": "http://arxiv.org/abs/2502.15075v2",
        "description": "Large Language Models (LLMs) have introduced significant advancements to the\ncapabilities of Natural Language Processing (NLP) in recent years. However, as\nthese models continue to scale in size, memory constraints pose substantial\nchallenge. Key and Value cache (KV cache) quantization has been well-documented\nas a promising solution to this limitation. In this work, we provide two novel\ntheorems aimed at enhancing KV quantization methods. Our first theorem, termed\nKey-Value Norm Disparity, states that the key weight matrices by nature carry\nricher information compared to the value weight matrices, as evidenced by\nhigher spectral and Frobenius norms across most of the layers. Our second\ntheorem, Key-Driven Quantization, posits that prioritizing the quantization\nprecision of keys over values induces significant improvements to the overall\nquantization performance. In particular, assigning greater precision to the\nkeys compared to the values achieves a higher degree of precision reduction\nwith minimal impact on model accuracy. We validate these theorems through\ntheory and extensive experiments on several state-of-the-art LLM architectures\nand benchmarks. These findings offer valuable guidelines for improving KV cache\nquantization strategies, facilitating more efficient memory utilization without\ncompromising model performance across diverse NLP tasks. Source code is\navailable at https://github.com/mohsenhariri/spectral-kv.",
        "snippets": [
            "Large Language Models (LLMs) have introduced significant advancements to the\ncapabilities of Natural Language Processing (NLP) in recent years. However, as\nthese models continue to scale in size, memory constraints pose substantial\nchallenge. Key and Value cache (KV cache) quantization has been well-documented\nas a promising solution to this limitation. In this work, we provide two novel\ntheorems aimed at enhancing KV quantization methods. Our first theorem, termed\nKey-Value Norm Disparity, states that the key weight matrices by nature carry\nricher information compared to the value weight matrices, as evidenced by\nhigher spectral and Frobenius norms across most of the layers. Our second\ntheorem, Key-Driven Quantization, posits that prioritizing the quantization\nprecision of keys over values induces significant improvements to the overall\nquantization performance. In particular, assigning greater precision to the\nkeys compared to the values achieves a higher degree of precision reduction\nwith minimal impact on model accuracy. We validate these theorems through\ntheory and extensive experiments on several state-of-the-art LLM architectures\nand benchmarks. These findings offer valuable guidelines for improving KV cache\nquantization strategies, facilitating more efficient memory utilization without\ncompromising model performance across diverse NLP tasks. Source code is\navailable at https://github.com/mohsenhariri/spectral-kv."
        ],
        "title": "Quantize What Counts: Bit Allocation Insights Informed by Spectral Gaps in Keys and Values",
        "meta": {
            "query": "LLM caching strategies for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.17922v2": {
        "url": "http://arxiv.org/abs/2503.17922v2",
        "description": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
        "snippets": [
            "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
        ],
        "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for Efficient LLM Inference",
        "meta": {
            "query": "LLM caching strategies for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2504.06261v3": {
        "url": "http://arxiv.org/abs/2504.06261v3",
        "description": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the LLM instances to come up with their own collaboration\nstrategy for the problem at hand, all the while \"seeing\" each other's memory in\nthe concurrent KV cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\nmemory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)\nto avoid recomputation while improving parallel hardware utilization. We find\nthat modern reasoning-capable LLMs can perform inference with shared Key-Value\ncache out of the box, without additional fine-tuning.",
        "snippets": [
            "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the LLM instances to come up with their own collaboration\nstrategy for the problem at hand, all the while \"seeing\" each other's memory in\nthe concurrent KV cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\nmemory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE)\nto avoid recomputation while improving parallel hardware utilization. We find\nthat modern reasoning-capable LLMs can perform inference with shared Key-Value\ncache out of the box, without additional fine-tuning."
        ],
        "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
        "meta": {
            "query": "LLM caching strategies for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2412.12706v2": {
        "url": "http://arxiv.org/abs/2412.12706v2",
        "description": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV.",
        "snippets": [
            "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension separately. However,\nthese works leaving the trade-off between these two orthogonal dimensions\nlargely under-explored. In this paper, we comprehensively investigate the\ntoken-precision trade-off in KV cache compression.Experiments demonstrate that\nstoring more tokens in the KV cache with lower precision,a strategy we term\nquantized pruning, can significantly enhance the long-context performance of\nLLMs. In-depth analysis of the token-precision trade-off across key aspects\ndemonstrates that, quantized pruning achieves substantial improvements in\nretrieval-related tasks and consistently performs well across varying input\nlengths. Furthermore, quantized pruning demonstrates notable stability and\neffectiveness across different KV pruning methods, quantization strategies, and\nmodel scales. These findings offer valuable insights into optimizing KV cache\ncompression through balanced token-precision trade-off strategies. Our code is\navailable at https://github.com/zhzihao/QPruningKV."
        ],
        "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision Trade-off in KV Cache Compression",
        "meta": {
            "query": "LLM caching strategies for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2405.10443v4": {
        "url": "http://arxiv.org/abs/2405.10443v4",
        "description": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
        "snippets": [
            "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
        ],
        "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation",
        "meta": {
            "query": "LLM caching strategies for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2312.05516v3": {
        "url": "http://arxiv.org/abs/2312.05516v3",
        "description": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.",
        "snippets": [
            "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency."
        ],
        "title": "Stateful Large Language Model Serving with Pensieve",
        "meta": {
            "query": "LLM caching strategies for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2402.12065v2": {
        "url": "http://arxiv.org/abs/2402.12065v2",
        "description": "Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial memory requirements and the computational demands of\nauto-regressive text generation process. This paper addresses these challenges\nby focusing on the quantization of LLMs, a technique that reduces memory\nconsumption by converting model parameters and activations into low-bit\nintegers. We critically analyze the existing quantization approaches,\nidentifying their limitations in balancing the accuracy and efficiency of the\nquantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ\nframework especially designed for quantizing weights and the key/value (KV)\ncache of LLMs. Specifically, we incorporates past-only quantization to improve\nthe computation of attention. Additionally, we introduce two-dimensional\nquantization strategy to handle the distribution of KV cache, along with a\ncross-block reconstruction regularization for parameter optimization.\nExperiments show that WKVQuant achieves almost comparable memory savings to\nweight-activation quantization, while also approaching the performance of\nweight-only quantization.",
        "snippets": [
            "Large Language Models (LLMs) face significant deployment challenges due to\ntheir substantial memory requirements and the computational demands of\nauto-regressive text generation process. This paper addresses these challenges\nby focusing on the quantization of LLMs, a technique that reduces memory\nconsumption by converting model parameters and activations into low-bit\nintegers. We critically analyze the existing quantization approaches,\nidentifying their limitations in balancing the accuracy and efficiency of the\nquantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ\nframework especially designed for quantizing weights and the key/value (KV)\ncache of LLMs. Specifically, we incorporates past-only quantization to improve\nthe computation of attention. Additionally, we introduce two-dimensional\nquantization strategy to handle the distribution of KV cache, along with a\ncross-block reconstruction regularization for parameter optimization.\nExperiments show that WKVQuant achieves almost comparable memory savings to\nweight-activation quantization, while also approaching the performance of\nweight-only quantization."
        ],
        "title": "WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More",
        "meta": {
            "query": "LLM caching strategies for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.13212v1": {
        "url": "http://arxiv.org/abs/2410.13212v1",
        "description": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters.",
        "snippets": [
            "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters."
        ],
        "title": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise Asymmetric Quantization Configurations",
        "meta": {
            "query": "LLM caching strategies for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2501.04987v3": {
        "url": "http://arxiv.org/abs/2501.04987v3",
        "description": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
        "snippets": [
            "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
        ],
        "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
        "meta": {
            "query": "LLM caching strategies for natural language processing tasks"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1310.3584v1": {
        "url": "http://arxiv.org/abs/1310.3584v1",
        "description": "Many Information Centric Networking (ICN) proposals use a network of caches\nto bring the contents closer to the consumers, reduce the load on producers and\ndecrease the unnecessary retransmission for ISPs. Nevertheless, the existing\ncache management scheme for the network of caches obtain poor performance. The\nmain reason for performance degradation in a network of caches is the filter\neffect of the replacement policy. A cache serves the requests that generate\ncache-hits and forwards the requests that generate cache-misses. This filtering\nchanges the pattern of requests and leads to decreased hit ratios in the\nsubsequent caches. In this paper, we propose a coordinated caching scheme to\nsolve the filter effect problem by introducing the selection policy. This\npolicy manages a cache such that: i) the cache obtains a high hit ratio ii) the\nmissed requests from the cache can be used by subsequent caches to obtain a\nhigh hit ratio. Our coordinated selection scheme achieves an overall hit ratio\nof a network of caches equivalent to that of edge routers with big caches.\nMoreover, our scheme decreases the average number of evictions per cache slot\nby four order of magnitude compared to the LRU universal caching.",
        "snippets": [
            "Many Information Centric Networking (ICN) proposals use a network of caches\nto bring the contents closer to the consumers, reduce the load on producers and\ndecrease the unnecessary retransmission for ISPs. Nevertheless, the existing\ncache management scheme for the network of caches obtain poor performance. The\nmain reason for performance degradation in a network of caches is the filter\neffect of the replacement policy. A cache serves the requests that generate\ncache-hits and forwards the requests that generate cache-misses. This filtering\nchanges the pattern of requests and leads to decreased hit ratios in the\nsubsequent caches. In this paper, we propose a coordinated caching scheme to\nsolve the filter effect problem by introducing the selection policy. This\npolicy manages a cache such that: i) the cache obtains a high hit ratio ii) the\nmissed requests from the cache can be used by subsequent caches to obtain a\nhigh hit ratio. Our coordinated selection scheme achieves an overall hit ratio\nof a network of caches equivalent to that of edge routers with big caches.\nMoreover, our scheme decreases the average number of evictions per cache slot\nby four order of magnitude compared to the LRU universal caching."
        ],
        "title": "Selection Policy: Fighting against Filter Effect in Network of Caches",
        "meta": {
            "query": "cache eviction policies in LLM serving systems"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1512.00727v2": {
        "url": "http://arxiv.org/abs/1512.00727v2",
        "description": "This paper proposes to use a frequency based cache admission policy in order\nto boost the effectiveness of caches subject to skewed access distributions.\nGiven a newly accessed item and an eviction candidate from the cache, our\nscheme decides, based on the recent access history, whether it is worth\nadmitting the new item into the cache at the expense of the eviction candidate.\n  Realizing this concept is enabled through a novel approximate LFU structure\ncalled TinyLFU, which maintains an approximate representation of the access\nfrequency of a large sample of recently accessed items. TinyLFU is very compact\nand light-weight as it builds upon Bloom filter theory.\n  We study the properties of TinyLFU through simulations of both synthetic\nworkloads as well as multiple real traces from several sources. These\nsimulations demonstrate the performance boost obtained by enhancing various\nreplacement policies with the TinyLFU eviction policy. Also, a new combined\nreplacement and eviction policy scheme nicknamed W-TinyLFU is presented.\nW-TinyLFU is demonstrated to obtain equal or better hit-ratios than other state\nof the art replacement policies on these traces. It is the only scheme to\nobtain such good results on all traces.",
        "snippets": [
            "This paper proposes to use a frequency based cache admission policy in order\nto boost the effectiveness of caches subject to skewed access distributions.\nGiven a newly accessed item and an eviction candidate from the cache, our\nscheme decides, based on the recent access history, whether it is worth\nadmitting the new item into the cache at the expense of the eviction candidate.\n  Realizing this concept is enabled through a novel approximate LFU structure\ncalled TinyLFU, which maintains an approximate representation of the access\nfrequency of a large sample of recently accessed items. TinyLFU is very compact\nand light-weight as it builds upon Bloom filter theory.\n  We study the properties of TinyLFU through simulations of both synthetic\nworkloads as well as multiple real traces from several sources. These\nsimulations demonstrate the performance boost obtained by enhancing various\nreplacement policies with the TinyLFU eviction policy. Also, a new combined\nreplacement and eviction policy scheme nicknamed W-TinyLFU is presented.\nW-TinyLFU is demonstrated to obtain equal or better hit-ratios than other state\nof the art replacement policies on these traces. It is the only scheme to\nobtain such good results on all traces."
        ],
        "title": "TinyLFU: A Highly Efficient Cache Admission Policy",
        "meta": {
            "query": "cache eviction policies in LLM serving systems"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.02750v1": {
        "url": "http://arxiv.org/abs/2502.02750v1",
        "description": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
        "snippets": [
            "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency."
        ],
        "title": "Cache is King: Smart Page Eviction with eBPF",
        "meta": {
            "query": "cache eviction policies in LLM serving systems"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2106.07423v1": {
        "url": "http://arxiv.org/abs/2106.07423v1",
        "description": "In this paper, we propose an Efficient Two-Level I/O Caching Architecture\n(ETICA) for virtualized platforms that can significantly improve I/O latency,\nendurance, and cost (in terms of cache size) while preserving the reliability\nof write-pending data blocks. As opposed to previous one-level I/O caching\nschemes in virtualized platforms, our proposed architecture 1) provides two\nlevels of cache by employing both Dynamic Random-Access Memory (DRAM) and SSD\nin the I/O caching layer of virtualized platforms and 2) effectively partitions\nthe cache space between running VMs to achieve maximum performance and minimum\ncache size. To manage the two-level cache, unlike the previous reuse distance\ncalculation schemes such as Useful Reuse Distance (URD), which only consider\nthe request type and neglect the impact of cache write policy, we propose a new\nmetric, Policy Optimized reuse Distance (POD). The key idea of POD is to\neffectively calculate the reuse distance and estimate the amount of two-level\nDRAM+SSD cache space to allocate by considering both 1) the request type and 2)\nthe cache write policy. Doing so results in enhanced performance and reduced\ncache size due to the allocation of cache blocks only for the requests that\nwould be served by the I/O cache. ETICA maintains the reliability of\nwrite-pending data blocks and improves performance by 1) assigning an effective\nand fixed write policy at each level of the I/O cache hierarchy and 2)\nemploying effective promotion and eviction methods between cache levels. Our\nextensive experiments conducted with a real implementation of the proposed\ntwo-level storage caching architecture show that ETICA provides 45% higher\nperformance, compared to the state-of-the-art caching schemes in virtualized\nplatforms, while improving both cache size and SSD endurance by 51.7% and\n33.8%, respectively.",
        "snippets": [
            "In this paper, we propose an Efficient Two-Level I/O Caching Architecture\n(ETICA) for virtualized platforms that can significantly improve I/O latency,\nendurance, and cost (in terms of cache size) while preserving the reliability\nof write-pending data blocks. As opposed to previous one-level I/O caching\nschemes in virtualized platforms, our proposed architecture 1) provides two\nlevels of cache by employing both Dynamic Random-Access Memory (DRAM) and SSD\nin the I/O caching layer of virtualized platforms and 2) effectively partitions\nthe cache space between running VMs to achieve maximum performance and minimum\ncache size. To manage the two-level cache, unlike the previous reuse distance\ncalculation schemes such as Useful Reuse Distance (URD), which only consider\nthe request type and neglect the impact of cache write policy, we propose a new\nmetric, Policy Optimized reuse Distance (POD). The key idea of POD is to\neffectively calculate the reuse distance and estimate the amount of two-level\nDRAM+SSD cache space to allocate by considering both 1) the request type and 2)\nthe cache write policy. Doing so results in enhanced performance and reduced\ncache size due to the allocation of cache blocks only for the requests that\nwould be served by the I/O cache. ETICA maintains the reliability of\nwrite-pending data blocks and improves performance by 1) assigning an effective\nand fixed write policy at each level of the I/O cache hierarchy and 2)\nemploying effective promotion and eviction methods between cache levels. Our\nextensive experiments conducted with a real implementation of the proposed\ntwo-level storage caching architecture show that ETICA provides 45% higher\nperformance, compared to the state-of-the-art caching schemes in virtualized\nplatforms, while improving both cache size and SSD endurance by 51.7% and\n33.8%, respectively."
        ],
        "title": "ETICA: Efficient Two-Level I/O Caching Architecture for Virtualized Platforms",
        "meta": {
            "query": "cache eviction policies in LLM serving systems"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2110.11602v1": {
        "url": "http://arxiv.org/abs/2110.11602v1",
        "description": "Cache eviction algorithms are used widely in operating systems, databases and\nother systems that use caches to speed up execution by caching data that is\nused by the application. There are many policies such as MRU (Most Recently\nUsed), MFU (Most Frequently Used), LRU (Least Recently Used) and LFU (Least\nFrequently Used) which each have their advantages and drawbacks and are hence\nused in specific scenarios. By far, the most widely used algorithm is LRU, both\nfor its $O(1)$ speed of operation as well as its close resemblance to the kind\nof behaviour that is expected by most applications. The LFU algorithm also has\nbehaviour desirable by many real world workloads. However, in many places, the\nLRU algorithm is is preferred over the LFU algorithm because of its lower run\ntime complexity of $O(1)$ versus $O(\\log n)$. We present here an LFU cache\neviction algorithm that has a runtime complexity of $O(1)$ for all of its\noperations, which include insertion, access and deletion(eviction).",
        "snippets": [
            "Cache eviction algorithms are used widely in operating systems, databases and\nother systems that use caches to speed up execution by caching data that is\nused by the application. There are many policies such as MRU (Most Recently\nUsed), MFU (Most Frequently Used), LRU (Least Recently Used) and LFU (Least\nFrequently Used) which each have their advantages and drawbacks and are hence\nused in specific scenarios. By far, the most widely used algorithm is LRU, both\nfor its $O(1)$ speed of operation as well as its close resemblance to the kind\nof behaviour that is expected by most applications. The LFU algorithm also has\nbehaviour desirable by many real world workloads. However, in many places, the\nLRU algorithm is is preferred over the LFU algorithm because of its lower run\ntime complexity of $O(1)$ versus $O(\\log n)$. We present here an LFU cache\neviction algorithm that has a runtime complexity of $O(1)$ for all of its\noperations, which include insertion, access and deletion(eviction)."
        ],
        "title": "An O(1) algorithm for implementing the LFU cache eviction scheme",
        "meta": {
            "query": "cache eviction policies in LLM serving systems"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1604.03175v1": {
        "url": "http://arxiv.org/abs/1604.03175v1",
        "description": "We study the problem of optimal content placement over a network of caches, a\nproblem naturally arising in several networking applications, including ICNs,\nCDNs, and P2P systems. Given a demand of content request rates and paths\nfollowed, we wish to determine the content placement that maximizes the\nexpected caching gain, i.e., the reduction of routing costs due to intermediate\ncaching. The offline version of this problem is NP-hard and, in general, the\ndemand and topology may be a priori unknown. Hence, a distributed, adaptive,\nconstant approximation content placement algorithm is desired. We show that\npath replication, a simple algorithm frequently encountered in literature, can\nbe arbitrarily suboptimal when combined with traditional eviction policies,\nlike LRU, LFU, or FIFO. We propose a distributed, adaptive algorithm that\nperforms stochastic gradient ascent on a concave relaxation of the expected\ncaching gain, and constructs a probabilistic content placement within 1-1/e\nfactor from the optimal, in expectation. Motivated by our analysis, we also\npropose a novel greedy eviction policy to be used with path replication, and\nshow through numerical evaluations that both algorithms significantly\noutperform path replication with traditional eviction policies over a broad\narray of network topologies.",
        "snippets": [
            "We study the problem of optimal content placement over a network of caches, a\nproblem naturally arising in several networking applications, including ICNs,\nCDNs, and P2P systems. Given a demand of content request rates and paths\nfollowed, we wish to determine the content placement that maximizes the\nexpected caching gain, i.e., the reduction of routing costs due to intermediate\ncaching. The offline version of this problem is NP-hard and, in general, the\ndemand and topology may be a priori unknown. Hence, a distributed, adaptive,\nconstant approximation content placement algorithm is desired. We show that\npath replication, a simple algorithm frequently encountered in literature, can\nbe arbitrarily suboptimal when combined with traditional eviction policies,\nlike LRU, LFU, or FIFO. We propose a distributed, adaptive algorithm that\nperforms stochastic gradient ascent on a concave relaxation of the expected\ncaching gain, and constructs a probabilistic content placement within 1-1/e\nfactor from the optimal, in expectation. Motivated by our analysis, we also\npropose a novel greedy eviction policy to be used with path replication, and\nshow through numerical evaluations that both algorithms significantly\noutperform path replication with traditional eviction policies over a broad\narray of network topologies."
        ],
        "title": "Adaptive Caching Networks with Optimality Guarantees",
        "meta": {
            "query": "cache eviction policies in LLM serving systems"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2403.04643v2": {
        "url": "http://arxiv.org/abs/2403.04643v2",
        "description": "The emergence of LLMs has ignited a fresh surge of breakthroughs in NLP\napplications, particularly in domains such as question-answering systems and\ntext generation. As the need for longer context grows, a significant bottleneck\nin model deployment emerges due to the linear expansion of the Key-Value (KV)\ncache with the context length. Existing methods primarily rely on various\nhypotheses, such as sorting the KV cache based on attention scores for\nreplacement or eviction, to compress the KV cache and improve model throughput.\nHowever, heuristics used by these strategies may wrongly evict essential KV\ncache, which can significantly degrade model performance. In this paper, we\npropose QAQ, a Quality Adaptive Quantization scheme for the KV cache. We\ntheoretically demonstrate that key cache and value cache exhibit distinct\nsensitivities to quantization, leading to the formulation of separate\nquantization strategies for their non-uniform quantization. Through the\nintegration of dedicated outlier handling, as well as an improved\nattention-aware approach, QAQ achieves up to 10x the compression ratio of the\nKV cache size with a neglectable impact on model performance. QAQ significantly\nreduces the practical hurdles of deploying LLMs, opening up new possibilities\nfor longer-context applications. The code is available at\ngithub.com/ClubieDong/KVCacheQuantization.",
        "snippets": [
            "The emergence of LLMs has ignited a fresh surge of breakthroughs in NLP\napplications, particularly in domains such as question-answering systems and\ntext generation. As the need for longer context grows, a significant bottleneck\nin model deployment emerges due to the linear expansion of the Key-Value (KV)\ncache with the context length. Existing methods primarily rely on various\nhypotheses, such as sorting the KV cache based on attention scores for\nreplacement or eviction, to compress the KV cache and improve model throughput.\nHowever, heuristics used by these strategies may wrongly evict essential KV\ncache, which can significantly degrade model performance. In this paper, we\npropose QAQ, a Quality Adaptive Quantization scheme for the KV cache. We\ntheoretically demonstrate that key cache and value cache exhibit distinct\nsensitivities to quantization, leading to the formulation of separate\nquantization strategies for their non-uniform quantization. Through the\nintegration of dedicated outlier handling, as well as an improved\nattention-aware approach, QAQ achieves up to 10x the compression ratio of the\nKV cache size with a neglectable impact on model performance. QAQ significantly\nreduces the practical hurdles of deploying LLMs, opening up new possibilities\nfor longer-context applications. The code is available at\ngithub.com/ClubieDong/KVCacheQuantization."
        ],
        "title": "QAQ: Quality Adaptive Quantization for LLM KV Cache",
        "meta": {
            "query": "cache eviction policies in LLM serving systems"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.05004v1": {
        "url": "http://arxiv.org/abs/2410.05004v1",
        "description": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT.",
        "snippets": [
            "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT."
        ],
        "title": "Fast State Restoration in LLM Serving with HCache",
        "meta": {
            "query": "cache eviction policies in LLM serving systems"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2002.08080v2": {
        "url": "http://arxiv.org/abs/2002.08080v2",
        "description": "We consider distributed and dynamic caching of coded content at small base\nstations (SBSs) in an area served by a macro base station (MBS). Specifically,\ncontent is encoded using a maximum distance separable code and cached according\nto a time-to-live (TTL) cache eviction policy, which allows coded packets to be\nremoved from the caches at periodic times. Mobile users requesting a particular\ncontent download coded packets from SBSs within communication range. If\nadditional packets are required to decode the file, these are downloaded from\nthe MBS. We formulate an optimization problem that is efficiently solved\nnumerically, providing TTL caching policies minimizing the overall network\nload. We demonstrate that distributed coded caching using TTL caching policies\ncan offer significant reductions in terms of network load when request arrivals\nare bursty. We show how the distributed coded caching problem utilizing TTL\ncaching policies can be analyzed as a specific single cache, convex\noptimization problem. Our problem encompasses static caching and the single\ncache as special cases. We prove that, interestingly, static caching is optimal\nunder a Poisson request process, and that for a single cache the optimization\nproblem has a surprisingly simple solution.",
        "snippets": [
            "We consider distributed and dynamic caching of coded content at small base\nstations (SBSs) in an area served by a macro base station (MBS). Specifically,\ncontent is encoded using a maximum distance separable code and cached according\nto a time-to-live (TTL) cache eviction policy, which allows coded packets to be\nremoved from the caches at periodic times. Mobile users requesting a particular\ncontent download coded packets from SBSs within communication range. If\nadditional packets are required to decode the file, these are downloaded from\nthe MBS. We formulate an optimization problem that is efficiently solved\nnumerically, providing TTL caching policies minimizing the overall network\nload. We demonstrate that distributed coded caching using TTL caching policies\ncan offer significant reductions in terms of network load when request arrivals\nare bursty. We show how the distributed coded caching problem utilizing TTL\ncaching policies can be analyzed as a specific single cache, convex\noptimization problem. Our problem encompasses static caching and the single\ncache as special cases. We prove that, interestingly, static caching is optimal\nunder a Poisson request process, and that for a single cache the optimization\nproblem has a surprisingly simple solution."
        ],
        "title": "Dynamic Coded Caching in Wireless Networks",
        "meta": {
            "query": "cache eviction policies in LLM serving systems"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2403.09636v2": {
        "url": "http://arxiv.org/abs/2403.09636v2",
        "description": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
        "snippets": [
            "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
        ],
        "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
        "meta": {
            "query": "cache eviction policies in LLM serving systems"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2504.07494v1": {
        "url": "http://arxiv.org/abs/2504.07494v1",
        "description": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
        "snippets": [
            "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems."
        ],
        "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving",
        "meta": {
            "query": "cache eviction policies in LLM serving systems"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.18292v1": {
        "url": "http://arxiv.org/abs/2503.18292v1",
        "description": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average).",
        "snippets": [
            "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average)."
        ],
        "title": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity",
        "meta": {
            "query": "cache eviction policies in LLM serving systems"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2301.11886v1": {
        "url": "http://arxiv.org/abs/2301.11886v1",
        "description": "Recent work shows the effectiveness of Machine Learning (ML) to reduce cache\nmiss ratios by making better eviction decisions than heuristics. However,\nstate-of-the-art ML caches require many predictions to make an eviction\ndecision, making them impractical for high-throughput caching systems. This\npaper introduces Machine learning At the Tail (MAT), a framework to build\nefficient ML-based caching systems by integrating an ML module with a\ntraditional cache system based on a heuristic algorithm. MAT treats the\nheuristic algorithm as a filter to receive high-quality samples to train an ML\nmodel and likely candidate objects for evictions. We evaluate MAT on 8\nproduction workloads, spanning storage, in-memory caching, and CDNs. The\nsimulation experiments show MAT reduces the number of costly ML\npredictions-per-eviction from 63 to 2, while achieving comparable miss ratios\nto the state-of-the-art ML cache system. We compare a MAT prototype system with\nan LRU-based caching system in the same setting and show that they achieve\nsimilar request rates.",
        "snippets": [
            "Recent work shows the effectiveness of Machine Learning (ML) to reduce cache\nmiss ratios by making better eviction decisions than heuristics. However,\nstate-of-the-art ML caches require many predictions to make an eviction\ndecision, making them impractical for high-throughput caching systems. This\npaper introduces Machine learning At the Tail (MAT), a framework to build\nefficient ML-based caching systems by integrating an ML module with a\ntraditional cache system based on a heuristic algorithm. MAT treats the\nheuristic algorithm as a filter to receive high-quality samples to train an ML\nmodel and likely candidate objects for evictions. We evaluate MAT on 8\nproduction workloads, spanning storage, in-memory caching, and CDNs. The\nsimulation experiments show MAT reduces the number of costly ML\npredictions-per-eviction from 63 to 2, while achieving comparable miss ratios\nto the state-of-the-art ML cache system. We compare a MAT prototype system with\nan LRU-based caching system in the same setting and show that they achieve\nsimilar request rates."
        ],
        "title": "A Learned Cache Eviction Framework with Minimal Overhead",
        "meta": {
            "query": "cache eviction policies in LLM serving systems"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2312.06235v1": {
        "url": "http://arxiv.org/abs/2312.06235v1",
        "description": "Randomizing the mapping of addresses to cache entries has proven to be an\neffective technique for hardening caches against contention-based attacks like\nPrime+Prome. While attacks and defenses are still evolving, it is clear that\nrandomized caches significantly increase the security against such attacks.\nHowever, one aspect that is missing from most analyses of randomized cache\narchitectures is the choice of the replacement policy. Often, only the random-\nand LRU replacement policies are investigated. However, LRU is not applicable\nto randomized caches due to its immense hardware overhead, while the random\nreplacement policy is not ideal from a performance and security perspective.\n  In this paper, we explore replacement policies for randomized caches. We\ndevelop two new replacement policies and evaluate a total of five replacement\npolicies regarding their security against Prime+Prune+Probe attackers.\nMoreover, we analyze the effect of the replacement policy on the system's\nperformance and quantify the introduced hardware overhead. We implement\nrandomized caches with configurable replacement policies in software and\nhardware using a custom cache simulator, gem5, and the CV32E40P RISC-V core.\nAmong others, we show that the construction of eviction sets with our new\npolicy, VARP-64, requires over 25-times more cache accesses than with the\nrandom replacement policy while also enhancing overall performance.",
        "snippets": [
            "Randomizing the mapping of addresses to cache entries has proven to be an\neffective technique for hardening caches against contention-based attacks like\nPrime+Prome. While attacks and defenses are still evolving, it is clear that\nrandomized caches significantly increase the security against such attacks.\nHowever, one aspect that is missing from most analyses of randomized cache\narchitectures is the choice of the replacement policy. Often, only the random-\nand LRU replacement policies are investigated. However, LRU is not applicable\nto randomized caches due to its immense hardware overhead, while the random\nreplacement policy is not ideal from a performance and security perspective.\n  In this paper, we explore replacement policies for randomized caches. We\ndevelop two new replacement policies and evaluate a total of five replacement\npolicies regarding their security against Prime+Prune+Probe attackers.\nMoreover, we analyze the effect of the replacement policy on the system's\nperformance and quantify the introduced hardware overhead. We implement\nrandomized caches with configurable replacement policies in software and\nhardware using a custom cache simulator, gem5, and the CV32E40P RISC-V core.\nAmong others, we show that the construction of eviction sets with our new\npolicy, VARP-64, requires over 25-times more cache accesses than with the\nrandom replacement policy while also enhancing overall performance."
        ],
        "title": "On The Effect of Replacement Policies on The Security of Randomized Cache Architectures",
        "meta": {
            "query": "cache eviction policies in LLM serving systems"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1901.01187v1": {
        "url": "http://arxiv.org/abs/1901.01187v1",
        "description": "In this paper, we propose PopNetCod, a popularity-based caching policy for\nnetwork coding enabled Named Data Networking. PopNetCod is a distributed\ncaching policy, in which each router measures the local popularity of the\ncontent objects by analyzing the requests that it receives. It then uses this\ninformation to decide which Data packets to cache or evict from its content\nstore. Since network coding is used, partial caching of content objects is\nsupported, which facilitates the management of the content store. The routers\ndecide the Data packets that they cache or evict in an online manner when they\nreceive requests for Data packets. This allows the most popular Data packets to\nbe cached closer to the network edges. The evaluation of PopNetCod shows an\nimproved cache-hit rate compared to the widely used Leave Copy Everywhere\nplacement policy and the Least Recently Used eviction policy. The improved\ncache-hit rate helps the clients to achieve higher goodput, while it also\nreduces the load on the source servers.",
        "snippets": [
            "In this paper, we propose PopNetCod, a popularity-based caching policy for\nnetwork coding enabled Named Data Networking. PopNetCod is a distributed\ncaching policy, in which each router measures the local popularity of the\ncontent objects by analyzing the requests that it receives. It then uses this\ninformation to decide which Data packets to cache or evict from its content\nstore. Since network coding is used, partial caching of content objects is\nsupported, which facilitates the management of the content store. The routers\ndecide the Data packets that they cache or evict in an online manner when they\nreceive requests for Data packets. This allows the most popular Data packets to\nbe cached closer to the network edges. The evaluation of PopNetCod shows an\nimproved cache-hit rate compared to the widely used Leave Copy Everywhere\nplacement policy and the Least Recently Used eviction policy. The improved\ncache-hit rate helps the clients to achieve higher goodput, while it also\nreduces the load on the source servers."
        ],
        "title": "PopNetCod: A Popularity-based Caching Policy for Network Coding enabled Named Data Networking",
        "meta": {
            "query": "cache eviction policies in LLM serving systems"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2501.01792v1": {
        "url": "http://arxiv.org/abs/2501.01792v1",
        "description": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
        "snippets": [
            "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
        ],
        "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.16210v1": {
        "url": "http://arxiv.org/abs/2505.16210v1",
        "description": "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used.",
        "snippets": [
            "Large Language Models (LLMs) have demonstrated remarkable proficiency across\na wide range of tasks. However, LLMs often require larger batch sizes to\nenhance throughput or longer context lengths to meet task demands, which\nsignificantly increases the memory resource consumption of the Key-Value (KV)\ncache during inference, becoming a major bottleneck in LLM deployment. To\naddress this issue, quantization is a common and straightforward approach.\nCurrently, quantization methods for activations are limited to 8-bit, and\nquantization to even lower bits can lead to substantial accuracy drops. To\nfurther save space by quantizing the KV cache to even lower bits, we analyzed\nthe element distribution of the KV cache and designed the NQKV algorithm. Since\nthe elements within each block of the KV cache follow a normal distribution,\nNQKV employs per-block quantile quantization to achieve\ninformation-theoretically optimal quantization error. Without significantly\ncompromising model output quality, NQKV enables the OPT model to perform\ninference with an 2x larger batch size or a 4x longer context length, and it\nimproves throughput by 9.3x compared to when the KV cache is not used."
        ],
        "title": "NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1312.2306v1": {
        "url": "http://arxiv.org/abs/1312.2306v1",
        "description": "Embedded system software is highly constrained from performance, memory\nfootprint, energy consumption and implementing cost view point. It is always\ndesirable to obtain better Instructions per Cycle. Instruction cache has major\ncontribution in improving IPC. Cache memories are realized on the same chip\nwhere the processor is running. This considerably increases the system cost as\nwell. Hence, it is required to maintain a trade off between cache sizes and\nperformance improvement offered. Determining the number of cache lines and size\nof cache line are important parameters for cache designing. The design space\nfor cache is quite large. It is time taking to execute the given application\nwith different cache sizes on an instruction set simulator to figure out the\noptimal cache size. In this paper, a technique is proposed to identify a number\nof cache lines and cache line size for the L1 instruction cache that will offer\nbest or nearly best IPC. Cache size is derived, at a higher abstraction level,\nfrom basic block analysis in the Low Level Virtual Machine environment. The\ncache size estimated is cross validated by simulating the set of benchmark\napplications with different cache sizes in simple scalar simulator. The\nproposed method seems to be superior in terms of estimation accuracy and\nestimation time as compared to the existing methods for estimation of optimal\ncache size parameters like cache line size, number of cache lines.",
        "snippets": [
            "Embedded system software is highly constrained from performance, memory\nfootprint, energy consumption and implementing cost view point. It is always\ndesirable to obtain better Instructions per Cycle. Instruction cache has major\ncontribution in improving IPC. Cache memories are realized on the same chip\nwhere the processor is running. This considerably increases the system cost as\nwell. Hence, it is required to maintain a trade off between cache sizes and\nperformance improvement offered. Determining the number of cache lines and size\nof cache line are important parameters for cache designing. The design space\nfor cache is quite large. It is time taking to execute the given application\nwith different cache sizes on an instruction set simulator to figure out the\noptimal cache size. In this paper, a technique is proposed to identify a number\nof cache lines and cache line size for the L1 instruction cache that will offer\nbest or nearly best IPC. Cache size is derived, at a higher abstraction level,\nfrom basic block analysis in the Low Level Virtual Machine environment. The\ncache size estimated is cross validated by simulating the set of benchmark\napplications with different cache sizes in simple scalar simulator. The\nproposed method seems to be superior in terms of estimation accuracy and\nestimation time as compared to the existing methods for estimation of optimal\ncache size parameters like cache line size, number of cache lines."
        ],
        "title": "Dominant block guided optimal cache size estimation to maximize IPC of embedded software",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2110.11110v1": {
        "url": "http://arxiv.org/abs/2110.11110v1",
        "description": "This paper considers the secretive coded caching problem with shared caches\nin which no user must have access to the files that it did not demand. In a\nshared cache network, the users are served by a smaller number of helper caches\nand each user is connected to exactly one helper cache. To ensure the secrecy\nconstraint in shared cache networks, each user is required to have an\nindividual cache of at least unit file size. For this setting, a secretive\ncoded caching scheme was proposed recently in the literature\n(\\enquote{Secretive Coded Caching with Shared Caches}, in \\textit{IEEE\nCommunications Letters}, 2021), and it requires a subpacketization level which\nis in the exponential order of the number of helper caches. By utilizing the\nPDA constructions, we propose a procedure to obtain new secretive coded caching\nschemes for shared caches with reduced subpacketization levels. We also show\nthat the existing secretive coded caching scheme for shared caches can be\nrecovered using our procedure. Furthermore, we derive a lower bound on the\nsecretive transmission rate using cut-set arguments and demonstrate the\norder-optimality of the proposed secretive coded caching scheme.",
        "snippets": [
            "This paper considers the secretive coded caching problem with shared caches\nin which no user must have access to the files that it did not demand. In a\nshared cache network, the users are served by a smaller number of helper caches\nand each user is connected to exactly one helper cache. To ensure the secrecy\nconstraint in shared cache networks, each user is required to have an\nindividual cache of at least unit file size. For this setting, a secretive\ncoded caching scheme was proposed recently in the literature\n(\\enquote{Secretive Coded Caching with Shared Caches}, in \\textit{IEEE\nCommunications Letters}, 2021), and it requires a subpacketization level which\nis in the exponential order of the number of helper caches. By utilizing the\nPDA constructions, we propose a procedure to obtain new secretive coded caching\nschemes for shared caches with reduced subpacketization levels. We also show\nthat the existing secretive coded caching scheme for shared caches can be\nrecovered using our procedure. Furthermore, we derive a lower bound on the\nsecretive transmission rate using cut-set arguments and demonstrate the\norder-optimality of the proposed secretive coded caching scheme."
        ],
        "title": "A Secretive Coded Caching for Shared Cache Systems using PDAs",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.18517v1": {
        "url": "http://arxiv.org/abs/2410.18517v1",
        "description": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
        "snippets": [
            "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
        ],
        "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2407.12866v1": {
        "url": "http://arxiv.org/abs/2407.12866v1",
        "description": "The efficiency of large language models (LLMs) remains a critical challenge,\nparticularly in contexts where computational resources are limited. Traditional\nattention mechanisms in these models, while powerful, require significant\ncomputational and memory resources due to the necessity of recalculating and\nstoring attention weights across different layers. This paper introduces a\nnovel Shared Attention (SA) mechanism, designed to enhance the efficiency of\nLLMs by directly sharing computed attention weights across multiple layers.\nUnlike previous methods that focus on sharing intermediate Key-Value (KV)\ncaches, our approach utilizes the isotropic tendencies of attention\ndistributions observed in advanced LLMs post-pretraining to reduce both the\ncomputational flops and the size of the KV cache required during inference. We\nempirically demonstrate that implementing SA across various LLMs results in\nminimal accuracy loss on standard benchmarks. Our findings suggest that SA not\nonly conserves computational resources but also maintains robust model\nperformance, thereby facilitating the deployment of more efficient LLMs in\nresource-constrained environments.",
        "snippets": [
            "The efficiency of large language models (LLMs) remains a critical challenge,\nparticularly in contexts where computational resources are limited. Traditional\nattention mechanisms in these models, while powerful, require significant\ncomputational and memory resources due to the necessity of recalculating and\nstoring attention weights across different layers. This paper introduces a\nnovel Shared Attention (SA) mechanism, designed to enhance the efficiency of\nLLMs by directly sharing computed attention weights across multiple layers.\nUnlike previous methods that focus on sharing intermediate Key-Value (KV)\ncaches, our approach utilizes the isotropic tendencies of attention\ndistributions observed in advanced LLMs post-pretraining to reduce both the\ncomputational flops and the size of the KV cache required during inference. We\nempirically demonstrate that implementing SA across various LLMs results in\nminimal accuracy loss on standard benchmarks. Our findings suggest that SA not\nonly conserves computational resources but also maintains robust model\nperformance, thereby facilitating the deployment of more efficient LLMs in\nresource-constrained environments."
        ],
        "title": "Beyond KV Caching: Shared Attention for Efficient LLMs",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.18893v1": {
        "url": "http://arxiv.org/abs/2503.18893v1",
        "description": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.",
        "snippets": [
            "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV."
        ],
        "title": "xKV: Cross-Layer SVD for KV-Cache Compression",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2305.17118v2": {
        "url": "http://arxiv.org/abs/2305.17118v2",
        "description": "Large language models(LLMs) have sparked a new wave of exciting AI\napplications. Hosting these models at scale requires significant memory\nresources. One crucial memory bottleneck for the deployment stems from the\ncontext window. It is commonly recognized that model weights are memory hungry;\nhowever, the size of key-value embedding stored during the generation process\n(KV cache) can easily surpass the model size. The enormous size of the KV cache\nputs constraints on the inference batch size, which is crucial for high\nthroughput inference workload. Inspired by an interesting observation of the\nattention scores, we hypothesize the persistence of importance: only pivotal\ntokens, which had a substantial influence at one step, will significantly\ninfluence future generations. Based on our empirical verification and\ntheoretical analysis around this hypothesis, we propose Scissorhands, a system\nthat maintains the memory usage of the KV cache at a fixed budget without\nfinetuning the model. In essence, Scissorhands manages the KV cache by storing\nthe pivotal tokens with a higher probability. We validate that Scissorhands\nreduces the inference memory usage of the KV cache by up to 5X without\ncompromising model quality. We further demonstrate that Scissorhands can be\ncombined with 4-bit quantization, traditionally used to compress model weights,\nto achieve up to 20X compression.",
        "snippets": [
            "Large language models(LLMs) have sparked a new wave of exciting AI\napplications. Hosting these models at scale requires significant memory\nresources. One crucial memory bottleneck for the deployment stems from the\ncontext window. It is commonly recognized that model weights are memory hungry;\nhowever, the size of key-value embedding stored during the generation process\n(KV cache) can easily surpass the model size. The enormous size of the KV cache\nputs constraints on the inference batch size, which is crucial for high\nthroughput inference workload. Inspired by an interesting observation of the\nattention scores, we hypothesize the persistence of importance: only pivotal\ntokens, which had a substantial influence at one step, will significantly\ninfluence future generations. Based on our empirical verification and\ntheoretical analysis around this hypothesis, we propose Scissorhands, a system\nthat maintains the memory usage of the KV cache at a fixed budget without\nfinetuning the model. In essence, Scissorhands manages the KV cache by storing\nthe pivotal tokens with a higher probability. We validate that Scissorhands\nreduces the inference memory usage of the KV cache by up to 5X without\ncompromising model quality. We further demonstrate that Scissorhands can be\ncombined with 4-bit quantization, traditionally used to compress model weights,\nto achieve up to 20X compression."
        ],
        "title": "Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2309.06180v1": {
        "url": "http://arxiv.org/abs/2309.06180v1",
        "description": "High throughput serving of large language models (LLMs) requires batching\nsufficiently many requests at a time. However, existing systems struggle\nbecause the key-value cache (KV cache) memory for each request is huge and\ngrows and shrinks dynamically. When managed inefficiently, this memory can be\nsignificantly wasted by fragmentation and redundant duplication, limiting the\nbatch size. To address this problem, we propose PagedAttention, an attention\nalgorithm inspired by the classical virtual memory and paging techniques in\noperating systems. On top of it, we build vLLM, an LLM serving system that\nachieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV\ncache within and across requests to further reduce memory usage. Our\nevaluations show that vLLM improves the throughput of popular LLMs by\n2-4$\\times$ with the same level of latency compared to the state-of-the-art\nsystems, such as FasterTransformer and Orca. The improvement is more pronounced\nwith longer sequences, larger models, and more complex decoding algorithms.\nvLLM's source code is publicly available at\nhttps://github.com/vllm-project/vllm",
        "snippets": [
            "High throughput serving of large language models (LLMs) requires batching\nsufficiently many requests at a time. However, existing systems struggle\nbecause the key-value cache (KV cache) memory for each request is huge and\ngrows and shrinks dynamically. When managed inefficiently, this memory can be\nsignificantly wasted by fragmentation and redundant duplication, limiting the\nbatch size. To address this problem, we propose PagedAttention, an attention\nalgorithm inspired by the classical virtual memory and paging techniques in\noperating systems. On top of it, we build vLLM, an LLM serving system that\nachieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV\ncache within and across requests to further reduce memory usage. Our\nevaluations show that vLLM improves the throughput of popular LLMs by\n2-4$\\times$ with the same level of latency compared to the state-of-the-art\nsystems, such as FasterTransformer and Orca. The improvement is more pronounced\nwith longer sequences, larger models, and more complex decoding algorithms.\nvLLM's source code is publicly available at\nhttps://github.com/vllm-project/vllm"
        ],
        "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2207.05975v1": {
        "url": "http://arxiv.org/abs/2207.05975v1",
        "description": "Caching is a crucial component of many computer systems, so naturally it is a\nwell-studied topic in algorithm design. Much of traditional caching research\nstudies cache management for a single-user or single-processor environment. In\nthis paper, we propose two related generalizations of the classical caching\nproblem that capture issues that arise in a multi-user or multi-processor\nenvironment. In the caching with reserves problem, a caching algorithm is\nrequired to maintain at least $k_i$ pages belonging to user $i$ in the cache at\nany time, for some given reserve capacities $k_i$. In the public-private\ncaching problem, the cache of total size $k$ is partitioned into subcaches, a\nprivate cache of size $k_i$ for each user $i$ and a shared public cache usable\nby any user. In both of these models, as in the classical caching framework,\nthe objective of the algorithm is to dynamically maintain the cache so as to\nminimize the total number of cache misses.\n  We show that caching with reserves and public-private caching models are\nequivalent up to constant factors, and thus focus on the former. Unlike\nclassical caching, both of these models turn out to be NP-hard even in the\noffline setting, where the page sequence is known in advance. For the offline\nsetting, we design a 2-approximation algorithm, whose analysis carefully keeps\ntrack of a potential function to bound the cost. In the online setting, we\nfirst design an $O(\\ln k)$-competitive fractional algorithm using the\nprimal-dual framework, and then show how to convert it online to a randomized\nintegral algorithm with the same guarantee.",
        "snippets": [
            "Caching is a crucial component of many computer systems, so naturally it is a\nwell-studied topic in algorithm design. Much of traditional caching research\nstudies cache management for a single-user or single-processor environment. In\nthis paper, we propose two related generalizations of the classical caching\nproblem that capture issues that arise in a multi-user or multi-processor\nenvironment. In the caching with reserves problem, a caching algorithm is\nrequired to maintain at least $k_i$ pages belonging to user $i$ in the cache at\nany time, for some given reserve capacities $k_i$. In the public-private\ncaching problem, the cache of total size $k$ is partitioned into subcaches, a\nprivate cache of size $k_i$ for each user $i$ and a shared public cache usable\nby any user. In both of these models, as in the classical caching framework,\nthe objective of the algorithm is to dynamically maintain the cache so as to\nminimize the total number of cache misses.\n  We show that caching with reserves and public-private caching models are\nequivalent up to constant factors, and thus focus on the former. Unlike\nclassical caching, both of these models turn out to be NP-hard even in the\noffline setting, where the page sequence is known in advance. For the offline\nsetting, we design a 2-approximation algorithm, whose analysis carefully keeps\ntrack of a potential function to bound the cost. In the online setting, we\nfirst design an $O(\\ln k)$-competitive fractional algorithm using the\nprimal-dual framework, and then show how to convert it online to a randomized\nintegral algorithm with the same guarantee."
        ],
        "title": "Caching with Reserves",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.14647v1": {
        "url": "http://arxiv.org/abs/2503.14647v1",
        "description": "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing.",
        "snippets": [
            "Across large language model (LLM) applications, we observe an emerging trend\nfor reusing KV caches to save the prefill delays of processing repeated input\ntexts in different LLM inputs. This has led to a broad design space, including\ncolocating stored KV caches with (or close to) GPUs to various KV cache\ncompression. However, a key question remains unanswered: can these delay\nreductions also be economically favorable? Specifically, we ask whether a\ndeveloper can use public cloud services to store precomputed KV caches and\nreuse them to save delay without incurring more costs in terms of compute,\nstorage, and network. To answer this question, we propose an validated\nanalytical model for the cloud cost (in compute, storage, and network) of\nstoring and reusing KV caches based on various workload parameters, such as\nreuse frequency, generated text lengths, model sizes, etc. Preliminary results\nshow that KV cache reusing is able to save both delay and cloud cost across a\nrange of workloads with long context. And we call more efforts on building more\neconomical context augmented LLM by KV cache reusing."
        ],
        "title": "Towards More Economical Context-Augmented LLM Generation by Reusing Stored KV Cache",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2411.17089v2": {
        "url": "http://arxiv.org/abs/2411.17089v2",
        "description": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR.",
        "snippets": [
            "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR."
        ],
        "title": "KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial Recomputation",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2006.11283v2": {
        "url": "http://arxiv.org/abs/2006.11283v2",
        "description": "We propose a decentralized caching policy for wireless heterogeneous networks\nthat makes content placement decisions based on pairwise interactions between\ncache nodes. We call our proposed scheme {\\gamma}-exclusion cache placement\n(GEC), where a parameter {\\gamma} controls an exclusion radius that discourages\nnearby caches from storing redundant content. GEC takes into account item\npopularity and the nodes' caching priorities and leverages negative dependence\nto relax the classic 0-1 knapsack problem to yield spatially balanced sampling\nacross caches. We show that GEC guarantees a better concentration (reduced\nvariance) of the required cache storage size than the state of the art, and\nthat the cache size constraints can be satisfied with high probability. Given a\ncache hit probability target, we compare the 95\\% confidence intervals of the\nrequired cache sizes for three caching schemes: (i) independent placement, (ii)\nhard exclusion caching (HEC), and (iii) the proposed GEC approach. For uniform\nspatial traffic, we demonstrate that GEC provides approximately a 3x and 2x\nreduction in required cache size over (i) and (ii), respectively. For\nnon-uniform spatial traffic based on realistic peak-hour variations in urban\nscenarios, the gains are even greater.",
        "snippets": [
            "We propose a decentralized caching policy for wireless heterogeneous networks\nthat makes content placement decisions based on pairwise interactions between\ncache nodes. We call our proposed scheme {\\gamma}-exclusion cache placement\n(GEC), where a parameter {\\gamma} controls an exclusion radius that discourages\nnearby caches from storing redundant content. GEC takes into account item\npopularity and the nodes' caching priorities and leverages negative dependence\nto relax the classic 0-1 knapsack problem to yield spatially balanced sampling\nacross caches. We show that GEC guarantees a better concentration (reduced\nvariance) of the required cache storage size than the state of the art, and\nthat the cache size constraints can be satisfied with high probability. Given a\ncache hit probability target, we compare the 95\\% confidence intervals of the\nrequired cache sizes for three caching schemes: (i) independent placement, (ii)\nhard exclusion caching (HEC), and (iii) the proposed GEC approach. For uniform\nspatial traffic, we demonstrate that GEC provides approximately a 3x and 2x\nreduction in required cache size over (i) and (ii), respectively. For\nnon-uniform spatial traffic based on realistic peak-hour variations in urban\nscenarios, the gains are even greater."
        ],
        "title": "Spatial Concentration of Caching in Wireless Heterogeneous Networks",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.17787v1": {
        "url": "http://arxiv.org/abs/2505.17787v1",
        "description": "Large language models (LLMs) have gained great success in various domains.\nExisting systems cache Key and Value within the attention block to avoid\nredundant computations. However, the size of key-value cache (KV cache) is\nunpredictable and can even be tens of times larger than the weights in the long\ncontext length scenario. In this work, we propose Titanus, a software-hardware\nco-design to efficiently compress the KV cache on-the-fly. We first propose the\ncascade pruning-quantization (CPQ) method to reduce the KV cache movement. The\nhierarchical quantization extension strategy is introduced to tackle the\nnon-independent per-channel quantization issue. To further reduce KV cache\nmovement, we transfer only the non-zero KV cache between the accelerator and\noff-chip memory. Moreover, we customize a two-stage design space exploration\nframework for the CPQ method. A novel pipeline and parallelism dataflow is\ndesigned to reduce the first token generation time. Experiments show that\nTitanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency\n(throughput) compared to Nvidia A100 GPU and FlightLLM respectively. The code\nfor Titanus is available at\nhttps://github.com/peilin-chen/Titanus-for-LLM-acceleration.",
        "snippets": [
            "Large language models (LLMs) have gained great success in various domains.\nExisting systems cache Key and Value within the attention block to avoid\nredundant computations. However, the size of key-value cache (KV cache) is\nunpredictable and can even be tens of times larger than the weights in the long\ncontext length scenario. In this work, we propose Titanus, a software-hardware\nco-design to efficiently compress the KV cache on-the-fly. We first propose the\ncascade pruning-quantization (CPQ) method to reduce the KV cache movement. The\nhierarchical quantization extension strategy is introduced to tackle the\nnon-independent per-channel quantization issue. To further reduce KV cache\nmovement, we transfer only the non-zero KV cache between the accelerator and\noff-chip memory. Moreover, we customize a two-stage design space exploration\nframework for the CPQ method. A novel pipeline and parallelism dataflow is\ndesigned to reduce the first token generation time. Experiments show that\nTitanus achieves 159.9x (49.6x) and 34.8x (29.2x) energy efficiency\n(throughput) compared to Nvidia A100 GPU and FlightLLM respectively. The code\nfor Titanus is available at\nhttps://github.com/peilin-chen/Titanus-for-LLM-acceleration."
        ],
        "title": "Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM Acceleration",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.19258v3": {
        "url": "http://arxiv.org/abs/2410.19258v3",
        "description": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
        "snippets": [
            "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
        ],
        "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2405.12591v1": {
        "url": "http://arxiv.org/abs/2405.12591v1",
        "description": "Key-value~(KV) caching is an important technique to accelerate the inference\nof large language models~(LLMs), but incurs significant memory overhead. To\ncompress the size of KV cache, existing methods often compromise precision or\nrequire extra data for calibration, limiting their practicality in LLM\ndeployment. In this paper, we introduce \\textbf{DecoQuant}, a novel data-free\nlow-bit quantization technique based on tensor decomposition methods, to\neffectively compress KV cache. Our core idea is to adjust the outlier\ndistribution of the original matrix by performing tensor decomposition, so that\nthe quantization difficulties are migrated from the matrix to decomposed local\ntensors. Specially, we find that outliers mainly concentrate on small local\ntensors, while large tensors tend to have a narrower value range. Based on this\nfinding, we propose to apply low-bit quantization to the large tensor, while\nmaintaining high-precision representation for the small tensor. Furthermore, we\nutilize the proposed quantization method to compress the KV cache of LLMs to\naccelerate the inference and develop an efficient dequantization kernel\ntailored specifically for DecoQuant. Through extensive experiments, DecoQuant\ndemonstrates remarkable efficiency gains, showcasing up to a $\\sim$75\\%\nreduction in memory footprint while maintaining comparable generation quality.",
        "snippets": [
            "Key-value~(KV) caching is an important technique to accelerate the inference\nof large language models~(LLMs), but incurs significant memory overhead. To\ncompress the size of KV cache, existing methods often compromise precision or\nrequire extra data for calibration, limiting their practicality in LLM\ndeployment. In this paper, we introduce \\textbf{DecoQuant}, a novel data-free\nlow-bit quantization technique based on tensor decomposition methods, to\neffectively compress KV cache. Our core idea is to adjust the outlier\ndistribution of the original matrix by performing tensor decomposition, so that\nthe quantization difficulties are migrated from the matrix to decomposed local\ntensors. Specially, we find that outliers mainly concentrate on small local\ntensors, while large tensors tend to have a narrower value range. Based on this\nfinding, we propose to apply low-bit quantization to the large tensor, while\nmaintaining high-precision representation for the small tensor. Furthermore, we\nutilize the proposed quantization method to compress the KV cache of LLMs to\naccelerate the inference and develop an efficient dequantization kernel\ntailored specifically for DecoQuant. Through extensive experiments, DecoQuant\ndemonstrates remarkable efficiency gains, showcasing up to a $\\sim$75\\%\nreduction in memory footprint while maintaining comparable generation quality."
        ],
        "title": "Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2405.03917v1": {
        "url": "http://arxiv.org/abs/2405.03917v1",
        "description": "Efficient deployment of Large Language Models (LLMs) requires batching\nmultiple requests together to improve throughput. As the batch size, context\nlength, or model size increases, the size of the key and value (KV) cache can\nquickly become the main contributor to GPU memory usage and the bottleneck of\ninference latency. Quantization has emerged as an effective technique for KV\ncache compression, but existing methods still fail at very low bit widths. We\nobserve that distinct channels of a key/value activation embedding are highly\ninter-dependent, and the joint entropy of multiple channels grows at a slower\nrate than the sum of their marginal entropies. Based on this insight, we\npropose Coupled Quantization (CQ), which couples multiple key/value channels\ntogether to exploit their inter-dependency and encode the activations in a more\ninformation-efficient manner. Extensive experiments reveal that CQ outperforms\nor is competitive with existing baselines in preserving model quality.\nFurthermore, we demonstrate that CQ can preserve model quality with KV cache\nquantized down to 1-bit.",
        "snippets": [
            "Efficient deployment of Large Language Models (LLMs) requires batching\nmultiple requests together to improve throughput. As the batch size, context\nlength, or model size increases, the size of the key and value (KV) cache can\nquickly become the main contributor to GPU memory usage and the bottleneck of\ninference latency. Quantization has emerged as an effective technique for KV\ncache compression, but existing methods still fail at very low bit widths. We\nobserve that distinct channels of a key/value activation embedding are highly\ninter-dependent, and the joint entropy of multiple channels grows at a slower\nrate than the sum of their marginal entropies. Based on this insight, we\npropose Coupled Quantization (CQ), which couples multiple key/value channels\ntogether to exploit their inter-dependency and encode the activations in a more\ninformation-efficient manner. Extensive experiments reveal that CQ outperforms\nor is competitive with existing baselines in preserving model quality.\nFurthermore, we demonstrate that CQ can preserve model quality with KV cache\nquantized down to 1-bit."
        ],
        "title": "KV Cache is 1 Bit Per Channel: Efficient Large Language Model Inference with Coupled Quantization",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1707.04179v1": {
        "url": "http://arxiv.org/abs/1707.04179v1",
        "description": "This paper investigates one of the fundamental issues in cache-enabled\nheterogeneous networks (HetNets): how many cache instances should be deployed\nat different base stations, in order to provide guaranteed service in a\ncost-effective manner. Specifically, we consider two-tier HetNets with\nhierarchical caching, where the most popular files are cached at small cell\nbase stations (SBSs) while the less popular ones are cached at macro base\nstations (MBSs). For a given network cache deployment budget, the cache sizes\nfor MBSs and SBSs are optimized to maximize network capacity while satisfying\nthe file transmission rate requirements. As cache sizes of MBSs and SBSs affect\nthe traffic load distribution, inter-tier traffic steering is also employed for\nload balancing. Based on stochastic geometry analysis, the optimal cache sizes\nfor MBSs and SBSs are obtained, which are threshold-based with respect to cache\nbudget in the networks constrained by SBS backhauls. Simulation results are\nprovided to evaluate the proposed schemes and demonstrate the applications in\ncost-effective network deployment.",
        "snippets": [
            "This paper investigates one of the fundamental issues in cache-enabled\nheterogeneous networks (HetNets): how many cache instances should be deployed\nat different base stations, in order to provide guaranteed service in a\ncost-effective manner. Specifically, we consider two-tier HetNets with\nhierarchical caching, where the most popular files are cached at small cell\nbase stations (SBSs) while the less popular ones are cached at macro base\nstations (MBSs). For a given network cache deployment budget, the cache sizes\nfor MBSs and SBSs are optimized to maximize network capacity while satisfying\nthe file transmission rate requirements. As cache sizes of MBSs and SBSs affect\nthe traffic load distribution, inter-tier traffic steering is also employed for\nload balancing. Based on stochastic geometry analysis, the optimal cache sizes\nfor MBSs and SBSs are obtained, which are threshold-based with respect to cache\nbudget in the networks constrained by SBS backhauls. Simulation results are\nprovided to evaluate the proposed schemes and demonstrate the applications in\ncost-effective network deployment."
        ],
        "title": "Cost-Effective Cache Deployment in Mobile Heterogeneous Networks",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2405.14366v2": {
        "url": "http://arxiv.org/abs/2405.14366v2",
        "description": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
        "snippets": [
            "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance."
        ],
        "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.14740v2": {
        "url": "http://arxiv.org/abs/2410.14740v2",
        "description": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.",
        "snippets": [
            "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD."
        ],
        "title": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM Inference with Mixed-Precision and Multi-level Caching",
        "meta": {
            "query": "cache size requirements for LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.16002v2": {
        "url": "http://arxiv.org/abs/2502.16002v2",
        "description": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines.",
        "snippets": [
            "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."
        ],
        "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
        "meta": {
            "query": "KV reuse characteristics in LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2412.03594v2": {
        "url": "http://arxiv.org/abs/2412.03594v2",
        "description": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments.",
        "snippets": [
            "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments."
        ],
        "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix Sharing and Throughput-oriented Token Batching",
        "meta": {
            "query": "KV reuse characteristics in LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.10951v2": {
        "url": "http://arxiv.org/abs/2505.10951v2",
        "description": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT).",
        "snippets": [
            "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to incorporate structured knowledge via graph retrieval as\ncontextual input, enhancing more accurate and context-aware reasoning. We\nobserve that for different queries, it could retrieve similar subgraphs as\nprompts, and thus we propose SubGCache, which aims to reduce inference latency\nby reusing computation across queries with similar structural prompts (i.e.,\nsubgraphs). Specifically, SubGCache clusters queries based on subgraph\nembeddings, constructs a representative subgraph for each cluster, and\npre-computes the key-value (KV) cache of the representative subgraph. For each\nquery with its retrieved subgraph within a cluster, it reuses the pre-computed\nKV cache of the representative subgraph of the cluster without computing the KV\ntensors again for saving computation. Experiments on two new datasets across\nmultiple LLM backbones and graph-based RAG frameworks demonstrate that\nSubGCache consistently reduces inference latency with comparable and even\nimproved generation quality, achieving up to 6.68$\\times$ reduction in\ntime-to-first-token (TTFT)."
        ],
        "title": "SubGCache: Accelerating Graph-based RAG with Subgraph-level KV Cache",
        "meta": {
            "query": "KV reuse characteristics in LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2407.00023v2": {
        "url": "http://arxiv.org/abs/2407.00023v2",
        "description": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency.",
        "snippets": [
            "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency."
        ],
        "title": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving",
        "meta": {
            "query": "KV reuse characteristics in LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2404.00242v4": {
        "url": "http://arxiv.org/abs/2404.00242v4",
        "description": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
        "snippets": [
            "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT."
        ],
        "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference",
        "meta": {
            "query": "KV reuse characteristics in LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.22618v1": {
        "url": "http://arxiv.org/abs/2505.22618v1",
        "description": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
        "snippets": [
            "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
        ],
        "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding",
        "meta": {
            "query": "KV reuse characteristics in LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.14398v1": {
        "url": "http://arxiv.org/abs/2505.14398v1",
        "description": "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques.",
        "snippets": [
            "While humans naturally learn and adapt from past experiences, large language\nmodels (LLMs) and their agentic counterparts struggle to retain reasoning from\nprevious tasks and apply them in future contexts. To address this limitation,\nwe propose a novel framework, log-augmented generation (LAG) that directly\nreuses prior computation and reasoning from past logs at test time to enhance\nmodel's ability to learn from previous tasks and perform better on new, unseen\nchallenges, all while keeping the system efficient and scalable. Specifically,\nour system represents task logs using key-value (KV) caches, encoding the full\nreasoning context of prior tasks while storing KV caches for only a selected\nsubset of tokens. When a new task arises, LAG retrieves the KV values from\nrelevant logs to augment generation. Our approach differs from reflection-based\nmemory mechanisms by directly reusing prior reasoning and computations without\nrequiring additional steps for knowledge extraction or distillation. Our method\nalso goes beyond existing KV caching techniques, which primarily target\nefficiency gains rather than improving accuracy. Experiments on knowledge- and\nreasoning-intensive datasets demonstrate that our method significantly\noutperforms standard agentic systems that do not utilize logs, as well as\nexisting solutions based on reflection and KV cache techniques."
        ],
        "title": "Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation",
        "meta": {
            "query": "KV reuse characteristics in LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2504.07479v1": {
        "url": "http://arxiv.org/abs/2504.07479v1",
        "description": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference.",
        "snippets": [
            "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference."
        ],
        "title": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache Pruning for Efficient Long-Context LLM Inference",
        "meta": {
            "query": "KV reuse characteristics in LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2403.11369v2": {
        "url": "http://arxiv.org/abs/2403.11369v2",
        "description": "This paper investigates the question of what makes math word problems (MWPs)\nin English challenging for large language models (LLMs). We conduct an in-depth\nanalysis of the key linguistic and mathematical characteristics of MWPs. In\naddition, we train feature-based classifiers to better understand the impact of\neach feature on the overall difficulty of MWPs for prominent LLMs and\ninvestigate whether this helps predict how well LLMs fare against specific\ncategories of MWPs.",
        "snippets": [
            "This paper investigates the question of what makes math word problems (MWPs)\nin English challenging for large language models (LLMs). We conduct an in-depth\nanalysis of the key linguistic and mathematical characteristics of MWPs. In\naddition, we train feature-based classifiers to better understand the impact of\neach feature on the overall difficulty of MWPs for prominent LLMs and\ninvestigate whether this helps predict how well LLMs fare against specific\ncategories of MWPs."
        ],
        "title": "What Makes Math Word Problems Challenging for LLMs?",
        "meta": {
            "query": "KV reuse characteristics in LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2412.14838v4": {
        "url": "http://arxiv.org/abs/2412.14838v4",
        "description": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
        "snippets": [
            "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
        ],
        "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs",
        "meta": {
            "query": "KV reuse characteristics in LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2403.11421v1": {
        "url": "http://arxiv.org/abs/2403.11421v1",
        "description": "Cost of serving large language models (LLM) is high, but the expensive and\nscarce GPUs are poorly efficient when generating tokens sequentially, unless\nthe batch of sequences is enlarged. However, the batch size is limited by some\nconstantly reused intermediate results, namely KV-Cache. They occupy too much\nmemory to fit more sequences into a GPU simultaneously. While they could be\noffloaded to host memory, the CPU-GPU bandwidth is an inevitable bottleneck.\n  We find a way to decompose the transformer models into two parts of different\ncharacteristics, one of which includes the memory-bound KV-Cache accessing. Our\nkey insight is that the aggregated memory capacity, bandwidth, and computing\npower of CPUs across multiple nodes is an efficient option to process this\npart. Performance improvement comes from reduced data transmission overhead and\nboosted GPU throughput to process the other model part. Moreover, we address\nefficiency challenges brought by heterogeneity at both temporal and\ninter-device scopes using scheduling and performance modeling techniques.\nEvaluation results show that our system achieves 1.88x - 5.04x the throughput\nof vLLM when serving modern LLMs with the same GPU.",
        "snippets": [
            "Cost of serving large language models (LLM) is high, but the expensive and\nscarce GPUs are poorly efficient when generating tokens sequentially, unless\nthe batch of sequences is enlarged. However, the batch size is limited by some\nconstantly reused intermediate results, namely KV-Cache. They occupy too much\nmemory to fit more sequences into a GPU simultaneously. While they could be\noffloaded to host memory, the CPU-GPU bandwidth is an inevitable bottleneck.\n  We find a way to decompose the transformer models into two parts of different\ncharacteristics, one of which includes the memory-bound KV-Cache accessing. Our\nkey insight is that the aggregated memory capacity, bandwidth, and computing\npower of CPUs across multiple nodes is an efficient option to process this\npart. Performance improvement comes from reduced data transmission overhead and\nboosted GPU throughput to process the other model part. Moreover, we address\nefficiency challenges brought by heterogeneity at both temporal and\ninter-device scopes using scheduling and performance modeling techniques.\nEvaluation results show that our system achieves 1.88x - 5.04x the throughput\nof vLLM when serving modern LLMs with the same GPU."
        ],
        "title": "FastDecode: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines",
        "meta": {
            "query": "KV reuse characteristics in LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2411.02820v3": {
        "url": "http://arxiv.org/abs/2411.02820v3",
        "description": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
        "snippets": [
            "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
        ],
        "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM Serving",
        "meta": {
            "query": "KV reuse characteristics in LLMs"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2106.03840v2": {
        "url": "http://arxiv.org/abs/2106.03840v2",
        "description": "Key-value (KV) separation is a technique that introduces randomness in the\nI/O access patterns to reduce I/O amplification in LSM-based key-value stores\nfor fast storage devices (NVMe). KV separation has a significant drawback that\nmakes it less attractive: Delete and especially update operations that are\nimportant in modern workloads result in frequent and expensive garbage\ncollection (GC) in the value log. In this paper, we design and implement\nParallax, which proposes hybrid KV placement that reduces GC overhead\nsignificantly and maximizes the benefits of using a log. We first model the\nbenefits of KV separation for different KV pair sizes. We use this model to\nclassify KV pairs in three categories small, medium, and large. Then, Parallax\nuses different approaches for each KV category: It always places large values\nin a log and small values in place. For medium values it uses a mixed strategy\nthat combines the benefits of using a log and eliminates GC overhead as\nfollows: It places medium values in a log for all but the last few (typically\none or two) levels in the LSM structure, where it performs a full compaction,\nmerges values in place, and reclaims log space without the need for GC. We\nevaluate Parallax against RocksDB that places all values in place and BlobDB\nthat always performs KV separation. We find that Parallax increases throughput\nby up to 12.4x and 17.83x, decreases I/O amplification by up to 27.1x and 26x,\nand increases CPU efficiency by up to 18.7x and 28x respectively, for all but\nscan-based YCSB workloads.",
        "snippets": [
            "Key-value (KV) separation is a technique that introduces randomness in the\nI/O access patterns to reduce I/O amplification in LSM-based key-value stores\nfor fast storage devices (NVMe). KV separation has a significant drawback that\nmakes it less attractive: Delete and especially update operations that are\nimportant in modern workloads result in frequent and expensive garbage\ncollection (GC) in the value log. In this paper, we design and implement\nParallax, which proposes hybrid KV placement that reduces GC overhead\nsignificantly and maximizes the benefits of using a log. We first model the\nbenefits of KV separation for different KV pair sizes. We use this model to\nclassify KV pairs in three categories small, medium, and large. Then, Parallax\nuses different approaches for each KV category: It always places large values\nin a log and small values in place. For medium values it uses a mixed strategy\nthat combines the benefits of using a log and eliminates GC overhead as\nfollows: It places medium values in a log for all but the last few (typically\none or two) levels in the LSM structure, where it performs a full compaction,\nmerges values in place, and reclaims log space without the need for GC. We\nevaluate Parallax against RocksDB that places all values in place and BlobDB\nthat always performs KV separation. We find that Parallax increases throughput\nby up to 12.4x and 17.83x, decreases I/O amplification by up to 27.1x and 26x,\nand increases CPU efficiency by up to 18.7x and 28x respectively, for all but\nscan-based YCSB workloads."
        ],
        "title": "Balancing Garbage Collection vs I/O Amplification using hybrid Key-Value Placement in LSM-based Key-Value Stores",
        "meta": {
            "query": "realworld KV workload patterns in large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2506.02006v1": {
        "url": "http://arxiv.org/abs/2506.02006v1",
        "description": "Efficiently serving large language models (LLMs) under dynamic and bursty\nworkloads remains a key challenge for real-world deployment. Existing serving\nframeworks and static model compression techniques fail to adapt to workload\nfluctuations, leading to either service-level objective (SLO) violations under\nfull-precision serving or persistent accuracy degradation with static\nquantization. We present MorphServe, a dynamic, workload-aware LLM serving\nframework based on morphological adaptation. MorphServe introduces two\nasynchronous, token-level runtime mechanisms: quantized layer swapping, which\nselectively replaces less impactful layers with quantized alternatives during\nhigh-load periods, and pressure-aware KV cache resizing, which dynamically\nadjusts KV cache capacity in response to memory pressure. These mechanisms\nenable state-preserving transitions with minimum runtime overhead and are fully\ncompatible with modern scheduling and attention techniques. Extensive\nexperiments on Vicuna and Llama family models with real-world workloads\ndemonstrate that MorphServe reduces average SLO violations by 92.45 percent and\nimproves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving,\nwithout compromising generation quality. These results establish MorphServe as\na practical and elastic solution for LLM deployment in dynamic environments.",
        "snippets": [
            "Efficiently serving large language models (LLMs) under dynamic and bursty\nworkloads remains a key challenge for real-world deployment. Existing serving\nframeworks and static model compression techniques fail to adapt to workload\nfluctuations, leading to either service-level objective (SLO) violations under\nfull-precision serving or persistent accuracy degradation with static\nquantization. We present MorphServe, a dynamic, workload-aware LLM serving\nframework based on morphological adaptation. MorphServe introduces two\nasynchronous, token-level runtime mechanisms: quantized layer swapping, which\nselectively replaces less impactful layers with quantized alternatives during\nhigh-load periods, and pressure-aware KV cache resizing, which dynamically\nadjusts KV cache capacity in response to memory pressure. These mechanisms\nenable state-preserving transitions with minimum runtime overhead and are fully\ncompatible with modern scheduling and attention techniques. Extensive\nexperiments on Vicuna and Llama family models with real-world workloads\ndemonstrate that MorphServe reduces average SLO violations by 92.45 percent and\nimproves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving,\nwithout compromising generation quality. These results establish MorphServe as\na practical and elastic solution for LLM deployment in dynamic environments."
        ],
        "title": "Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing",
        "meta": {
            "query": "realworld KV workload patterns in large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.05772v1": {
        "url": "http://arxiv.org/abs/2505.05772v1",
        "description": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures.",
        "snippets": [
            "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures."
        ],
        "title": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM",
        "meta": {
            "query": "realworld KV workload patterns in large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2504.03651v1": {
        "url": "http://arxiv.org/abs/2504.03651v1",
        "description": "Large language models have been widely deployed in various applications,\nencompassing both interactive online tasks and batched offline tasks. Given the\nburstiness and latency sensitivity of online tasks, over-provisioning resources\nis common practice. This allows for the integration of latency-insensitive\noffline tasks during periods of low online load, enhancing resource\nutilization. However, strategically serving online and offline tasks through a\npreemption mechanism fails to fully leverage the flexibility of offline tasks\nand suffers from KV cache recomputation and irregular workloads.\n  In this paper, we introduce Echo, a collaborative online-offline task serving\nsystem, including a scheduler, a KV cache manager, and estimation toolkits. The\nscheduler and KV cache manager work tightly to maximize the throughput of\noffline tasks, while the estimator further predicts execution time to ensure\nonline task SLOs. The scheduler leverages the batch information of last\niteration to reduce the search space for finding the optimal schedule. The KV\ncache manager sets the priority of the KV cache based on the type of tasks and\nthe opportunity of prefix sharing to reduce the recomputation. Finally, the\nestimation toolkits predict the execution time, future memory consumption, and\nthe throughput of offline tasks to guide the scheduler, KV cache manager, and\nthe system deployer. Evaluation based on real-world workloads demonstrates that\nEcho can increase offline task throughput by up to $3.3\\times$, while\nsatisfying online task SLOs.",
        "snippets": [
            "Large language models have been widely deployed in various applications,\nencompassing both interactive online tasks and batched offline tasks. Given the\nburstiness and latency sensitivity of online tasks, over-provisioning resources\nis common practice. This allows for the integration of latency-insensitive\noffline tasks during periods of low online load, enhancing resource\nutilization. However, strategically serving online and offline tasks through a\npreemption mechanism fails to fully leverage the flexibility of offline tasks\nand suffers from KV cache recomputation and irregular workloads.\n  In this paper, we introduce Echo, a collaborative online-offline task serving\nsystem, including a scheduler, a KV cache manager, and estimation toolkits. The\nscheduler and KV cache manager work tightly to maximize the throughput of\noffline tasks, while the estimator further predicts execution time to ensure\nonline task SLOs. The scheduler leverages the batch information of last\niteration to reduce the search space for finding the optimal schedule. The KV\ncache manager sets the priority of the KV cache based on the type of tasks and\nthe opportunity of prefix sharing to reduce the recomputation. Finally, the\nestimation toolkits predict the execution time, future memory consumption, and\nthe throughput of offline tasks to guide the scheduler, KV cache manager, and\nthe system deployer. Evaluation based on real-world workloads demonstrates that\nEcho can increase offline task throughput by up to $3.3\\times$, while\nsatisfying online task SLOs."
        ],
        "title": "Echo: Efficient Co-Scheduling of Hybrid Online-Offline Tasks for Large Language Model Serving",
        "meta": {
            "query": "realworld KV workload patterns in large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.17606v1": {
        "url": "http://arxiv.org/abs/2502.17606v1",
        "description": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively.",
        "snippets": [
            "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively."
        ],
        "title": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based Key-Value Stores",
        "meta": {
            "query": "realworld KV workload patterns in large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.16257v1": {
        "url": "http://arxiv.org/abs/2503.16257v1",
        "description": "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts.",
        "snippets": [
            "Video large language models (VideoLLMs) have demonstrated the capability to\nprocess longer video inputs and enable complex reasoning and analysis. However,\ndue to the thousands of visual tokens from the video frames, key-value (KV)\ncache can significantly increase memory requirements, becoming a bottleneck for\ninference speed and memory usage. KV cache quantization is a widely used\napproach to address this problem. In this paper, we find that 2-bit KV\nquantization of VideoLLMs can hardly hurt the model performance, while the\nlimit of KV cache quantization in even lower bits has not been investigated. To\nbridge this gap, we introduce VidKV, a plug-and-play KV cache quantization\nmethod to compress the KV cache to lower than 2 bits. Specifically, (1) for\nkey, we propose a mixed-precision quantization strategy in the channel\ndimension, where we perform 2-bit quantization for anomalous channels and 1-bit\nquantization combined with FFT for normal channels; (2) for value, we implement\n1.58-bit quantization while selectively filtering semantically salient visual\ntokens for targeted preservation, for a better trade-off between precision and\nmodel performance. Importantly, our findings suggest that the value cache of\nVideoLLMs should be quantized in a per-channel fashion instead of the per-token\nfashion proposed by prior KV cache quantization works for LLMs. Empirically,\nextensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show\nthat VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit\nprecision with almost no performance drop compared to the FP16 counterparts."
        ],
        "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language Models",
        "meta": {
            "query": "realworld KV workload patterns in large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2406.02069v4": {
        "url": "http://arxiv.org/abs/2406.02069v4",
        "description": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance.",
        "snippets": [
            "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance."
        ],
        "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling",
        "meta": {
            "query": "realworld KV workload patterns in large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.17138v2": {
        "url": "http://arxiv.org/abs/2505.17138v2",
        "description": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
        "snippets": [
            "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
        ],
        "title": "RAP: Runtime-Adaptive Pruning for LLM Inference",
        "meta": {
            "query": "realworld KV workload patterns in large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2501.15021v1": {
        "url": "http://arxiv.org/abs/2501.15021v1",
        "description": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput.",
        "snippets": [
            "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput."
        ],
        "title": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for Vision-Language Models",
        "meta": {
            "query": "realworld KV workload patterns in large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2408.04107v3": {
        "url": "http://arxiv.org/abs/2408.04107v3",
        "description": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose FDC, a fast KV\ndimensionality compression system that eliminates the decompression overhead\nincurred in the existing KV dimensionality compression system, Palu, and\nreduces attention time. Moreover, FDC employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, FDC enhances the attention kernel to balance the\nuneven workloads caused by the adaptive compression approach to further reduce\nattention computation latency. Comprehensive experiments demonstrate that\ncompared to Palu, FDC can reduce Job Completion Time (JCT) by up to 64%, and\ndelivers up to 1.97X throughput under the same latency, while maintaining 99%\nof the accuracy without compression. When state-of-the-art eviction and\nquantization methods are combined with FDC, they exhibit similar improvements\ncompared to those combined with Palu. We open-sourced the code.",
        "snippets": [
            "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose FDC, a fast KV\ndimensionality compression system that eliminates the decompression overhead\nincurred in the existing KV dimensionality compression system, Palu, and\nreduces attention time. Moreover, FDC employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, FDC enhances the attention kernel to balance the\nuneven workloads caused by the adaptive compression approach to further reduce\nattention computation latency. Comprehensive experiments demonstrate that\ncompared to Palu, FDC can reduce Job Completion Time (JCT) by up to 64%, and\ndelivers up to 1.97X throughput under the same latency, while maintaining 99%\nof the accuracy without compression. When state-of-the-art eviction and\nquantization methods are combined with FDC, they exhibit similar improvements\ncompared to those combined with Palu. We open-sourced the code."
        ],
        "title": "FDC: Fast KV Dimensionality Compression for Efficient LLM Inference",
        "meta": {
            "query": "realworld KV workload patterns in large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2412.04652v1": {
        "url": "http://arxiv.org/abs/2412.04652v1",
        "description": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
        "snippets": [
            "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP"
        ],
        "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
        "meta": {
            "query": "realworld KV workload patterns in large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2403.09054v2": {
        "url": "http://arxiv.org/abs/2403.09054v2",
        "description": "Transformers have emerged as the underpinning architecture for Large Language\nModels (LLMs). In generative language models, the inference process involves\ntwo primary phases: prompt processing and token generation. Token generation,\nwhich constitutes the majority of the computational workload, primarily entails\nvector-matrix multiplications and interactions with the Key-Value (KV) Cache.\nThis phase is constrained by memory bandwidth due to the overhead of\ntransferring weights and KV cache values from the memory system to the\ncomputing units. This memory bottleneck becomes particularly pronounced in\napplications that require long-context and extensive text generation, both of\nwhich are increasingly crucial for LLMs.\n  This paper introduces \"Keyformer\", an innovative inference-time approach, to\nmitigate the challenges associated with KV cache size and memory bandwidth\nutilization. Keyformer leverages the observation that approximately 90% of the\nattention weight in generative inference focuses on a specific subset of\ntokens, referred to as \"key\" tokens. Keyformer retains only the key tokens in\nthe KV cache by identifying these crucial tokens using a novel score function.\nThis approach effectively reduces both the KV cache size and memory bandwidth\nusage without compromising model accuracy. We evaluate Keyformer's performance\nacross three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ\nvarious positional embedding algorithms. Our assessment encompasses a variety\nof tasks, with a particular emphasis on summarization and conversation tasks\ninvolving extended contexts. Keyformer's reduction of KV cache reduces\ninference latency by 2.1x and improves token generation throughput by 2.4x,\nwhile preserving the model's accuracy.",
        "snippets": [
            "Transformers have emerged as the underpinning architecture for Large Language\nModels (LLMs). In generative language models, the inference process involves\ntwo primary phases: prompt processing and token generation. Token generation,\nwhich constitutes the majority of the computational workload, primarily entails\nvector-matrix multiplications and interactions with the Key-Value (KV) Cache.\nThis phase is constrained by memory bandwidth due to the overhead of\ntransferring weights and KV cache values from the memory system to the\ncomputing units. This memory bottleneck becomes particularly pronounced in\napplications that require long-context and extensive text generation, both of\nwhich are increasingly crucial for LLMs.\n  This paper introduces \"Keyformer\", an innovative inference-time approach, to\nmitigate the challenges associated with KV cache size and memory bandwidth\nutilization. Keyformer leverages the observation that approximately 90% of the\nattention weight in generative inference focuses on a specific subset of\ntokens, referred to as \"key\" tokens. Keyformer retains only the key tokens in\nthe KV cache by identifying these crucial tokens using a novel score function.\nThis approach effectively reduces both the KV cache size and memory bandwidth\nusage without compromising model accuracy. We evaluate Keyformer's performance\nacross three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ\nvarious positional embedding algorithms. Our assessment encompasses a variety\nof tasks, with a particular emphasis on summarization and conversation tasks\ninvolving extended contexts. Keyformer's reduction of KV cache reduces\ninference latency by 2.1x and improves token generation throughput by 2.4x,\nwhile preserving the model's accuracy."
        ],
        "title": "Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference",
        "meta": {
            "query": "realworld KV workload patterns in large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2504.09261v1": {
        "url": "http://arxiv.org/abs/2504.09261v1",
        "description": "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively.",
        "snippets": [
            "Visual Autoregressive (VAR) models have emerged as a powerful approach for\nmulti-modal content creation, offering high efficiency and quality across\ndiverse multimedia applications. However, they face significant memory\nbottlenecks due to extensive KV cache accumulation during inference. Existing\nKV cache compression techniques for large language models are suboptimal for\nVAR models due to, as we identify in this paper, two distinct categories of\nattention heads in VAR models: Structural Heads, which preserve spatial\ncoherence through diagonal attention patterns, and Contextual Heads, which\nmaintain semantic consistency through vertical attention patterns. These\ndifferences render single-strategy KV compression techniques ineffective for\nVAR models. To address this, we propose HACK, a training-free Head-Aware\nCompression method for KV cache. HACK allocates asymmetric cache budgets and\nemploys pattern-specific compression strategies tailored to the essential\ncharacteristics of each head category. Experiments on Infinity-2B, Infinity-8B,\nand VAR-d30 demonstrate its effectiveness in text-to-image and\nclass-conditional generation tasks. HACK can hack down up to 50\\% and 70\\% of\ncache with minimal performance degradation for VAR-d30 and Infinity-8B,\nrespectively. Even with 70\\% and 90\\% KV cache compression in VAR-d30 and\nInfinity-8B, HACK still maintains high-quality generation while reducing memory\nusage by 44.2\\% and 58.9\\%, respectively."
        ],
        "title": "Head-Aware KV Cache Compression for Efficient Visual Autoregressive Modeling",
        "meta": {
            "query": "realworld KV workload patterns in large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2401.17644v5": {
        "url": "http://arxiv.org/abs/2401.17644v5",
        "description": "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry.",
        "snippets": [
            "Serving systems for Large Language Models (LLMs) are often optimized to\nimprove quality of service (QoS) and throughput. However, due to the lack of\nopen-source LLM serving workloads, these systems are frequently evaluated under\nunrealistic workload assumptions. Consequently, performance may degrade when\nsystems are deployed in real-world scenarios. This work presents BurstGPT, an\nLLM serving workload with 10.31 million traces from regional Azure OpenAI GPT\nservices over 213 days. BurstGPT captures LLM serving characteristics from\nuser, model and system perspectives: (1) User request concurrency: burstiness\nvariations of requests in Azure OpenAI GPT services, revealing diversified\nconcurrency patterns in different services and model types. (2) User\nconversation patterns: counts and intervals within conversations for service\noptimizations. (3) Model response lengths: auto-regressive serving processes of\nGPT models, showing statistical relations between requests and their responses.\n(4) System response failures: failures of conversation and API services,\nshowing intensive resource needs and limited availability of LLM services in\nAzure. The details of the characteristics can serve multiple purposes in LLM\nserving optimizations, such as system evaluation and trace provisioning. In our\ndemo evaluation with BurstGPT, frequent variations in BurstGPT reveal declines\nin efficiency, stability, or reliability in realistic LLM serving. We identify\nthat the generalization of KV cache management, scheduling and disaggregation\noptimizations can be improved under realistic workload evaluations. BurstGPT is\npublicly available now at https://github.com/HPMLL/BurstGPT and is widely used\nto develop prototypes of LLM serving frameworks in the industry."
        ],
        "title": "BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems",
        "meta": {
            "query": "realworld KV workload patterns in large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.07590v1": {
        "url": "http://arxiv.org/abs/2410.07590v1",
        "description": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems.",
        "snippets": [
            "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems."
        ],
        "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed KV Caches for Chunked Text",
        "meta": {
            "query": "realworld KV workload patterns in large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2412.17854v1": {
        "url": "http://arxiv.org/abs/2412.17854v1",
        "description": "Tenant evictions threaten housing stability and are a major concern for many\ncities. An open question concerns whether data-driven methods enhance outreach\nprograms that target at-risk tenants to mitigate their risk of eviction. We\npropose a novel active geospatial search (AGS) modeling framework for this\nproblem. AGS integrates property-level information in a search policy that\nidentifies a sequence of rental units to canvas to both determine their\neviction risk and provide support if needed. We propose a hierarchical\nreinforcement learning approach to learn a search policy for AGS that scales to\nlarge urban areas containing thousands of parcels, balancing exploration and\nexploitation and accounting for travel costs and a budget constraint.\nCrucially, the search policy adapts online to newly discovered information\nabout evictions. Evaluation using eviction data for a large urban area\ndemonstrates that the proposed framework and algorithmic approach are\nconsiderably more effective at sequentially identifying eviction cases than\nbaseline methods.",
        "snippets": [
            "Tenant evictions threaten housing stability and are a major concern for many\ncities. An open question concerns whether data-driven methods enhance outreach\nprograms that target at-risk tenants to mitigate their risk of eviction. We\npropose a novel active geospatial search (AGS) modeling framework for this\nproblem. AGS integrates property-level information in a search policy that\nidentifies a sequence of rental units to canvas to both determine their\neviction risk and provide support if needed. We propose a hierarchical\nreinforcement learning approach to learn a search policy for AGS that scales to\nlarge urban areas containing thousands of parcels, balancing exploration and\nexploitation and accounting for travel costs and a budget constraint.\nCrucially, the search policy adapts online to newly discovered information\nabout evictions. Evaluation using eviction data for a large urban area\ndemonstrates that the proposed framework and algorithmic approach are\nconsiderably more effective at sequentially identifying eviction cases than\nbaseline methods."
        ],
        "title": "Active Geospatial Search for Efficient Tenant Eviction Outreach",
        "meta": {
            "query": "cache eviction policies for large language models in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2006.16239v2": {
        "url": "http://arxiv.org/abs/2006.16239v2",
        "description": "Program execution speed critically depends on increasing cache hits, as cache\nhits are orders of magnitude faster than misses. To increase cache hits, we\nfocus on the problem of cache replacement: choosing which cache line to evict\nupon inserting a new line. This is challenging because it requires planning far\nahead and currently there is no known practical solution. As a result, current\nreplacement policies typically resort to heuristics designed for specific\ncommon access patterns, which fail on more diverse and complex access patterns.\nIn contrast, we propose an imitation learning approach to automatically learn\ncache access patterns by leveraging Belady's, an oracle policy that computes\nthe optimal eviction decision given the future cache accesses. While directly\napplying Belady's is infeasible since the future is unknown, we train a policy\nconditioned only on past accesses that accurately approximates Belady's even on\ndiverse and complex access patterns, and call this approach Parrot. When\nevaluated on 13 of the most memory-intensive SPEC applications, Parrot\nincreases cache miss rates by 20% over the current state of the art. In\naddition, on a large-scale web search benchmark, Parrot increases cache hit\nrates by 61% over a conventional LRU policy. We release a Gym environment to\nfacilitate research in this area, as data is plentiful, and further\nadvancements can have significant real-world impact.",
        "snippets": [
            "Program execution speed critically depends on increasing cache hits, as cache\nhits are orders of magnitude faster than misses. To increase cache hits, we\nfocus on the problem of cache replacement: choosing which cache line to evict\nupon inserting a new line. This is challenging because it requires planning far\nahead and currently there is no known practical solution. As a result, current\nreplacement policies typically resort to heuristics designed for specific\ncommon access patterns, which fail on more diverse and complex access patterns.\nIn contrast, we propose an imitation learning approach to automatically learn\ncache access patterns by leveraging Belady's, an oracle policy that computes\nthe optimal eviction decision given the future cache accesses. While directly\napplying Belady's is infeasible since the future is unknown, we train a policy\nconditioned only on past accesses that accurately approximates Belady's even on\ndiverse and complex access patterns, and call this approach Parrot. When\nevaluated on 13 of the most memory-intensive SPEC applications, Parrot\nincreases cache miss rates by 20% over the current state of the art. In\naddition, on a large-scale web search benchmark, Parrot increases cache hit\nrates by 61% over a conventional LRU policy. We release a Gym environment to\nfacilitate research in this area, as data is plentiful, and further\nadvancements can have significant real-world impact."
        ],
        "title": "An Imitation Learning Approach for Cache Replacement",
        "meta": {
            "query": "cache eviction policies for large language models in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.17603v1": {
        "url": "http://arxiv.org/abs/2503.17603v1",
        "description": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache.",
        "snippets": [
            "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache."
        ],
        "title": "A Generative Caching System for Large Language Models",
        "meta": {
            "query": "cache optimization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.23294v1": {
        "url": "http://arxiv.org/abs/2503.23294v1",
        "description": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets.",
        "snippets": [
            "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets."
        ],
        "title": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context LLM Inference",
        "meta": {
            "query": "cache optimization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2412.05896v1": {
        "url": "http://arxiv.org/abs/2412.05896v1",
        "description": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x.",
        "snippets": [
            "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x."
        ],
        "title": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM Inference",
        "meta": {
            "query": "cache optimization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2411.05276v3": {
        "url": "http://arxiv.org/abs/2411.05276v3",
        "description": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
        "snippets": [
            "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
        ],
        "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic Embedding Caching",
        "meta": {
            "query": "cache optimization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2501.09383v1": {
        "url": "http://arxiv.org/abs/2501.09383v1",
        "description": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
        "snippets": [
            "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments."
        ],
        "title": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
        "meta": {
            "query": "cache optimization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2308.00562v1": {
        "url": "http://arxiv.org/abs/2308.00562v1",
        "description": "A simultaneously transmitting and reflecting surface (STARS) enabled edge\ncaching system is proposed for reducing backhaul traffic and ensuring the\nquality of service. A novel Caching-at-STARS structure, where a dedicated smart\ncontroller and cache memory are installed at the STARS, is proposed to satisfy\nuser demands with fewer hops and desired channel conditions. Then, a joint\ncaching replacement and information-centric hybrid beamforming optimization\nproblem is formulated for minimizing the network power consumption. As\nlong-term decision processes, the optimization problems based on independent\nand coupled phase-shift models of Caching-at-STARS contain both continuous and\ndiscrete decision variables, and are suitable for solving with deep\nreinforcement learning (DRL) algorithm. For the independent phase-shift\nCaching-at-STARS model, we develop a frequency-aware based twin delayed deep\ndeterministic policy gradient (FA-TD3) algorithm that leverages user historical\nrequest information to serialize high-dimensional caching replacement decision\nvariables. For the coupled phase-shift Caching-at-STARS model, we conceive a\ncooperative TD3 \\& deep-Q network (TD3-DQN) algorithm comprised of FA-TD3 and\nDQN agents to decide on continuous and discrete variables respectively by\nobserving the network external and internal environment. The numerical results\ndemonstrate that: 1) The Caching-at-STARS-enabled edge caching system has\nadvantages over traditional edge caching, especially in scenarios where Zipf\nskewness factors or cache capacity is large; 2) Caching-at-STARS outperforms\nthe RIS-assisted edge caching systems; 3) The proposed FA-TD3 and cooperative\nTD3-DQN algorithms are superior in reducing network power consumption than\nconventional TD3.",
        "snippets": [
            "A simultaneously transmitting and reflecting surface (STARS) enabled edge\ncaching system is proposed for reducing backhaul traffic and ensuring the\nquality of service. A novel Caching-at-STARS structure, where a dedicated smart\ncontroller and cache memory are installed at the STARS, is proposed to satisfy\nuser demands with fewer hops and desired channel conditions. Then, a joint\ncaching replacement and information-centric hybrid beamforming optimization\nproblem is formulated for minimizing the network power consumption. As\nlong-term decision processes, the optimization problems based on independent\nand coupled phase-shift models of Caching-at-STARS contain both continuous and\ndiscrete decision variables, and are suitable for solving with deep\nreinforcement learning (DRL) algorithm. For the independent phase-shift\nCaching-at-STARS model, we develop a frequency-aware based twin delayed deep\ndeterministic policy gradient (FA-TD3) algorithm that leverages user historical\nrequest information to serialize high-dimensional caching replacement decision\nvariables. For the coupled phase-shift Caching-at-STARS model, we conceive a\ncooperative TD3 \\& deep-Q network (TD3-DQN) algorithm comprised of FA-TD3 and\nDQN agents to decide on continuous and discrete variables respectively by\nobserving the network external and internal environment. The numerical results\ndemonstrate that: 1) The Caching-at-STARS-enabled edge caching system has\nadvantages over traditional edge caching, especially in scenarios where Zipf\nskewness factors or cache capacity is large; 2) Caching-at-STARS outperforms\nthe RIS-assisted edge caching systems; 3) The proposed FA-TD3 and cooperative\nTD3-DQN algorithms are superior in reducing network power consumption than\nconventional TD3."
        ],
        "title": "Caching-at-STARS: the Next Generation Edge Caching",
        "meta": {
            "query": "cache optimization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.10714v2": {
        "url": "http://arxiv.org/abs/2503.10714v2",
        "description": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
        "snippets": [
            "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
        ],
        "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient Long-Context LLMs",
        "meta": {
            "query": "cache optimization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2407.18003v4": {
        "url": "http://arxiv.org/abs/2407.18003v4",
        "description": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
        "snippets": [
            "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
        ],
        "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption",
        "meta": {
            "query": "cache optimization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.18334v2": {
        "url": "http://arxiv.org/abs/2503.18334v2",
        "description": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
        "snippets": [
            "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
        ],
        "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language Models",
        "meta": {
            "query": "cache optimization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.09397v1": {
        "url": "http://arxiv.org/abs/2410.09397v1",
        "description": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference.",
        "snippets": [
            "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference."
        ],
        "title": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for Backward Passes",
        "meta": {
            "query": "cache optimization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2406.17565v3": {
        "url": "http://arxiv.org/abs/2406.17565v3",
        "description": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
        "snippets": [
            "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
        ],
        "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool",
        "meta": {
            "query": "cache optimization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2404.04793v2": {
        "url": "http://arxiv.org/abs/2404.04793v2",
        "description": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention.",
        "snippets": [
            "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention."
        ],
        "title": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget",
        "meta": {
            "query": "cache optimization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2306.17254v1": {
        "url": "http://arxiv.org/abs/2306.17254v1",
        "description": "NVMe SSD caching has demonstrated impressive capabilities in solving cloud\nblock storage's I/O bottleneck and enhancing application performance in public,\nprivate, and hybrid cloud environments. However, traditional host-side caching\nsolutions have several serious limitations. First, the cache cannot be shared\nacross hosts, leading to low cache utilization. Second, the commonly-used\nfix-sized cache block allocation mechanism is unable to provide good cache\nperformance with low memory overhead for diverse cloud workloads with vastly\ndifferent I/O patterns. This paper presents AdaCache, a novel userspace\ndisaggregated cache system that utilizes adaptive cache block allocation for\ncloud block storage. First, AdaCache proposes an innovative adaptive cache\nblock allocation scheme that allocates cache blocks based on the request size\nto achieve both good cache performance and low memory overhead. Second,\nAdaCache proposes a group-based cache organization that stores cache blocks\ninto groups to solve the fragmentation problem brought by variable-sized cache\nblocks. Third, AdaCache designs a two-level cache replacement policy that\nreplaces cache blocks in both single blocks and groups to improve the hit\nratio. Experimental results with real-world traces show that AdaCache can\nsubstantially improve I/O performance and reduce storage access caused by cache\nmiss with a much lower memory usage compared to traditional fix-sized cache\nsystems.",
        "snippets": [
            "NVMe SSD caching has demonstrated impressive capabilities in solving cloud\nblock storage's I/O bottleneck and enhancing application performance in public,\nprivate, and hybrid cloud environments. However, traditional host-side caching\nsolutions have several serious limitations. First, the cache cannot be shared\nacross hosts, leading to low cache utilization. Second, the commonly-used\nfix-sized cache block allocation mechanism is unable to provide good cache\nperformance with low memory overhead for diverse cloud workloads with vastly\ndifferent I/O patterns. This paper presents AdaCache, a novel userspace\ndisaggregated cache system that utilizes adaptive cache block allocation for\ncloud block storage. First, AdaCache proposes an innovative adaptive cache\nblock allocation scheme that allocates cache blocks based on the request size\nto achieve both good cache performance and low memory overhead. Second,\nAdaCache proposes a group-based cache organization that stores cache blocks\ninto groups to solve the fragmentation problem brought by variable-sized cache\nblocks. Third, AdaCache designs a two-level cache replacement policy that\nreplaces cache blocks in both single blocks and groups to improve the hit\nratio. Experimental results with real-world traces show that AdaCache can\nsubstantially improve I/O performance and reduce storage access caused by cache\nmiss with a much lower memory usage compared to traditional fix-sized cache\nsystems."
        ],
        "title": "AdaCache: A Disaggregated Cache System with Adaptive Block Size for Cloud Block Storage",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2309.10239v1": {
        "url": "http://arxiv.org/abs/2309.10239v1",
        "description": "In-memory caching systems are fundamental building blocks in cloud services.\nHowever, due to the coupled CPU and memory on monolithic servers, existing\ncaching systems cannot elastically adjust resources in a resource-efficient and\nagile manner. To achieve better elasticity, we propose to port in-memory\ncaching systems to the disaggregated memory (DM) architecture, where compute\nand memory resources are decoupled and can be allocated flexibly. However,\nconstructing an elastic caching system on DM is challenging since accessing\ncached objects with CPU-bypass remote memory accesses hinders the execution of\ncaching algorithms. Moreover, the elastic changes of compute and memory\nresources on DM affect the access patterns of cached data, compromising the hit\nrates of caching algorithms. We design Ditto, the first caching system on DM,\nto address these challenges. Ditto first proposes a client-centric caching\nframework to efficiently execute various caching algorithms in the compute pool\nof DM, relying only on remote memory accesses. Then, Ditto employs a\ndistributed adaptive caching scheme that adaptively switches to the best-fit\ncaching algorithm in real-time based on the performance of multiple caching\nalgorithms to improve cache hit rates. Our experiments show that Ditto\neffectively adapts to the changing resources on DM and outperforms the\nstate-of-the-art caching systems by up to 3.6x in real-world workloads and 9x\nin YCSB",
        "snippets": [
            "In-memory caching systems are fundamental building blocks in cloud services.\nHowever, due to the coupled CPU and memory on monolithic servers, existing\ncaching systems cannot elastically adjust resources in a resource-efficient and\nagile manner. To achieve better elasticity, we propose to port in-memory\ncaching systems to the disaggregated memory (DM) architecture, where compute\nand memory resources are decoupled and can be allocated flexibly. However,\nconstructing an elastic caching system on DM is challenging since accessing\ncached objects with CPU-bypass remote memory accesses hinders the execution of\ncaching algorithms. Moreover, the elastic changes of compute and memory\nresources on DM affect the access patterns of cached data, compromising the hit\nrates of caching algorithms. We design Ditto, the first caching system on DM,\nto address these challenges. Ditto first proposes a client-centric caching\nframework to efficiently execute various caching algorithms in the compute pool\nof DM, relying only on remote memory accesses. Then, Ditto employs a\ndistributed adaptive caching scheme that adaptively switches to the best-fit\ncaching algorithm in real-time based on the performance of multiple caching\nalgorithms to improve cache hit rates. Our experiments show that Ditto\neffectively adapts to the changing resources on DM and outperforms the\nstate-of-the-art caching systems by up to 3.6x in real-world workloads and 9x\nin YCSB"
        ],
        "title": "Ditto: An Elastic and Adaptive Memory-Disaggregated Caching System",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1810.09442v1": {
        "url": "http://arxiv.org/abs/1810.09442v1",
        "description": "Major chip manufacturers have all introduced multicore microprocessors.\nMulti-socket systems built from these processors are used for running various\nserver applications. However to the best of our knowledge current commercial\noperating systems are not optimized for multi-threaded workloads running on\nsuch servers. Cache-to-cache transfers and remote memory accesses impact the\nperformance of such workloads. This paper presents a unified approach to\noptimizing OS scheduling algorithms for both cache-to-cache transfers and\nremote DRAM accesses that also takes cache affinity into account. By observing\nthe patterns of local and remote cache-to-cache transfers as well as local and\nremote DRAM accesses for every thread in each scheduling quantum and applying\ndifferent algorithms, we come up with a new schedule of threads for the next\nquantum taking cache affinity into account. This new schedule cuts down both\nremote cache-to-cache transfers and remote DRAM accesses for the next\nscheduling quantum and improves overall performance. We present two algorithms\nof varying complexity for optimizing cache-to-cache transfers. One of these is\na new algorithm which is relatively simpler and performs better when combined\nwith algorithms that optimize remote DRAM accesses. For optimizing remote DRAM\naccesses we present two algorithms. Though both algorithms differ in\nalgorithmic complexity we find that for our workloads they perform equally\nwell. We used three different synthetic workloads to evaluate these algorithms.\nWe also performed sensitivity analysis with respect to varying remote\ncache-to-cache transfer latency and remote DRAM latency. We show that these\nalgorithms can cut down overall latency by up to 16.79% depending on the\nalgorithm used.",
        "snippets": [
            "Major chip manufacturers have all introduced multicore microprocessors.\nMulti-socket systems built from these processors are used for running various\nserver applications. However to the best of our knowledge current commercial\noperating systems are not optimized for multi-threaded workloads running on\nsuch servers. Cache-to-cache transfers and remote memory accesses impact the\nperformance of such workloads. This paper presents a unified approach to\noptimizing OS scheduling algorithms for both cache-to-cache transfers and\nremote DRAM accesses that also takes cache affinity into account. By observing\nthe patterns of local and remote cache-to-cache transfers as well as local and\nremote DRAM accesses for every thread in each scheduling quantum and applying\ndifferent algorithms, we come up with a new schedule of threads for the next\nquantum taking cache affinity into account. This new schedule cuts down both\nremote cache-to-cache transfers and remote DRAM accesses for the next\nscheduling quantum and improves overall performance. We present two algorithms\nof varying complexity for optimizing cache-to-cache transfers. One of these is\na new algorithm which is relatively simpler and performs better when combined\nwith algorithms that optimize remote DRAM accesses. For optimizing remote DRAM\naccesses we present two algorithms. Though both algorithms differ in\nalgorithmic complexity we find that for our workloads they perform equally\nwell. We used three different synthetic workloads to evaluate these algorithms.\nWe also performed sensitivity analysis with respect to varying remote\ncache-to-cache transfer latency and remote DRAM latency. We show that these\nalgorithms can cut down overall latency by up to 16.79% depending on the\nalgorithm used."
        ],
        "title": "OS Scheduling Algorithms for Improving the Performance of Multithreaded Workloads",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1409.8324v3": {
        "url": "http://arxiv.org/abs/1409.8324v3",
        "description": "Read-only caches are widely used in cloud infrastructures to reduce access\nlatency and load on backend databases. Operators view coherent caches as\nimpractical at genuinely large scale and many client-facing caches are updated\nin an asynchronous manner with best-effort pipelines. Existing solutions that\nsupport cache consistency are inapplicable to this scenario since they require\na round trip to the database on every cache transaction.\n  Existing incoherent cache technologies are oblivious to transactional data\naccess, even if the backend database supports transactions. We propose T-Cache,\na novel caching policy for read-only transactions in which inconsistency is\ntolerable (won't cause safety violations) but undesirable (has a cost). T-Cache\nimproves cache consistency despite asynchronous and unreliable communication\nbetween the cache and the database. We define cache-serializability, a variant\nof serializability that is suitable for incoherent caches, and prove that with\nunbounded resources T-Cache implements this new specification. With limited\nresources, T-Cache allows the system manager to choose a trade-off between\nperformance and consistency.\n  Our evaluation shows that T-Cache detects many inconsistencies with only\nnominal overhead. We use synthetic workloads to demonstrate the efficacy of\nT-Cache when data accesses are clustered and its adaptive reaction to workload\nchanges. With workloads based on the real-world topologies, T-Cache detects\n43-70% of the inconsistencies and increases the rate of consistent transactions\nby 33-58%.",
        "snippets": [
            "Read-only caches are widely used in cloud infrastructures to reduce access\nlatency and load on backend databases. Operators view coherent caches as\nimpractical at genuinely large scale and many client-facing caches are updated\nin an asynchronous manner with best-effort pipelines. Existing solutions that\nsupport cache consistency are inapplicable to this scenario since they require\na round trip to the database on every cache transaction.\n  Existing incoherent cache technologies are oblivious to transactional data\naccess, even if the backend database supports transactions. We propose T-Cache,\na novel caching policy for read-only transactions in which inconsistency is\ntolerable (won't cause safety violations) but undesirable (has a cost). T-Cache\nimproves cache consistency despite asynchronous and unreliable communication\nbetween the cache and the database. We define cache-serializability, a variant\nof serializability that is suitable for incoherent caches, and prove that with\nunbounded resources T-Cache implements this new specification. With limited\nresources, T-Cache allows the system manager to choose a trade-off between\nperformance and consistency.\n  Our evaluation shows that T-Cache detects many inconsistencies with only\nnominal overhead. We use synthetic workloads to demonstrate the efficacy of\nT-Cache when data accesses are clustered and its adaptive reaction to workload\nchanges. With workloads based on the real-world topologies, T-Cache detects\n43-70% of the inconsistencies and increases the rate of consistent transactions\nby 33-58%."
        ],
        "title": "Cache Serializability: Reducing Inconsistency in Edge Transactions",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.21919v1": {
        "url": "http://arxiv.org/abs/2505.21919v1",
        "description": "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]\nand Sherman [2]) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference.",
        "snippets": [
            "The increasing adoption of large language models (LLMs) with extended context\nwindows necessitates efficient Key-Value Cache (KVC) management to optimize\ninference performance. Inference workloads like Retrieval-Augmented Generation\n(RAG) and agents exhibit high cache reusability, making efficient caching\ncritical to reducing redundancy and improving speed. We analyze real-world KVC\naccess patterns using publicly available traces and evaluate commercial\nkey-value stores like Redis and state-of-the-art RDMA-based systems (CHIME \nand Sherman ) for KVC metadata management. Our work demonstrates the lack of\ntailored storage solution for KVC prefilling, underscores the need for an\nefficient distributed caching system with optimized metadata management for LLM\nworkloads, and provides insights into designing improved KVC management systems\nfor scalable, low-latency inference."
        ],
        "title": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2203.10766v2": {
        "url": "http://arxiv.org/abs/2203.10766v2",
        "description": "Cloud block storage systems support diverse types of applications in modern\ncloud services. Characterizing their I/O activities is critical for guiding\nbetter system designs and optimizations. In this paper, we present an in-depth\ncomparative analysis of production cloud block storage workloads through the\nblock-level I/O traces of billions of I/O requests collected from two\nproduction systems, Alibaba Cloud and Tencent Cloud Block Storage. We study\ntheir characteristics of load intensities, spatial patterns, and temporal\npatterns. We also compare the cloud block storage workloads with the notable\npublic block-level I/O workloads from the enterprise data centers at Microsoft\nResearch Cambridge, and identify the commonalities and differences of the three\nsources of traces. To this end, we provide 6 findings through the high-level\nanalysis and 16 findings through the detailed analysis on load intensity,\nspatial patterns, and temporal patterns. We discuss the implications of our\nfindings on load balancing, cache efficiency, and storage cluster management in\ncloud block storage systems.",
        "snippets": [
            "Cloud block storage systems support diverse types of applications in modern\ncloud services. Characterizing their I/O activities is critical for guiding\nbetter system designs and optimizations. In this paper, we present an in-depth\ncomparative analysis of production cloud block storage workloads through the\nblock-level I/O traces of billions of I/O requests collected from two\nproduction systems, Alibaba Cloud and Tencent Cloud Block Storage. We study\ntheir characteristics of load intensities, spatial patterns, and temporal\npatterns. We also compare the cloud block storage workloads with the notable\npublic block-level I/O workloads from the enterprise data centers at Microsoft\nResearch Cambridge, and identify the commonalities and differences of the three\nsources of traces. To this end, we provide 6 findings through the high-level\nanalysis and 16 findings through the detailed analysis on load intensity,\nspatial patterns, and temporal patterns. We discuss the implications of our\nfindings on load balancing, cache efficiency, and storage cluster management in\ncloud block storage systems."
        ],
        "title": "An In-Depth Comparative Analysis of Cloud Block Storage Workloads: Findings and Implications",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1803.03914v3": {
        "url": "http://arxiv.org/abs/1803.03914v3",
        "description": "Content-delivery applications can achieve scalability and reduce wide-area\nnetwork traffic using geographically distributed caches. However, each deployed\ncache has an associated cost, and under time-varying request rates (e.g., a\ndaily cycle) there may be long periods when the request rate from the local\nregion is not high enough to justify this cost. Cloud computing offers a\nsolution to problems of this kind, by supporting dynamic allocation and release\nof resources. In this paper, we analyze the potential benefits from dynamically\ninstantiating caches using resources from cloud service providers. We develop\nnovel analytic caching models that accommodate time-varying request rates,\ntransient behavior as a cache fills following instantiation, and selective\ncache insertion policies. Within the context of a simple cost model, we then\ndevelop bounds and compare policies with optimized parameter selections to\nobtain insights into key cost/performance tradeoffs. We find that dynamic cache\ninstantiation can provide substantial cost reductions, that potential\nreductions strongly dependent on the object popularity skew, and that selective\ncache insertion can be even more beneficial in this context than with\nconventional edge caches. Finally, our contributions also include accurate and\neasy-to-compute approximations that are shown applicable to LRU caches under\ntime-varying workloads.",
        "snippets": [
            "Content-delivery applications can achieve scalability and reduce wide-area\nnetwork traffic using geographically distributed caches. However, each deployed\ncache has an associated cost, and under time-varying request rates (e.g., a\ndaily cycle) there may be long periods when the request rate from the local\nregion is not high enough to justify this cost. Cloud computing offers a\nsolution to problems of this kind, by supporting dynamic allocation and release\nof resources. In this paper, we analyze the potential benefits from dynamically\ninstantiating caches using resources from cloud service providers. We develop\nnovel analytic caching models that accommodate time-varying request rates,\ntransient behavior as a cache fills following instantiation, and selective\ncache insertion policies. Within the context of a simple cost model, we then\ndevelop bounds and compare policies with optimized parameter selections to\nobtain insights into key cost/performance tradeoffs. We find that dynamic cache\ninstantiation can provide substantial cost reductions, that potential\nreductions strongly dependent on the object popularity skew, and that selective\ncache insertion can be even more beneficial in this context than with\nconventional edge caches. Finally, our contributions also include accurate and\neasy-to-compute approximations that are shown applicable to LRU caches under\ntime-varying workloads."
        ],
        "title": "Optimized Dynamic Cache Instantiation and Accurate LRU Approximations under Time-varying Request Volume",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2303.08396v2": {
        "url": "http://arxiv.org/abs/2303.08396v2",
        "description": "Hyperscalars run services across a large fleet of servers, serving billions\nof users worldwide. These services, however, behave differently than commonly\navailable benchmark suites, resulting in server architectures that are not\noptimized for cloud workloads. With datacenters becoming a primary server\nprocessor market, optimizing server processors for cloud workloads by better\nunderstanding their behavior has become crucial. To address this, in this\npaper, we present MemProf, a memory profiler that profiles the three major\nreasons for stalls in cloud workloads: code-fetch, memory bandwidth, and memory\nlatency. We use MemProf to understand the behavior of cloud workloads and\npropose and evaluate micro-architectural and memory system design improvements\nthat help cloud workloads' performance.\n  MemProf's code analysis shows that cloud workloads execute the same code\nacross CPU cores. Using this, we propose shared micro-architectural\nstructures--a shared L2 I-TLB and a shared L2 cache. Next, to help with memory\nbandwidth stalls, using workloads' memory bandwidth distribution, we find that\nonly a few pages contribute to most of the system bandwidth. We use this\nfinding to evaluate a new high-bandwidth, small-capacity memory tier and show\nthat it performs 1.46x better than the current baseline configuration. Finally,\nwe look into ways to improve memory latency for cloud workloads. Profiling\nusing MemProf reveals that L2 hardware prefetchers, a common solution to reduce\nmemory latency, have very low coverage and consume a significant amount of\nmemory bandwidth. To help improve hardware prefetcher performance, we built a\nmemory tracing tool to collect and validate production memory access traces.",
        "snippets": [
            "Hyperscalars run services across a large fleet of servers, serving billions\nof users worldwide. These services, however, behave differently than commonly\navailable benchmark suites, resulting in server architectures that are not\noptimized for cloud workloads. With datacenters becoming a primary server\nprocessor market, optimizing server processors for cloud workloads by better\nunderstanding their behavior has become crucial. To address this, in this\npaper, we present MemProf, a memory profiler that profiles the three major\nreasons for stalls in cloud workloads: code-fetch, memory bandwidth, and memory\nlatency. We use MemProf to understand the behavior of cloud workloads and\npropose and evaluate micro-architectural and memory system design improvements\nthat help cloud workloads' performance.\n  MemProf's code analysis shows that cloud workloads execute the same code\nacross CPU cores. Using this, we propose shared micro-architectural\nstructures--a shared L2 I-TLB and a shared L2 cache. Next, to help with memory\nbandwidth stalls, using workloads' memory bandwidth distribution, we find that\nonly a few pages contribute to most of the system bandwidth. We use this\nfinding to evaluate a new high-bandwidth, small-capacity memory tier and show\nthat it performs 1.46x better than the current baseline configuration. Finally,\nwe look into ways to improve memory latency for cloud workloads. Profiling\nusing MemProf reveals that L2 hardware prefetchers, a common solution to reduce\nmemory latency, have very low coverage and consume a significant amount of\nmemory bandwidth. To help improve hardware prefetcher performance, we built a\nmemory tracing tool to collect and validate production memory access traces."
        ],
        "title": "Workload Behavior Driven Memory Subsystem Design for Hyperscale",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2504.11816v1": {
        "url": "http://arxiv.org/abs/2504.11816v1",
        "description": "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads.",
        "snippets": [
            "LLM inference is essential for applications like text summarization,\ntranslation, and data analysis, but the high cost of GPU instances from Cloud\nService Providers (CSPs) like AWS is a major burden. This paper proposes\nInferSave, a cost-efficient VM selection framework for cloud based LLM\ninference. InferSave optimizes KV cache offloading based on Service Level\nObjectives (SLOs) and workload charac teristics, estimating GPU memory needs,\nand recommending cost-effective VM instances. Additionally, the Compute Time\nCalibration Function (CTCF) improves instance selection accuracy by adjusting\nfor discrepancies between theoretical and actual GPU performance. Experiments\non AWS GPU instances show that selecting lower-cost instances without KV cache\noffloading improves cost efficiency by up to 73.7% for online workloads, while\nKV cache offloading saves up to 20.19% for offline workloads."
        ],
        "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2501.14770v2": {
        "url": "http://arxiv.org/abs/2501.14770v2",
        "description": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short.",
        "snippets": [
            "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short."
        ],
        "title": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine Learning Approaches",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1805.06747v1": {
        "url": "http://arxiv.org/abs/1805.06747v1",
        "description": "In recent years, SSDs have gained tremendous attention in computing and\nstorage systems due to significant performance improvement over HDDs. The cost\nper capacity of SSDs, however, prevents them from entirely replacing HDDs in\nsuch systems. One approach to effectively take advantage of SSDs is to use them\nas a caching layer to store performance critical data blocks to reduce the\nnumber of accesses to disk subsystem. Due to characteristics of Flash-based\nSSDs such as limited write endurance and long latency on write operations,\nemploying caching algorithms at the Operating System (OS) level necessitates to\ntake such characteristics into consideration. Previous caching techniques are\noptimized towards only one type of application, which affects both generality\nand applicability. In addition, they are not adaptive when the workload pattern\nchanges over time. This paper presents an efficient Reconfigurable Cache\nArchitecture (ReCA) for storage systems using a comprehensive workload\ncharacterization to find an optimal cache configuration for I/O intensive\napplications. For this purpose, we first investigate various types of I/O\nworkloads and classify them into five major classes. Based on this\ncharacterization, an optimal cache configuration is presented for each class of\nworkloads. Then, using the main features of each class, we continuously monitor\nthe characteristics of an application during system runtime and the cache\norganization is reconfigured if the application changes from one class to\nanother class of workloads. The cache reconfiguration is done online and\nworkload classes can be extended to emerging I/O workloads in order to maintain\nits efficiency with the characteristics of I/O requests. Experimental results\nobtained by implementing ReCA in a server running Linux show that the proposed\narchitecture improves performance and lifetime up to 24\\% and 33\\%,\nrespectively.",
        "snippets": [
            "In recent years, SSDs have gained tremendous attention in computing and\nstorage systems due to significant performance improvement over HDDs. The cost\nper capacity of SSDs, however, prevents them from entirely replacing HDDs in\nsuch systems. One approach to effectively take advantage of SSDs is to use them\nas a caching layer to store performance critical data blocks to reduce the\nnumber of accesses to disk subsystem. Due to characteristics of Flash-based\nSSDs such as limited write endurance and long latency on write operations,\nemploying caching algorithms at the Operating System (OS) level necessitates to\ntake such characteristics into consideration. Previous caching techniques are\noptimized towards only one type of application, which affects both generality\nand applicability. In addition, they are not adaptive when the workload pattern\nchanges over time. This paper presents an efficient Reconfigurable Cache\nArchitecture (ReCA) for storage systems using a comprehensive workload\ncharacterization to find an optimal cache configuration for I/O intensive\napplications. For this purpose, we first investigate various types of I/O\nworkloads and classify them into five major classes. Based on this\ncharacterization, an optimal cache configuration is presented for each class of\nworkloads. Then, using the main features of each class, we continuously monitor\nthe characteristics of an application during system runtime and the cache\norganization is reconfigured if the application changes from one class to\nanother class of workloads. The cache reconfiguration is done online and\nworkload classes can be extended to emerging I/O workloads in order to maintain\nits efficiency with the characteristics of I/O requests. Experimental results\nobtained by implementing ReCA in a server running Linux show that the proposed\narchitecture improves performance and lifetime up to 24\\% and 33\\%,\nrespectively."
        ],
        "title": "ReCA: an Efficient Reconfigurable Cache Architecture for Storage Systems with Online Workload Characterization",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1909.09463v1": {
        "url": "http://arxiv.org/abs/1909.09463v1",
        "description": "Major chip manufacturers have all introduced multicore microprocessors.\nMulti-socket systems built from these processors are used for running various\nserver applications. Depending on the application, remote cache-to-cache\ntransfers can severely impact the performance of such workloads. This paper\npresents a cache optimization that can cut down remote cache-to-cache\ntransfers. By keeping track of remote cache lines loaded from remote caches\ninto last-level-cache and by biasing the cache replacement policy towards such\nremote cache lines we can reduce the number of cache misses. This in turn\nresults in improvement of overall performance. I present the design details in\nthis paper. I do a qualitative comparison of various solutions to the problem\nof performance impact of remote cache-to-cache transfers. This work can be\nextended by doing a quantitative evaluation.",
        "snippets": [
            "Major chip manufacturers have all introduced multicore microprocessors.\nMulti-socket systems built from these processors are used for running various\nserver applications. Depending on the application, remote cache-to-cache\ntransfers can severely impact the performance of such workloads. This paper\npresents a cache optimization that can cut down remote cache-to-cache\ntransfers. By keeping track of remote cache lines loaded from remote caches\ninto last-level-cache and by biasing the cache replacement policy towards such\nremote cache lines we can reduce the number of cache misses. This in turn\nresults in improvement of overall performance. I present the design details in\nthis paper. I do a qualitative comparison of various solutions to the problem\nof performance impact of remote cache-to-cache transfers. This work can be\nextended by doing a quantitative evaluation."
        ],
        "title": "Cache Optimization for Sharing Intensive Workloads on Multi-socket Multi-core servers",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1805.00976v1": {
        "url": "http://arxiv.org/abs/1805.00976v1",
        "description": "In recent years, high interest in using Virtual Machines (VMs) in data\ncenters and Cloud computing has significantly increased the demand for\nhigh-performance data storage systems. Recent studies suggest using SSDs as a\ncaching layer for HDD-based storage subsystems in virtualization platforms.\nSuch studies neglect to address the endurance and cost of SSDs, which can\nsignificantly affect the efficiency of I/O caching. Moreover, previous studies\nonly configure the cache size to provide the required performance level for\neach VM, while neglecting other important parameters such as cache write policy\nand request type, which can adversely affect both performance-per-cost and\nendurance.\n  In this paper, we present a new high-Endurance and Cost-efficient I/O Caching\n(ECI-Cache) scheme for virtualized platforms, which can significantly improve\nboth the performance-per-cost and endurance of storage subsystems as opposed to\npreviously proposed I/O caching schemes. Unlike traditional I/O caching schemes\nwhich allocate cache size only based on reuse distance of accesses, we propose\na new metric, Useful Reuse Distance (URD), which considers the request type in\nreuse distance calculation, resulting in improved performance-per-cost and\nendurance for the SSD cache. Via online characterization of workloads and using\nURD, ECI-Cache partitions the SSD cache across VMs and is able to dynamically\nadjust the cache size and write policy for each VM. To evaluate the proposed\nscheme, we have implemented ECI-Cache in an open source hypervisor, QEMU\n(version 2.8.0), on a server running the CentOS 7 operating system (kernel\nversion 3.10.0-327). Experimental results show that our proposed scheme\nimproves the performance, performance-per-cost, and endurance of the SSD cache\nby 17%, 30% and 65%, respectively, compared to the state-of-the-art dynamic\ncache partitioning scheme.",
        "snippets": [
            "In recent years, high interest in using Virtual Machines (VMs) in data\ncenters and Cloud computing has significantly increased the demand for\nhigh-performance data storage systems. Recent studies suggest using SSDs as a\ncaching layer for HDD-based storage subsystems in virtualization platforms.\nSuch studies neglect to address the endurance and cost of SSDs, which can\nsignificantly affect the efficiency of I/O caching. Moreover, previous studies\nonly configure the cache size to provide the required performance level for\neach VM, while neglecting other important parameters such as cache write policy\nand request type, which can adversely affect both performance-per-cost and\nendurance.\n  In this paper, we present a new high-Endurance and Cost-efficient I/O Caching\n(ECI-Cache) scheme for virtualized platforms, which can significantly improve\nboth the performance-per-cost and endurance of storage subsystems as opposed to\npreviously proposed I/O caching schemes. Unlike traditional I/O caching schemes\nwhich allocate cache size only based on reuse distance of accesses, we propose\na new metric, Useful Reuse Distance (URD), which considers the request type in\nreuse distance calculation, resulting in improved performance-per-cost and\nendurance for the SSD cache. Via online characterization of workloads and using\nURD, ECI-Cache partitions the SSD cache across VMs and is able to dynamically\nadjust the cache size and write policy for each VM. To evaluate the proposed\nscheme, we have implemented ECI-Cache in an open source hypervisor, QEMU\n(version 2.8.0), on a server running the CentOS 7 operating system (kernel\nversion 3.10.0-327). Experimental results show that our proposed scheme\nimproves the performance, performance-per-cost, and endurance of the SSD cache\nby 17%, 30% and 65%, respectively, compared to the state-of-the-art dynamic\ncache partitioning scheme."
        ],
        "title": "ECI-Cache: A High-Endurance and Cost-Efficient I/O Caching Scheme for Virtualized Platforms",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2007.15859v1": {
        "url": "http://arxiv.org/abs/2007.15859v1",
        "description": "Caching techniques are widely used in the era of cloud computing from\napplications, such as Web caches to infrastructures, Memcached and memory\ncaches in computer architectures. Prediction of cached data can greatly help\nimprove cache management and performance. The recent advancement of deep\nlearning techniques enables the design of novel intelligent cache replacement\npolicies. In this work, we propose a learning-aided approach to predict future\ndata accesses. We find that a powerful LSTM-based recurrent neural network\nmodel can provide high prediction accuracy based on only a cache trace as\ninput. The high accuracy results from a carefully crafted locality-driven\nfeature design. Inspired by the high prediction accuracy, we propose a pseudo\nOPT policy and evaluate it upon 13 real-world storage workloads from Microsoft\nResearch. Results demonstrate that the new cache policy improves state-of-art\npractical policies by up to 19.2% and incurs only 2.3% higher miss ratio than\nOPT on average.",
        "snippets": [
            "Caching techniques are widely used in the era of cloud computing from\napplications, such as Web caches to infrastructures, Memcached and memory\ncaches in computer architectures. Prediction of cached data can greatly help\nimprove cache management and performance. The recent advancement of deep\nlearning techniques enables the design of novel intelligent cache replacement\npolicies. In this work, we propose a learning-aided approach to predict future\ndata accesses. We find that a powerful LSTM-based recurrent neural network\nmodel can provide high prediction accuracy based on only a cache trace as\ninput. The high accuracy results from a carefully crafted locality-driven\nfeature design. Inspired by the high prediction accuracy, we propose a pseudo\nOPT policy and evaluate it upon 13 real-world storage workloads from Microsoft\nResearch. Results demonstrate that the new cache policy improves state-of-art\npractical policies by up to 19.2% and incurs only 2.3% higher miss ratio than\nOPT on average."
        ],
        "title": "Learning Forward Reuse Distance",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1910.00134v1": {
        "url": "http://arxiv.org/abs/1910.00134v1",
        "description": "In recent years, machine intelligence (MI) applications have emerged as a\nmajor driver for the computing industry. Optimizing these workloads is\nimportant but complicated. As memory demands grow and data movement overheads\nincreasingly limit performance, determining the best GPU caching policy to use\nfor a diverse range of MI workloads represents one important challenge. To\nstudy this, we evaluate 17 MI applications and characterize their behaviors\nusing a range of GPU caching strategies. In our evaluations, we find that the\nchoice of caching policy in GPU caches involves multiple performance trade-offs\nand interactions, and there is no one-size-fits-all GPU caching policy for MI\nworkloads. Based on detailed simulation results, we motivate and evaluate a set\nof cache optimizations that consistently match the performance of the best\nstatic GPU caching policies.",
        "snippets": [
            "In recent years, machine intelligence (MI) applications have emerged as a\nmajor driver for the computing industry. Optimizing these workloads is\nimportant but complicated. As memory demands grow and data movement overheads\nincreasingly limit performance, determining the best GPU caching policy to use\nfor a diverse range of MI workloads represents one important challenge. To\nstudy this, we evaluate 17 MI applications and characterize their behaviors\nusing a range of GPU caching strategies. In our evaluations, we find that the\nchoice of caching policy in GPU caches involves multiple performance trade-offs\nand interactions, and there is no one-size-fits-all GPU caching policy for MI\nworkloads. Based on detailed simulation results, we motivate and evaluate a set\nof cache optimizations that consistently match the performance of the best\nstatic GPU caching policies."
        ],
        "title": "Optimizing GPU Cache Policies for MI Workloads",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1902.00795v1": {
        "url": "http://arxiv.org/abs/1902.00795v1",
        "description": "Caches are an important component of modern computing systems given their\nsignificant impact on performance. In particular, caches play a key role in the\ncloud due to the nature of large-scale, data-intensive processing. One of the\nkey challenges for the cloud providers is how to share the caching capacity\namong tenants, under the circumstance that each often requires a different\ndegree of quality of service (QoS) with respect to data access performance. The\ninvariant is that the individual tenants' QoS requirements should be satisfied\nwhile the cache usage is optimized in a system-wide manner. In this paper, we\nintroduce a learning-based approach for dynamic cache management in a cloud,\nwhich is based on the estimation of data access pattern of a tenant and the\nprediction of cache performance for the access pattern in question. We consider\na variety of probability distributions to estimate the data access pattern, and\nexamine a set of learning-based regression techniques to predict the cache hit\nrate for the access pattern. The predicted cache hit rate is then used to make\na decision whether reallocating cache space is needed to meet the QoS\nrequirement for the tenant. Our experimental results with an extensive set of\nsynthetic traces and the YCSB benchmark show that the proposed method\nconsistently optimizes the cache space while satisfying the QoS requirement.",
        "snippets": [
            "Caches are an important component of modern computing systems given their\nsignificant impact on performance. In particular, caches play a key role in the\ncloud due to the nature of large-scale, data-intensive processing. One of the\nkey challenges for the cloud providers is how to share the caching capacity\namong tenants, under the circumstance that each often requires a different\ndegree of quality of service (QoS) with respect to data access performance. The\ninvariant is that the individual tenants' QoS requirements should be satisfied\nwhile the cache usage is optimized in a system-wide manner. In this paper, we\nintroduce a learning-based approach for dynamic cache management in a cloud,\nwhich is based on the estimation of data access pattern of a tenant and the\nprediction of cache performance for the access pattern in question. We consider\na variety of probability distributions to estimate the data access pattern, and\nexamine a set of learning-based regression techniques to predict the cache hit\nrate for the access pattern. The predicted cache hit rate is then used to make\na decision whether reallocating cache space is needed to meet the QoS\nrequirement for the tenant. Our experimental results with an extensive set of\nsynthetic traces and the YCSB benchmark show that the proposed method\nconsistently optimizes the cache space while satisfying the QoS requirement."
        ],
        "title": "Learning-based Dynamic Cache Management in a Cloud",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1603.01352v1": {
        "url": "http://arxiv.org/abs/1603.01352v1",
        "description": "Adding new hardware features to a cloud computing server requires testing\nboth the functionalities and the performance of the new hardware mechanisms.\nHowever, commonly used cloud computing server workloads are not\nwell-represented by the SPEC integer and floating-point benchmark and Parsec\nsuites typically used by the computer architecture community. Existing cloud\nbenchmark suites for scale-out or scale-up computing are not representative of\nthe most common cloud usage, and are very difficult to run on a cycle-accurate\nsimulator that can accurately model new hardware, like gem5. In this paper, we\npresent PALMScloud, a suite of cloud computing benchmarks for performance\nevaluation of cloud servers, that is ready to run on the gem5 cycle-accurate\nsimulator. We demonstrate how our cloud computing benchmarks are used in\nevaluating the cache performance of a new secure cache called Newcache as a\ncase study. We hope that these cloud benchmarks, ready to run on a dual-machine\ngem5 simulator or on real machines, can be useful to other researchers\ninterested in improving hardware micro-architecture and cloud server\nperformance.",
        "snippets": [
            "Adding new hardware features to a cloud computing server requires testing\nboth the functionalities and the performance of the new hardware mechanisms.\nHowever, commonly used cloud computing server workloads are not\nwell-represented by the SPEC integer and floating-point benchmark and Parsec\nsuites typically used by the computer architecture community. Existing cloud\nbenchmark suites for scale-out or scale-up computing are not representative of\nthe most common cloud usage, and are very difficult to run on a cycle-accurate\nsimulator that can accurately model new hardware, like gem5. In this paper, we\npresent PALMScloud, a suite of cloud computing benchmarks for performance\nevaluation of cloud servers, that is ready to run on the gem5 cycle-accurate\nsimulator. We demonstrate how our cloud computing benchmarks are used in\nevaluating the cache performance of a new secure cache called Newcache as a\ncase study. We hope that these cloud benchmarks, ready to run on a dual-machine\ngem5 simulator or on real machines, can be useful to other researchers\ninterested in improving hardware micro-architecture and cloud server\nperformance."
        ],
        "title": "Cloud Server Benchmarks for Performance Evaluation of New Hardware Architecture",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1909.09599v1": {
        "url": "http://arxiv.org/abs/1909.09599v1",
        "description": "Modern multi-core processors share cache resources for maximum cache\nutilization and performance gains. However, this leaves the cache vulnerable to\nside-channel attacks, where timing differences in shared cache behavior are\nexploited to infer information on the victim's execution patterns, ultimately\nleaking private information. The root cause for these attacks is mutually\ndistrusting processes sharing cache entries and accessing them in a\ndeterministic manner. Various defenses against cache side-channel attacks have\nbeen proposed. However, they either degrade performance significantly, impose\nimpractical restrictions, or can only defeat certain classes of these attacks.\nMore importantly, they assume that side-channel-resilient caches are required\nfor the entire execution workload and do not allow to selectively enable the\nmitigation only for the security-critical portion of the workload. We present a\ngeneric mechanism for a flexible and soft partitioning of set-associative\ncaches and propose a hybrid cache architecture, called HybCache. HybCache can\nbe configured to selectively apply side-channel-resilient cache behavior only\nfor isolated execution domains, while providing the non-isolated execution with\nconventional cache behavior, capacity and performance. An isolation domain can\ninclude one or more processes, specific portions of code, or a Trusted\nExecution Environment. We show that, with minimal hardware modifications and\nkernel support, HybCache can provide side-channel-resilient cache only for\nisolated execution with a performance overhead of 3.5-5%, while incurring no\nperformance overhead for the remaining execution workload. We provide a\nsimulator-based and hardware implementation of HybCache to evaluate the\nperformance and area overheads, and show how it mitigates typical access-based\nand contention-based cache attacks.",
        "snippets": [
            "Modern multi-core processors share cache resources for maximum cache\nutilization and performance gains. However, this leaves the cache vulnerable to\nside-channel attacks, where timing differences in shared cache behavior are\nexploited to infer information on the victim's execution patterns, ultimately\nleaking private information. The root cause for these attacks is mutually\ndistrusting processes sharing cache entries and accessing them in a\ndeterministic manner. Various defenses against cache side-channel attacks have\nbeen proposed. However, they either degrade performance significantly, impose\nimpractical restrictions, or can only defeat certain classes of these attacks.\nMore importantly, they assume that side-channel-resilient caches are required\nfor the entire execution workload and do not allow to selectively enable the\nmitigation only for the security-critical portion of the workload. We present a\ngeneric mechanism for a flexible and soft partitioning of set-associative\ncaches and propose a hybrid cache architecture, called HybCache. HybCache can\nbe configured to selectively apply side-channel-resilient cache behavior only\nfor isolated execution domains, while providing the non-isolated execution with\nconventional cache behavior, capacity and performance. An isolation domain can\ninclude one or more processes, specific portions of code, or a Trusted\nExecution Environment. We show that, with minimal hardware modifications and\nkernel support, HybCache can provide side-channel-resilient cache only for\nisolated execution with a performance overhead of 3.5-5%, while incurring no\nperformance overhead for the remaining execution workload. We provide a\nsimulator-based and hardware implementation of HybCache to evaluate the\nperformance and area overheads, and show how it mitigates typical access-based\nand contention-based cache attacks."
        ],
        "title": "HybCache: Hybrid Side-Channel-Resilient Caches for Trusted Execution Environments",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2305.06696v1": {
        "url": "http://arxiv.org/abs/2305.06696v1",
        "description": "In recent years, graph-processing has become an essential class of workloads\nwith applications in a rapidly growing number of fields. Graph-processing\ntypically uses large input sets, often in multi-gigabyte scale, and\ndata-dependent graph traversal methods exhibiting irregular memory access\npatterns. Recent work demonstrates that, due to the highly irregular memory\naccess patterns of data-dependent graph traversals, state-of-the-art\ngraph-processing workloads spend up to 80 % of the total execution time waiting\nfor memory accesses to be served by the DRAM. The vast disparity between the\nLast Level Cache (LLC) and main memory latencies is a problem that has been\naddressed for years in computer architecture. One of the prevailing approaches\nwhen it comes to mitigating this performance gap between modern CPUs and DRAM\nis cache replacement policies. In this work, we characterize the challenges\ndrawn by graph-processing workloads and evaluate the most relevant cache\nreplacement policies.",
        "snippets": [
            "In recent years, graph-processing has become an essential class of workloads\nwith applications in a rapidly growing number of fields. Graph-processing\ntypically uses large input sets, often in multi-gigabyte scale, and\ndata-dependent graph traversal methods exhibiting irregular memory access\npatterns. Recent work demonstrates that, due to the highly irregular memory\naccess patterns of data-dependent graph traversals, state-of-the-art\ngraph-processing workloads spend up to 80 % of the total execution time waiting\nfor memory accesses to be served by the DRAM. The vast disparity between the\nLast Level Cache (LLC) and main memory latencies is a problem that has been\naddressed for years in computer architecture. One of the prevailing approaches\nwhen it comes to mitigating this performance gap between modern CPUs and DRAM\nis cache replacement policies. In this work, we characterize the challenges\ndrawn by graph-processing workloads and evaluate the most relevant cache\nreplacement policies."
        ],
        "title": "Characterizing the impact of last-level cache replacement policies on big-data workloads",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1912.01555v1": {
        "url": "http://arxiv.org/abs/1912.01555v1",
        "description": "In this paper, we present a comprehensive analysis investigating the\nreliability of SSD-based I/O caching architectures used in enterprise storage\nsystems under power failure and high-operating temperature. We explore variety\nof SSDs from top vendors and investigate the cache reliability in mirrored\nconfiguration. To this end, we first develop a physical fault injection and\nfailure detection platform and then investigate the impact of workload\ndependent parameters on the reliability of I/O cache in the presence of two\ncommon failure types in data centers, power outage and high temperature faults.\nWe implement an I/O cache scheme using an open-source I/O cache module in Linux\noperating system. The experimental results obtained by conducting more than\ntwenty thousand of physical fault injections on the implemented I/O cache with\ndifferent write policies reveal that the failure rate of the I/O cache is\nsignificantly affected by workload dependent parameters. Our results show that\nunlike workload requests access pattern, the other workload dependent\nparameters such as request size, Working Set Size (WSS), and sequence of the\naccesses have considerable impact on the I/O cache failure rate. We observe a\nsignificant growth in the failure rate in the workloads by decreasing the size\nof the requests (by more than 14X). Furthermore, we observe that in addition to\nwrites, the read accesses to the I/O cache are subjected to failure in presence\nof sudden power outage (the failure mainly occurs during promoting data to the\ncache). In addition, we observe that I/O cache experiences no data failure upon\nhigh temperature faults.",
        "snippets": [
            "In this paper, we present a comprehensive analysis investigating the\nreliability of SSD-based I/O caching architectures used in enterprise storage\nsystems under power failure and high-operating temperature. We explore variety\nof SSDs from top vendors and investigate the cache reliability in mirrored\nconfiguration. To this end, we first develop a physical fault injection and\nfailure detection platform and then investigate the impact of workload\ndependent parameters on the reliability of I/O cache in the presence of two\ncommon failure types in data centers, power outage and high temperature faults.\nWe implement an I/O cache scheme using an open-source I/O cache module in Linux\noperating system. The experimental results obtained by conducting more than\ntwenty thousand of physical fault injections on the implemented I/O cache with\ndifferent write policies reveal that the failure rate of the I/O cache is\nsignificantly affected by workload dependent parameters. Our results show that\nunlike workload requests access pattern, the other workload dependent\nparameters such as request size, Working Set Size (WSS), and sequence of the\naccesses have considerable impact on the I/O cache failure rate. We observe a\nsignificant growth in the failure rate in the workloads by decreasing the size\nof the requests (by more than 14X). Furthermore, we observe that in addition to\nwrites, the read accesses to the I/O cache are subjected to failure in presence\nof sudden power outage (the failure mainly occurs during promoting data to the\ncache). In addition, we observe that I/O cache experiences no data failure upon\nhigh temperature faults."
        ],
        "title": "Evaluating Reliability of SSD-Based I/O Caches in Enterprise Storage Systems",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2504.02220v1": {
        "url": "http://arxiv.org/abs/2504.02220v1",
        "description": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics.",
        "snippets": [
            "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics."
        ],
        "title": "Comparative Analysis of Distributed Caching Algorithms: Performance Metrics and Implementation Considerations",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.15192v2": {
        "url": "http://arxiv.org/abs/2502.15192v2",
        "description": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects.",
        "snippets": [
            "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects."
        ],
        "title": "SPAARC: Spatial Proximity and Association based prefetching for Augmented Reality in edge Cache",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2006.08067v2": {
        "url": "http://arxiv.org/abs/2006.08067v2",
        "description": "Distributed caches are widely deployed to serve social networks and web\napplications at billion-user scales. This paper presents Cache-on-Track (CoT),\na decentralized, elastic, and predictive caching framework for cloud\nenvironments. CoT proposes a new cache replacement policy specifically tailored\nfor small front-end caches that serve skewed workloads. Front-end servers use a\nheavy hitter tracking algorithm to continuously track the top-k hot keys. CoT\ndynamically caches the hottest C keys out of the tracked keys. Our experiments\nshow that CoT's replacement policy consistently outperforms the hit-rates of\nLRU, LFU, and ARC for the same cache size on different skewed workloads. Also,\n\\algoname slightly outperforms the hit-rate of LRU-2 when both policies are\nconfigured with the same tracking (history) size. CoT achieves server size\nload-balance with 50\\% to 93.75\\% less front-end cache in comparison to other\nreplacement policies.",
        "snippets": [
            "Distributed caches are widely deployed to serve social networks and web\napplications at billion-user scales. This paper presents Cache-on-Track (CoT),\na decentralized, elastic, and predictive caching framework for cloud\nenvironments. CoT proposes a new cache replacement policy specifically tailored\nfor small front-end caches that serve skewed workloads. Front-end servers use a\nheavy hitter tracking algorithm to continuously track the top-k hot keys. CoT\ndynamically caches the hottest C keys out of the tracked keys. Our experiments\nshow that CoT's replacement policy consistently outperforms the hit-rates of\nLRU, LFU, and ARC for the same cache size on different skewed workloads. Also,\n\\algoname slightly outperforms the hit-rate of LRU-2 when both policies are\nconfigured with the same tracking (history) size. CoT achieves server size\nload-balance with 50\\% to 93.75\\% less front-end cache in comparison to other\nreplacement policies."
        ],
        "title": "CoT: Decentralized Elastic Caches for Cloud Environments",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1703.08280v1": {
        "url": "http://arxiv.org/abs/1703.08280v1",
        "description": "Memory caches are being aggressively used in today's data-parallel systems\nsuch as Spark, Tez, and Piccolo. However, prevalent systems employ rather\nsimple cache management policies--notably the Least Recently Used (LRU)\npolicy--that are oblivious to the application semantics of data dependency,\nexpressed as a directed acyclic graph (DAG). Without this knowledge, memory\ncaching can at best be performed by \"guessing\" the future data access patterns\nbased on historical information (e.g., the access recency and/or frequency),\nwhich frequently results in inefficient, erroneous caching with low hit ratio\nand a long response time. In this paper, we propose a novel cache replacement\npolicy, Least Reference Count (LRC), which exploits the application-specific\nDAG information to optimize the cache management. LRC evicts the cached data\nblocks whose reference count is the smallest. The reference count is defined,\nfor each data block, as the number of dependent child blocks that have not been\ncomputed yet. We demonstrate the efficacy of LRC through both empirical\nanalysis and cluster deployments against popular benchmarking workloads. Our\nSpark implementation shows that, compared with LRU, LRC speeds up typical\napplications by 60%.",
        "snippets": [
            "Memory caches are being aggressively used in today's data-parallel systems\nsuch as Spark, Tez, and Piccolo. However, prevalent systems employ rather\nsimple cache management policies--notably the Least Recently Used (LRU)\npolicy--that are oblivious to the application semantics of data dependency,\nexpressed as a directed acyclic graph (DAG). Without this knowledge, memory\ncaching can at best be performed by \"guessing\" the future data access patterns\nbased on historical information (e.g., the access recency and/or frequency),\nwhich frequently results in inefficient, erroneous caching with low hit ratio\nand a long response time. In this paper, we propose a novel cache replacement\npolicy, Least Reference Count (LRC), which exploits the application-specific\nDAG information to optimize the cache management. LRC evicts the cached data\nblocks whose reference count is the smallest. The reference count is defined,\nfor each data block, as the number of dependent child blocks that have not been\ncomputed yet. We demonstrate the efficacy of LRC through both empirical\nanalysis and cluster deployments against popular benchmarking workloads. Our\nSpark implementation shows that, compared with LRU, LRC speeds up typical\napplications by 60%."
        ],
        "title": "LRC: Dependency-Aware Cache Management for Data Analytics Clusters",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2211.15739v1": {
        "url": "http://arxiv.org/abs/2211.15739v1",
        "description": "Workloads in modern cloud data centers are becoming increasingly complex. The\nnumber of workloads running in cloud data centers has been growing\nexponentially for the last few years, and cloud service providers (CSP) have\nbeen supporting on-demand services in real-time. Realizing the growing\ncomplexity of cloud environment and cloud workloads, hardware vendors such as\nIntel and AMD are increasingly introducing cloud-specific workload acceleration\nfeatures in their CPU platforms. These features are typically targeted towards\npopular and commonly-used cloud workloads. Nonetheless, uncommon,\ncustomer-specific workloads (unknown workloads), if their characteristics are\ndifferent from common workloads (known workloads), may not realize the\npotential of the underlying platform. To address this problem of realizing the\nfull potential of the underlying platform, we develop a machine learning based\ntechnique to characterize, profile and predict workloads running in the cloud\nenvironment. Experimental evaluation of our technique demonstrates good\nprediction performance. We also develop techniques to analyze the performance\nof the model in a standalone manner.",
        "snippets": [
            "Workloads in modern cloud data centers are becoming increasingly complex. The\nnumber of workloads running in cloud data centers has been growing\nexponentially for the last few years, and cloud service providers (CSP) have\nbeen supporting on-demand services in real-time. Realizing the growing\ncomplexity of cloud environment and cloud workloads, hardware vendors such as\nIntel and AMD are increasingly introducing cloud-specific workload acceleration\nfeatures in their CPU platforms. These features are typically targeted towards\npopular and commonly-used cloud workloads. Nonetheless, uncommon,\ncustomer-specific workloads (unknown workloads), if their characteristics are\ndifferent from common workloads (known workloads), may not realize the\npotential of the underlying platform. To address this problem of realizing the\nfull potential of the underlying platform, we develop a machine learning based\ntechnique to characterize, profile and predict workloads running in the cloud\nenvironment. Experimental evaluation of our technique demonstrates good\nprediction performance. We also develop techniques to analyze the performance\nof the model in a standalone manner."
        ],
        "title": "CWD: A Machine Learning based Approach to Detect Unknown Cloud Workloads",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1504.06736v2": {
        "url": "http://arxiv.org/abs/1504.06736v2",
        "description": "Systems for processing big data---e.g., Hadoop, Spark, and massively parallel\ndatabases---need to run workloads on behalf of multiple tenants simultaneously.\nThe abundant disk-based storage in these systems is usually complemented by a\nsmaller, but much faster, {\\em cache}. Cache is a precious resource: Tenants\nwho get to use cache can see two orders of magnitude performance improvement.\nCache is also a limited and hence shared resource: Unlike a resource like a CPU\ncore which can be used by only one tenant at a time, a cached data item can be\naccessed by multiple tenants at the same time. Cache, therefore, has to be\nshared by a multi-tenancy-aware policy across tenants, each having a unique set\nof priorities and workload characteristics.\n  In this paper, we develop cache allocation strategies that speed up the\noverall workload while being {\\em fair} to each tenant. We build a novel\nfairness model targeted at the shared resource setting that incorporates not\nonly the more standard concepts of Pareto-efficiency and sharing incentive, but\nalso define envy freeness via the notion of {\\em core} from cooperative game\ntheory. Our cache management platform, ROBUS, uses randomization over small\ntime batches, and we develop a proportionally fair allocation mechanism that\nsatisfies the core property in expectation. We show that this algorithm and\nrelated fair algorithms can be approximated to arbitrary precision in\npolynomial time. We evaluate these algorithms on a ROBUS prototype implemented\non Spark with RDD store used as cache. Our evaluation on a synthetically\ngenerated industry-standard workload shows that our algorithms provide a\nspeedup close to performance optimal algorithms while guaranteeing fairness\nacross tenants.",
        "snippets": [
            "Systems for processing big data---e.g., Hadoop, Spark, and massively parallel\ndatabases---need to run workloads on behalf of multiple tenants simultaneously.\nThe abundant disk-based storage in these systems is usually complemented by a\nsmaller, but much faster, {\\em cache}. Cache is a precious resource: Tenants\nwho get to use cache can see two orders of magnitude performance improvement.\nCache is also a limited and hence shared resource: Unlike a resource like a CPU\ncore which can be used by only one tenant at a time, a cached data item can be\naccessed by multiple tenants at the same time. Cache, therefore, has to be\nshared by a multi-tenancy-aware policy across tenants, each having a unique set\nof priorities and workload characteristics.\n  In this paper, we develop cache allocation strategies that speed up the\noverall workload while being {\\em fair} to each tenant. We build a novel\nfairness model targeted at the shared resource setting that incorporates not\nonly the more standard concepts of Pareto-efficiency and sharing incentive, but\nalso define envy freeness via the notion of {\\em core} from cooperative game\ntheory. Our cache management platform, ROBUS, uses randomization over small\ntime batches, and we develop a proportionally fair allocation mechanism that\nsatisfies the core property in expectation. We show that this algorithm and\nrelated fair algorithms can be approximated to arbitrary precision in\npolynomial time. We evaluate these algorithms on a ROBUS prototype implemented\non Spark with RDD store used as cache. Our evaluation on a synthetically\ngenerated industry-standard workload shows that our algorithms provide a\nspeedup close to performance optimal algorithms while guaranteeing fairness\nacross tenants."
        ],
        "title": "ROBUS: Fair Cache Allocation for Multi-tenant Data-parallel Workloads",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1906.01260v1": {
        "url": "http://arxiv.org/abs/1906.01260v1",
        "description": "The in-memory cache system is an important component in a cloud for the data\naccess performance. As the tenants may have different performance goals for\ndata access depending on the nature of their tasks, effectively managing the\nmemory cache is a crucial concern in such a shared computing environment. Two\nextreme methods for managing the memory cache are unlimited sharing and\ncomplete isolation, both of which would be inefficient with the expensive\nstorage complexity to meet the per-tenant performance requirement. In this\npaper, we present a new cache model that incorporates global caching (based on\nunlimited sharing) and static caching (offering complete isolation) for a\nprivate cloud, in which it is critical to offer the guaranteed performance\nwhile minimizing the operating cost. This paper also presents a cache insertion\nalgorithm tailored to the proposed cache model. From an extensive set of\nexperiments conducted on the simulation and emulation settings, the results\nconfirm the validity of the presented cache architecture and insertion\nalgorithm showing the optimized use of the cache space for meeting the\nper-tenant performance requirement.",
        "snippets": [
            "The in-memory cache system is an important component in a cloud for the data\naccess performance. As the tenants may have different performance goals for\ndata access depending on the nature of their tasks, effectively managing the\nmemory cache is a crucial concern in such a shared computing environment. Two\nextreme methods for managing the memory cache are unlimited sharing and\ncomplete isolation, both of which would be inefficient with the expensive\nstorage complexity to meet the per-tenant performance requirement. In this\npaper, we present a new cache model that incorporates global caching (based on\nunlimited sharing) and static caching (offering complete isolation) for a\nprivate cloud, in which it is critical to offer the guaranteed performance\nwhile minimizing the operating cost. This paper also presents a cache insertion\nalgorithm tailored to the proposed cache model. From an extensive set of\nexperiments conducted on the simulation and emulation settings, the results\nconfirm the validity of the presented cache architecture and insertion\nalgorithm showing the optimized use of the cache space for meeting the\nper-tenant performance requirement."
        ],
        "title": "A Hybrid Cache Architecture for Meeting Per-Tenant Performance Goals in a Private Cloud",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2012.04880v1": {
        "url": "http://arxiv.org/abs/2012.04880v1",
        "description": "With diverse IoT workloads, placing compute and analytics close to where data\nis collected is becoming increasingly important. We seek to understand what is\nthe performance and the cost implication of running analytics on IoT data at\nthe various available platforms. These workloads can be compute-light, such as\noutlier detection on sensor data, or compute-intensive, such as object\ndetection from video feeds obtained from drones. In our paper, JANUS, we\nprofile the performance/$ and the compute versus communication cost for a\ncompute-light IoT workload and a compute-intensive IoT workload. In addition,\nwe also look at the pros and cons of some of the proprietary deep-learning\nobject detection packages, such as Amazon Rekognition, Google Vision, and Azure\nCognitive Services, to contrast with open-source and tunable solutions, such as\nFaster R-CNN (FRCNN). We find that AWS IoT Greengrass delivers at least 2X\nlower latency and 1.25X lower cost compared to all other cloud platforms for\nthe compute-light outlier detection workload. For the compute-intensive\nstreaming video analytics task, an opensource solution to object detection\nrunning on cloud VMs saves on dollar costs compared to proprietary solutions\nprovided by Amazon, Microsoft, and Google, but loses out on latency (up to 6X).\nIf it runs on a low-powered edge device, the latency is up to 49X lower.",
        "snippets": [
            "With diverse IoT workloads, placing compute and analytics close to where data\nis collected is becoming increasingly important. We seek to understand what is\nthe performance and the cost implication of running analytics on IoT data at\nthe various available platforms. These workloads can be compute-light, such as\noutlier detection on sensor data, or compute-intensive, such as object\ndetection from video feeds obtained from drones. In our paper, JANUS, we\nprofile the performance/$ and the compute versus communication cost for a\ncompute-light IoT workload and a compute-intensive IoT workload. In addition,\nwe also look at the pros and cons of some of the proprietary deep-learning\nobject detection packages, such as Amazon Rekognition, Google Vision, and Azure\nCognitive Services, to contrast with open-source and tunable solutions, such as\nFaster R-CNN (FRCNN). We find that AWS IoT Greengrass delivers at least 2X\nlower latency and 1.25X lower cost compared to all other cloud platforms for\nthe compute-light outlier detection workload. For the compute-intensive\nstreaming video analytics task, an opensource solution to object detection\nrunning on cloud VMs saves on dollar costs compared to proprietary solutions\nprovided by Amazon, Microsoft, and Google, but loses out on latency (up to 6X).\nIf it runs on a low-powered edge device, the latency is up to 49X lower."
        ],
        "title": "JANUS: Benchmarking Commercial and Open-Source Cloud and Edge Platforms for Object and Anomaly Detection Workloads",
        "meta": {
            "query": "workload patterns and caching performance in cloud computing"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2304.10268v5": {
        "url": "http://arxiv.org/abs/2304.10268v5",
        "description": "Caches are used to reduce the speed differential between the CPU and memory\nto improve the performance of modern processors. However, attackers can use\ncontention-based cache timing attacks to steal sensitive information from\nvictim processes through carefully designed cache eviction sets. And L1 data\ncache attacks are widely exploited and pose a significant privacy and\nconfidentiality threat. Existing hardware-based countermeasures mainly focus on\ncache partitioning, randomization, and cache line flushing, which unfortunately\neither incur high overhead or can be circumvented by sophisticated attacks. In\nthis paper, we propose a novel hardware-software co-design called BackCache\nwith the idea of always achieving cache hits instead of cache misses to\nmitigate contention-based cache timing attacks on the L1 data cache. BackCache\nplaces the evicted cache lines from the L1 data cache into a fully-associative\nbackup cache to hide the evictions. To improve the security of BackCache, we\nintroduce a randomly used replacement policy (RURP) and a dynamic backup cache\nresizing mechanism. We also present a theoretical security analysis to\ndemonstrate the effectiveness of BackCache. Our evaluation on the gem5\nsimulator shows that BackCache can degrade the performance by 2.61%, 2.66%, and\n3.36% For OS kernel, single-thread, and multi-thread benchmarks.",
        "snippets": [
            "Caches are used to reduce the speed differential between the CPU and memory\nto improve the performance of modern processors. However, attackers can use\ncontention-based cache timing attacks to steal sensitive information from\nvictim processes through carefully designed cache eviction sets. And L1 data\ncache attacks are widely exploited and pose a significant privacy and\nconfidentiality threat. Existing hardware-based countermeasures mainly focus on\ncache partitioning, randomization, and cache line flushing, which unfortunately\neither incur high overhead or can be circumvented by sophisticated attacks. In\nthis paper, we propose a novel hardware-software co-design called BackCache\nwith the idea of always achieving cache hits instead of cache misses to\nmitigate contention-based cache timing attacks on the L1 data cache. BackCache\nplaces the evicted cache lines from the L1 data cache into a fully-associative\nbackup cache to hide the evictions. To improve the security of BackCache, we\nintroduce a randomly used replacement policy (RURP) and a dynamic backup cache\nresizing mechanism. We also present a theoretical security analysis to\ndemonstrate the effectiveness of BackCache. Our evaluation on the gem5\nsimulator shows that BackCache can degrade the performance by 2.61%, 2.66%, and\n3.36% For OS kernel, single-thread, and multi-thread benchmarks."
        ],
        "title": "BackCache: Mitigating Contention-Based Cache Timing Attacks by Hiding Cache Line Evictions",
        "meta": {
            "query": "cache eviction policies for LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1702.04078v4": {
        "url": "http://arxiv.org/abs/1702.04078v4",
        "description": "To cope with the ongoing changing demands of the internet, 'in-network\ncaching' has been presented as an application solution for two decades. With\nthe advent of information-centric network (ICN) architecture, 'in-network\ncaching' becomes a network level solution. Some unique features of ICNs, e.g.,\nrapidly changing cache states, higher request arrival rates, smaller cache\nsizes, and other factors, impose diverse requirements on the content eviction\npolicies. In particular, eviction policies should be fast and lightweight. In\nthis study, we propose cache replication and eviction schemes, Conditional\nLeave Cope Everywhere (CLCE) and Least Frequent Recently Used (LFRU), which are\nwell suited for the ICN type of cache networks (CNs). The CLCE replication\nscheme reduces the redundant caching of contents; hence improves the cache\nspace utilization. LFRU approximates the Least Frequently Used (LFU) scheme\ncoupled with the Least Recently Used (LRU) scheme and is practically\nimplementable for rapidly changing cache networks like ICNs.",
        "snippets": [
            "To cope with the ongoing changing demands of the internet, 'in-network\ncaching' has been presented as an application solution for two decades. With\nthe advent of information-centric network (ICN) architecture, 'in-network\ncaching' becomes a network level solution. Some unique features of ICNs, e.g.,\nrapidly changing cache states, higher request arrival rates, smaller cache\nsizes, and other factors, impose diverse requirements on the content eviction\npolicies. In particular, eviction policies should be fast and lightweight. In\nthis study, we propose cache replication and eviction schemes, Conditional\nLeave Cope Everywhere (CLCE) and Least Frequent Recently Used (LFRU), which are\nwell suited for the ICN type of cache networks (CNs). The CLCE replication\nscheme reduces the redundant caching of contents; hence improves the cache\nspace utilization. LFRU approximates the Least Frequently Used (LFU) scheme\ncoupled with the Least Recently Used (LRU) scheme and is practically\nimplementable for rapidly changing cache networks like ICNs."
        ],
        "title": "A Cache Management Scheme for Efficient Content Eviction and Replication in Cache Networks",
        "meta": {
            "query": "cache eviction policies for LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1810.06930v1": {
        "url": "http://arxiv.org/abs/1810.06930v1",
        "description": "We propose a caching policy that uses a feedforward neural network (FNN) to\npredict content popularity. Our scheme outperforms popular eviction policies\nlike LRU or ARC, but also a new policy relying on the more complex recurrent\nneural networks. At the same time, replacing the FNN predictor with a naive\nlinear estimator does not degrade caching performance significantly,\nquestioning then the role of neural networks for these applications.",
        "snippets": [
            "We propose a caching policy that uses a feedforward neural network (FNN) to\npredict content popularity. Our scheme outperforms popular eviction policies\nlike LRU or ARC, but also a new policy relying on the more complex recurrent\nneural networks. At the same time, replacing the FNN predictor with a naive\nlinear estimator does not degrade caching performance significantly,\nquestioning then the role of neural networks for these applications."
        ],
        "title": "Feedforward Neural Networks for Caching: Enough or Too Much?",
        "meta": {
            "query": "cache eviction policies for LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1904.06278v1": {
        "url": "http://arxiv.org/abs/1904.06278v1",
        "description": "Caches have become the prime method for unintended information extraction\nacross logical isolation boundaries. Even Spectre and Meltdown rely on the\ncache side channel, as it provides great resolution and is widely available on\nall major CPU platforms. As a consequence, several methods to stop cache\nattacks by detecting them have been proposed. Detection is strongly aided by\nthe fact that observing cache activity of co-resident processes is not possible\nwithout altering the cache state and thereby forcing evictions on the observed\nprocesses. In this work, we show that this widely held assumption is incorrect.\nThrough clever usage of the cache replacement policy it is possible to track a\nvictims process cache accesses without forcing evictions on the victim's data.\nHence, online detection mechanisms that rely on these evictions can be\ncircumvented as they do not detect be the introduced RELOAD+REFRESH attack. The\nattack requires a profound understanding of the cache replacement policy. We\npresent a methodology to recover the replacement policy and apply it to the\nlast five generations of Intel processors. We further show empirically that the\nperformance of RELOAD+REFRESH on cryptographic implementations is comparable to\nthat of other widely used cache attacks, while its detectability becomes\nextremely difficult, due to the negligible effect on the victims cache access\npattern.",
        "snippets": [
            "Caches have become the prime method for unintended information extraction\nacross logical isolation boundaries. Even Spectre and Meltdown rely on the\ncache side channel, as it provides great resolution and is widely available on\nall major CPU platforms. As a consequence, several methods to stop cache\nattacks by detecting them have been proposed. Detection is strongly aided by\nthe fact that observing cache activity of co-resident processes is not possible\nwithout altering the cache state and thereby forcing evictions on the observed\nprocesses. In this work, we show that this widely held assumption is incorrect.\nThrough clever usage of the cache replacement policy it is possible to track a\nvictims process cache accesses without forcing evictions on the victim's data.\nHence, online detection mechanisms that rely on these evictions can be\ncircumvented as they do not detect be the introduced RELOAD+REFRESH attack. The\nattack requires a profound understanding of the cache replacement policy. We\npresent a methodology to recover the replacement policy and apply it to the\nlast five generations of Intel processors. We further show empirically that the\nperformance of RELOAD+REFRESH on cryptographic implementations is comparable to\nthat of other widely used cache attacks, while its detectability becomes\nextremely difficult, due to the negligible effect on the victims cache access\npattern."
        ],
        "title": "RELOAD+REFRESH: Abusing Cache Replacement Policies to Perform Stealthy Cache Attacks",
        "meta": {
            "query": "cache eviction policies for LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2407.08454v2": {
        "url": "http://arxiv.org/abs/2407.08454v2",
        "description": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.",
        "snippets": [
            "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets."
        ],
        "title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks",
        "meta": {
            "query": "cache eviction policies for LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.00817v1": {
        "url": "http://arxiv.org/abs/2505.00817v1",
        "description": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains.",
        "snippets": [
            "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains."
        ],
        "title": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from Large Language Models",
        "meta": {
            "query": "cache eviction policies for LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.24133v2": {
        "url": "http://arxiv.org/abs/2505.24133v2",
        "description": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
        "snippets": [
            "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
        ],
        "title": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration",
        "meta": {
            "query": "KV$ caching characterization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.16163v1": {
        "url": "http://arxiv.org/abs/2503.16163v1",
        "description": "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio.",
        "snippets": [
            "Transformer-based large language models (LLMs) have already achieved\nremarkable results on long-text tasks, but the limited GPU memory (VRAM)\nresources struggle to accommodate the linearly growing demand for key-value\n(KV) cache as the sequence length increases, which has become a bottleneck for\nthe application of LLMs on long sequences. Existing KV cache compression\nmethods include eviction, merging, or quantization of the KV cache to reduce\nits size. However, compression results in irreversible information forgetting,\npotentially affecting the accuracy of subsequent decoding. In this paper, we\npropose SpeCache, which takes full advantage of the large and easily expandable\nCPU memory to offload the complete KV cache, and dynamically fetches KV pairs\nback in each decoding step based on their importance measured by low-bit KV\ncache copy in VRAM. To avoid inference latency caused by CPU-GPU communication,\nSpeCache speculatively predicts the KV pairs that the next token might attend\nto, allowing us to prefetch them before the next decoding step which enables\nparallelization of prefetching and computation. Experiments on LongBench and\nNeedle-in-a-Haystack benchmarks verify that SpeCache effectively reduces VRAM\nusage while avoiding information forgetting for long sequences without\nre-training, even with a 10x high KV cache compression ratio."
        ],
        "title": "SpeCache: Speculative Key-Value Caching for Efficient Generation of LLMs",
        "meta": {
            "query": "KV$ caching characterization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.15252v1": {
        "url": "http://arxiv.org/abs/2410.15252v1",
        "description": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression.",
        "snippets": [
            "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression."
        ],
        "title": "Lossless KV Cache Compression to 2%",
        "meta": {
            "query": "KV$ caching characterization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.17599v2": {
        "url": "http://arxiv.org/abs/2502.17599v2",
        "description": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
        "snippets": [
            "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
        ],
        "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference",
        "meta": {
            "query": "KV$ caching characterization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2412.09036v1": {
        "url": "http://arxiv.org/abs/2412.09036v1",
        "description": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.",
        "snippets": [
            "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance."
        ],
        "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty",
        "meta": {
            "query": "KV$ caching characterization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.13109v1": {
        "url": "http://arxiv.org/abs/2505.13109v1",
        "description": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
        "snippets": [
            "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
        ],
        "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
        "meta": {
            "query": "KV$ caching characterization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2310.01801v4": {
        "url": "http://arxiv.org/abs/2310.01801v4",
        "description": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
        "snippets": [
            "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
        ],
        "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
        "meta": {
            "query": "KV$ caching characterization for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.01330v1": {
        "url": "http://arxiv.org/abs/2503.01330v1",
        "description": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio.",
        "snippets": [
            "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio."
        ],
        "title": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large Language Models",
        "meta": {
            "query": "large language model serving workload patterns and cache eviction policies"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2404.14961v1": {
        "url": "http://arxiv.org/abs/2404.14961v1",
        "description": "Modern large-scale recommender systems are built upon computation-intensive\ninfrastructure and usually suffer from a huge difference in traffic between\npeak and off-peak periods. In peak periods, it is challenging to perform\nreal-time computation for each request due to the limited budget of\ncomputational resources. The recommendation with a cache is a solution to this\nproblem, where a user-wise result cache is used to provide recommendations when\nthe recommender system cannot afford a real-time computation. However, the\ncached recommendations are usually suboptimal compared to real-time\ncomputation, and it is challenging to determine the items in the cache for each\nuser. In this paper, we provide a cache-aware reinforcement learning (CARL)\nmethod to jointly optimize the recommendation by real-time computation and by\nthe cache. We formulate the problem as a Markov decision process with user\nstates and a cache state, where the cache state represents whether the\nrecommender system performs recommendations by real-time computation or by the\ncache. The computational load of the recommender system determines the cache\nstate. We perform reinforcement learning based on such a model to improve user\nengagement over multiple requests. Moreover, we show that the cache will\nintroduce a challenge called critic dependency, which deteriorates the\nperformance of reinforcement learning. To tackle this challenge, we propose an\neigenfunction learning (EL) method to learn independent critics for CARL.\nExperiments show that CARL can significantly improve the users' engagement when\nconsidering the result cache. CARL has been fully launched in Kwai app, serving\nover 100 million users.",
        "snippets": [
            "Modern large-scale recommender systems are built upon computation-intensive\ninfrastructure and usually suffer from a huge difference in traffic between\npeak and off-peak periods. In peak periods, it is challenging to perform\nreal-time computation for each request due to the limited budget of\ncomputational resources. The recommendation with a cache is a solution to this\nproblem, where a user-wise result cache is used to provide recommendations when\nthe recommender system cannot afford a real-time computation. However, the\ncached recommendations are usually suboptimal compared to real-time\ncomputation, and it is challenging to determine the items in the cache for each\nuser. In this paper, we provide a cache-aware reinforcement learning (CARL)\nmethod to jointly optimize the recommendation by real-time computation and by\nthe cache. We formulate the problem as a Markov decision process with user\nstates and a cache state, where the cache state represents whether the\nrecommender system performs recommendations by real-time computation or by the\ncache. The computational load of the recommender system determines the cache\nstate. We perform reinforcement learning based on such a model to improve user\nengagement over multiple requests. Moreover, we show that the cache will\nintroduce a challenge called critic dependency, which deteriorates the\nperformance of reinforcement learning. To tackle this challenge, we propose an\neigenfunction learning (EL) method to learn independent critics for CARL.\nExperiments show that CARL can significantly improve the users' engagement when\nconsidering the result cache. CARL has been fully launched in Kwai app, serving\nover 100 million users."
        ],
        "title": "Cache-Aware Reinforcement Learning in Large-Scale Recommender Systems",
        "meta": {
            "query": "open challenges in optimizing cache performance for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1807.00207v1": {
        "url": "http://arxiv.org/abs/1807.00207v1",
        "description": "Caching networks are designed to reduce traffic load at backhaul links, by\nserving demands from edge-nodes. In the past decades, many studies have been\ndone to address the caching problem. However, in practice, finding an optimal\ncaching policy is still challenging due to dynamicity of traffic and\nscalability caused by complex impact of caching strategy chosen by each\nindividual cache on other parts of network. In this paper, we focus on cache\nplacement to optimize the performance metrics such as hit ratio in cooperative\nlarge-scale caching networks. Our proposed solution, cooperative multi-agent\nbased cache placement (CoM-Cache) is based on multi-agent reinforcement\nlearning framework and can seamlessly track the content popularity dynamics in\nan on-line fashion. CoM-Cache is enable to solve the problems over a spectrum\nfrom isolated to interconnected caches and is designed flexibly to fit any\ncaching networks. To deal with dimensionality issue, CoM-Cache exploits the\nproperty of locality of interactions among caches. The experimental results\nreport CoM-Cache outperforms base-line schemes, however at the expense of\nreasonable additional complexity.",
        "snippets": [
            "Caching networks are designed to reduce traffic load at backhaul links, by\nserving demands from edge-nodes. In the past decades, many studies have been\ndone to address the caching problem. However, in practice, finding an optimal\ncaching policy is still challenging due to dynamicity of traffic and\nscalability caused by complex impact of caching strategy chosen by each\nindividual cache on other parts of network. In this paper, we focus on cache\nplacement to optimize the performance metrics such as hit ratio in cooperative\nlarge-scale caching networks. Our proposed solution, cooperative multi-agent\nbased cache placement (CoM-Cache) is based on multi-agent reinforcement\nlearning framework and can seamlessly track the content popularity dynamics in\nan on-line fashion. CoM-Cache is enable to solve the problems over a spectrum\nfrom isolated to interconnected caches and is designed flexibly to fit any\ncaching networks. To deal with dimensionality issue, CoM-Cache exploits the\nproperty of locality of interactions among caches. The experimental results\nreport CoM-Cache outperforms base-line schemes, however at the expense of\nreasonable additional complexity."
        ],
        "title": "Multi-agent Learning for Cooperative Large-scale Caching Networks",
        "meta": {
            "query": "open challenges in optimizing cache performance for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2411.18191v2": {
        "url": "http://arxiv.org/abs/2411.18191v2",
        "description": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
        "snippets": [
            "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
        ],
        "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel Attacks",
        "meta": {
            "query": "open challenges in optimizing cache performance for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1809.04676v2": {
        "url": "http://arxiv.org/abs/1809.04676v2",
        "description": "Basic block reordering is an important step for profile-guided binary\noptimization. The state-of-the-art goal for basic block reordering is to\nmaximize the number of fall-through branches. However, we demonstrate that such\norderings may impose suboptimal performance on instruction and I-TLB caches. We\npropose a new algorithm that relies on a model combining the effects of\nfall-through and caching behavior. As details of modern processor caching is\nquite complex and often unknown, we show how to use machine learning in\nselecting parameters that best trade off different caching effects to maximize\nbinary performance.\n  An extensive evaluation on a variety of applications, including Facebook\nproduction workloads, the open-source compilers Clang and GCC, and SPEC CPU\nbenchmarks, indicate that the new method outperforms existing block reordering\ntechniques, improving the resulting performance of applications with large code\nsize. We have open sourced the code of the new algorithm as a part of a\npost-link binary optimization tool, BOLT.",
        "snippets": [
            "Basic block reordering is an important step for profile-guided binary\noptimization. The state-of-the-art goal for basic block reordering is to\nmaximize the number of fall-through branches. However, we demonstrate that such\norderings may impose suboptimal performance on instruction and I-TLB caches. We\npropose a new algorithm that relies on a model combining the effects of\nfall-through and caching behavior. As details of modern processor caching is\nquite complex and often unknown, we show how to use machine learning in\nselecting parameters that best trade off different caching effects to maximize\nbinary performance.\n  An extensive evaluation on a variety of applications, including Facebook\nproduction workloads, the open-source compilers Clang and GCC, and SPEC CPU\nbenchmarks, indicate that the new method outperforms existing block reordering\ntechniques, improving the resulting performance of applications with large code\nsize. We have open sourced the code of the new algorithm as a part of a\npost-link binary optimization tool, BOLT."
        ],
        "title": "Improved Basic Block Reordering",
        "meta": {
            "query": "open challenges in optimizing cache performance for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2407.21018v3": {
        "url": "http://arxiv.org/abs/2407.21018v3",
        "description": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK.",
        "snippets": [
            "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK."
        ],
        "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
        "meta": {
            "query": "open challenges in optimizing cache performance for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2306.02003v2": {
        "url": "http://arxiv.org/abs/2306.02003v2",
        "description": "Large Language Models (LLMs) and other large foundation models have achieved\nnoteworthy success, but their size exacerbates existing resource consumption\nand latency challenges. In particular, the large-scale deployment of these\nmodels is hindered by the significant resource requirements during inference.\nIn this paper, we study two approaches for mitigating these challenges:\nemploying a cache to store previous queries and learning a model multiplexer to\nchoose from an ensemble of models for query processing.\n  Theoretically, we provide an optimal algorithm for jointly optimizing both\napproaches to reduce the inference cost in both offline and online tabular\nsettings. By combining a caching algorithm, namely Greedy Dual Size with\nFrequency (GDSF) or Least Expected Cost (LEC), with a model multiplexer, we\nachieve optimal rates in both offline and online settings. Empirically,\nsimulations show that the combination of our caching and model multiplexing\nalgorithms greatly improves over the baselines, with up to $50\\times$\nimprovement over the baseline when the ratio between the maximum cost and\nminimum cost is $100$. Experiments on real datasets show a $4.3\\times$\nimprovement in FLOPs over the baseline when the ratio for FLOPs is $10$, and a\n$1.8\\times$ improvement in latency when the ratio for average latency is\n$1.85$.",
        "snippets": [
            "Large Language Models (LLMs) and other large foundation models have achieved\nnoteworthy success, but their size exacerbates existing resource consumption\nand latency challenges. In particular, the large-scale deployment of these\nmodels is hindered by the significant resource requirements during inference.\nIn this paper, we study two approaches for mitigating these challenges:\nemploying a cache to store previous queries and learning a model multiplexer to\nchoose from an ensemble of models for query processing.\n  Theoretically, we provide an optimal algorithm for jointly optimizing both\napproaches to reduce the inference cost in both offline and online tabular\nsettings. By combining a caching algorithm, namely Greedy Dual Size with\nFrequency (GDSF) or Least Expected Cost (LEC), with a model multiplexer, we\nachieve optimal rates in both offline and online settings. Empirically,\nsimulations show that the combination of our caching and model multiplexing\nalgorithms greatly improves over the baselines, with up to $50\\times$\nimprovement over the baseline when the ratio between the maximum cost and\nminimum cost is $100$. Experiments on real datasets show a $4.3\\times$\nimprovement in FLOPs over the baseline when the ratio for FLOPs is $10$, and a\n$1.8\\times$ improvement in latency when the ratio for average latency is\n$1.85$."
        ],
        "title": "On Optimal Caching and Model Multiplexing for Large Model Inference",
        "meta": {
            "query": "open challenges in optimizing cache performance for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2504.02268v1": {
        "url": "http://arxiv.org/abs/2504.02268v1",
        "description": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations.",
        "snippets": [
            "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations."
        ],
        "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and Synthetic Data",
        "meta": {
            "query": "open challenges in optimizing cache performance for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.07120v1": {
        "url": "http://arxiv.org/abs/2503.07120v1",
        "description": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}.",
        "snippets": [
            "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}."
        ],
        "title": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature Caching",
        "meta": {
            "query": "open challenges in optimizing cache performance for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2307.11069v1": {
        "url": "http://arxiv.org/abs/2307.11069v1",
        "description": "Large scientific collaborations often have multiple scientists accessing the\nsame set of files while doing different analyses, which create repeated\naccesses to the large amounts of shared data located far away. These data\naccesses have long latency due to distance and occupy the limited bandwidth\navailable over the wide-area network. To reduce the wide-area network traffic\nand the data access latency, regional data storage caches have been installed\nas a new networking service. To study the effectiveness of such a cache system\nin scientific applications, we examine the Southern California Petabyte Scale\nCache for a high-energy physics experiment. By examining about 3TB of\noperational logs, we show that this cache removed 67.6% of file requests from\nthe wide-area network and reduced the traffic volume on wide-area network by\n12.3TB (or 35.4%) an average day. The reduction in the traffic volume (35.4%)\nis less than the reduction in file counts (67.6%) because the larger files are\nless likely to be reused. Due to this difference in data access patterns, the\ncache system has implemented a policy to avoid evicting smaller files when\nprocessing larger files. We also build a machine learning model to study the\npredictability of the cache behavior. Tests show that this model is able to\naccurately predict the cache accesses, cache misses, and network throughput,\nmaking the model useful for future studies on resource provisioning and\nplanning.",
        "snippets": [
            "Large scientific collaborations often have multiple scientists accessing the\nsame set of files while doing different analyses, which create repeated\naccesses to the large amounts of shared data located far away. These data\naccesses have long latency due to distance and occupy the limited bandwidth\navailable over the wide-area network. To reduce the wide-area network traffic\nand the data access latency, regional data storage caches have been installed\nas a new networking service. To study the effectiveness of such a cache system\nin scientific applications, we examine the Southern California Petabyte Scale\nCache for a high-energy physics experiment. By examining about 3TB of\noperational logs, we show that this cache removed 67.6% of file requests from\nthe wide-area network and reduced the traffic volume on wide-area network by\n12.3TB (or 35.4%) an average day. The reduction in the traffic volume (35.4%)\nis less than the reduction in file counts (67.6%) because the larger files are\nless likely to be reused. Due to this difference in data access patterns, the\ncache system has implemented a policy to avoid evicting smaller files when\nprocessing larger files. We also build a machine learning model to study the\npredictability of the cache behavior. Tests show that this model is able to\naccurately predict the cache accesses, cache misses, and network throughput,\nmaking the model useful for future studies on resource provisioning and\nplanning."
        ],
        "title": "Effectiveness and predictability of in-network storage cache for scientific workflows",
        "meta": {
            "query": "limitations of workloadaware cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1411.4759v4": {
        "url": "http://arxiv.org/abs/1411.4759v4",
        "description": "In this paper we analyze Least Recently Used (LRU) caches operating under the\nShot Noise requests Model (SNM). The SNM was recently proposed to better\ncapture the main characteristics of today Video on Demand (VoD) traffic. We\ninvestigate the validity of Che's approximation through an asymptotic analysis\nof the cache eviction time. In particular, we provide a large deviation\nprinciple, a law of large numbers and a central limit theorem for the cache\neviction time, as the cache size grows large. Finally, we derive upper and\nlower bounds for the \"hit\" probability in tandem networks of caches under Che's\napproximation.",
        "snippets": [
            "In this paper we analyze Least Recently Used (LRU) caches operating under the\nShot Noise requests Model (SNM). The SNM was recently proposed to better\ncapture the main characteristics of today Video on Demand (VoD) traffic. We\ninvestigate the validity of Che's approximation through an asymptotic analysis\nof the cache eviction time. In particular, we provide a large deviation\nprinciple, a law of large numbers and a central limit theorem for the cache\neviction time, as the cache size grows large. Finally, we derive upper and\nlower bounds for the \"hit\" probability in tandem networks of caches under Che's\napproximation."
        ],
        "title": "Modeling LRU caches with Shot Noise request processes",
        "meta": {
            "query": "limitations of workloadaware cache eviction policies for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2310.14631v1": {
        "url": "http://arxiv.org/abs/2310.14631v1",
        "description": "The ever-growing end user data demands, and the simultaneous reductions in\nmemory costs are fueling edge-caching deployments. Caching at the edge is\nsubstantially different from that at the core and needs to take into account\nthe nature of individual data demands. For example, an individual user may not\nbe interested in requesting the same data item again, if it has recently\nrequested it. Such individual dynamics are not apparent in the aggregated data\nrequests at the core and have not been considered in popularity-driven caching\ndesigns for the core. Hence, these traditional caching policies could induce\nsignificant inefficiencies when applied at the edges. To address this issue, we\ndevelop new edge caching policies optimized for the individual demands that\nalso leverage overhearing opportunities at the wireless edge. With the\nobjective of maximizing the hit ratio, the proposed policies will actively\nevict the data items that are not likely to be requested in the near future,\nand strategically bring them back into the cache through overhearing when they\nare likely to be popular again. Both theoretical analysis and numerical\nsimulations demonstrate that the proposed edge caching policies could\noutperform the popularity-driven policies that are optimal at the core.",
        "snippets": [
            "The ever-growing end user data demands, and the simultaneous reductions in\nmemory costs are fueling edge-caching deployments. Caching at the edge is\nsubstantially different from that at the core and needs to take into account\nthe nature of individual data demands. For example, an individual user may not\nbe interested in requesting the same data item again, if it has recently\nrequested it. Such individual dynamics are not apparent in the aggregated data\nrequests at the core and have not been considered in popularity-driven caching\ndesigns for the core. Hence, these traditional caching policies could induce\nsignificant inefficiencies when applied at the edges. To address this issue, we\ndevelop new edge caching policies optimized for the individual demands that\nalso leverage overhearing opportunities at the wireless edge. With the\nobjective of maximizing the hit ratio, the proposed policies will actively\nevict the data items that are not likely to be requested in the near future,\nand strategically bring them back into the cache through overhearing when they\nare likely to be popular again. Both theoretical analysis and numerical\nsimulations demonstrate that the proposed edge caching policies could\noutperform the popularity-driven policies that are optimal at the core."
        ],
        "title": "Optimal Edge Caching For Individualized Demand Dynamics",
        "meta": {
            "query": "challenges in designing cache eviction policies for dynamic workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2006.08487v1": {
        "url": "http://arxiv.org/abs/2006.08487v1",
        "description": "Last-Level Cache (LLC) represents the bulk of a modern CPU processor's\ntransistor budget and is essential for application performance as LLC enables\nfast access to data in contrast to much slower main memory. However,\napplications with large working set size often exhibit streaming and/or\nthrashing access patterns at LLC. As a result, a large fraction of the LLC\ncapacity is occupied by dead blocks that will not be referenced again, leading\nto inefficient utilization of the LLC capacity. To improve cache efficiency,\nthe state-of-the-art cache management techniques employ prediction mechanisms\nthat learn from the past access patterns with an aim to accurately identify as\nmany dead blocks as possible. Once identified, dead blocks are evicted from LLC\nto make space for potentially high reuse cache blocks.\n  In this thesis, we identify variability in the reuse behavior of cache blocks\nas the key limiting factor in maximizing cache efficiency for state-of-the-art\npredictive techniques. Variability in reuse prediction is inevitable due to\nnumerous factors that are outside the control of LLC. The sources of\nvariability include control-flow variation, speculative execution and\ncontention from cores sharing the cache, among others. Variability in reuse\nprediction challenges existing techniques in reliably identifying the end of a\nblock's useful lifetime, thus causing lower prediction accuracy, coverage, or\nboth. To address this challenge, this thesis aims to design robust cache\nmanagement mechanisms and policies for LLC in the face of variability in reuse\nprediction to minimize cache misses, while keeping the cost and complexity of\nthe hardware implementation low. To that end, we propose two cache management\ntechniques, one domain-agnostic and one domain-specialized, to improve cache\nefficiency by addressing variability in reuse prediction.",
        "snippets": [
            "Last-Level Cache (LLC) represents the bulk of a modern CPU processor's\ntransistor budget and is essential for application performance as LLC enables\nfast access to data in contrast to much slower main memory. However,\napplications with large working set size often exhibit streaming and/or\nthrashing access patterns at LLC. As a result, a large fraction of the LLC\ncapacity is occupied by dead blocks that will not be referenced again, leading\nto inefficient utilization of the LLC capacity. To improve cache efficiency,\nthe state-of-the-art cache management techniques employ prediction mechanisms\nthat learn from the past access patterns with an aim to accurately identify as\nmany dead blocks as possible. Once identified, dead blocks are evicted from LLC\nto make space for potentially high reuse cache blocks.\n  In this thesis, we identify variability in the reuse behavior of cache blocks\nas the key limiting factor in maximizing cache efficiency for state-of-the-art\npredictive techniques. Variability in reuse prediction is inevitable due to\nnumerous factors that are outside the control of LLC. The sources of\nvariability include control-flow variation, speculative execution and\ncontention from cores sharing the cache, among others. Variability in reuse\nprediction challenges existing techniques in reliably identifying the end of a\nblock's useful lifetime, thus causing lower prediction accuracy, coverage, or\nboth. To address this challenge, this thesis aims to design robust cache\nmanagement mechanisms and policies for LLC in the face of variability in reuse\nprediction to minimize cache misses, while keeping the cost and complexity of\nthe hardware implementation low. To that end, we propose two cache management\ntechniques, one domain-agnostic and one domain-specialized, to improve cache\nefficiency by addressing variability in reuse prediction."
        ],
        "title": "Addressing Variability in Reuse Prediction for Last-Level Caches",
        "meta": {
            "query": "challenges in designing cache eviction policies for dynamic workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.14317v2": {
        "url": "http://arxiv.org/abs/2502.14317v2",
        "description": "Extrapolating ultra-long contexts (text length >128K) remains a major\nchallenge for large language models (LLMs), as most training-free extrapolation\nmethods are not only severely limited by memory bottlenecks, but also suffer\nfrom the attention sink, which restricts their scalability and effectiveness in\npractice. In this work, we propose ParallelComp, a parallel long-context\ncompression method that effectively overcomes the memory bottleneck, enabling\n8B-parameter LLMs to extrapolate from 8K to 128K tokens on a single A100 80GB\nGPU in a training-free setting. ParallelComp splits the input into chunks,\ndynamically evicting redundant chunks and irrelevant tokens, supported by a\nparallel KV cache eviction mechanism. Importantly, we present a systematic\ntheoretical and empirical analysis of attention biases in parallel\nattention-including the attention sink, recency bias, and middle bias-and\nreveal that these biases exhibit distinctive patterns under ultra-long context\nsettings. We further design a KV cache eviction technique to mitigate this\nphenomenon. Experimental results show that ParallelComp enables an 8B model\n(trained on 8K context) to achieve 91.17% of GPT-4's performance under\nultra-long contexts, outperforming closed-source models such as Claude-2 and\nKimi-Chat. We achieve a 1.76x improvement in chunk throughput, thereby\nachieving a 23.50x acceleration in the prefill stage with negligible\nperformance loss and pave the way for scalable and robust ultra-long contexts\nextrapolation in LLMs. We release the code at\nhttps://github.com/menik1126/ParallelComp.",
        "snippets": [
            "Extrapolating ultra-long contexts (text length >128K) remains a major\nchallenge for large language models (LLMs), as most training-free extrapolation\nmethods are not only severely limited by memory bottlenecks, but also suffer\nfrom the attention sink, which restricts their scalability and effectiveness in\npractice. In this work, we propose ParallelComp, a parallel long-context\ncompression method that effectively overcomes the memory bottleneck, enabling\n8B-parameter LLMs to extrapolate from 8K to 128K tokens on a single A100 80GB\nGPU in a training-free setting. ParallelComp splits the input into chunks,\ndynamically evicting redundant chunks and irrelevant tokens, supported by a\nparallel KV cache eviction mechanism. Importantly, we present a systematic\ntheoretical and empirical analysis of attention biases in parallel\nattention-including the attention sink, recency bias, and middle bias-and\nreveal that these biases exhibit distinctive patterns under ultra-long context\nsettings. We further design a KV cache eviction technique to mitigate this\nphenomenon. Experimental results show that ParallelComp enables an 8B model\n(trained on 8K context) to achieve 91.17% of GPT-4's performance under\nultra-long contexts, outperforming closed-source models such as Claude-2 and\nKimi-Chat. We achieve a 1.76x improvement in chunk throughput, thereby\nachieving a 23.50x acceleration in the prefill stage with negligible\nperformance loss and pave the way for scalable and robust ultra-long contexts\nextrapolation in LLMs. We release the code at\nhttps://github.com/menik1126/ParallelComp."
        ],
        "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
        "meta": {
            "query": "challenges in designing cache eviction policies for dynamic workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2212.13671v1": {
        "url": "http://arxiv.org/abs/2212.13671v1",
        "description": "When facing objects/files of differing sizes in content delivery networks\n(CDNs) caches, pursuing an optimal object miss ratio (OMR) by approximating\nBelady no longer ensures an optimal byte miss ratio (BMR), creating confusion\nabout how to achieve a superior BMR in CDNs. To address this issue, we\nexperimentally observe that there exists a time window to delay the eviction of\nthe object with the longest reuse distance to improve BMR without increasing\nOMR. As a result, we introduce a deep reinforcement learning (RL) model to\ncapture this time window by dynamically monitoring the changes in OMR and BMR,\nand implementing a BMR-friendly policy in the time window. Based on this\npolicy, we propose a Belady and Size Eviction (LRU-BaSE) algorithm, reducing\nBMR while maintaining OMR. To make LRU-BaSE efficient and practical, we address\nthe feedback delay problem of RL with a two-pronged approach. On the one hand,\nour observation of a rear section of the LRU cache queue containing most of the\neviction candidates allows LRU-BaSE to shorten the decision region. On the\nother hand, the request distribution on CDNs makes it feasible to divide the\nlearning region into multiple sub-regions that are each learned with reduced\ntime and increased accuracy. In real CDN systems, compared to LRU, LRU-BaSE can\nreduce \"backing to OS\" traffic and access latency by 30.05\\% and 17.07\\%,\nrespectively, on average. The results on the simulator confirm that LRU-BaSE\noutperforms the state-of-the-art cache replacement policies, where LRU-BaSE's\nBMR is 0.63\\% and 0.33\\% less than that of Belady and Practical Flow-based\nOffline Optimal (PFOO), respectively, on average. In addition, compared to\nLearning Relaxed Belady (LRB), LRU-BaSE can yield relatively stable performance\nwhen facing workload drift.",
        "snippets": [
            "When facing objects/files of differing sizes in content delivery networks\n(CDNs) caches, pursuing an optimal object miss ratio (OMR) by approximating\nBelady no longer ensures an optimal byte miss ratio (BMR), creating confusion\nabout how to achieve a superior BMR in CDNs. To address this issue, we\nexperimentally observe that there exists a time window to delay the eviction of\nthe object with the longest reuse distance to improve BMR without increasing\nOMR. As a result, we introduce a deep reinforcement learning (RL) model to\ncapture this time window by dynamically monitoring the changes in OMR and BMR,\nand implementing a BMR-friendly policy in the time window. Based on this\npolicy, we propose a Belady and Size Eviction (LRU-BaSE) algorithm, reducing\nBMR while maintaining OMR. To make LRU-BaSE efficient and practical, we address\nthe feedback delay problem of RL with a two-pronged approach. On the one hand,\nour observation of a rear section of the LRU cache queue containing most of the\neviction candidates allows LRU-BaSE to shorten the decision region. On the\nother hand, the request distribution on CDNs makes it feasible to divide the\nlearning region into multiple sub-regions that are each learned with reduced\ntime and increased accuracy. In real CDN systems, compared to LRU, LRU-BaSE can\nreduce \"backing to OS\" traffic and access latency by 30.05\\% and 17.07\\%,\nrespectively, on average. The results on the simulator confirm that LRU-BaSE\noutperforms the state-of-the-art cache replacement policies, where LRU-BaSE's\nBMR is 0.63\\% and 0.33\\% less than that of Belady and Practical Flow-based\nOffline Optimal (PFOO), respectively, on average. In addition, compared to\nLearning Relaxed Belady (LRB), LRU-BaSE can yield relatively stable performance\nwhen facing workload drift."
        ],
        "title": "Optimizing Replacement Policies for Content Delivery Network Caching: Beyond Belady to Attain A Seemingly Unattainable Byte Miss Ratio",
        "meta": {
            "query": "challenges in designing cache eviction policies for dynamic workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1909.13839v1": {
        "url": "http://arxiv.org/abs/1909.13839v1",
        "description": "This study investigates the use of reinforcement learning to guide a general\npurpose cache manager decisions. Cache managers directly impact the overall\nperformance of computer systems. They govern decisions about which objects\nshould be cached, the duration they should be cached for, and decides on which\nobjects to evict from the cache if it is full. These three decisions impact\nboth the cache hit rate and size of the storage that is needed to achieve that\ncache hit rate. An optimal cache manager will avoid unnecessary operations,\nmaximise the cache hit rate which results in fewer round trips to a slower\nbackend storage system, and minimise the size of storage needed to achieve a\nhigh hit-rate.\n  This project investigates using reinforcement learning in cache management by\ndesigning three separate agents for each of the cache manager tasks.\nFurthermore, the project investigates two advanced reinforcement learning\narchitectures for multi-decision problems: a single multi-task agent and a\nmulti-agent. We also introduce a framework to simplify the modelling of\ncomputer systems problems as a reinforcement learning task. The framework\nabstracts delayed experiences observations and reward assignment in computer\nsystems while providing a flexible way to scale to multiple agents.\n  Simulation results based on an established database benchmark system show\nthat reinforcement learning agents can achieve a higher cache hit rate over\nheuristic driven algorithms while minimising the needed space. They are also\nable to adapt to a changing workload and dynamically adjust their caching\nstrategy accordingly. The proposed cache manager model is generic and\napplicable to other types of caches, such as file system caches. This project\nis the first, to our knowledge, to model cache manager decisions as a\nmulti-task control problem.",
        "snippets": [
            "This study investigates the use of reinforcement learning to guide a general\npurpose cache manager decisions. Cache managers directly impact the overall\nperformance of computer systems. They govern decisions about which objects\nshould be cached, the duration they should be cached for, and decides on which\nobjects to evict from the cache if it is full. These three decisions impact\nboth the cache hit rate and size of the storage that is needed to achieve that\ncache hit rate. An optimal cache manager will avoid unnecessary operations,\nmaximise the cache hit rate which results in fewer round trips to a slower\nbackend storage system, and minimise the size of storage needed to achieve a\nhigh hit-rate.\n  This project investigates using reinforcement learning in cache management by\ndesigning three separate agents for each of the cache manager tasks.\nFurthermore, the project investigates two advanced reinforcement learning\narchitectures for multi-decision problems: a single multi-task agent and a\nmulti-agent. We also introduce a framework to simplify the modelling of\ncomputer systems problems as a reinforcement learning task. The framework\nabstracts delayed experiences observations and reward assignment in computer\nsystems while providing a flexible way to scale to multiple agents.\n  Simulation results based on an established database benchmark system show\nthat reinforcement learning agents can achieve a higher cache hit rate over\nheuristic driven algorithms while minimising the needed space. They are also\nable to adapt to a changing workload and dynamically adjust their caching\nstrategy accordingly. The proposed cache manager model is generic and\napplicable to other types of caches, such as file system caches. This project\nis the first, to our knowledge, to model cache manager decisions as a\nmulti-task control problem."
        ],
        "title": "RLCache: Automated Cache Management Using Reinforcement Learning",
        "meta": {
            "query": "challenges in designing cache eviction policies for dynamic workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2209.14673v1": {
        "url": "http://arxiv.org/abs/2209.14673v1",
        "description": "Randomized, skewed caches (RSCs) such as CEASER-S have recently received much\nattention to defend against contention-based cache side channels. By\nrandomizing and regularly changing the mapping(s) of addresses to cache sets,\nthese techniques are designed to obfuscate the leakage of memory access\npatterns. However, new attack techniques, e.g., Prime+Prune+Probe, soon\ndemonstrated the limits of RSCs as they allow attackers to more quickly learn\nwhich addresses contend in the cache and use this information to circumvent the\nrandomization. To yet maintain side-channel resilience, RSCs must change the\nrandom mapping(s) more frequently with adverse effects on performance and\nimplementation complexity. This work aims to make randomization-based\napproaches more robust to allow for reduced re-keying rates and presents\nChameleon Cache. Chameleon Cache extends RSCs with a victim cache (VC) to\ndecouple contention in the RSC from evictions observed by the user. The VC\nallows Chameleon Cache to make additional use of the multiple mappings RSCs\nprovide to translate addresses to cache set indices: when a cache line is\nevicted from the RSC to the VC under one of its mappings, the VC automatically\nreinserts this evicted line back into the RSC by using a different mapping. As\na result, the effects of previous RSC set contention are hidden and Chameleon\nCache exhibits side-channel resistance and eviction patterns similar to fully\nassociative caches with random replacement. We show that Chameleon Cache has\nperformance overheads of < 1% and stress that VCs are more generically helpful\nto increase side-channel resistance and re-keying intervals of randomized\ncaches.",
        "snippets": [
            "Randomized, skewed caches (RSCs) such as CEASER-S have recently received much\nattention to defend against contention-based cache side channels. By\nrandomizing and regularly changing the mapping(s) of addresses to cache sets,\nthese techniques are designed to obfuscate the leakage of memory access\npatterns. However, new attack techniques, e.g., Prime+Prune+Probe, soon\ndemonstrated the limits of RSCs as they allow attackers to more quickly learn\nwhich addresses contend in the cache and use this information to circumvent the\nrandomization. To yet maintain side-channel resilience, RSCs must change the\nrandom mapping(s) more frequently with adverse effects on performance and\nimplementation complexity. This work aims to make randomization-based\napproaches more robust to allow for reduced re-keying rates and presents\nChameleon Cache. Chameleon Cache extends RSCs with a victim cache (VC) to\ndecouple contention in the RSC from evictions observed by the user. The VC\nallows Chameleon Cache to make additional use of the multiple mappings RSCs\nprovide to translate addresses to cache set indices: when a cache line is\nevicted from the RSC to the VC under one of its mappings, the VC automatically\nreinserts this evicted line back into the RSC by using a different mapping. As\na result, the effects of previous RSC set contention are hidden and Chameleon\nCache exhibits side-channel resistance and eviction patterns similar to fully\nassociative caches with random replacement. We show that Chameleon Cache has\nperformance overheads of < 1% and stress that VCs are more generically helpful\nto increase side-channel resistance and re-keying intervals of randomized\ncaches."
        ],
        "title": "Chameleon Cache: Approximating Fully Associative Caches with Random Replacement to Prevent Contention-Based Cache Attacks",
        "meta": {
            "query": "challenges in designing cache eviction policies for dynamic workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2504.11208v2": {
        "url": "http://arxiv.org/abs/2504.11208v2",
        "description": "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively.",
        "snippets": [
            "An essential step for mounting cache attacks is finding eviction sets,\ncollections of memory locations that contend on cache space. On Intel\nprocessors, one of the main challenges for identifying contending addresses is\nthe sliced cache design, where the processor hashes the physical address to\ndetermine where in the cache a memory location is stored. While past works have\ndemonstrated that the hash function can be reversed, they also showed that it\ndepends on physical address bits that the adversary does not know.\n  In this work, we make three main contributions to the art of finding eviction\nsets. We first exploit microarchitectural races to compare memory access times\nand identify the cache slice to which an address maps. We then use the known\nhash function to both reduce the error rate in our slice identification method\nand to reduce the work by extrapolating slice mappings to untested memory\naddresses. Finally, we show how to propagate information on eviction sets\nacross different page offsets for the hitherto unexplored case of non-linear\nhash functions.\n  Our contributions allow for entire LLC eviction set generation in 0.7 seconds\non the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear\nfunctions. This represents a significant improvement compared to\nstate-of-the-art techniques taking 9x and 10x longer, respectively."
        ],
        "title": "Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink of an Eye",
        "meta": {
            "query": "challenges in designing cache eviction policies for dynamic workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2109.03021v1": {
        "url": "http://arxiv.org/abs/2109.03021v1",
        "description": "Software caches optimize the performance of diverse storage systems,\ndatabases and other software systems. Existing works on software caches\nautomatically resort to fully associative cache designs. Our work shows that\nlimited associativity caches are a promising direction for concurrent software\ncaches. Specifically, we demonstrate that limited associativity enables simple\nyet efficient realizations of multiple cache management schemes that can be\ntrivially parallelized. We show that the obtained hit ratio is usually similar\nto fully associative caches of the same management policy, but the throughput\nis improved by up to X5 compared to production-grade caching libraries,\nespecially in multi-threaded executions.",
        "snippets": [
            "Software caches optimize the performance of diverse storage systems,\ndatabases and other software systems. Existing works on software caches\nautomatically resort to fully associative cache designs. Our work shows that\nlimited associativity caches are a promising direction for concurrent software\ncaches. Specifically, we demonstrate that limited associativity enables simple\nyet efficient realizations of multiple cache management schemes that can be\ntrivially parallelized. We show that the obtained hit ratio is usually similar\nto fully associative caches of the same management policy, but the throughput\nis improved by up to X5 compared to production-grade caching libraries,\nespecially in multi-threaded executions."
        ],
        "title": "Limited Associativity Makes Concurrent Software Caches a Breeze",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1311.1667v1": {
        "url": "http://arxiv.org/abs/1311.1667v1",
        "description": "3D integration has the potential to improve the scalability and performance\nof Chip Multiprocessors (CMP). A closed form analytical solution for optimizing\n3D CMP cache hierarchy is developed. It allows optimal partitioning of the\ncache hierarchy levels into 3D silicon layers and optimal allocation of area\namong cache hierarchy levels under constrained area and power budgets. The\noptimization framework is extended by incorporating the impact of multithreaded\ndata sharing on the private cache miss rate. An analytical model for cache\naccess time as a function of cache size and a number of 3D partitions is\nproposed and verified using CACTI simulation.",
        "snippets": [
            "3D integration has the potential to improve the scalability and performance\nof Chip Multiprocessors (CMP). A closed form analytical solution for optimizing\n3D CMP cache hierarchy is developed. It allows optimal partitioning of the\ncache hierarchy levels into 3D silicon layers and optimal allocation of area\namong cache hierarchy levels under constrained area and power budgets. The\noptimization framework is extended by incorporating the impact of multithreaded\ndata sharing on the private cache miss rate. An analytical model for cache\naccess time as a function of cache size and a number of 3D partitions is\nproposed and verified using CACTI simulation."
        ],
        "title": "3D Cache Hierarchy Optimization",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1810.07503v1": {
        "url": "http://arxiv.org/abs/1810.07503v1",
        "description": "Recently, physical layer (PHY) caching has been proposed to exploit the\ndynamic side information induced by caches at base stations (BSs) to support\nCoordinated Multi-Point (CoMP) and achieve huge degrees of freedom (DoF) gains.\nDue to the limited cache storage capacity, the performance of PHY caching\ndepends heavily on the cache content placement algorithm. In existing\nalgorithms, the cache content placement is adaptive to the long-term popularity\ndistribution in an offline manner. We propose an online PHY caching framework\nwhich adapts the cache content placement to microscopic spatial and temporary\npopularity variations to fully exploit the benefits of PHY caching.\nSpecifically, the joint optimization of online cache content placement and\ncontent delivery is formulated as a mixed-timescale drift minimization problem\nto increase the CoMP opportunity and reduce the cache content placement cost.\nWe propose a low-complexity algorithm to obtain a throughput-optimal solution.\nMoreover, we provide a closed-form characterization of the maximum sum DoF in\nthe stability region and study the impact of key system parameters on the\nstability region. Simulations show that the proposed online PHY caching\nframework achieves large gain over existing solutions.",
        "snippets": [
            "Recently, physical layer (PHY) caching has been proposed to exploit the\ndynamic side information induced by caches at base stations (BSs) to support\nCoordinated Multi-Point (CoMP) and achieve huge degrees of freedom (DoF) gains.\nDue to the limited cache storage capacity, the performance of PHY caching\ndepends heavily on the cache content placement algorithm. In existing\nalgorithms, the cache content placement is adaptive to the long-term popularity\ndistribution in an offline manner. We propose an online PHY caching framework\nwhich adapts the cache content placement to microscopic spatial and temporary\npopularity variations to fully exploit the benefits of PHY caching.\nSpecifically, the joint optimization of online cache content placement and\ncontent delivery is formulated as a mixed-timescale drift minimization problem\nto increase the CoMP opportunity and reduce the cache content placement cost.\nWe propose a low-complexity algorithm to obtain a throughput-optimal solution.\nMoreover, we provide a closed-form characterization of the maximum sum DoF in\nthe stability region and study the impact of key system parameters on the\nstability region. Simulations show that the proposed online PHY caching\nframework achieves large gain over existing solutions."
        ],
        "title": "Mixed-Timescale Online PHY Caching for Dual-Mode MIMO Cooperative Networks",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1510.07865v2": {
        "url": "http://arxiv.org/abs/1510.07865v2",
        "description": "In this paper, we devise the optimal caching placement to maximize the\noffloading probability for a two-tier wireless caching system, where the\nhelpers and a part of users have caching ability. The offloading comes from the\nlocal caching, D2D sharing and the helper transmission. In particular, to\nmaximize the offloading probability we reformulate the caching placement\nproblem for users and helpers into a difference of convex (DC) problem which\ncan be effectively solved by DC programming. Moreover, we analyze the two\nextreme cases where there is only help-tier caching network and only user-tier.\nSpecifically, the placement problem for the helper-tier caching network is\nreduced to a convex problem, and can be effectively solved by the classical\nwater-filling method. We notice that users and helpers prefer to cache popular\ncontents under low node density and prefer to cache different contents evenly\nunder high node density. Simulation results indicate a great performance gain\nof the proposed caching placement over existing approaches.",
        "snippets": [
            "In this paper, we devise the optimal caching placement to maximize the\noffloading probability for a two-tier wireless caching system, where the\nhelpers and a part of users have caching ability. The offloading comes from the\nlocal caching, D2D sharing and the helper transmission. In particular, to\nmaximize the offloading probability we reformulate the caching placement\nproblem for users and helpers into a difference of convex (DC) problem which\ncan be effectively solved by DC programming. Moreover, we analyze the two\nextreme cases where there is only help-tier caching network and only user-tier.\nSpecifically, the placement problem for the helper-tier caching network is\nreduced to a convex problem, and can be effectively solved by the classical\nwater-filling method. We notice that users and helpers prefer to cache popular\ncontents under low node density and prefer to cache different contents evenly\nunder high node density. Simulation results indicate a great performance gain\nof the proposed caching placement over existing approaches."
        ],
        "title": "Optimal Caching Placement for D2D Assisted Wireless Caching Networks",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1605.09519v2": {
        "url": "http://arxiv.org/abs/1605.09519v2",
        "description": "This paper investigates optimal caching placement for wireless femto-caching\nnetwork. The average bit error rate (BER) is formulated as a function of\ncaching placement under wireless fading. To minimize the average BER, we\npropose a greedy algorithm finding optimal caching placement with low\ncomputational complexity. Exploiting the property of the optimal caching\nplacement which we derive, the proposed algorithm can be performed over\nconsiderably reduced search space. Contrary to the optimal caching placement\nwithout consideration of wireless fading aspects, we reveal that optimal\ncaching placement can be reached by balancing a tradeoff between two different\ngains: file diversity gain and channel diversity gain. Moreover, we also\nidentify the conditions that the optimal placement can be found without running\nthe proposed greedy algorithm and derive the corresponding optimal caching\nplacement in closed form.",
        "snippets": [
            "This paper investigates optimal caching placement for wireless femto-caching\nnetwork. The average bit error rate (BER) is formulated as a function of\ncaching placement under wireless fading. To minimize the average BER, we\npropose a greedy algorithm finding optimal caching placement with low\ncomputational complexity. Exploiting the property of the optimal caching\nplacement which we derive, the proposed algorithm can be performed over\nconsiderably reduced search space. Contrary to the optimal caching placement\nwithout consideration of wireless fading aspects, we reveal that optimal\ncaching placement can be reached by balancing a tradeoff between two different\ngains: file diversity gain and channel diversity gain. Moreover, we also\nidentify the conditions that the optimal placement can be found without running\nthe proposed greedy algorithm and derive the corresponding optimal caching\nplacement in closed form."
        ],
        "title": "Optimal caching placement for wireless femto-caching network",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1705.05590v3": {
        "url": "http://arxiv.org/abs/1705.05590v3",
        "description": "Edge-caching has received much attention as an efficient technique to reduce\ndelivery latency and network congestion during peak-traffic times by bringing\ndata closer to end users. Existing works usually design caching algorithms\nseparately from physical layer design. In this paper, we analyse edge-caching\nwireless networks by taking into account the caching capability when designing\nthe signal transmission. Particularly, we investigate multi-layer caching where\nboth base station (BS) and users are capable of storing content data in their\nlocal cache and analyse the performance of edge-caching wireless networks under\ntwo notable uncoded and coded caching strategies. Firstly, we propose a coded\ncaching strategy that is applied to arbitrary values of cache size. The\nrequired backhaul and access rates are derived as a function of the BS and user\ncache size. Secondly, closed-form expressions for the system energy efficiency\n(EE) corresponding to the two caching methods are derived. Based on the derived\nformulas, the system EE is maximized via precoding vectors design and\noptimization while satisfying a predefined user request rate. Thirdly, two\noptimization problems are proposed to minimize the content delivery time for\nthe two caching strategies. Finally, numerical results are presented to verify\nthe effectiveness of the two caching methods.",
        "snippets": [
            "Edge-caching has received much attention as an efficient technique to reduce\ndelivery latency and network congestion during peak-traffic times by bringing\ndata closer to end users. Existing works usually design caching algorithms\nseparately from physical layer design. In this paper, we analyse edge-caching\nwireless networks by taking into account the caching capability when designing\nthe signal transmission. Particularly, we investigate multi-layer caching where\nboth base station (BS) and users are capable of storing content data in their\nlocal cache and analyse the performance of edge-caching wireless networks under\ntwo notable uncoded and coded caching strategies. Firstly, we propose a coded\ncaching strategy that is applied to arbitrary values of cache size. The\nrequired backhaul and access rates are derived as a function of the BS and user\ncache size. Secondly, closed-form expressions for the system energy efficiency\n(EE) corresponding to the two caching methods are derived. Based on the derived\nformulas, the system EE is maximized via precoding vectors design and\noptimization while satisfying a predefined user request rate. Thirdly, two\noptimization problems are proposed to minimize the content delivery time for\nthe two caching strategies. Finally, numerical results are presented to verify\nthe effectiveness of the two caching methods."
        ],
        "title": "Edge-Caching Wireless Networks: Performance Analysis and Optimization",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1609.09682v1": {
        "url": "http://arxiv.org/abs/1609.09682v1",
        "description": "Caching popular content at the edge of future mobile networks has been widely\nconsidered in order to alleviate the impact of the data tsunami on both the\naccess and backhaul networks. A number of interesting techniques have been\nproposed, including femto-caching and \"delayed\" or opportunistic cache access.\nNevertheless, the majority of these approaches suffer from the rather limited\nstorage capacity of the edge caches, compared to the tremendous and rapidly\nincreasing size of the Internet content catalog. We propose to depart from the\nassumption of hard cache misses, common in most existing works, and consider\n\"soft\" cache misses, where if the original content is not available, an\nalternative content that is locally cached can be recommended. Given that\nInternet content consumption is increasingly entertainment-oriented, we believe\nthat a related content could often lead to complete or at least partial user\nsatisfaction, without the need to retrieve the original content over expensive\nlinks. In this paper, we formulate the problem of optimal edge caching with\nsoft cache hits, in the context of delayed access, and analyze the expected\ngains. We then show using synthetic and real datasets of related video contents\nthat promising caching gains could be achieved in practice.",
        "snippets": [
            "Caching popular content at the edge of future mobile networks has been widely\nconsidered in order to alleviate the impact of the data tsunami on both the\naccess and backhaul networks. A number of interesting techniques have been\nproposed, including femto-caching and \"delayed\" or opportunistic cache access.\nNevertheless, the majority of these approaches suffer from the rather limited\nstorage capacity of the edge caches, compared to the tremendous and rapidly\nincreasing size of the Internet content catalog. We propose to depart from the\nassumption of hard cache misses, common in most existing works, and consider\n\"soft\" cache misses, where if the original content is not available, an\nalternative content that is locally cached can be recommended. Given that\nInternet content consumption is increasingly entertainment-oriented, we believe\nthat a related content could often lead to complete or at least partial user\nsatisfaction, without the need to retrieve the original content over expensive\nlinks. In this paper, we formulate the problem of optimal edge caching with\nsoft cache hits, in the context of delayed access, and analyze the expected\ngains. We then show using synthetic and real datasets of related video contents\nthat promising caching gains could be achieved in practice."
        ],
        "title": "Soft Cache Hits and the Impact of Alternative Content Recommendations on Mobile Edge Caching",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1511.03961v2": {
        "url": "http://arxiv.org/abs/1511.03961v2",
        "description": "Building on the recent coded-caching breakthrough by Maddah-Ali and Niesen,\nthe work here considers the $K$-user cache-aided wireless multi-antenna (MISO)\nsymmetric broadcast channel (BC) with random fading and imperfect feedback, and\nanalyzes the throughput performance as a function of feedback statistics and\ncache size. In this setting, our work identifies the optimal cache-aided\ndegrees-of-freedom (DoF) within a factor of 4, by identifying near-optimal\nschemes that exploit the new synergy between coded caching and delayed CSIT, as\nwell as by exploiting the unexplored interplay between caching and\nfeedback-quality. The derived limits interestingly reveal that --- the\ncombination of imperfect quality current CSIT, delayed CSIT, and coded caching,\nguarantees that --- the DoF gains have an initial offset defined by the quality\nof current CSIT, and then that the additional gains attributed to coded caching\nare exponential, in the sense that any linear decrease in the required DoF\nperformance, allows for an exponential reduction in the required cache size.",
        "snippets": [
            "Building on the recent coded-caching breakthrough by Maddah-Ali and Niesen,\nthe work here considers the $K$-user cache-aided wireless multi-antenna (MISO)\nsymmetric broadcast channel (BC) with random fading and imperfect feedback, and\nanalyzes the throughput performance as a function of feedback statistics and\ncache size. In this setting, our work identifies the optimal cache-aided\ndegrees-of-freedom (DoF) within a factor of 4, by identifying near-optimal\nschemes that exploit the new synergy between coded caching and delayed CSIT, as\nwell as by exploiting the unexplored interplay between caching and\nfeedback-quality. The derived limits interestingly reveal that --- the\ncombination of imperfect quality current CSIT, delayed CSIT, and coded caching,\nguarantees that --- the DoF gains have an initial offset defined by the quality\nof current CSIT, and then that the additional gains attributed to coded caching\nare exponential, in the sense that any linear decrease in the required DoF\nperformance, allows for an exponential reduction in the required cache size."
        ],
        "title": "Fundamental Limits of Cache-Aided Wireless BC: Interplay of Coded-Caching and CSIT Feedback",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2310.07243v1": {
        "url": "http://arxiv.org/abs/2310.07243v1",
        "description": "Caching is crucial for enabling high-throughput networks for data intensive\napplications. Traditional caching technology relies on DRAM, as it can transfer\ndata at a high rate. However, DRAM capacity is subject to contention by most\nsystem components and thus is very limited, implying that DRAM-only caches\ncannot scale to meet growing demand. Fortunately, persistent memory and flash\nstorage technologies are rapidly evolving and can be utilized alongside DRAM to\nincrease cache capacities. To do so without compromising network performance\nrequires caching techniques adapted to the characteristics of these\ntechnologies. In this paper, we model the cache as a collection of storage\nblocks with different rate parameters and utilization costs. We introduce an\noptimization technique based on the drift-plus-penalty method and apply it in a\nframework which enables joint caching and forwarding. We show that it achieves\nan optimal trade-off between throughput and cache utilization costs in a\nvirtual control plane. We then develop a corresponding practical policy in the\ndata plane. Finally, through simulations in several settings, we demonstrate\nthe superior performance of our proposed approach with respect to total user\ndelay and cache utilization costs.",
        "snippets": [
            "Caching is crucial for enabling high-throughput networks for data intensive\napplications. Traditional caching technology relies on DRAM, as it can transfer\ndata at a high rate. However, DRAM capacity is subject to contention by most\nsystem components and thus is very limited, implying that DRAM-only caches\ncannot scale to meet growing demand. Fortunately, persistent memory and flash\nstorage technologies are rapidly evolving and can be utilized alongside DRAM to\nincrease cache capacities. To do so without compromising network performance\nrequires caching techniques adapted to the characteristics of these\ntechnologies. In this paper, we model the cache as a collection of storage\nblocks with different rate parameters and utilization costs. We introduce an\noptimization technique based on the drift-plus-penalty method and apply it in a\nframework which enables joint caching and forwarding. We show that it achieves\nan optimal trade-off between throughput and cache utilization costs in a\nvirtual control plane. We then develop a corresponding practical policy in the\ndata plane. Finally, through simulations in several settings, we demonstrate\nthe superior performance of our proposed approach with respect to total user\ndelay and cache utilization costs."
        ],
        "title": "Cost-aware Joint Caching and Forwarding in Networks with Heterogeneous Cache Resources",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1902.07014v1": {
        "url": "http://arxiv.org/abs/1902.07014v1",
        "description": "Edge caching is being explored as a promising technology to alleviate the\nnetwork burden of cellular networks by separating the computing functionalities\naway from cellular base stations. However, the service capability of existing\ncaching scheme is limited by fixed edge infrastructure when facing the\nuncertainties of users' requests and locations. The vehicular caching, which\nuses the moving vehicles as cache carriers, is regard as an efficient method to\nsolve the problem above. This paper studies the effectiveness of vehicular\ncaching scheme in content centric networks by developing optimization model\ntowards the minimization of network energy consumption. Particularly, we model\nthe interactions between caching vehicles and mobile users as a 2-D Markov\nprocess, in order to characterize the network availability of mobile users.\nBased on the developed model, we propose an online vehicular caching design by\noptimizing network energy efficiency. Specifically, the problem of caching\ndecision making is firstly formulated as a fractional optimization model,\ntowards the optimal energy efficiency. Using nonlinear fractional programming\ntechnology and Lyapunov optimization theory, we derive the theoretical solution\nfor the optimization model. An online caching algorithm to enable the optimal\nvehicular caching is developed based on the solution. Finally, extensive\nsimulations are conducted to examine the performance of our proposal. By\ncomparison, our online caching scheme outperforms the existing scheme in terms\nof energy efficiency, hit ratio, cache utilization, and system gain.",
        "snippets": [
            "Edge caching is being explored as a promising technology to alleviate the\nnetwork burden of cellular networks by separating the computing functionalities\naway from cellular base stations. However, the service capability of existing\ncaching scheme is limited by fixed edge infrastructure when facing the\nuncertainties of users' requests and locations. The vehicular caching, which\nuses the moving vehicles as cache carriers, is regard as an efficient method to\nsolve the problem above. This paper studies the effectiveness of vehicular\ncaching scheme in content centric networks by developing optimization model\ntowards the minimization of network energy consumption. Particularly, we model\nthe interactions between caching vehicles and mobile users as a 2-D Markov\nprocess, in order to characterize the network availability of mobile users.\nBased on the developed model, we propose an online vehicular caching design by\noptimizing network energy efficiency. Specifically, the problem of caching\ndecision making is firstly formulated as a fractional optimization model,\ntowards the optimal energy efficiency. Using nonlinear fractional programming\ntechnology and Lyapunov optimization theory, we derive the theoretical solution\nfor the optimization model. An online caching algorithm to enable the optimal\nvehicular caching is developed based on the solution. Finally, extensive\nsimulations are conducted to examine the performance of our proposal. By\ncomparison, our online caching scheme outperforms the existing scheme in terms\nof energy efficiency, hit ratio, cache utilization, and system gain."
        ],
        "title": "A Mobility-Aware Vehicular Caching Scheme in Content Centric Networks: Model and Optimization",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1810.01172v1": {
        "url": "http://arxiv.org/abs/1810.01172v1",
        "description": "Harnessing information about the user mobility pattern and daily demand can\nenhance the network capability to improve the quality of experience (QoE) at\nVehicular Ad-Hoc Networks (VANETs). Proactive caching, as one of the key\nfeatures offered by 5G networks, has lately received much interest. However,\nmore research is still needed to convey large-sized multimedia content\nincluding video, audio and pictures to the high speed moving vehicles. In this\npaper, we study the gains achieved by proactive caching in Roadside Units\n(RSUs) where we take into consideration the effect of the vehicle velocity on\nthe optimal caching decision. Information about the user demand and mobility is\nharnessed to cache some files in RSUs, which will communicate with vehicles\ntraversing along the visited roads before the actual demand. Our main objective\nis to minimize the total network latency. Towards this objective, we formulate\ntwo optimization problems for non-cooperative and cooperative caching schemes\nto find the optimal caching policy to decide which files to be cached by the\nRSUs. Due to the complexity of these problems, we propose a sub-optimal caching\npolicy for each scheme. We compare the performance of the optimal caching\npolicy to that of the sub-optimal caching policy. Numerical results show that\nproactive caching has a significant performance gain when compared to the\nbaseline reactive scenario. Moreover, results reveal that the cooperative\ncaching scheme is more efficient than the non-cooperative scheme.",
        "snippets": [
            "Harnessing information about the user mobility pattern and daily demand can\nenhance the network capability to improve the quality of experience (QoE) at\nVehicular Ad-Hoc Networks (VANETs). Proactive caching, as one of the key\nfeatures offered by 5G networks, has lately received much interest. However,\nmore research is still needed to convey large-sized multimedia content\nincluding video, audio and pictures to the high speed moving vehicles. In this\npaper, we study the gains achieved by proactive caching in Roadside Units\n(RSUs) where we take into consideration the effect of the vehicle velocity on\nthe optimal caching decision. Information about the user demand and mobility is\nharnessed to cache some files in RSUs, which will communicate with vehicles\ntraversing along the visited roads before the actual demand. Our main objective\nis to minimize the total network latency. Towards this objective, we formulate\ntwo optimization problems for non-cooperative and cooperative caching schemes\nto find the optimal caching policy to decide which files to be cached by the\nRSUs. Due to the complexity of these problems, we propose a sub-optimal caching\npolicy for each scheme. We compare the performance of the optimal caching\npolicy to that of the sub-optimal caching policy. Numerical results show that\nproactive caching has a significant performance gain when compared to the\nbaseline reactive scenario. Moreover, results reveal that the cooperative\ncaching scheme is more efficient than the non-cooperative scheme."
        ],
        "title": "Towards Mobility-Aware Proactive Caching for Vehicular Ad hoc Networks",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1910.08291v1": {
        "url": "http://arxiv.org/abs/1910.08291v1",
        "description": "In device-to-device (D2D)-enabled caching cellular networks, the user\nterminals (UTs) collaboratively store and share a large volume of popular\ncontents from the base station (BS) for traffic offloading and delivery delay\nreduction. In this article, the multi-winner auction based caching placement in\nD2D-enabled caching cellular networks is investigated for UT edge caching\nincentive and content caching redundancy reduction. Firstly, a multi-winner\nonce auction for UT edge caching is modeled which auctions multiple contents\nfor multiple UTs. Then the optimization problem for content caching revenue\nmaximization is formulated. Specifically, the \"cache conflict\" restriction\nrelationship among UTs is used as one of the constraints in the problem to\nreduce the content caching redundancy in a UT movement scenario. The problem is\nsolved by semidefinite programming (SDP) relaxation to obtain an approximate\noptimal caching placement. Moreover, the payment strategy of the auction is\ndeveloped as a Nash bargaining game for personal profit fairness among the UTs\nwho win the auction for content caching. Subsequently, a multi-winner once\nauction based caching (MOAC) placement algorithm is proposed. In addition, due\nto the high complexity of MOAC, we further propose a heuristic multi-winner\nrepeated auction based caching placement (MRAC) algorithm, which can greatly\nreduce the complexity with only tiny performance loss. Simulation results show\nthat the proposed algorithms can reduce the traffic load and average content\naccess delay effectively compared with the existing caching placement\nalgorithms.",
        "snippets": [
            "In device-to-device (D2D)-enabled caching cellular networks, the user\nterminals (UTs) collaboratively store and share a large volume of popular\ncontents from the base station (BS) for traffic offloading and delivery delay\nreduction. In this article, the multi-winner auction based caching placement in\nD2D-enabled caching cellular networks is investigated for UT edge caching\nincentive and content caching redundancy reduction. Firstly, a multi-winner\nonce auction for UT edge caching is modeled which auctions multiple contents\nfor multiple UTs. Then the optimization problem for content caching revenue\nmaximization is formulated. Specifically, the \"cache conflict\" restriction\nrelationship among UTs is used as one of the constraints in the problem to\nreduce the content caching redundancy in a UT movement scenario. The problem is\nsolved by semidefinite programming (SDP) relaxation to obtain an approximate\noptimal caching placement. Moreover, the payment strategy of the auction is\ndeveloped as a Nash bargaining game for personal profit fairness among the UTs\nwho win the auction for content caching. Subsequently, a multi-winner once\nauction based caching (MOAC) placement algorithm is proposed. In addition, due\nto the high complexity of MOAC, we further propose a heuristic multi-winner\nrepeated auction based caching placement (MRAC) algorithm, which can greatly\nreduce the complexity with only tiny performance loss. Simulation results show\nthat the proposed algorithms can reduce the traffic load and average content\naccess delay effectively compared with the existing caching placement\nalgorithms."
        ],
        "title": "D2D-Enabled Mobile User Edge Caching: A Multi-Winner Auction Approach",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1908.06595v1": {
        "url": "http://arxiv.org/abs/1908.06595v1",
        "description": "In traditional cache-enabled small-cell networks (SCNs), a user can suffer\nstrong interference due to contentcentric base station association. This may\ndegenerate the advantage of collaborative content caching among multiple small\nbase stations (SBSs), including probabilistic caching and coded caching. In\nthis work, we tackle this issue by deploying multiple antennas at each SBS for\ninterference management. Two types of beamforming are considered. One is\nmatched-filter (MF) to strengthen the effective channel gain of the desired\nsignal, and the other is zero-forcing (ZF) to cancel interference within a\nselected SBS cooperation group. We apply these two beamforming techniques in\nboth probabilistic caching and coded caching, and conduct performance analysis\nusing stochastic geometry. We obtain exact and approximate compact integral\nexpressions of system performances measured by average fractional offloaded\ntraffic (AFOT) and average ergodic spectral efficiency (AESE). Based on these\nexpressions, we then optimize the caching parameters for AFOT or AESE\nmaximization. For probabilistic caching, optimal caching solutions are\nobtained. For coded caching, an efficient greedy-based algorithm is proposed.\nNumerical results show that multiple antennas can boost the advantage of\nprobabilistic caching and coded caching over the traditional most popular\ncaching with the proper use of beamforming.",
        "snippets": [
            "In traditional cache-enabled small-cell networks (SCNs), a user can suffer\nstrong interference due to contentcentric base station association. This may\ndegenerate the advantage of collaborative content caching among multiple small\nbase stations (SBSs), including probabilistic caching and coded caching. In\nthis work, we tackle this issue by deploying multiple antennas at each SBS for\ninterference management. Two types of beamforming are considered. One is\nmatched-filter (MF) to strengthen the effective channel gain of the desired\nsignal, and the other is zero-forcing (ZF) to cancel interference within a\nselected SBS cooperation group. We apply these two beamforming techniques in\nboth probabilistic caching and coded caching, and conduct performance analysis\nusing stochastic geometry. We obtain exact and approximate compact integral\nexpressions of system performances measured by average fractional offloaded\ntraffic (AFOT) and average ergodic spectral efficiency (AESE). Based on these\nexpressions, we then optimize the caching parameters for AFOT or AESE\nmaximization. For probabilistic caching, optimal caching solutions are\nobtained. For coded caching, an efficient greedy-based algorithm is proposed.\nNumerical results show that multiple antennas can boost the advantage of\nprobabilistic caching and coded caching over the traditional most popular\ncaching with the proper use of beamforming."
        ],
        "title": "Modeling, Analysis, and Optimization of Caching in Multi-Antenna Small-Cell Networks",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/0903.4898v1": {
        "url": "http://arxiv.org/abs/0903.4898v1",
        "description": "It is well known that the static caching algorithm that keeps the most\nfrequently requested documents in the cache is optimal in case when documents\nare of the same size and requests are independent and equally distributed.\nHowever, it is hard to develop explicit and provably optimal caching algorithms\nwhen requests are statistically correlated. In this paper, we show that keeping\nthe most frequently requested documents in the cache is still optimal for large\ncache sizes even if the requests are strongly correlated.",
        "snippets": [
            "It is well known that the static caching algorithm that keeps the most\nfrequently requested documents in the cache is optimal in case when documents\nare of the same size and requests are independent and equally distributed.\nHowever, it is hard to develop explicit and provably optimal caching algorithms\nwhen requests are statistically correlated. In this paper, we show that keeping\nthe most frequently requested documents in the cache is still optimal for large\ncache sizes even if the requests are strongly correlated."
        ],
        "title": "Asymptotic Optimality of the Static Frequency Caching in the Presence of Correlated Requests",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1606.05396v1": {
        "url": "http://arxiv.org/abs/1606.05396v1",
        "description": "This work explores coded caching in the symmetric $K$-user cache-aided MISO\nBC with imperfect CSIT-type feedback, for the specific case where the cache\nsize is much smaller than the library size. Building on the recently explored\nsynergy between caching and delayed-CSIT, and building on the tradeoff between\ncaching and CSIT quality, the work proposes new schemes that boost the impact\nof small caches, focusing on the case where the cumulative cache size is\nsmaller than the library size. For this small-cache setting, based on the\nproposed near-optimal schemes, the work identifies the optimal cache-aided\ndegrees-of-freedom (DoF) performance within a factor of 4.",
        "snippets": [
            "This work explores coded caching in the symmetric $K$-user cache-aided MISO\nBC with imperfect CSIT-type feedback, for the specific case where the cache\nsize is much smaller than the library size. Building on the recently explored\nsynergy between caching and delayed-CSIT, and building on the tradeoff between\ncaching and CSIT quality, the work proposes new schemes that boost the impact\nof small caches, focusing on the case where the cumulative cache size is\nsmaller than the library size. For this small-cache setting, based on the\nproposed near-optimal schemes, the work identifies the optimal cache-aided\ndegrees-of-freedom (DoF) performance within a factor of 4."
        ],
        "title": "Feedback-Aided Coded Caching for the MISO BC with Small Caches",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.02349v1": {
        "url": "http://arxiv.org/abs/2502.02349v1",
        "description": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times.",
        "snippets": [
            "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times."
        ],
        "title": "Random Adaptive Cache Placement Policy",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1802.10326v2": {
        "url": "http://arxiv.org/abs/1802.10326v2",
        "description": "In this paper, we consider a hybrid millimeter wave (mmWave) and micro wave\n($\\mu$Wave) network from the perspective of \\emph{wireless caching} and study\nthe optimal probabilistic content/file caching placement at desirable base\nstations (BSs) using a stochastic geometric framework. Considering the average\nsuccess probability (ASP) of file delivery as the performance metric, we derive\nexpressions for the association probability of the typical user to the mmWave\nand $\\mu$Wave networks. Accordingly, we provide an upper bound for the ASP of\nfile delivery and formulate the content caching placement scheme as an\noptimization problem with respect to caching probabilities, that jointly\noptimizes the ASP of file delivery considering both content placement and\ndelivery phases. In particular, we consider the caching placement strategy\nunder both noise-limited and interference-limited environments. We numerically\nevaluate the performance of the proposed caching schemes under essential\nfactors, such as blockages in the mmWave network, cluster radius, BS density,\nand path loss and compare it with uniform caching placement, caching $M$ most\npopular contents, and random caching placement. Numerical results demonstrate\nthe superiority of the proposed caching scheme over others, albeit certain\ntrade-offs.",
        "snippets": [
            "In this paper, we consider a hybrid millimeter wave (mmWave) and micro wave\n($\\mu$Wave) network from the perspective of \\emph{wireless caching} and study\nthe optimal probabilistic content/file caching placement at desirable base\nstations (BSs) using a stochastic geometric framework. Considering the average\nsuccess probability (ASP) of file delivery as the performance metric, we derive\nexpressions for the association probability of the typical user to the mmWave\nand $\\mu$Wave networks. Accordingly, we provide an upper bound for the ASP of\nfile delivery and formulate the content caching placement scheme as an\noptimization problem with respect to caching probabilities, that jointly\noptimizes the ASP of file delivery considering both content placement and\ndelivery phases. In particular, we consider the caching placement strategy\nunder both noise-limited and interference-limited environments. We numerically\nevaluate the performance of the proposed caching schemes under essential\nfactors, such as blockages in the mmWave network, cluster radius, BS density,\nand path loss and compare it with uniform caching placement, caching $M$ most\npopular contents, and random caching placement. Numerical results demonstrate\nthe superiority of the proposed caching scheme over others, albeit certain\ntrade-offs."
        ],
        "title": "An Analysis on Caching Placement for Millimeter/Micro Wave Hybrid Networks",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1602.03635v2": {
        "url": "http://arxiv.org/abs/1602.03635v2",
        "description": "It has been recently advocated that in large communication systems it is\nbeneficial both for the users and for the network as a whole to store content\ncloser to users. One particular implementation of such an approach is to\nco-locate caches with wireless base stations. In this paper we study\ngeographically distributed caching of a fixed collection of files. We model\ncache placement with the help of stochastic geometry and optimize the\nallocation of storage capacity among files in order to minimize the cache miss\nprobability. We consider both per cache capacity constraints as well as an\naverage capacity constraint over all caches. The case of per cache capacity\nconstraints can be efficiently solved using dynamic programming, whereas the\ncase of the average constraint leads to a convex optimization problem. We\ndemonstrate that the average constraint leads to significantly smaller cache\nmiss probability. Finally, we suggest a simple LRU-based policy for\ngeographically distributed caching and show that its performance is close to\nthe optimal.",
        "snippets": [
            "It has been recently advocated that in large communication systems it is\nbeneficial both for the users and for the network as a whole to store content\ncloser to users. One particular implementation of such an approach is to\nco-locate caches with wireless base stations. In this paper we study\ngeographically distributed caching of a fixed collection of files. We model\ncache placement with the help of stochastic geometry and optimize the\nallocation of storage capacity among files in order to minimize the cache miss\nprobability. We consider both per cache capacity constraints as well as an\naverage capacity constraint over all caches. The case of per cache capacity\nconstraints can be efficiently solved using dynamic programming, whereas the\ncase of the average constraint leads to a convex optimization problem. We\ndemonstrate that the average constraint leads to significantly smaller cache\nmiss probability. Finally, we suggest a simple LRU-based policy for\ngeographically distributed caching and show that its performance is close to\nthe optimal."
        ],
        "title": "Optimization of Caching Devices with Geometric Constraints",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2209.04164v1": {
        "url": "http://arxiv.org/abs/2209.04164v1",
        "description": "Joint caching and transmission optimization problem is challenging due to the\ndeep coupling between decisions. This paper proposes an iterative distributed\nmulti-agent learning approach to jointly optimize caching and transmission. The\ngoal of this approach is to minimize the total transmission delay of all users.\nIn this iterative approach, each iteration includes caching optimization and\ntransmission optimization. A multi-agent reinforcement learning (MARL)-based\ncaching network is developed to cache popular tasks, such as answering which\nfiles to evict from the cache and which files to storage. Based on the cached\nfiles of the caching network, the transmission network transmits cached files\nfor users by single transmission (ST) or joint transmission (JT) with\nmulti-agent Bayesian learning automaton (MABLA) method. And then users access\nthe edge servers with the minimum transmission delay. The experimental results\ndemonstrate the performance of the proposed multi-agent learning approach.",
        "snippets": [
            "Joint caching and transmission optimization problem is challenging due to the\ndeep coupling between decisions. This paper proposes an iterative distributed\nmulti-agent learning approach to jointly optimize caching and transmission. The\ngoal of this approach is to minimize the total transmission delay of all users.\nIn this iterative approach, each iteration includes caching optimization and\ntransmission optimization. A multi-agent reinforcement learning (MARL)-based\ncaching network is developed to cache popular tasks, such as answering which\nfiles to evict from the cache and which files to storage. Based on the cached\nfiles of the caching network, the transmission network transmits cached files\nfor users by single transmission (ST) or joint transmission (JT) with\nmulti-agent Bayesian learning automaton (MABLA) method. And then users access\nthe edge servers with the minimum transmission delay. The experimental results\ndemonstrate the performance of the proposed multi-agent learning approach."
        ],
        "title": "Joint Caching and Transmission in the Mobile Edge Network: A Multi-Agent Learning Approach",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1610.09526v3": {
        "url": "http://arxiv.org/abs/1610.09526v3",
        "description": "Existing designs for content dissemination do not fully explore and exploit\npotential caching and computation capabilities in advanced wireless networks.\nIn this paper, we propose two partition-based caching designs, i.e., a coded\ncaching design based on Random Linear Network Coding and an uncoded caching\ndesign. We consider the analysis and optimization of the two caching designs in\na large-scale successive interference cancelation (SIC)-enabled wireless\nnetwork. First, under each caching design, by utilizing tools from stochastic\ngeometry and adopting appropriate approximations, we derive a tractable\nexpression for the successful transmission probability in the general file size\nregime. To further obtain design insights, we also derive closed-form\nexpressions for the successful transmission probability in the small and large\nfile size regimes, respectively. Then, under each caching design, we consider\nthe successful transmission probability maximization in the general file size\nregime, which is an NP-hard problem. By exploring structural properties, we\nsuccessfully transform the original optimization problem into a Multiple-Choice\nKnapsack Problem (MCKP), and obtain a near optimal solution with 1/2\napproximation guarantee and polynomial complexity. We also obtain closed-form\nasymptotically optimal solutions. The analysis and optimization results show\nthe advantage of the coded caching design over the uncoded caching design, and\nreveal the impact of caching and SIC capabilities. Finally, by numerical\nresults, we show that the two proposed caching designs achieve significant\nperformance gains over some baseline caching designs.",
        "snippets": [
            "Existing designs for content dissemination do not fully explore and exploit\npotential caching and computation capabilities in advanced wireless networks.\nIn this paper, we propose two partition-based caching designs, i.e., a coded\ncaching design based on Random Linear Network Coding and an uncoded caching\ndesign. We consider the analysis and optimization of the two caching designs in\na large-scale successive interference cancelation (SIC)-enabled wireless\nnetwork. First, under each caching design, by utilizing tools from stochastic\ngeometry and adopting appropriate approximations, we derive a tractable\nexpression for the successful transmission probability in the general file size\nregime. To further obtain design insights, we also derive closed-form\nexpressions for the successful transmission probability in the small and large\nfile size regimes, respectively. Then, under each caching design, we consider\nthe successful transmission probability maximization in the general file size\nregime, which is an NP-hard problem. By exploring structural properties, we\nsuccessfully transform the original optimization problem into a Multiple-Choice\nKnapsack Problem (MCKP), and obtain a near optimal solution with 1/2\napproximation guarantee and polynomial complexity. We also obtain closed-form\nasymptotically optimal solutions. The analysis and optimization results show\nthe advantage of the coded caching design over the uncoded caching design, and\nreveal the impact of caching and SIC capabilities. Finally, by numerical\nresults, we show that the two proposed caching designs achieve significant\nperformance gains over some baseline caching designs."
        ],
        "title": "Partition-based Caching in Large-Scale SIC-Enabled Wireless Networks",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1709.06278v1": {
        "url": "http://arxiv.org/abs/1709.06278v1",
        "description": "Caching at base stations is a promising technology to satisfy the increasing\ncapacity requirements and reduce the backhaul loads in future wireless\nnetworks. Careful design of random caching can fully exploit the file\npopularity and achieve good performance. However, previous works on random\ncaching scheme usually assumed single antenna at BSs and users, which is not\nthe case in practical multi-antenna networks. In this paper, we consider the\nanalysis and optimization in the cache-enabled multi-antenna networks with\nlimited backhaul. We first derive a closed-form expression and a simple tight\nupper bound of the successful transmission probability, using tools from\nstochastic geometry and a gamma approximation. Based on the analytic results,\nwe then consider the area spectrum efficiency maximization by optimizing design\nparameters, which is a complicated mixed-integer optimization problem. After\nanalyzing the optimal properties, we obtain a local optimal solution with lower\ncomplexity. To further simplify the optimization, we then solve an asymptotic\noptimization problem in the high user density region, using the upper bound as\nthe objective function. Numerical simulations show that the asymptotic optimal\ncaching scheme achieves better performance over existing caching schemes. The\nanalysis and optimization results provide insightful design guidelines for\nrandom caching in practical networks.",
        "snippets": [
            "Caching at base stations is a promising technology to satisfy the increasing\ncapacity requirements and reduce the backhaul loads in future wireless\nnetworks. Careful design of random caching can fully exploit the file\npopularity and achieve good performance. However, previous works on random\ncaching scheme usually assumed single antenna at BSs and users, which is not\nthe case in practical multi-antenna networks. In this paper, we consider the\nanalysis and optimization in the cache-enabled multi-antenna networks with\nlimited backhaul. We first derive a closed-form expression and a simple tight\nupper bound of the successful transmission probability, using tools from\nstochastic geometry and a gamma approximation. Based on the analytic results,\nwe then consider the area spectrum efficiency maximization by optimizing design\nparameters, which is a complicated mixed-integer optimization problem. After\nanalyzing the optimal properties, we obtain a local optimal solution with lower\ncomplexity. To further simplify the optimization, we then solve an asymptotic\noptimization problem in the high user density region, using the upper bound as\nthe objective function. Numerical simulations show that the asymptotic optimal\ncaching scheme achieves better performance over existing caching schemes. The\nanalysis and optimization results provide insightful design guidelines for\nrandom caching in practical networks."
        ],
        "title": "Random Caching in Backhaul-Limited Multi-Antenna Networks: Analysis and Area Spectrum Efficiency Optimization",
        "meta": {
            "query": "workloaddependent cache performance optimization"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1912.01082v5": {
        "url": "http://arxiv.org/abs/1912.01082v5",
        "description": "This paper studies the caching system of multiple cache-enabled users with\nrandom demands. Under nonuniform file popularity, we thoroughly characterize\nthe optimal uncoded cache placement structure for the coded caching scheme\n(CCS). Formulating the cache placement as an optimization problem to minimize\nthe average delivery rate, we identify the file group structure in the optimal\nsolution. We show that, regardless of the file popularity distribution, there\nare \\emph{at most three file groups} in the optimal cache placement{, where\nfiles within a group have the same cache placement}. We further characterize\nthe complete structure of the optimal cache placement and obtain the\nclosed-form solution in each of the three file group structures. A simple\nalgorithm is developed to obtain the final optimal cache placement by comparing\na set of candidate closed-form solutions computed in parallel. We provide\ninsight into the file groups formed by the optimal cache placement. The optimal\nplacement solution also indicates that coding between file groups may be\nexplored during delivery, in contrast to the existing suboptimal file grouping\nschemes. Using the file group structure in the optimal cache placement for the\nCCS, we propose a new information-theoretic converse bound for coded caching\nthat is tighter than the existing best one. Moreover, we characterize the file\nsubpacketization in the CCS with the optimal cache placement solution and show\nthat the maximum subpacketization level in the worst case scales as\n$\\mathcal{O}(2^K/\\sqrt{K})$ for $K$ users.",
        "snippets": [
            "This paper studies the caching system of multiple cache-enabled users with\nrandom demands. Under nonuniform file popularity, we thoroughly characterize\nthe optimal uncoded cache placement structure for the coded caching scheme\n(CCS). Formulating the cache placement as an optimization problem to minimize\nthe average delivery rate, we identify the file group structure in the optimal\nsolution. We show that, regardless of the file popularity distribution, there\nare \\emph{at most three file groups} in the optimal cache placement{, where\nfiles within a group have the same cache placement}. We further characterize\nthe complete structure of the optimal cache placement and obtain the\nclosed-form solution in each of the three file group structures. A simple\nalgorithm is developed to obtain the final optimal cache placement by comparing\na set of candidate closed-form solutions computed in parallel. We provide\ninsight into the file groups formed by the optimal cache placement. The optimal\nplacement solution also indicates that coding between file groups may be\nexplored during delivery, in contrast to the existing suboptimal file grouping\nschemes. Using the file group structure in the optimal cache placement for the\nCCS, we propose a new information-theoretic converse bound for coded caching\nthat is tighter than the existing best one. Moreover, we characterize the file\nsubpacketization in the CCS with the optimal cache placement solution and show\nthat the maximum subpacketization level in the worst case scales as\n$\\mathcal{O}(2^K/\\sqrt{K})$ for $K$ users."
        ],
        "title": "Fundamental Structure of Optimal Cache Placement for Coded Caching with Nonuniform Demands",
        "meta": {
            "query": "LLM caching characterization studies"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1604.08178v1": {
        "url": "http://arxiv.org/abs/1604.08178v1",
        "description": "Centralized coded caching of popular contents is studied for users with\nheterogeneous distortion requirements, corresponding to diverse processing and\ndisplay capabilities of mobile devices. Users' distortion requirements are\nassumed to be fixed and known, while their particular demands are revealed only\nafter the placement phase. Modeling each file in the database as an independent\nand identically distributed Gaussian vector, the minimum delivery rate that can\nsatisfy any demand combination within the corresponding distortion target is\nstudied. The optimal delivery rate is characterized for the special case of two\nusers and two files for any pair of distortion requirements. For the general\nsetting with multiple users and files, a layered caching and delivery scheme,\nwhich exploits the successive refinability of Gaussian sources, is proposed.\nThis scheme caches each content in multiple layers, and it is optimized by\nsolving two subproblems: lossless caching of each layer with heterogeneous\ncache capacities, and allocation of available caches among layers. The delivery\nrate minimization problem for each layer is solved numerically, while two\nschemes, called the proportional cache allocation (PCA) and ordered cache\nallocation (OCA), are proposed for cache allocation. These schemes are compared\nwith each other and the cut-set bound through numerical simulations.",
        "snippets": [
            "Centralized coded caching of popular contents is studied for users with\nheterogeneous distortion requirements, corresponding to diverse processing and\ndisplay capabilities of mobile devices. Users' distortion requirements are\nassumed to be fixed and known, while their particular demands are revealed only\nafter the placement phase. Modeling each file in the database as an independent\nand identically distributed Gaussian vector, the minimum delivery rate that can\nsatisfy any demand combination within the corresponding distortion target is\nstudied. The optimal delivery rate is characterized for the special case of two\nusers and two files for any pair of distortion requirements. For the general\nsetting with multiple users and files, a layered caching and delivery scheme,\nwhich exploits the successive refinability of Gaussian sources, is proposed.\nThis scheme caches each content in multiple layers, and it is optimized by\nsolving two subproblems: lossless caching of each layer with heterogeneous\ncache capacities, and allocation of available caches among layers. The delivery\nrate minimization problem for each layer is solved numerically, while two\nschemes, called the proportional cache allocation (PCA) and ordered cache\nallocation (OCA), are proposed for cache allocation. These schemes are compared\nwith each other and the cut-set bound through numerical simulations."
        ],
        "title": "Centralized Coded Caching for Heterogeneous Lossy Requests",
        "meta": {
            "query": "LLM caching characterization studies"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1702.08044v1": {
        "url": "http://arxiv.org/abs/1702.08044v1",
        "description": "Degraded K-user broadcast channels (BC) are studied when receivers are\nfacilitated with cache memories. Lower and upper bounds are derived on the\ncapacity-memory tradeoff, i.e., on the largest rate of reliable communication\nover the BC as a function of the receivers' cache sizes, and the bounds are\nshown to match for some special cases. The lower bounds are achieved by two new\ncoding schemes that benefit from non-uniform cache assignment. Lower and upper\nbounds are also established on the global capacity-memory tradeoff, i.e., on\nthe largest capacity-memory tradeoff that can be attained by optimizing the\nreceivers' cache sizes subject to a total cache memory budget. The bounds\ncoincide when the total cache memory budget is sufficiently small or\nsufficiently large, characterized in terms of the BC statistics. For small\ncache memories, it is optimal to assign all the cache memory to the weakest\nreceiver. In this regime, the global capacity-memory tradeoff grows as the\ntotal cache memory budget divided by the number of files in the system. In\nother words, a perfect global caching gain is achievable in this regime and the\nperformance corresponds to a system where all cache contents in the network are\navailable to all receivers. For large cache memories, it is optimal to assign a\npositive cache memory to every receiver such that the weaker receivers are\nassigned larger cache memories compared to the stronger receivers. In this\nregime, the growth rate of the global capacity-memory tradeoff is further\ndivided by the number of users, which corresponds to a local caching gain.\nNumerical indicate suggest that a uniform cache-assignment of the total cache\nmemory is suboptimal in all regimes unless the BC is completely symmetric. For\nerasure BCs, this claim is proved analytically in the regime of small\ncache-sizes.",
        "snippets": [
            "Degraded K-user broadcast channels (BC) are studied when receivers are\nfacilitated with cache memories. Lower and upper bounds are derived on the\ncapacity-memory tradeoff, i.e., on the largest rate of reliable communication\nover the BC as a function of the receivers' cache sizes, and the bounds are\nshown to match for some special cases. The lower bounds are achieved by two new\ncoding schemes that benefit from non-uniform cache assignment. Lower and upper\nbounds are also established on the global capacity-memory tradeoff, i.e., on\nthe largest capacity-memory tradeoff that can be attained by optimizing the\nreceivers' cache sizes subject to a total cache memory budget. The bounds\ncoincide when the total cache memory budget is sufficiently small or\nsufficiently large, characterized in terms of the BC statistics. For small\ncache memories, it is optimal to assign all the cache memory to the weakest\nreceiver. In this regime, the global capacity-memory tradeoff grows as the\ntotal cache memory budget divided by the number of files in the system. In\nother words, a perfect global caching gain is achievable in this regime and the\nperformance corresponds to a system where all cache contents in the network are\navailable to all receivers. For large cache memories, it is optimal to assign a\npositive cache memory to every receiver such that the weaker receivers are\nassigned larger cache memories compared to the stronger receivers. In this\nregime, the growth rate of the global capacity-memory tradeoff is further\ndivided by the number of users, which corresponds to a local caching gain.\nNumerical indicate suggest that a uniform cache-assignment of the total cache\nmemory is suboptimal in all regimes unless the BC is completely symmetric. For\nerasure BCs, this claim is proved analytically in the regime of small\ncache-sizes."
        ],
        "title": "Benefits of Cache Assignment on Degraded Broadcast Channels",
        "meta": {
            "query": "LLM caching characterization studies"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1906.09970v1": {
        "url": "http://arxiv.org/abs/1906.09970v1",
        "description": "Content delivery in a multi-user cache-aided broadcast network is studied,\nwhere a server holding a database of correlated contents communicates with the\nusers over a Gaussian broadcast channel (BC). The minimum transmission power\nrequired to satisfy all possible demand combinations is studied, when the users\nare equipped with caches of equal size. Assuming uncoded cache placement, a\nlower bound on the required transmit power as a function of the cache capacity\nis derived. An achievable centralized caching scheme is proposed, which not\nonly utilizes the user's local caches, but also exploits the correlation among\nthe contents in the database. The performance of the scheme, which provides an\nupper bound on the required transmit power for a given cache capacity, is\ncharacterized. Our results indicate that exploiting the correlations among the\ncontents in a cache-aided Gaussain BC can provide significant energy savings.",
        "snippets": [
            "Content delivery in a multi-user cache-aided broadcast network is studied,\nwhere a server holding a database of correlated contents communicates with the\nusers over a Gaussian broadcast channel (BC). The minimum transmission power\nrequired to satisfy all possible demand combinations is studied, when the users\nare equipped with caches of equal size. Assuming uncoded cache placement, a\nlower bound on the required transmit power as a function of the cache capacity\nis derived. An achievable centralized caching scheme is proposed, which not\nonly utilizes the user's local caches, but also exploits the correlation among\nthe contents in the database. The performance of the scheme, which provides an\nupper bound on the required transmit power for a given cache capacity, is\ncharacterized. Our results indicate that exploiting the correlations among the\ncontents in a cache-aided Gaussain BC can provide significant energy savings."
        ],
        "title": "Centralized Caching and Delivery of Correlated Contents over Gaussian Broadcast Channels",
        "meta": {
            "query": "LLM caching characterization studies"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1802.10073v1": {
        "url": "http://arxiv.org/abs/1802.10073v1",
        "description": "This paper considers heterogeneous coded caching where the users have unequal\ndistortion requirements. The server is connected to the users via an error-free\nmulticast link and designs the users' cache sizes subject to a total memory\nbudget. In particular, in the placement phase, the server jointly designs the\nusers' cache sizes and the cache contents. To serve the users' requests, in the\ndelivery phase, the server transmits signals that satisfy the users' distortion\nrequirements. An optimization problem with the objective of minimizing the\nworst-case delivery load subject to the total cache memory budget and users'\ndistortion requirements is formulated. The optimal solution for uncoded\nplacement and linear delivery is characterized explicitly and is shown to\nexhibit a threshold policy with respect to the total cache memory budget. As a\nbyproduct of the study, a caching scheme for systems with fixed cache sizes\nthat outperforms the state-of-art is presented.",
        "snippets": [
            "This paper considers heterogeneous coded caching where the users have unequal\ndistortion requirements. The server is connected to the users via an error-free\nmulticast link and designs the users' cache sizes subject to a total memory\nbudget. In particular, in the placement phase, the server jointly designs the\nusers' cache sizes and the cache contents. To serve the users' requests, in the\ndelivery phase, the server transmits signals that satisfy the users' distortion\nrequirements. An optimization problem with the objective of minimizing the\nworst-case delivery load subject to the total cache memory budget and users'\ndistortion requirements is formulated. The optimal solution for uncoded\nplacement and linear delivery is characterized explicitly and is shown to\nexhibit a threshold policy with respect to the total cache memory budget. As a\nbyproduct of the study, a caching scheme for systems with fixed cache sizes\nthat outperforms the state-of-art is presented."
        ],
        "title": "On Coded Caching with Heterogeneous Distortion Requirements",
        "meta": {
            "query": "LLM caching characterization studies"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1912.11847v1": {
        "url": "http://arxiv.org/abs/1912.11847v1",
        "description": "In recent years, optimization of the success transmission probability in\nwireless cache-enabled networks has been studied extensively. However, few\nworks have concerned about the real-time performance of the cache-enabled\nnetworks. In this paper, we investigate the performance of the cache-enabled\nwith real-time guarantees by adopting the age of information (AoI) as the\nmetric to characterize the timeliness of the delivered information. We\nestablish a spatial-temporal model by utilizing the stochastic geometry and\nqueuing theory which captures both the temporal traffic dynamics and the\ninterferers' geographic distribution. Under the random caching framework, we\nachieve the closed-form expression of AoI by adopting the maximum average\nreceived power criterion for the user association. Finally, we formulate a\nconvex optimization problem for the minimization of the Peak AoI (PAoI) and\nobtain the optimal caching probabilities by utilizing the Karush-Kuhn-Tucker\n(KKT) conditions. Numerical results demonstrate that the random caching\nstrategy is a better choice rather than the most popular content and uniform\ncaching strategy to improve the real-time performance for the cached file as\nwell as maintaining the file diversity at a relatively high level.",
        "snippets": [
            "In recent years, optimization of the success transmission probability in\nwireless cache-enabled networks has been studied extensively. However, few\nworks have concerned about the real-time performance of the cache-enabled\nnetworks. In this paper, we investigate the performance of the cache-enabled\nwith real-time guarantees by adopting the age of information (AoI) as the\nmetric to characterize the timeliness of the delivered information. We\nestablish a spatial-temporal model by utilizing the stochastic geometry and\nqueuing theory which captures both the temporal traffic dynamics and the\ninterferers' geographic distribution. Under the random caching framework, we\nachieve the closed-form expression of AoI by adopting the maximum average\nreceived power criterion for the user association. Finally, we formulate a\nconvex optimization problem for the minimization of the Peak AoI (PAoI) and\nobtain the optimal caching probabilities by utilizing the Karush-Kuhn-Tucker\n(KKT) conditions. Numerical results demonstrate that the random caching\nstrategy is a better choice rather than the most popular content and uniform\ncaching strategy to improve the real-time performance for the cached file as\nwell as maintaining the file diversity at a relatively high level."
        ],
        "title": "Edge Caching with Real-Time Guarantees",
        "meta": {
            "query": "LLM caching characterization studies"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2201.02855v1": {
        "url": "http://arxiv.org/abs/2201.02855v1",
        "description": "Spin-Transfer Torque Magnetic RAM (STT-MRAM) is known as the most promising\nreplacement for SRAM technology in large Last-Level Caches (LLCs). Despite its\nhigh-density, non-volatility, near-zero leakage power, and immunity to\nradiation as the major advantages, STT-MRAM-based cache suffers from high error\nrates mainly due to retention failure, read disturbance, and write failure.\nExisting studies are limited to estimating the rate of only one or two of these\nerror types for STT-MRAM cache. However, the overall vulnerability of STT-MRAM\ncaches, which its estimation is a must to design cost-efficient reliable\ncaches, has not been offered in any of previous studies. In this paper, we\npropose a system-level framework for reliability exploration and\ncharacterization of errors behavior in STT-MRAM caches. To this end, we\nformulate the cache vulnerability considering the inter-correlation of the\nerror types including all three errors as well as the dependency of error rates\nto workloads behavior and Process Variations (PVs). Our analysis reveals that\nSTT-MRAM cache vulnerability is highly workload-dependent and varies by orders\nof magnitude in different cache access patterns. Our analytical study also\nshows that this vulnerability divergence significantly increases by process\nvariations in STT-MRAM cells. To evaluate the framework, we implement the error\ntypes in the gem5 full-system simulator, and the experimental results show that\nthe total error rate in a shared LLC varies by 32.0x for different workloads. A\nfurther 6.5x vulnerability variation is observed when considering PVs in the\nSTT-MRAM cells. In addition, the contribution of each error type in total LLC\nvulnerability highly varies in different cache access patterns and moreover,\nerror rates are differently affected by PVs.",
        "snippets": [
            "Spin-Transfer Torque Magnetic RAM (STT-MRAM) is known as the most promising\nreplacement for SRAM technology in large Last-Level Caches (LLCs). Despite its\nhigh-density, non-volatility, near-zero leakage power, and immunity to\nradiation as the major advantages, STT-MRAM-based cache suffers from high error\nrates mainly due to retention failure, read disturbance, and write failure.\nExisting studies are limited to estimating the rate of only one or two of these\nerror types for STT-MRAM cache. However, the overall vulnerability of STT-MRAM\ncaches, which its estimation is a must to design cost-efficient reliable\ncaches, has not been offered in any of previous studies. In this paper, we\npropose a system-level framework for reliability exploration and\ncharacterization of errors behavior in STT-MRAM caches. To this end, we\nformulate the cache vulnerability considering the inter-correlation of the\nerror types including all three errors as well as the dependency of error rates\nto workloads behavior and Process Variations (PVs). Our analysis reveals that\nSTT-MRAM cache vulnerability is highly workload-dependent and varies by orders\nof magnitude in different cache access patterns. Our analytical study also\nshows that this vulnerability divergence significantly increases by process\nvariations in STT-MRAM cells. To evaluate the framework, we implement the error\ntypes in the gem5 full-system simulator, and the experimental results show that\nthe total error rate in a shared LLC varies by 32.0x for different workloads. A\nfurther 6.5x vulnerability variation is observed when considering PVs in the\nSTT-MRAM cells. In addition, the contribution of each error type in total LLC\nvulnerability highly varies in different cache access patterns and moreover,\nerror rates are differently affected by PVs."
        ],
        "title": "A System-Level Framework for Analytical and Empirical Reliability Exploration of STT-MRAM Caches",
        "meta": {
            "query": "LLM caching characterization studies"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1812.08720v1": {
        "url": "http://arxiv.org/abs/1812.08720v1",
        "description": "In recent years, enterprise Solid-State Drives (SSDs) are used in the caching\nlayer of high-performance servers to close the growing performance gap between\nprocessing units and storage subsystem. SSD-based I/O caching is typically not\neffective in workloads with burst accesses in which the caching layer itself\nbecomes the performance bottleneck because of the large number of accesses.\nExisting I/O cache architectures mainly focus on maximizing the cache hit ratio\nwhile they neglect the average queue time of accesses. Previous studies\nsuggested bypassing the cache when burst accesses are identified. These\nschemes, however, are not applicable to a general cache configuration and also\nresult in significant performance degradation on burst accesses. In this paper,\nwe propose a novel I/O cache load balancing scheme (LBICA) with adaptive write\npolicy management to prevent the I/O cache from becoming performance bottleneck\nin burst accesses. Our proposal, unlike previous schemes, which disable the I/O\ncache or bypass the requests into the disk subsystem in burst accesses,\nselectively reduces the number of waiting accesses in the SSD queue and\nbalances the load between the I/O cache and the disk subsystem while providing\nthe maximum performance. The proposed scheme characterizes the workload based\non the type of in-queue requests and assigns an effective cache write policy.\nWe aim to bypass the accesses which 1) are served faster by the disk subsystem\nor 2) cannot be merged with other accesses in the I/O cache queue. Doing so,\nthe selected requests are responded by the disk layer, preventing from\noverloading the I/O cache. Our evaluations on a physical system shows that\nLBICA reduces the load on the I/O cache by 48% and improves the performance of\nburst workloads by 30% compared to the latest state-of-the-art load balancing\nscheme.",
        "snippets": [
            "In recent years, enterprise Solid-State Drives (SSDs) are used in the caching\nlayer of high-performance servers to close the growing performance gap between\nprocessing units and storage subsystem. SSD-based I/O caching is typically not\neffective in workloads with burst accesses in which the caching layer itself\nbecomes the performance bottleneck because of the large number of accesses.\nExisting I/O cache architectures mainly focus on maximizing the cache hit ratio\nwhile they neglect the average queue time of accesses. Previous studies\nsuggested bypassing the cache when burst accesses are identified. These\nschemes, however, are not applicable to a general cache configuration and also\nresult in significant performance degradation on burst accesses. In this paper,\nwe propose a novel I/O cache load balancing scheme (LBICA) with adaptive write\npolicy management to prevent the I/O cache from becoming performance bottleneck\nin burst accesses. Our proposal, unlike previous schemes, which disable the I/O\ncache or bypass the requests into the disk subsystem in burst accesses,\nselectively reduces the number of waiting accesses in the SSD queue and\nbalances the load between the I/O cache and the disk subsystem while providing\nthe maximum performance. The proposed scheme characterizes the workload based\non the type of in-queue requests and assigns an effective cache write policy.\nWe aim to bypass the accesses which 1) are served faster by the disk subsystem\nor 2) cannot be merged with other accesses in the I/O cache queue. Doing so,\nthe selected requests are responded by the disk layer, preventing from\noverloading the I/O cache. Our evaluations on a physical system shows that\nLBICA reduces the load on the I/O cache by 48% and improves the performance of\nburst workloads by 30% compared to the latest state-of-the-art load balancing\nscheme."
        ],
        "title": "LBICA: A Load Balancer for I/O Cache Architectures",
        "meta": {
            "query": "LLM caching characterization studies"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1605.07729v1": {
        "url": "http://arxiv.org/abs/1605.07729v1",
        "description": "This paper studies the storage-latency tradeoff in the $3\\times3$ wireless\ninterference network with caches equipped at all transmitters and receivers.\nThe tradeoff is characterized by the so-called fractional delivery time (FDT)\nat given normalized transmitter and receiver cache sizes. We first propose a\ngeneric cooperative transmitter/receiver caching strategy with adjustable file\nsplitting ratios. Based on this caching strategy, we then design the delivery\nphase carefully to turn the considered interference channel opportunistically\ninto broadcast channel, multicast channel, X channel, or a hybrid form of these\nchannels. After that, we obtain an achievable upper bound of the minimum FDT by\nsolving a linear programming problem of the file splitting ratios. The\nachievable FDT is a convex and piece-wise linear decreasing function of the\ncache sizes. Receiver local caching gain, coded multicasting gain, and\ntransmitter cooperation gain (interference alignment and interference\nneutralization) are leveraged in different cache size regions.",
        "snippets": [
            "This paper studies the storage-latency tradeoff in the $3\\times3$ wireless\ninterference network with caches equipped at all transmitters and receivers.\nThe tradeoff is characterized by the so-called fractional delivery time (FDT)\nat given normalized transmitter and receiver cache sizes. We first propose a\ngeneric cooperative transmitter/receiver caching strategy with adjustable file\nsplitting ratios. Based on this caching strategy, we then design the delivery\nphase carefully to turn the considered interference channel opportunistically\ninto broadcast channel, multicast channel, X channel, or a hybrid form of these\nchannels. After that, we obtain an achievable upper bound of the minimum FDT by\nsolving a linear programming problem of the file splitting ratios. The\nachievable FDT is a convex and piece-wise linear decreasing function of the\ncache sizes. Receiver local caching gain, coded multicasting gain, and\ntransmitter cooperation gain (interference alignment and interference\nneutralization) are leveraged in different cache size regions."
        ],
        "title": "Cooperative Tx/Rx Caching in Interference Channels: A Storage-Latency Tradeoff Study",
        "meta": {
            "query": "LLM caching characterization studies"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2109.14680v1": {
        "url": "http://arxiv.org/abs/2109.14680v1",
        "description": "Coded caching utilizes proper file subpacketization and coded delivery to\nmake full use of the multicast opportunities in content delivery, to alleviate\nfile transfer load in massive content delivery scenarios. Most existing work\nconsiders deterministic environments. An important practical topic is to\ncharacterize the impact of the uncertainty from user inactivity on coded\ncaching. We consider a one server cache-enabled network under homogeneous file\nand network settings in presence of user inactivity. Unlike random or\nprobabilistic caching studied in the literature, deterministic coded caching is\nconsidered, with the objective to minimize the worst-case backhaul load by\noptimizing the file subpacketization and the caching strategy. First, a coded\ncaching method is used, where each file is split into the same type of\nfragments labeled using sets with fixed cardinality, and the optimality of the\nselected cardinality is proved. Optimal file subpacketization by splitting the\nfile into multiple types of fragments labeled with multiple cardinalities is\nthen discussed. We show that the closed-form optimum turns out to be given by a\nfixed cardinality -- optimizing for user inactivity only affects file delivery,\ncache placement is not affected. A decentralized version is also discussed and\nanalyzed, where each user fills its storage independently at random without\ncentralized coordination, and user inactivity is taken into account in file\ndelivery. Simulation results show that the optimization based centralized coded\ncaching scheme provides performance comparable to the ideal scenario assuming\nfull knowledge of user inactivity in the placement phase, while decentralized\ncaching performs slightly worse against user inactivity.",
        "snippets": [
            "Coded caching utilizes proper file subpacketization and coded delivery to\nmake full use of the multicast opportunities in content delivery, to alleviate\nfile transfer load in massive content delivery scenarios. Most existing work\nconsiders deterministic environments. An important practical topic is to\ncharacterize the impact of the uncertainty from user inactivity on coded\ncaching. We consider a one server cache-enabled network under homogeneous file\nand network settings in presence of user inactivity. Unlike random or\nprobabilistic caching studied in the literature, deterministic coded caching is\nconsidered, with the objective to minimize the worst-case backhaul load by\noptimizing the file subpacketization and the caching strategy. First, a coded\ncaching method is used, where each file is split into the same type of\nfragments labeled using sets with fixed cardinality, and the optimality of the\nselected cardinality is proved. Optimal file subpacketization by splitting the\nfile into multiple types of fragments labeled with multiple cardinalities is\nthen discussed. We show that the closed-form optimum turns out to be given by a\nfixed cardinality -- optimizing for user inactivity only affects file delivery,\ncache placement is not affected. A decentralized version is also discussed and\nanalyzed, where each user fills its storage independently at random without\ncentralized coordination, and user inactivity is taken into account in file\ndelivery. Simulation results show that the optimization based centralized coded\ncaching scheme provides performance comparable to the ideal scenario assuming\nfull knowledge of user inactivity in the placement phase, while decentralized\ncaching performs slightly worse against user inactivity."
        ],
        "title": "Fundamental Rate-Memory Tradeoff for Coded Caching in Presence of User Inactivity",
        "meta": {
            "query": "LLM caching characterization studies"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2401.08138v1": {
        "url": "http://arxiv.org/abs/2401.08138v1",
        "description": "Large language models (LLMs) enable state-of-the-art semantic capabilities to\nbe added to software systems such as semantic search of unstructured documents\nand text generation. However, these models are computationally expensive. At\nscale, the cost of serving thousands of users increases massively affecting\nalso user experience. To address this problem, semantic caches are used to\ncheck for answers to similar queries (that may have been phrased differently)\nwithout hitting the LLM service. Due to the nature of these semantic cache\ntechniques that rely on query embeddings, there is a high chance of errors\nimpacting user confidence in the system. Adopting semantic cache techniques\nusually requires testing the effectiveness of a semantic cache (accurate cache\nhits and misses) which requires a labelled test set of similar queries and\nresponses which is often unavailable. In this paper, we present VaryGen, an\napproach for using LLMs for test input generation that produces similar\nquestions from unstructured text documents. Our novel approach uses the\nreasoning capabilities of LLMs to 1) adapt queries to the domain, 2) synthesise\nsubtle variations to queries, and 3) evaluate the synthesised test dataset. We\nevaluated our approach in the domain of a student question and answer system by\nqualitatively analysing 100 generated queries and result pairs, and conducting\nan empirical case study with an open source semantic cache. Our results show\nthat query pairs satisfy human expectations of similarity and our generated\ndata demonstrates failure cases of a semantic cache. Additionally, we also\nevaluate our approach on Qasper dataset. This work is an important first step\ninto test input generation for semantic applications and presents\nconsiderations for practitioners when calibrating a semantic cache.",
        "snippets": [
            "Large language models (LLMs) enable state-of-the-art semantic capabilities to\nbe added to software systems such as semantic search of unstructured documents\nand text generation. However, these models are computationally expensive. At\nscale, the cost of serving thousands of users increases massively affecting\nalso user experience. To address this problem, semantic caches are used to\ncheck for answers to similar queries (that may have been phrased differently)\nwithout hitting the LLM service. Due to the nature of these semantic cache\ntechniques that rely on query embeddings, there is a high chance of errors\nimpacting user confidence in the system. Adopting semantic cache techniques\nusually requires testing the effectiveness of a semantic cache (accurate cache\nhits and misses) which requires a labelled test set of similar queries and\nresponses which is often unavailable. In this paper, we present VaryGen, an\napproach for using LLMs for test input generation that produces similar\nquestions from unstructured text documents. Our novel approach uses the\nreasoning capabilities of LLMs to 1) adapt queries to the domain, 2) synthesise\nsubtle variations to queries, and 3) evaluate the synthesised test dataset. We\nevaluated our approach in the domain of a student question and answer system by\nqualitatively analysing 100 generated queries and result pairs, and conducting\nan empirical case study with an open source semantic cache. Our results show\nthat query pairs satisfy human expectations of similarity and our generated\ndata demonstrates failure cases of a semantic cache. Additionally, we also\nevaluate our approach on Qasper dataset. This work is an important first step\ninto test input generation for semantic applications and presents\nconsiderations for practitioners when calibrating a semantic cache."
        ],
        "title": "LLMs for Test Input Generation for Semantic Caches",
        "meta": {
            "query": "LLM caching characterization studies"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.15304v1": {
        "url": "http://arxiv.org/abs/2502.15304v1",
        "description": "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs.",
        "snippets": [
            "For the efficient inference of Large Language Models (LLMs), the effective\ncompression of key-value (KV) cache is essential. Three main types of KV cache\ncompression techniques, namely sparsity, channel compression, and quantization,\nhave been identified. This study presents SVDq, a Singular Value Decomposition\n(SVD) - based mixed precision quantization method for K cache. Initially, K\ncache is transformed into latent channels using SVD basis representations.\nSince the values in latent channels decay rapidly and become negligible after\nonly a few latent channels, our method then incorporates importance-aware\nquantization and compression for latent channels. This enables the effective\nallocation of higher precision to more significant channels. Theoretically, we\nprove that SVDq results in quantization errors (x0.1 or even lower) that are\nmuch lower than those of per-channel key quantization in the original space.\nOur findings based on RULER and LongBench benchmarks demonstrate that SVDq can\nachieve an equivalent key cache precision as low as 1.25-bit. When combined\nwith key sparsity, it can reach a key compression ratio of up to 410x for\nattention computation, all while maintaining comparable model performance.\nNotably, our method is nearly lossless for LongBench datasets. This indicates\nthat SVDq enables high-precision low-bit quantization, providing a more\nefficient solution for KV cache compression in LLMs."
        ],
        "title": "SVDq: 1.25-bit and 410x Key Cache Compression for LLM Attention",
        "meta": {
            "query": "LLM caching characterization studies"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2411.15785v2": {
        "url": "http://arxiv.org/abs/2411.15785v2",
        "description": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models.",
        "snippets": [
            "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models."
        ],
        "title": "A Method for Building Large Language Models with Predefined KV Cache Capacity",
        "meta": {
            "query": "workloaddependent cache performance large language models caching"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.23956v1": {
        "url": "http://arxiv.org/abs/2503.23956v1",
        "description": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
        "snippets": [
            "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
        ],
        "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference",
        "meta": {
            "query": "workloaddependent cache performance large language models caching"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.15734v1": {
        "url": "http://arxiv.org/abs/2502.15734v1",
        "description": "Retrieval-Augmented Generation (RAG) is often used with Large Language Models\n(LLMs) to infuse domain knowledge or user-specific information. In RAG, given a\nuser query, a retriever extracts chunks of relevant text from a knowledge base.\nThese chunks are sent to an LLM as part of the input prompt. Typically, any\ngiven chunk is repeatedly retrieved across user questions. However, currently,\nfor every question, attention-layers in LLMs fully compute the key values (KVs)\nrepeatedly for the input chunks, as state-of-the-art methods cannot reuse\nKV-caches when chunks appear at arbitrary locations with arbitrary contexts.\nNaive reuse leads to output quality degradation. This leads to potentially\nredundant computations on expensive GPUs and increases latency. In this work,\nwe propose Cache-Craft, a system for managing and reusing precomputed KVs\ncorresponding to the text chunks (we call chunk-caches) in RAG-based systems.\nWe present how to identify chunk-caches that are reusable, how to efficiently\nperform a small fraction of recomputation to fix the cache to maintain output\nquality, and how to efficiently store and evict chunk-caches in the hardware\nfor maximizing reuse while masking any overheads. With real production\nworkloads as well as synthetic datasets, we show that Cache-Craft reduces\nredundant computation by 51% over SOTA prefix-caching and 75% over full\nrecomputation. Additionally, with continuous batching on a real production\nworkload, we get a 1.6X speed up in throughput and a 2X reduction in end-to-end\nresponse latency over prefix-caching while maintaining quality, for both the\nLLaMA-3-8B and LLaMA-3-70B models.",
        "snippets": [
            "Retrieval-Augmented Generation (RAG) is often used with Large Language Models\n(LLMs) to infuse domain knowledge or user-specific information. In RAG, given a\nuser query, a retriever extracts chunks of relevant text from a knowledge base.\nThese chunks are sent to an LLM as part of the input prompt. Typically, any\ngiven chunk is repeatedly retrieved across user questions. However, currently,\nfor every question, attention-layers in LLMs fully compute the key values (KVs)\nrepeatedly for the input chunks, as state-of-the-art methods cannot reuse\nKV-caches when chunks appear at arbitrary locations with arbitrary contexts.\nNaive reuse leads to output quality degradation. This leads to potentially\nredundant computations on expensive GPUs and increases latency. In this work,\nwe propose Cache-Craft, a system for managing and reusing precomputed KVs\ncorresponding to the text chunks (we call chunk-caches) in RAG-based systems.\nWe present how to identify chunk-caches that are reusable, how to efficiently\nperform a small fraction of recomputation to fix the cache to maintain output\nquality, and how to efficiently store and evict chunk-caches in the hardware\nfor maximizing reuse while masking any overheads. With real production\nworkloads as well as synthetic datasets, we show that Cache-Craft reduces\nredundant computation by 51% over SOTA prefix-caching and 75% over full\nrecomputation. Additionally, with continuous batching on a real production\nworkload, we get a 1.6X speed up in throughput and a 2X reduction in end-to-end\nresponse latency over prefix-caching while maintaining quality, for both the\nLLaMA-3-8B and LLaMA-3-70B models."
        ],
        "title": "Cache-Craft: Managing Chunk-Caches for Efficient Retrieval-Augmented Generation",
        "meta": {
            "query": "AttentionGate cache eviction policy for KV$ workload"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2504.15364v3": {
        "url": "http://arxiv.org/abs/2504.15364v3",
        "description": "We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods.",
        "snippets": [
            "We demonstrate that geometrically distinctive keys during LLM inference tend\nto have high attention scores. Based on the phenomenon we propose KeyDiff, a\ntraining-free KV cache eviction method based solely on key similarity. Unlike\nother KV cache eviction methods, KeyDiff can process arbitrarily long prompts\nwithin strict resource constraints and efficiently generate responses. We\nprovide a theoretical basis for KeyDiff by relating key diversity with\nattention scores. These results imply KeyDiff can efficiently identify the most\nimportant tokens to retain. Notably KeyDiff does not rely on attention scores,\nallowing the use of optimized attention mechanisms like FlashAttention. Under a\nstrict memory allowance, we demonstrate the effectiveness of KeyDiff for the\nLlama and Qwen model families by observing a performance gap of less than 0.04%\nwith 8K cache budget ($\\sim$23% KV cache reduction) from the non-evicting\nbaseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near\nbaseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning\nbenchmark and decrease end-to-end inference latency by up to 30% compared to\nthe other token-eviction methods."
        ],
        "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments",
        "meta": {
            "query": "AttentionGate cache eviction policy for KV$ workload"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.21465v3": {
        "url": "http://arxiv.org/abs/2410.21465v3",
        "description": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
        "snippets": [
            "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
        ],
        "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2501.12689v2": {
        "url": "http://arxiv.org/abs/2501.12689v2",
        "description": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average.",
        "snippets": [
            "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average."
        ],
        "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.23537v1": {
        "url": "http://arxiv.org/abs/2410.23537v1",
        "description": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
        "snippets": [
            "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
        ],
        "title": "ALISE: Accelerating Large Language Model Serving with Speculative Scheduling",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2504.09590v1": {
        "url": "http://arxiv.org/abs/2504.09590v1",
        "description": "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n[27]) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing [28], referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI.",
        "snippets": [
            "Recent breakthroughs in large Language Models (LLMs) have enabled various\ngenerative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT\n) powered by an LLM often concurrently support latency-critical requests\nfor interactive applications (e.g., question-answering systems, referred to as\nreal-time or RT requests) and throughput-oriented requests for back-of-house\nprocessing (e.g., documents batch processing , referred to best-effort or\nBE requests), with complex hybrid inference workloads to the underlying model.\nState-of-the-art (SOTA) LLM serving systems dedicate machines to each type of\nrequest, towards either low inference latency or high serving throughput,\nrespectively. This practice simplifies request scheduling and management but\nsuffers from poor resource utilization. We propose BROS, a hybrid LLM serving\nsystem that aims to collocate RT/BE requests, meeting RT requests' latency\nrequirements while maintaining BE requests' throughput. BROS formulates the\nproblem of hybrid RT/BE request scheduling and solves it with a dynamic\npriority-based algorithm. BROS designs a bidirectional KV cache management\nmechanism, allowing RT requests to share KV memory with BE requests to remove\nthe scheduling restrictions caused by insufficient KV memory and improve\nutilization. Extensive experiments validate that BROS achieves a good trade-off\nwhen serving hybrid RT and BE requests. It significantly reduces the latency of\nRT requests (up to 74.20%), improving their fine-grained service level\nobjectives (SLOs) attainments (up to 36.38x), with negligible throughput\nreduction for BE requests, showing significant advantages over SOTA systems\nlike vLLM and TGI."
        ],
        "title": "Efficient LLM Serving on Hybrid Real-time and Best-effort Requests",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2504.15720v1": {
        "url": "http://arxiv.org/abs/2504.15720v1",
        "description": "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions.",
        "snippets": [
            "Large language models (LLMs) with different architectures and sizes have been\ndeveloped. Serving each LLM with dedicated GPUs leads to resource waste and\nservice inefficiency due to the varying demand of LLM requests. A common\npractice is to share multiple LLMs. However, existing sharing systems either do\nnot consider the autoregressive pattern of LLM services, or only focus on\nimproving the throughput, which impairs the sharing performance, especially the\nserving latency. We present SeaLLM, which enables service-aware and\nlatency-optimized LLM sharing. SeaLLM improves the overall sharing performance\nby (1) a latency-optimized scheduling algorithm utilizing the characteristics\nof LLM services, (2) a placement algorithm to determine the placement plan and\nan adaptive replacement algorithm to decide the replacement interval, and (3) a\nunified key-value cache to share GPU memory among LLM services efficiently. Our\nevaluation under real-world traces and LLM services demonstrates that SeaLLM\nimproves the normalized latency by up to $13.60\\times$, the tail latency by up\nto $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to\nexisting solutions."
        ],
        "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large Language Model Inference",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2411.17741v1": {
        "url": "http://arxiv.org/abs/2411.17741v1",
        "description": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines.",
        "snippets": [
            "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines."
        ],
        "title": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM Inference Environments",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.00022v1": {
        "url": "http://arxiv.org/abs/2503.00022v1",
        "description": "Key-value (KV) caching has emerged as a crucial optimization technique for\naccelerating inference in large language models (LLMs). By allowing the\nattention operation to scale linearly rather than quadratically with the total\nsequence length, KV caching significantly enhances generation throughput.\nHowever, due to large context lengths in the modern LLMs, the memory footprint\nof the KV is a huge bottleneck for model deployment directly impacting the\nmodel's batch size, hindering its ability to deliver high-throughput. Existing\nresearch addresses this challenge using several techniques, such as discarding\nlow-attention tokens, quantization, and matrix approximation which typically\nlead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many\nKV compression technologies to improve the model accuracy at a much smaller\nmemory. KVCrush provides an alternate representation scheme for key-value\nstates, along with a low-overhead token pruning algorithm that accounts for the\ntoken distribution in the KV cache, which in turn allows for a a smaller\nfootprint while maintaining the accuracy of the model. Based on our results,\nKVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop\nand achieves state-of-the-art average accuracy with minimal overhead, incurring\nless than 0.5% total inference latency. KVCrush not only outperforms the\naccuracy of state-of-the-art importance-based token retention schemes but is\nalso compatible with typical practical LLM deployments using KV cache paging\nschemes such as vLLM and mixed precision quantization.",
        "snippets": [
            "Key-value (KV) caching has emerged as a crucial optimization technique for\naccelerating inference in large language models (LLMs). By allowing the\nattention operation to scale linearly rather than quadratically with the total\nsequence length, KV caching significantly enhances generation throughput.\nHowever, due to large context lengths in the modern LLMs, the memory footprint\nof the KV is a huge bottleneck for model deployment directly impacting the\nmodel's batch size, hindering its ability to deliver high-throughput. Existing\nresearch addresses this challenge using several techniques, such as discarding\nlow-attention tokens, quantization, and matrix approximation which typically\nlead to a negative impact on the model accuracy.\n  In this paper, We propose KVCrush technology which can be combined with many\nKV compression technologies to improve the model accuracy at a much smaller\nmemory. KVCrush provides an alternate representation scheme for key-value\nstates, along with a low-overhead token pruning algorithm that accounts for the\ntoken distribution in the KV cache, which in turn allows for a a smaller\nfootprint while maintaining the accuracy of the model. Based on our results,\nKVCrush reduces LongBench KV Cache size by 4x with less than 1% accuracy drop\nand achieves state-of-the-art average accuracy with minimal overhead, incurring\nless than 0.5% total inference latency. KVCrush not only outperforms the\naccuracy of state-of-the-art importance-based token retention schemes but is\nalso compatible with typical practical LLM deployments using KV cache paging\nschemes such as vLLM and mixed precision quantization."
        ],
        "title": "KVCrush: Key value cache size-reduction using similarity in head-behaviour",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2504.06319v1": {
        "url": "http://arxiv.org/abs/2504.06319v1",
        "description": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines.",
        "snippets": [
            "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines."
        ],
        "title": "Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2410.03960v3": {
        "url": "http://arxiv.org/abs/2410.03960v3",
        "description": "LLM inference for enterprise applications, such as summarization, RAG, and\ncode-generation, typically observe much longer prompt than generations, leading\nto high prefill cost and response latency. We present SwiftKV, a novel model\ntransformation and distillation procedure targeted at reducing the prefill\ncompute (in FLOPs) of prompt tokens while preserving high generation quality.\nFirst, SwiftKV prefills later layers' KV cache using an earlier layer's output,\nallowing prompt tokens to skip those later layers. Second, SwiftKV employs a\nlightweight knowledge-preserving distillation procedure that can adapt existing\nLLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV\ncache compression to improve inference performance in low-memory scenarios. Our\ncomprehensive experiments show that SwiftKV can effectively reduce prefill\ncomputation by 25-50% across several LLM families while incurring minimum\nquality degradation. In the end-to-end inference serving, SwiftKV realizes up\nto 2x higher aggregate throughput and 60% lower time per output token. It can\nachieve a staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at\nhttps://github.com/snowflakedb/arctictraining.",
        "snippets": [
            "LLM inference for enterprise applications, such as summarization, RAG, and\ncode-generation, typically observe much longer prompt than generations, leading\nto high prefill cost and response latency. We present SwiftKV, a novel model\ntransformation and distillation procedure targeted at reducing the prefill\ncompute (in FLOPs) of prompt tokens while preserving high generation quality.\nFirst, SwiftKV prefills later layers' KV cache using an earlier layer's output,\nallowing prompt tokens to skip those later layers. Second, SwiftKV employs a\nlightweight knowledge-preserving distillation procedure that can adapt existing\nLLMs with minimal accuracy impact. Third, SwiftKV can naturally incorporate KV\ncache compression to improve inference performance in low-memory scenarios. Our\ncomprehensive experiments show that SwiftKV can effectively reduce prefill\ncomputation by 25-50% across several LLM families while incurring minimum\nquality degradation. In the end-to-end inference serving, SwiftKV realizes up\nto 2x higher aggregate throughput and 60% lower time per output token. It can\nachieve a staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B. SwiftKV is open-sourced at\nhttps://github.com/snowflakedb/arctictraining."
        ],
        "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2408.11049v5": {
        "url": "http://arxiv.org/abs/2408.11049v5",
        "description": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
        "snippets": [
            "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
        ],
        "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2411.09317v1": {
        "url": "http://arxiv.org/abs/2411.09317v1",
        "description": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
        "snippets": [
            "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
        ],
        "title": "Pie: Pooling CPU Memory for LLM Inference",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2409.20002v3": {
        "url": "http://arxiv.org/abs/2409.20002v3",
        "description": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
        "snippets": [
            "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
        ],
        "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM Serving Systems",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2401.06761v1": {
        "url": "http://arxiv.org/abs/2401.06761v1",
        "description": "The massive adoption of large language models (LLMs) demands efficient\ndeployment strategies. However, the auto-regressive decoding process, which is\nfundamental to how most LLMs generate text, poses challenges to achieve\nefficient serving. In this work, we introduce a parallel auto-regressive\ngeneration method. By instruct-tuning on general domain data that contains\nhierarchical structures, we enable LLMs to independently plan their generation\nprocess and perform auto-parallel auto-regressive (APAR) generation,\nsignificantly reducing the number of generation steps. APAR alone can achieve\nup to 2x speed-up, and when combined with speculative decoding, the speed-up\ncan reach up to 4x. In addition, APAR reduces the key-value cache consumption\nand attention computation during generation. This leads to a throughput\nincrease of 20-70% and a latency reduce of 20-35% in high-throughput scenarios,\ncompared to state-of-the-art serving frameworks.",
        "snippets": [
            "The massive adoption of large language models (LLMs) demands efficient\ndeployment strategies. However, the auto-regressive decoding process, which is\nfundamental to how most LLMs generate text, poses challenges to achieve\nefficient serving. In this work, we introduce a parallel auto-regressive\ngeneration method. By instruct-tuning on general domain data that contains\nhierarchical structures, we enable LLMs to independently plan their generation\nprocess and perform auto-parallel auto-regressive (APAR) generation,\nsignificantly reducing the number of generation steps. APAR alone can achieve\nup to 2x speed-up, and when combined with speculative decoding, the speed-up\ncan reach up to 4x. In addition, APAR reduces the key-value cache consumption\nand attention computation during generation. This leads to a throughput\nincrease of 20-70% and a latency reduce of 20-35% in high-throughput scenarios,\ncompared to state-of-the-art serving frameworks."
        ],
        "title": "APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.21487v1": {
        "url": "http://arxiv.org/abs/2505.21487v1",
        "description": "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$.",
        "snippets": [
            "LLM decoding is bottlenecked for large batches and long contexts by loading\nthe key-value (KV) cache from high-bandwidth memory, which inflates per-token\nlatency, while the sequential nature of decoding limits parallelism. We analyze\nthe interplay among arithmetic intensity, parallelization, and model quality\nand question whether current architectures fully exploit modern hardware. This\nwork redesigns attention to perform more computation per byte loaded from\nmemory to maximize hardware efficiency without trading off parallel\nscalability. We first propose Grouped-Tied Attention (GTA), a simple variant\nthat combines and reuses key and value states, reducing memory transfers\nwithout compromising model quality. We then introduce Grouped Latent Attention\n(GLA), a parallel-friendly latent attention paired with low-level optimizations\nfor fast decoding while maintaining high model quality. Experiments show that\nGTA matches Grouped-Query Attention (GQA) quality while using roughly half the\nKV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier\nto shard. Our optimized GLA kernel is up to 2$\\times$ faster than FlashMLA, for\nexample, in a speculative decoding setting when the query length exceeds one.\nFurthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end\nlatency and increases throughput in online serving benchmarks by up to\n2$\\times$."
        ],
        "title": "Hardware-Efficient Attention for Fast Decoding",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2408.00008v2": {
        "url": "http://arxiv.org/abs/2408.00008v2",
        "description": "Large language models (LLMs) have surged in popularity and are extensively\nused in commercial applications, where the efficiency of model serving is\ncrucial for the user experience. Most current research focuses on optimizing\nindividual sub-procedures, e.g. local inference and communication, however,\nthere is no comprehensive framework that provides a holistic system view for\noptimizing LLM serving in an end-to-end manner. In this work, we conduct a\ndetailed analysis to identify major bottlenecks that impact end-to-end latency\nin LLM serving systems. Our analysis reveals that a comprehensive LLM serving\nendpoint must address a series of efficiency bottlenecks that extend beyond LLM\ninference. We then propose ScaleLLM, an optimized system for resource-efficient\nLLM serving. Our extensive experiments reveal that with 64 concurrent requests,\nScaleLLM achieves a 4.3x speed up over vLLM and outperforms state-of-the-arts\nwith 1.5x higher throughput.",
        "snippets": [
            "Large language models (LLMs) have surged in popularity and are extensively\nused in commercial applications, where the efficiency of model serving is\ncrucial for the user experience. Most current research focuses on optimizing\nindividual sub-procedures, e.g. local inference and communication, however,\nthere is no comprehensive framework that provides a holistic system view for\noptimizing LLM serving in an end-to-end manner. In this work, we conduct a\ndetailed analysis to identify major bottlenecks that impact end-to-end latency\nin LLM serving systems. Our analysis reveals that a comprehensive LLM serving\nendpoint must address a series of efficiency bottlenecks that extend beyond LLM\ninference. We then propose ScaleLLM, an optimized system for resource-efficient\nLLM serving. Our extensive experiments reveal that with 64 concurrent requests,\nScaleLLM achieves a 4.3x speed up over vLLM and outperforms state-of-the-arts\nwith 1.5x higher throughput."
        ],
        "title": "ScaleLLM: A Resource-Frugal LLM Serving Framework by Optimizing End-to-End Efficiency",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.01066v2": {
        "url": "http://arxiv.org/abs/2503.01066v2",
        "description": "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency.",
        "snippets": [
            "Continual learning has become a promising solution to refine large language\nmodels incrementally by leveraging user feedback. In particular, online\ncontinual learning - iteratively training the model with small batches of user\nfeedback - has demonstrated notable performance improvements. However, the\nexisting practice of separating training and serving processes forces the\nonline trainer to recompute the intermediate results already done during\nserving. Such redundant computations can account for 30%-42% of total training\ntime.\n  In this paper, we propose Alchemist, to the best of our knowledge, the first\nonline continual learning system that efficiently reuses serving activations to\nincrease training throughput. Alchemist introduces two key techniques: (1)\nrecording and storing activations and KV cache only during the prefill phase to\nminimize latency and memory overhead; and (2) smart activation offloading and\nhedging. Evaluations with inputs of varied token length sampled from ShareGPT\ndataset show that compared with a separate training cluster, Alchemist\nsignificantly increases training throughput by up to 1.72x, reduces up to 47%\nmemory usage during training, and supports up to 2x more training tokens - all\nwhile maintaining negligible impact on serving latency."
        ],
        "title": "Alchemist: Towards the Design of Efficient Online Continual Learning System",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2501.01005v2": {
        "url": "http://arxiv.org/abs/2501.01005v2",
        "description": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
        "snippets": [
            "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
        ],
        "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2504.11765v1": {
        "url": "http://arxiv.org/abs/2504.11765v1",
        "description": "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration.",
        "snippets": [
            "Recent large language models (LLMs) face increasing inference latency as\ninput context length and model size continue to grow. In particular, the\nretrieval-augmented generation (RAG) technique, which enhances LLM responses by\nincorporating external knowledge, exacerbates this issue by significantly\nincreasing the number of input tokens. This expansion in token length leads to\na substantial rise in computational overhead, particularly during the prefill\nstage, resulting in prolonged time-to-first-token (TTFT). To address this\nissue, this paper proposes a method to reduce TTFT by leveraging a disk-based\nkey-value (KV) cache to lessen the computational burden during the prefill\nstage. We also introduce a disk-based shared KV cache management system, called\nShared RAG-DCache, for multi-instance LLM RAG service environments. This\nsystem, together with an optimal system configuration, improves both throughput\nand latency under given resource constraints. Shared RAG-DCache exploits the\nlocality of documents related to user queries in RAG, as well as the queueing\ndelay in LLM inference services. It proactively generates and stores disk KV\ncaches for query-related documents and shares them across multiple LLM\ninstances to enhance inference performance. In experiments on a single host\nequipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in\nthroughput and up to a 12~65% reduction in latency, depending on the resource\nconfiguration."
        ],
        "title": "Shared Disk KV Cache Management for Efficient Multi-Instance Inference in RAG-Powered LLMs",
        "meta": {
            "query": "impact of caching on serving throughput and latency in LLM"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.01960v1": {
        "url": "http://arxiv.org/abs/2502.01960v1",
        "description": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss.",
        "snippets": [
            "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss."
        ],
        "title": "MPIC: Position-Independent Multimodal Context Caching System for Efficient MLLM Serving",
        "meta": {
            "query": "caching for large language models"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2402.15425v1": {
        "url": "http://arxiv.org/abs/2402.15425v1",
        "description": "Caches on the modern commodity CPUs have become one of the major sources of\nside-channel leakages and been abused as a new attack vector. To thwart the\ncache-based side-channel attacks, two types of countermeasures have been\nproposed: detection-based ones that limit the amount of microarchitectural\ntraces an attacker can leave, and cache prefetching-and-locking techniques that\nclaim to prevent such leakage by disallowing evictions on sensitive data. In\nthis paper, we present the Prime+Retouch attack that completely bypasses these\ndefense schemes by accurately inferring the cache activities with the metadata\nof the cache replacement policy. Prime+Retouch has three noticeable properties:\n1) it incurs no eviction on the victim's data, allowing us to bypass the two\nknown mitigation schemes, 2) it requires minimal synchronization of only one\nmemory access to the attacker's pre-primed cache lines, and 3) it leaks data\nvia non-shared memory, yet because underlying eviction metadata is shared.\n  We demonstrate Prime+Retouch in two architectures: predominant Intel x86 and\nemerging Apple M1. We elucidate how Prime+Retouch can break the T-table\nimplementation of AES with robust cache side-channel mitigations such as Cloak,\nunder both normal and SGX-protected environments. We also manifest feasibility\nof the Prime+Retouch attack on the M1 platform imposing more restrictions where\nthe precise measurement tools such as core clock cycle timer and performance\ncounters are inaccessible to the attacker. Furthermore, we first demystify\nundisclosed cache architecture and its eviction policy of L1 data cache on\nApple M1 architecture. We also devise a user-space noise-free cache monitoring\ntool by repurposing Intel TSX.",
        "snippets": [
            "Caches on the modern commodity CPUs have become one of the major sources of\nside-channel leakages and been abused as a new attack vector. To thwart the\ncache-based side-channel attacks, two types of countermeasures have been\nproposed: detection-based ones that limit the amount of microarchitectural\ntraces an attacker can leave, and cache prefetching-and-locking techniques that\nclaim to prevent such leakage by disallowing evictions on sensitive data. In\nthis paper, we present the Prime+Retouch attack that completely bypasses these\ndefense schemes by accurately inferring the cache activities with the metadata\nof the cache replacement policy. Prime+Retouch has three noticeable properties:\n1) it incurs no eviction on the victim's data, allowing us to bypass the two\nknown mitigation schemes, 2) it requires minimal synchronization of only one\nmemory access to the attacker's pre-primed cache lines, and 3) it leaks data\nvia non-shared memory, yet because underlying eviction metadata is shared.\n  We demonstrate Prime+Retouch in two architectures: predominant Intel x86 and\nemerging Apple M1. We elucidate how Prime+Retouch can break the T-table\nimplementation of AES with robust cache side-channel mitigations such as Cloak,\nunder both normal and SGX-protected environments. We also manifest feasibility\nof the Prime+Retouch attack on the M1 platform imposing more restrictions where\nthe precise measurement tools such as core clock cycle timer and performance\ncounters are inaccessible to the attacker. Furthermore, we first demystify\nundisclosed cache architecture and its eviction policy of L1 data cache on\nApple M1 architecture. We also devise a user-space noise-free cache monitoring\ntool by repurposing Intel TSX."
        ],
        "title": "Prime+Retouch: When Cache is Locked and Leaked",
        "meta": {
            "query": "cache eviction policies performance in LLM workloads"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.02504v1": {
        "url": "http://arxiv.org/abs/2503.02504v1",
        "description": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time.",
        "snippets": [
            "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time."
        ],
        "title": "Energy efficiency of cache eviction algorithms for Zipf distributed objects",
        "meta": {
            "query": "cache eviction policies performance in LLM workloads"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2501.14205v1": {
        "url": "http://arxiv.org/abs/2501.14205v1",
        "description": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks.",
        "snippets": [
            "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks."
        ],
        "title": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement Learning-based Model Caching and Inference Offloading",
        "meta": {
            "query": "caching strategies for LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2412.12488v1": {
        "url": "http://arxiv.org/abs/2412.12488v1",
        "description": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
        "snippets": [
            "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
        ],
        "title": "A System for Microserving of LLMs",
        "meta": {
            "query": "caching strategies for LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.18458v3": {
        "url": "http://arxiv.org/abs/2505.18458v3",
        "description": "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration.",
        "snippets": [
            "The integration of large language model (LLM) and data management (DATA) is\nrapidly redefining both domains. In this survey, we comprehensively review the\nbidirectional relationships. On the one hand, DATA4LLM, spanning large-scale\ndata processing, storage, and serving, feeds LLMs with high quality, diversity,\nand timeliness of data required for stages like pre-training, post-training,\nretrieval-augmented generation, and agentic workflows: (i) Data processing for\nLLMs includes scalable acquisition, deduplication, filtering, selection, domain\nmixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on\nefficient data and model formats, distributed and heterogeneous storage\nhierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data\nserving for LLMs tackles challenges in RAG (e.g., knowledge post-processing),\nLLM inference (e.g., prompt compression, data provenance), and training\nstrategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA,\nLLMs are emerging as general-purpose engines for data management. We review\nrecent advances in (i) data manipulation, including automatic data cleaning,\nintegration, discovery; (ii) data analysis, covering reasoning over structured,\nsemi-structured, and unstructured data, and (iii) system optimization (e.g.,\nconfiguration tuning, query rewriting, anomaly diagnosis), powered by LLM\ntechniques like retrieval-augmented prompting, task-specialized fine-tuning,\nand multi-agent collaboration."
        ],
        "title": "A Survey of LLM $\\times$ DATA",
        "meta": {
            "query": "caching strategies for LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1709.06951v2": {
        "url": "http://arxiv.org/abs/1709.06951v2",
        "description": "Conventional wireless caching assumes that content can be pushed to local\ncaching infrastructure during off-peak hours in an error-free manner; however,\nthis assumption is not applicable if local caches need to be frequently updated\nvia wireless transmission. This paper investigates a new approach to wireless\ncaching for the case when cache content has to be updated during on-peak hours.\nTwo non-orthogonal multiple access (NOMA) assisted caching strategies are\ndeveloped, namely the push-then-deliver strategy and the push-and-deliver\nstrategy. In the push-then-deliver strategy, the NOMA principle is applied to\npush more content files to the content servers during a short time interval\nreserved for content pushing in on-peak hours and to provide more connectivity\nfor content delivery, compared to the conventional orthogonal multiple access\n(OMA) strategy. The push-and-deliver strategy is motivated by the fact that\nsome users' requests cannot be accommodated locally and the base station has to\nserve them directly. These events during the content delivery phase are\nexploited as opportunities for content pushing, which further facilitates the\nfrequent update of the files cached at the content servers. It is also shown\nthat this strategy can be straightforwardly extended to device-to-device\ncaching, and various analytical results are developed to illustrate the\nsuperiority of the proposed caching strategies compared to OMA based schemes.",
        "snippets": [
            "Conventional wireless caching assumes that content can be pushed to local\ncaching infrastructure during off-peak hours in an error-free manner; however,\nthis assumption is not applicable if local caches need to be frequently updated\nvia wireless transmission. This paper investigates a new approach to wireless\ncaching for the case when cache content has to be updated during on-peak hours.\nTwo non-orthogonal multiple access (NOMA) assisted caching strategies are\ndeveloped, namely the push-then-deliver strategy and the push-and-deliver\nstrategy. In the push-then-deliver strategy, the NOMA principle is applied to\npush more content files to the content servers during a short time interval\nreserved for content pushing in on-peak hours and to provide more connectivity\nfor content delivery, compared to the conventional orthogonal multiple access\n(OMA) strategy. The push-and-deliver strategy is motivated by the fact that\nsome users' requests cannot be accommodated locally and the base station has to\nserve them directly. These events during the content delivery phase are\nexploited as opportunities for content pushing, which further facilitates the\nfrequent update of the files cached at the content servers. It is also shown\nthat this strategy can be straightforwardly extended to device-to-device\ncaching, and various analytical results are developed to illustrate the\nsuperiority of the proposed caching strategies compared to OMA based schemes."
        ],
        "title": "NOMA Assisted Wireless Caching: Strategies and Performance Analysis",
        "meta": {
            "query": "caching strategies for LLM serving"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2407.00079v3": {
        "url": "http://arxiv.org/abs/2407.00079v3",
        "description": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
        "snippets": [
            "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
        ],
        "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2412.18169v4": {
        "url": "http://arxiv.org/abs/2412.18169v4",
        "description": "Serving LLMs with a cluster of GPUs is common nowadays, where the serving\nsystem must meet strict latency SLOs required by applications. However, the\nstateful nature of LLM serving requires maintaining huge states (i.e., KVCache)\nin limited GPU memory. Under spikes in real-world workloads, GPU memory can be\neasily throttled, leading to orders of magnitude higher response latency due to\nqueuing introduced by waiting for KVCache to be reclaimed. Prior\nKVCache-centric approaches handle load throttling by dropping, migrating, or\nswapping KVCache. These methods fail to release sufficient memory quickly with\nrequests still queued.\n  This paper proposes the first parameter-centric approach to handling\nthrottling by selectively dropping replicated parameters to instantly free\nmemory for requests, based on an unnoticed observation that model parameters\nare commonly replicated across GPUs for serving LLMs. With additional memory,\nall requests can be served with a larger batch without queuing. To make the\nparameter-centric approach correct and efficient, we cooperatively execute\nrequests on GPUs with a complete copy of parameters using pipeline parallelism,\nand derive an appropriate drop plan without unnecessary cooperation. We also\ndesign techniques to minimize the performance overhead due to pipeline\nparallelism with the execution patterns of requests under drop. Evaluations\nshow that {\\sys} reduces the tail TTFT of requests under throttling by up to\n72.2 times compared to the state-of-the-art systems including Llumnix, vLLM and\nInferCept.",
        "snippets": [
            "Serving LLMs with a cluster of GPUs is common nowadays, where the serving\nsystem must meet strict latency SLOs required by applications. However, the\nstateful nature of LLM serving requires maintaining huge states (i.e., KVCache)\nin limited GPU memory. Under spikes in real-world workloads, GPU memory can be\neasily throttled, leading to orders of magnitude higher response latency due to\nqueuing introduced by waiting for KVCache to be reclaimed. Prior\nKVCache-centric approaches handle load throttling by dropping, migrating, or\nswapping KVCache. These methods fail to release sufficient memory quickly with\nrequests still queued.\n  This paper proposes the first parameter-centric approach to handling\nthrottling by selectively dropping replicated parameters to instantly free\nmemory for requests, based on an unnoticed observation that model parameters\nare commonly replicated across GPUs for serving LLMs. With additional memory,\nall requests can be served with a larger batch without queuing. To make the\nparameter-centric approach correct and efficient, we cooperatively execute\nrequests on GPUs with a complete copy of parameters using pipeline parallelism,\nand derive an appropriate drop plan without unnecessary cooperation. We also\ndesign techniques to minimize the performance overhead due to pipeline\nparallelism with the execution patterns of requests under drop. Evaluations\nshow that {\\sys} reduces the tail TTFT of requests under throttling by up to\n72.2 times compared to the state-of-the-art systems including Llumnix, vLLM and\nInferCept."
        ],
        "title": "KunServe: Efficient Parameter-centric Memory Management for LLM Serving",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2408.08147v1": {
        "url": "http://arxiv.org/abs/2408.08147v1",
        "description": "Serving disaggregated large language models (LLMs) over tens of thousands of\nxPU devices (GPUs or NPUs) with reliable performance faces multiple challenges.\n1) Ignoring the diversity (various prefixes and tidal requests), treating all\nthe prompts in a mixed pool is inadequate. To facilitate the similarity per\nscenario and minimize the inner mismatch on P/D (prefill and decoding)\nprocessing, fine-grained organization is required, dynamically adjusting P/D\nratios for better performance. 2) Due to inaccurate estimation on workload\n(queue status or maintained connections), the global scheduler easily incurs\nunnecessary timeouts in prefill. 3) Block-fixed device-to-device (D2D) KVCache\ntransfer over cluster-level RDMA (remote direct memory access) fails to achieve\ndesired D2D utilization as expected. To overcome previous problems, this paper\nproposes an end-to-end system P/D-Serve, complying with the paradigm of MLOps\n(machine learning operations), which models end-to-end (E2E) P/D performance\nand enables: 1) fine-grained P/D organization, mapping the service with RoCE\n(RDMA over converged ethernet) as needed, to facilitate similar processing and\ndynamic adjustments on P/D ratios; 2) on-demand forwarding upon rejections for\nidle prefill, decoupling the scheduler from regular inaccurate reports and\nlocal queues, to avoid timeouts in prefill; and 3) efficient KVCache transfer\nvia optimized D2D access. P/D-Serve is implemented upon Ascend and MindSpore,\nhas been deployed over tens of thousands of NPUs for more than eight months in\ncommercial use, and further achieves 60\\%, 42\\% and 46\\% improvements on E2E\nthroughput, time-to-first-token (TTFT) SLO (service level objective) and D2D\ntransfer time. As the E2E system with optimizations, P/D-Serve achieves 6.7x\nincrease on throughput, compared with aggregated LLMs.",
        "snippets": [
            "Serving disaggregated large language models (LLMs) over tens of thousands of\nxPU devices (GPUs or NPUs) with reliable performance faces multiple challenges.\n1) Ignoring the diversity (various prefixes and tidal requests), treating all\nthe prompts in a mixed pool is inadequate. To facilitate the similarity per\nscenario and minimize the inner mismatch on P/D (prefill and decoding)\nprocessing, fine-grained organization is required, dynamically adjusting P/D\nratios for better performance. 2) Due to inaccurate estimation on workload\n(queue status or maintained connections), the global scheduler easily incurs\nunnecessary timeouts in prefill. 3) Block-fixed device-to-device (D2D) KVCache\ntransfer over cluster-level RDMA (remote direct memory access) fails to achieve\ndesired D2D utilization as expected. To overcome previous problems, this paper\nproposes an end-to-end system P/D-Serve, complying with the paradigm of MLOps\n(machine learning operations), which models end-to-end (E2E) P/D performance\nand enables: 1) fine-grained P/D organization, mapping the service with RoCE\n(RDMA over converged ethernet) as needed, to facilitate similar processing and\ndynamic adjustments on P/D ratios; 2) on-demand forwarding upon rejections for\nidle prefill, decoupling the scheduler from regular inaccurate reports and\nlocal queues, to avoid timeouts in prefill; and 3) efficient KVCache transfer\nvia optimized D2D access. P/D-Serve is implemented upon Ascend and MindSpore,\nhas been deployed over tens of thousands of NPUs for more than eight months in\ncommercial use, and further achieves 60\\%, 42\\% and 46\\% improvements on E2E\nthroughput, time-to-first-token (TTFT) SLO (service level objective) and D2D\ntransfer time. As the E2E system with optimizations, P/D-Serve achieves 6.7x\nincrease on throughput, compared with aggregated LLMs."
        ],
        "title": "P/D-Serve: Serving Disaggregated Large Language Model at Scale",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2407.12820v2": {
        "url": "http://arxiv.org/abs/2407.12820v2",
        "description": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding.",
        "snippets": [
            "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding."
        ],
        "title": "PQCache: Product Quantization-based KVCache for Long Context LLM Inference",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.09999v2": {
        "url": "http://arxiv.org/abs/2505.09999v2",
        "description": "With the widespread adoption of Large Language Models (LLMs), serving LLM\ninference requests has become an increasingly important task, attracting active\nresearch advancements. Practical workloads play an essential role in this\nprocess: they are critical for motivating and benchmarking serving techniques\nand systems. However, the existing understanding of real-world LLM serving\nworkloads is limited due to the lack of a comprehensive workload\ncharacterization. Prior analyses remain insufficient in scale and scope, thus\nfailing to fully capture intricate workload characteristics.\n  In this paper, we fill the gap with an in-depth characterization of LLM\nserving workloads collected from our worldwide cloud inference serving service,\ncovering not only language models but also emerging multimodal and reasoning\nmodels, and unveiling important new findings in each case. Moreover, based on\nour findings, we propose ServeGen, a principled framework for generating\nrealistic LLM serving workloads by composing them on a per-client basis. A\npractical use case in production validates that ServeGen avoids 50%\nunder-provisioning compared to naive workload generation, demonstrating\nServeGen's advantage in performance benchmarking. ServeGen is available at\nhttps://github.com/alibaba/ServeGen.",
        "snippets": [
            "With the widespread adoption of Large Language Models (LLMs), serving LLM\ninference requests has become an increasingly important task, attracting active\nresearch advancements. Practical workloads play an essential role in this\nprocess: they are critical for motivating and benchmarking serving techniques\nand systems. However, the existing understanding of real-world LLM serving\nworkloads is limited due to the lack of a comprehensive workload\ncharacterization. Prior analyses remain insufficient in scale and scope, thus\nfailing to fully capture intricate workload characteristics.\n  In this paper, we fill the gap with an in-depth characterization of LLM\nserving workloads collected from our worldwide cloud inference serving service,\ncovering not only language models but also emerging multimodal and reasoning\nmodels, and unveiling important new findings in each case. Moreover, based on\nour findings, we propose ServeGen, a principled framework for generating\nrealistic LLM serving workloads by composing them on a per-client basis. A\npractical use case in production validates that ServeGen avoids 50%\nunder-provisioning compared to naive workload generation, demonstrating\nServeGen's advantage in performance benchmarking. ServeGen is available at\nhttps://github.com/alibaba/ServeGen."
        ],
        "title": "ServeGen: Workload Characterization and Generation of Large Language Model Serving in Production",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2401.02669v2": {
        "url": "http://arxiv.org/abs/2401.02669v2",
        "description": "Large Language Models (LLMs) demonstrate substantial potential across a\ndiverse array of domains via request serving. However, as trends continue to\npush for expanding context sizes, the autoregressive nature of LLMs results in\nhighly dynamic behavior of the attention layers, showcasing significant\ndifferences in computational characteristics and memory requirements from the\nnon-attention layers. This presents substantial challenges for resource\nmanagement and performance optimization in service systems. Existing static\nmodel parallelism and resource allocation strategies fall short when dealing\nwith this dynamicity. To address the issue, we propose Infinite-LLM, a novel\nLLM serving system designed to effectively handle dynamic context lengths.\nInfinite-LLM disaggregates attention layers from an LLM's inference process,\nfacilitating flexible and independent resource scheduling that optimizes\ncomputational performance and enhances memory utilization jointly. By\nleveraging a pooled GPU memory strategy across a cluster, Infinite-LLM not only\nsignificantly boosts system throughput but also supports extensive context\nlengths. Evaluated on a dataset with context lengths ranging from a few to\n2000K tokens across a cluster with 32 A100 GPUs, Infinite-LLM demonstrates\nthroughput improvement of 1.35-3.4x compared to state-of-the-art methods,\nenabling efficient and elastic LLM deployment.",
        "snippets": [
            "Large Language Models (LLMs) demonstrate substantial potential across a\ndiverse array of domains via request serving. However, as trends continue to\npush for expanding context sizes, the autoregressive nature of LLMs results in\nhighly dynamic behavior of the attention layers, showcasing significant\ndifferences in computational characteristics and memory requirements from the\nnon-attention layers. This presents substantial challenges for resource\nmanagement and performance optimization in service systems. Existing static\nmodel parallelism and resource allocation strategies fall short when dealing\nwith this dynamicity. To address the issue, we propose Infinite-LLM, a novel\nLLM serving system designed to effectively handle dynamic context lengths.\nInfinite-LLM disaggregates attention layers from an LLM's inference process,\nfacilitating flexible and independent resource scheduling that optimizes\ncomputational performance and enhances memory utilization jointly. By\nleveraging a pooled GPU memory strategy across a cluster, Infinite-LLM not only\nsignificantly boosts system throughput but also supports extensive context\nlengths. Evaluated on a dataset with context lengths ranging from a few to\n2000K tokens across a cluster with 32 A100 GPUs, Infinite-LLM demonstrates\nthroughput improvement of 1.35-3.4x compared to state-of-the-art methods,\nenabling efficient and elastic LLM deployment."
        ],
        "title": "Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2501.05601v1": {
        "url": "http://arxiv.org/abs/2501.05601v1",
        "description": "Recent studies have suggested that large language models (LLMs) underperform\non mathematical and computer science tasks when these problems are translated\nfrom Romanian into English, compared to their original Romanian format.\nAccurate translation is critical for applications ranging from automatic\ntranslations in programming competitions to the creation of high-quality\neducational materials, as well as minimizing errors or fraud in human\ntranslations. This study shows that robust large language models (LLMs) can\nmaintain or even enhance their performance in translating less common languages\nwhen given well-structured prompts. Our findings suggest that LLMs, with\nappropriate supervision, can be reliably used for the automatic translation of\nIOI (International Olympiad in Informatics)-style tasks. We evaluate several\ntranslation methods across multiple LLMs, including OpenRoLLM, Llama 3.1 8B,\nLlama 3.2 3B and GPT-4o, assessing their translation accuracy and performance\nstability through repeated runs. Additionally, we augment the OJI (Romanian\nCounty-Level Informatics Olympiad) Romanian dataset with accurate English\ntranslations, enhancing its utility for future LLM training and evaluation.\nThrough detailed syntactic and semantic analyses, we confirm that with human\noversight, LLMs can serve as a viable solution for multilingual\nproblem-solving. We also compare the translation quality of LLMs against human\ntranslators, as evaluated by a certified expert, underscoring the potential of\nLLMs in realworld scenarios.",
        "snippets": [
            "Recent studies have suggested that large language models (LLMs) underperform\non mathematical and computer science tasks when these problems are translated\nfrom Romanian into English, compared to their original Romanian format.\nAccurate translation is critical for applications ranging from automatic\ntranslations in programming competitions to the creation of high-quality\neducational materials, as well as minimizing errors or fraud in human\ntranslations. This study shows that robust large language models (LLMs) can\nmaintain or even enhance their performance in translating less common languages\nwhen given well-structured prompts. Our findings suggest that LLMs, with\nappropriate supervision, can be reliably used for the automatic translation of\nIOI (International Olympiad in Informatics)-style tasks. We evaluate several\ntranslation methods across multiple LLMs, including OpenRoLLM, Llama 3.1 8B,\nLlama 3.2 3B and GPT-4o, assessing their translation accuracy and performance\nstability through repeated runs. Additionally, we augment the OJI (Romanian\nCounty-Level Informatics Olympiad) Romanian dataset with accurate English\ntranslations, enhancing its utility for future LLM training and evaluation.\nThrough detailed syntactic and semantic analyses, we confirm that with human\noversight, LLMs can serve as a viable solution for multilingual\nproblem-solving. We also compare the translation quality of LLMs against human\ntranslators, as evaluated by a certified expert, underscoring the potential of\nLLMs in realworld scenarios."
        ],
        "title": "Exploring Large Language Models for Translating Romanian Computational Problems into English",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2506.02025v1": {
        "url": "http://arxiv.org/abs/2506.02025v1",
        "description": "High-Performance Computing (HPC) job scheduling involves balancing\nconflicting objectives such as minimizing makespan, reducing wait times,\noptimizing resource use, and ensuring fairness. Traditional methods, including\nheuristic-based (e.g., First-Come-First-Served) or intensive optimization\ntechniques, often lack adaptability to dynamic workloads and heterogeneous HPC\nsystems. To address this, we propose a novel Large Language Model (LLM)-based\nscheduler using a ReAct-style framework (Reason + Act), enabling iterative,\ninterpretable decision-making. The system incorporates a scratchpad memory to\ntrack scheduling history and refine decisions via natural language feedback,\nwhile a constraint enforcement module ensures feasibility and safety. We\nevaluate our approach using OpenAI's O4-Mini and Anthropic's Claude 3.7 across\nseven real-world HPC workload scenarios, including heterogeneous mixes, bursty\npatterns, and adversarial cases. Comparisons against FCFS, Shortest Job First,\nand Google OR-Tools (on 10 to 100 jobs) reveal that LLM-based scheduling\neffectively balances multiple objectives while offering transparent reasoning\nthrough natural language traces. The method excels in constraint satisfaction\nand adapts to diverse workloads without domain-specific training. However, a\ntrade-off between reasoning quality and computational overhead challenges\nreal-time deployment. This work presents the first comprehensive study of\nreasoning-capable LLMs for HPC scheduling, demonstrating their potential to\nhandle multiobjective optimization while highlighting limitations in\ncomputational efficiency. The findings provide insights into leveraging\nadvanced language models for complex scheduling problems in dynamic HPC\nenvironments.",
        "snippets": [
            "High-Performance Computing (HPC) job scheduling involves balancing\nconflicting objectives such as minimizing makespan, reducing wait times,\noptimizing resource use, and ensuring fairness. Traditional methods, including\nheuristic-based (e.g., First-Come-First-Served) or intensive optimization\ntechniques, often lack adaptability to dynamic workloads and heterogeneous HPC\nsystems. To address this, we propose a novel Large Language Model (LLM)-based\nscheduler using a ReAct-style framework (Reason + Act), enabling iterative,\ninterpretable decision-making. The system incorporates a scratchpad memory to\ntrack scheduling history and refine decisions via natural language feedback,\nwhile a constraint enforcement module ensures feasibility and safety. We\nevaluate our approach using OpenAI's O4-Mini and Anthropic's Claude 3.7 across\nseven real-world HPC workload scenarios, including heterogeneous mixes, bursty\npatterns, and adversarial cases. Comparisons against FCFS, Shortest Job First,\nand Google OR-Tools (on 10 to 100 jobs) reveal that LLM-based scheduling\neffectively balances multiple objectives while offering transparent reasoning\nthrough natural language traces. The method excels in constraint satisfaction\nand adapts to diverse workloads without domain-specific training. However, a\ntrade-off between reasoning quality and computational overhead challenges\nreal-time deployment. This work presents the first comprehensive study of\nreasoning-capable LLMs for HPC scheduling, demonstrating their potential to\nhandle multiobjective optimization while highlighting limitations in\ncomputational efficiency. The findings provide insights into leveraging\nadvanced language models for complex scheduling problems in dynamic HPC\nenvironments."
        ],
        "title": "Evaluating the Efficacy of LLM-Based Reasoning for Multiobjective HPC Job Scheduling",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.00722v2": {
        "url": "http://arxiv.org/abs/2502.00722v2",
        "description": "Recent advancements in Large Language Models (LLMs) have led to increasingly\ndiverse requests, accompanied with varying resource (compute and memory)\ndemands to serve them. However, this in turn degrades the cost-efficiency of\nLLM serving as common practices primarily rely on homogeneous GPU resources. In\nresponse to this problem, this work conducts a thorough study about serving\nLLMs over heterogeneous GPU resources on cloud platforms. The rationale is that\ndifferent GPU types exhibit distinct compute and memory characteristics,\naligning well with the divergent resource demands of diverse requests.\nParticularly, through comprehensive benchmarking, we discover that the\ncost-efficiency of LLM serving can be substantially optimized by meticulously\ndetermining GPU composition, deployment configurations, and workload\nassignments. Subsequently, we design a scheduling algorithm via mixed-integer\nlinear programming, aiming at deducing the most cost-efficient serving plan\nunder the constraints of price budget and real-time GPU availability.\nRemarkably, our approach effectively outperforms homogeneous and heterogeneous\nbaselines under a wide array of scenarios, covering diverse workload traces,\nvarying GPU availablilities, and multi-model serving. This casts new light on\nmore accessible and efficient LLM serving over heterogeneous cloud resources.",
        "snippets": [
            "Recent advancements in Large Language Models (LLMs) have led to increasingly\ndiverse requests, accompanied with varying resource (compute and memory)\ndemands to serve them. However, this in turn degrades the cost-efficiency of\nLLM serving as common practices primarily rely on homogeneous GPU resources. In\nresponse to this problem, this work conducts a thorough study about serving\nLLMs over heterogeneous GPU resources on cloud platforms. The rationale is that\ndifferent GPU types exhibit distinct compute and memory characteristics,\naligning well with the divergent resource demands of diverse requests.\nParticularly, through comprehensive benchmarking, we discover that the\ncost-efficiency of LLM serving can be substantially optimized by meticulously\ndetermining GPU composition, deployment configurations, and workload\nassignments. Subsequently, we design a scheduling algorithm via mixed-integer\nlinear programming, aiming at deducing the most cost-efficient serving plan\nunder the constraints of price budget and real-time GPU availability.\nRemarkably, our approach effectively outperforms homogeneous and heterogeneous\nbaselines under a wide array of scenarios, covering diverse workload traces,\nvarying GPU availablilities, and multi-model serving. This casts new light on\nmore accessible and efficient LLM serving over heterogeneous cloud resources."
        ],
        "title": "Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.05096v1": {
        "url": "http://arxiv.org/abs/2503.05096v1",
        "description": "Large Language Model (LLM) services often face challenges in achieving low\ninference latency and meeting Service Level Objectives (SLOs) under dynamic\nrequest patterns. Speculative decoding, which exploits lightweight models for\ndrafting and LLMs for verification, has emerged as a compelling technique to\naccelerate LLM inference. However, existing speculative decoding solutions\noften fail to adapt to varying workloads and system environments, resulting in\nperformance variability and SLO violations. In this paper, we introduce\nSpecServe, an efficient LLM inference system that dynamically adjusts\nspeculative strategies according to real-time request loads and system\nconfigurations. SpecServe proposes a theoretical model to understand and\npredict the efficiency of speculative decoding across diverse scenarios.\nAdditionally, it implements intelligent drafting and verification algorithms to\nguarantee optimal performance while achieving high SLO attainment. Experimental\nresults on real-world LLM traces demonstrate that SpecServe consistently meets\nSLOs and achieves substantial performance improvements, yielding\n1.14$\\times$-14.3$\\times$ speedups over state-of-the-art speculative inference\nsystems.",
        "snippets": [
            "Large Language Model (LLM) services often face challenges in achieving low\ninference latency and meeting Service Level Objectives (SLOs) under dynamic\nrequest patterns. Speculative decoding, which exploits lightweight models for\ndrafting and LLMs for verification, has emerged as a compelling technique to\naccelerate LLM inference. However, existing speculative decoding solutions\noften fail to adapt to varying workloads and system environments, resulting in\nperformance variability and SLO violations. In this paper, we introduce\nSpecServe, an efficient LLM inference system that dynamically adjusts\nspeculative strategies according to real-time request loads and system\nconfigurations. SpecServe proposes a theoretical model to understand and\npredict the efficiency of speculative decoding across diverse scenarios.\nAdditionally, it implements intelligent drafting and verification algorithms to\nguarantee optimal performance while achieving high SLO attainment. Experimental\nresults on real-world LLM traces demonstrate that SpecServe consistently meets\nSLOs and achieves substantial performance improvements, yielding\n1.14$\\times$-14.3$\\times$ speedups over state-of-the-art speculative inference\nsystems."
        ],
        "title": "SpecServe: Efficient and SLO-Aware Large Language Model Serving with Adaptive Speculative Decoding",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2401.07886v2": {
        "url": "http://arxiv.org/abs/2401.07886v2",
        "description": "Many applications must provide low-latency LLM service to users or risk\nunacceptable user experience. However, over-provisioning resources to serve\nfluctuating request patterns is often prohibitively expensive. In this work, we\npresent a best-effort serving system that employs deep reinforcement learning\nto adjust service quality based on the task distribution and system load. Our\nbest-effort system can maintain availability with over 10x higher client\nrequest rates, serves above 96% of peak performance 4.1x more often, and serves\nabove 98% of peak performance 2.3x more often than static serving on\nunpredictable workloads. Our learned router is robust to shifts in both the\narrival and task distribution. Compared to static serving, learned best-effort\nserving allows for cost-efficient serving through increased hardware utility.\nAdditionally, we argue that learned best-effort LLM serving is applicable in\nwide variety of settings and provides application developers great flexibility\nto meet their specific needs.",
        "snippets": [
            "Many applications must provide low-latency LLM service to users or risk\nunacceptable user experience. However, over-provisioning resources to serve\nfluctuating request patterns is often prohibitively expensive. In this work, we\npresent a best-effort serving system that employs deep reinforcement learning\nto adjust service quality based on the task distribution and system load. Our\nbest-effort system can maintain availability with over 10x higher client\nrequest rates, serves above 96% of peak performance 4.1x more often, and serves\nabove 98% of peak performance 2.3x more often than static serving on\nunpredictable workloads. Our learned router is robust to shifts in both the\narrival and task distribution. Compared to static serving, learned best-effort\nserving allows for cost-efficient serving through increased hardware utility.\nAdditionally, we argue that learned best-effort LLM serving is applicable in\nwide variety of settings and provides application developers great flexibility\nto meet their specific needs."
        ],
        "title": "Learned Best-Effort LLM Serving",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2404.02523v1": {
        "url": "http://arxiv.org/abs/2404.02523v1",
        "description": "Visual affordance learning is a key component for robots to understand how to\ninteract with objects. Conventional approaches in this field rely on\npre-defined objects and actions, falling short of capturing diverse\ninteractions in realworld scenarios. The key idea of our approach is employing\ntextual instruction, targeting various affordances for a wide range of objects.\nThis approach covers both hand-object and tool-object interactions. We\nintroduce text-driven affordance learning, aiming to learn contact points and\nmanipulation trajectories from an egocentric view following textual\ninstruction. In our task, contact points are represented as heatmaps, and the\nmanipulation trajectory as sequences of coordinates that incorporate both\nlinear and rotational movements for various manipulations. However, when we\ngather data for this task, manual annotations of these diverse interactions are\ncostly. To this end, we propose a pseudo dataset creation pipeline and build a\nlarge pseudo-training dataset: TextAFF80K, consisting of over 80K instances of\nthe contact points, trajectories, images, and text tuples. We extend existing\nreferring expression comprehension models for our task, and experimental\nresults show that our approach robustly handles multiple affordances, serving\nas a new standard for affordance learning in real-world scenarios.",
        "snippets": [
            "Visual affordance learning is a key component for robots to understand how to\ninteract with objects. Conventional approaches in this field rely on\npre-defined objects and actions, falling short of capturing diverse\ninteractions in realworld scenarios. The key idea of our approach is employing\ntextual instruction, targeting various affordances for a wide range of objects.\nThis approach covers both hand-object and tool-object interactions. We\nintroduce text-driven affordance learning, aiming to learn contact points and\nmanipulation trajectories from an egocentric view following textual\ninstruction. In our task, contact points are represented as heatmaps, and the\nmanipulation trajectory as sequences of coordinates that incorporate both\nlinear and rotational movements for various manipulations. However, when we\ngather data for this task, manual annotations of these diverse interactions are\ncostly. To this end, we propose a pseudo dataset creation pipeline and build a\nlarge pseudo-training dataset: TextAFF80K, consisting of over 80K instances of\nthe contact points, trajectories, images, and text tuples. We extend existing\nreferring expression comprehension models for our task, and experimental\nresults show that our approach robustly handles multiple affordances, serving\nas a new standard for affordance learning in real-world scenarios."
        ],
        "title": "Text-driven Affordance Learning from Egocentric Vision",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2501.14312v1": {
        "url": "http://arxiv.org/abs/2501.14312v1",
        "description": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency.",
        "snippets": [
            "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency."
        ],
        "title": "Locality-aware Fair Scheduling in LLM Serving",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.08415v2": {
        "url": "http://arxiv.org/abs/2503.08415v2",
        "description": "The increasing demand for large language model (LLM) serving has necessitated\nsignificant advancements in the optimization and profiling of LLM inference\nsystems. As these models become integral to a wide range of applications, the\nneed for efficient and scalable serving solutions has grown exponentially. This\nwork introduces TokenSim, a comprehensive hardware and software exploration\nsystem designed specifically for LLM inference. TokenSim is characterized by\nits support for extensible system optimizations including scheduling and memory\nmanagement. We validate the results with systems running with realworld\ndatasets, achieving an error rate of less than 1%. Furthermore, TokenSim\nfacilitates various insightful explorations into the performance and\noptimization of LLM serving systems.",
        "snippets": [
            "The increasing demand for large language model (LLM) serving has necessitated\nsignificant advancements in the optimization and profiling of LLM inference\nsystems. As these models become integral to a wide range of applications, the\nneed for efficient and scalable serving solutions has grown exponentially. This\nwork introduces TokenSim, a comprehensive hardware and software exploration\nsystem designed specifically for LLM inference. TokenSim is characterized by\nits support for extensible system optimizations including scheduling and memory\nmanagement. We validate the results with systems running with realworld\ndatasets, achieving an error rate of less than 1%. Furthermore, TokenSim\nfacilitates various insightful explorations into the performance and\noptimization of LLM serving systems."
        ],
        "title": "TokenSim: Enabling Hardware and Software Exploration for Large Language Model Inference Systems",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2408.13257v3": {
        "url": "http://arxiv.org/abs/2408.13257v3",
        "description": "Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has\nrecently garnered widespread attention in the research community. However, we\nobserve that existing benchmarks present several common barriers that make it\ndifficult to measure the significant challenges that models face in the real\nworld, including: 1) small data scale leads to a large performance variance; 2)\nreliance on model-based annotations results in restricted data quality; 3)\ninsufficient task difficulty, especially caused by the limited image\nresolution. To tackle these issues, we introduce MME-RealWorld. Specifically,\nwe collect more than $300$K images from public datasets and the Internet,\nfiltering $13,366$ high-quality images for annotation. This involves the\nefforts of professional $25$ annotators and $7$ experts in MLLMs, contributing\nto $29,429$ question-answer pairs that cover $43$ subtasks across $5$\nreal-world scenarios, extremely challenging even for humans. As far as we know,\nMME-RealWorld is the largest manually annotated benchmark to date, featuring\nthe highest resolution and a targeted focus on real-world applications. We\nfurther conduct a thorough evaluation involving $28$ prominent MLLMs, such as\nGPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n$60\\%$ accuracy. The challenges of perceiving high-resolution images and\nunderstanding complex real-world scenarios remain urgent issues to be\naddressed. The data and evaluation code are released at\nhttps://mme-realworld.github.io/ .",
        "snippets": [
            "Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has\nrecently garnered widespread attention in the research community. However, we\nobserve that existing benchmarks present several common barriers that make it\ndifficult to measure the significant challenges that models face in the real\nworld, including: 1) small data scale leads to a large performance variance; 2)\nreliance on model-based annotations results in restricted data quality; 3)\ninsufficient task difficulty, especially caused by the limited image\nresolution. To tackle these issues, we introduce MME-RealWorld. Specifically,\nwe collect more than $300$K images from public datasets and the Internet,\nfiltering $13,366$ high-quality images for annotation. This involves the\nefforts of professional $25$ annotators and $7$ experts in MLLMs, contributing\nto $29,429$ question-answer pairs that cover $43$ subtasks across $5$\nreal-world scenarios, extremely challenging even for humans. As far as we know,\nMME-RealWorld is the largest manually annotated benchmark to date, featuring\nthe highest resolution and a targeted focus on real-world applications. We\nfurther conduct a thorough evaluation involving $28$ prominent MLLMs, such as\nGPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n$60\\%$ accuracy. The challenges of perceiving high-resolution images and\nunderstanding complex real-world scenarios remain urgent issues to be\naddressed. The data and evaluation code are released at\nhttps://mme-realworld.github.io/ ."
        ],
        "title": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2412.18106v1": {
        "url": "http://arxiv.org/abs/2412.18106v1",
        "description": "Meeting growing demands for low latency and cost efficiency in\nproduction-grade large language model (LLM) serving systems requires\nintegrating advanced optimization techniques. However, dynamic and\nunpredictable input-output lengths of LLM, compounded by these optimizations,\nexacerbate the issues of workload variability, making it difficult to maintain\nhigh efficiency on AI accelerators, especially DSAs with tile-based programming\nmodels. To address this challenge, we introduce XY-Serve, a versatile, Ascend\nnative, end-to-end production LLM-serving system. The core idea is an\nabstraction mechanism that smooths out the workload variability by decomposing\ncomputations into unified, hardware-friendly, fine-grained meta primitives. For\nattention, we propose a meta-kernel that computes the basic pattern of\nmatmul-softmax-matmul with architectural-aware tile sizes. For GEMM, we\nintroduce a virtual padding scheme that adapts to dynamic shape changes while\nusing highly efficient GEMM primitives with assorted fixed tile sizes. XY-Serve\nsits harmoniously with vLLM. Experimental results show up to 89% end-to-end\nthroughput improvement compared with current publicly available baselines on\nAscend NPUs. Additionally, our approach outperforms existing GEMM (average\n14.6% faster) and attention (average 21.5% faster) kernels relative to existing\nlibraries. While the work is Ascend native, we believe the approach can be\nreadily applicable to SIMT architectures as well.",
        "snippets": [
            "Meeting growing demands for low latency and cost efficiency in\nproduction-grade large language model (LLM) serving systems requires\nintegrating advanced optimization techniques. However, dynamic and\nunpredictable input-output lengths of LLM, compounded by these optimizations,\nexacerbate the issues of workload variability, making it difficult to maintain\nhigh efficiency on AI accelerators, especially DSAs with tile-based programming\nmodels. To address this challenge, we introduce XY-Serve, a versatile, Ascend\nnative, end-to-end production LLM-serving system. The core idea is an\nabstraction mechanism that smooths out the workload variability by decomposing\ncomputations into unified, hardware-friendly, fine-grained meta primitives. For\nattention, we propose a meta-kernel that computes the basic pattern of\nmatmul-softmax-matmul with architectural-aware tile sizes. For GEMM, we\nintroduce a virtual padding scheme that adapts to dynamic shape changes while\nusing highly efficient GEMM primitives with assorted fixed tile sizes. XY-Serve\nsits harmoniously with vLLM. Experimental results show up to 89% end-to-end\nthroughput improvement compared with current publicly available baselines on\nAscend NPUs. Additionally, our approach outperforms existing GEMM (average\n14.6% faster) and attention (average 21.5% faster) kernels relative to existing\nlibraries. While the work is Ascend native, we believe the approach can be\nreadily applicable to SIMT architectures as well."
        ],
        "title": "Tackling the Dynamicity in a Production LLM Serving System with SOTA Optimizations via Hybrid Prefill/Decode/Verify Scheduling on Efficient Meta-kernels",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.05370v1": {
        "url": "http://arxiv.org/abs/2502.05370v1",
        "description": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions.",
        "snippets": [
            "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions."
        ],
        "title": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts Serving",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.11916v1": {
        "url": "http://arxiv.org/abs/2505.11916v1",
        "description": "Existing large language models (LLMs) serving systems typically employ\nPrefill-Decode disaggregated architecture to prevent computational interference\nbetween the prefill and decode phases. However, real-world LLM serving\nscenarios often exhibit significant fluctuations in request input/output\nlengths, causing traditional static prefill/decode node configuration ratio to\nresult in imbalanced computational loads between these two nodes, consequently\npreventing efficient utilization of computing resources to improve the system's\ngoodput. To address this challenge, we design and implement Arrow, an adaptive\nscheduler that leverages stateless instances and elastic instance pools to\nachieve efficient adaptive request and instance scheduling. Arrow dynamically\nadjusts the number of instances handling prefill and decode tasks based on\nreal-time cluster performance metrics, significantly enhancing the system's\ncapability to handle traffic spikes and load variations. Our evaluation under\ndiverse real-world workloads shows that Arrow achieves up to $5.62 \\times$ and\n$7.78 \\times$ higher request serving rates compared to state-of-the-art\nPD-colocated and PD-disaggregated serving systems respectively.",
        "snippets": [
            "Existing large language models (LLMs) serving systems typically employ\nPrefill-Decode disaggregated architecture to prevent computational interference\nbetween the prefill and decode phases. However, real-world LLM serving\nscenarios often exhibit significant fluctuations in request input/output\nlengths, causing traditional static prefill/decode node configuration ratio to\nresult in imbalanced computational loads between these two nodes, consequently\npreventing efficient utilization of computing resources to improve the system's\ngoodput. To address this challenge, we design and implement Arrow, an adaptive\nscheduler that leverages stateless instances and elastic instance pools to\nachieve efficient adaptive request and instance scheduling. Arrow dynamically\nadjusts the number of instances handling prefill and decode tasks based on\nreal-time cluster performance metrics, significantly enhancing the system's\ncapability to handle traffic spikes and load variations. Our evaluation under\ndiverse real-world workloads shows that Arrow achieves up to $5.62 \\times$ and\n$7.78 \\times$ higher request serving rates compared to state-of-the-art\nPD-colocated and PD-disaggregated serving systems respectively."
        ],
        "title": "Arrow: Adaptive Scheduling Mechanisms for Disaggregated LLM Inference Architecture",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2502.00022v1": {
        "url": "http://arxiv.org/abs/2502.00022v1",
        "description": "HRA (Human Reliability Analysis) data is crucial for advancing HRA\nmethodologies. however, existing data collection methods lack the necessary\ngranularity, and most approaches fail to capture dynamic features.\nAdditionally, many methods require expert knowledge as input, making them\ntime-consuming and labor-intensive. To address these challenges, we propose a\nnew paradigm for the automated collection of HRA data. Our approach focuses on\nkey indicators behind human error, specifically measuring workload in\ncollaborative settings. This study introduces a novel, scenario-driven method\nfor workload estimation, leveraging fine-tuned large language models (LLMs). By\ntraining LLMs on real-world operational data from high-temperature gas-cooled\nreactors (HTGRs), we simulate human behavior and cognitive load in real time\nacross various collaborative scenarios. The method dynamically adapts to\nchanges in operator workload, providing more accurate, flexible, and scalable\nworkload estimates. The results demonstrate that the proposed WELLA (Workload\nEstimation with LLMs and Agents) outperforms existing commercial LLM-based\nmethods in terms of prediction accuracy.",
        "snippets": [
            "HRA (Human Reliability Analysis) data is crucial for advancing HRA\nmethodologies. however, existing data collection methods lack the necessary\ngranularity, and most approaches fail to capture dynamic features.\nAdditionally, many methods require expert knowledge as input, making them\ntime-consuming and labor-intensive. To address these challenges, we propose a\nnew paradigm for the automated collection of HRA data. Our approach focuses on\nkey indicators behind human error, specifically measuring workload in\ncollaborative settings. This study introduces a novel, scenario-driven method\nfor workload estimation, leveraging fine-tuned large language models (LLMs). By\ntraining LLMs on real-world operational data from high-temperature gas-cooled\nreactors (HTGRs), we simulate human behavior and cognitive load in real time\nacross various collaborative scenarios. The method dynamically adapts to\nchanges in operator workload, providing more accurate, flexible, and scalable\nworkload estimates. The results demonstrate that the proposed WELLA (Workload\nEstimation with LLMs and Agents) outperforms existing commercial LLM-based\nmethods in terms of prediction accuracy."
        ],
        "title": "A Dynamic and High-Precision Method for Scenario-Based HRA Synthetic Data Collection in Multi-Agent Collaborative Environments Driven by LLMs",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2404.07947v1": {
        "url": "http://arxiv.org/abs/2404.07947v1",
        "description": "This paper presents ExeGPT, a distributed system designed for\nconstraint-aware LLM inference. ExeGPT finds and runs with an optimal execution\nschedule to maximize inference throughput while satisfying a given latency\nconstraint. By leveraging the distribution of input and output sequences, it\neffectively allocates resources and determines optimal execution\nconfigurations, including batch sizes and partial tensor parallelism. We also\nintroduce two scheduling strategies based on Round-Robin Allocation and\nWorkload-Aware Allocation policies, suitable for different NLP workloads. We\nevaluate ExeGPT on six LLM instances of T5, OPT, and GPT-3 and five NLP tasks,\neach with four distinct latency constraints. Compared to FasterTransformer,\nExeGPT achieves up to 15.2x improvements in throughput and 6x improvements in\nlatency. Overall, ExeGPT achieves an average throughput gain of 2.9x across\ntwenty evaluation scenarios. Moreover, when adapting to changing sequence\ndistributions, the cost of adjusting the schedule in ExeGPT is reasonably\nmodest. ExeGPT proves to be an effective solution for optimizing and executing\nLLM inference for diverse NLP workload and serving conditions.",
        "snippets": [
            "This paper presents ExeGPT, a distributed system designed for\nconstraint-aware LLM inference. ExeGPT finds and runs with an optimal execution\nschedule to maximize inference throughput while satisfying a given latency\nconstraint. By leveraging the distribution of input and output sequences, it\neffectively allocates resources and determines optimal execution\nconfigurations, including batch sizes and partial tensor parallelism. We also\nintroduce two scheduling strategies based on Round-Robin Allocation and\nWorkload-Aware Allocation policies, suitable for different NLP workloads. We\nevaluate ExeGPT on six LLM instances of T5, OPT, and GPT-3 and five NLP tasks,\neach with four distinct latency constraints. Compared to FasterTransformer,\nExeGPT achieves up to 15.2x improvements in throughput and 6x improvements in\nlatency. Overall, ExeGPT achieves an average throughput gain of 2.9x across\ntwenty evaluation scenarios. Moreover, when adapting to changing sequence\ndistributions, the cost of adjusting the schedule in ExeGPT is reasonably\nmodest. ExeGPT proves to be an effective solution for optimizing and executing\nLLM inference for diverse NLP workload and serving conditions."
        ],
        "title": "ExeGPT: Constraint-Aware Resource Scheduling for LLM Inference",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.14468v1": {
        "url": "http://arxiv.org/abs/2505.14468v1",
        "description": "Serverless computing has grown rapidly for serving Large Language Model (LLM)\ninference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid\nscaling. However, our analysis reveals that current serverless can effectively\nserve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to\nthree key limitations: 1) massive parameter redundancy among functions where\n99% of weights are unnecessarily duplicated, 2) costly artifact loading latency\nbeyond LLM loading, and 3) magnified resource contention when serving multiple\nLoRA LLMs. These inefficiencies lead to massive GPU wastage, increased\nTime-To-First-Token (TTFT), and high monetary costs.\n  We propose ServerlessLoRA, a novel serverless inference system designed for\nfaster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM\nsharing across isolated LoRA functions to reduce redundancy. We design a\npre-loading method that pre-loads comprehensive LoRA artifacts to minimize\ncold-start latency. Furthermore, ServerlessLoRA employs contention aware\nbatching and offloading to mitigate GPU resource conflicts during bursty\nworkloads. Experiment on industrial workloads demonstrates that ServerlessLoRA\nreduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to\nstate-of-the-art LLM inference solutions.",
        "snippets": [
            "Serverless computing has grown rapidly for serving Large Language Model (LLM)\ninference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid\nscaling. However, our analysis reveals that current serverless can effectively\nserve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to\nthree key limitations: 1) massive parameter redundancy among functions where\n99% of weights are unnecessarily duplicated, 2) costly artifact loading latency\nbeyond LLM loading, and 3) magnified resource contention when serving multiple\nLoRA LLMs. These inefficiencies lead to massive GPU wastage, increased\nTime-To-First-Token (TTFT), and high monetary costs.\n  We propose ServerlessLoRA, a novel serverless inference system designed for\nfaster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM\nsharing across isolated LoRA functions to reduce redundancy. We design a\npre-loading method that pre-loads comprehensive LoRA artifacts to minimize\ncold-start latency. Furthermore, ServerlessLoRA employs contention aware\nbatching and offloading to mitigate GPU resource conflicts during bursty\nworkloads. Experiment on industrial workloads demonstrates that ServerlessLoRA\nreduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to\nstate-of-the-art LLM inference solutions."
        ],
        "title": "ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1509.03721v1": {
        "url": "http://arxiv.org/abs/1509.03721v1",
        "description": "The initial location of data in DRAMs is determined and controlled by the\n'address-mapping' and even modern memory controllers use a fixed and\nrun-time-agnostic address mapping. On the other hand, the memory access pattern\nseen at the memory interface level will dynamically change at run-time. This\ndynamic nature of memory access pattern and the fixed behavior of address\nmapping process in DRAM controllers, implied by using a fixed address mapping\nscheme, means that DRAM performance cannot be exploited efficiently. DReAM is a\nnovel hardware technique that can detect a workload-specific address mapping at\nrun-time based on the application access pattern which improves the performance\nof DRAMs. The experimental results show that DReAM outperforms the best\nevaluated address mapping on average by 9%, for mapping-sensitive workloads, by\n2% for mapping-insensitive workloads, and up to 28% across all the workloads.\nDReAM can be seen as an insurance policy capable of detecting which scenarios\nare not well served by the predefined address mapping.",
        "snippets": [
            "The initial location of data in DRAMs is determined and controlled by the\n'address-mapping' and even modern memory controllers use a fixed and\nrun-time-agnostic address mapping. On the other hand, the memory access pattern\nseen at the memory interface level will dynamically change at run-time. This\ndynamic nature of memory access pattern and the fixed behavior of address\nmapping process in DRAM controllers, implied by using a fixed address mapping\nscheme, means that DRAM performance cannot be exploited efficiently. DReAM is a\nnovel hardware technique that can detect a workload-specific address mapping at\nrun-time based on the application access pattern which improves the performance\nof DRAMs. The experimental results show that DReAM outperforms the best\nevaluated address mapping on average by 9%, for mapping-sensitive workloads, by\n2% for mapping-insensitive workloads, and up to 28% across all the workloads.\nDReAM can be seen as an insurance policy capable of detecting which scenarios\nare not well served by the predefined address mapping."
        ],
        "title": "DReAM: Dynamic Re-arrangement of Address Mapping to Improve the Performance of DRAMs",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2503.14649v2": {
        "url": "http://arxiv.org/abs/2503.14649v2",
        "description": "Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions.",
        "snippets": [
            "Retrieval-augmented generation (RAG), which combines large language models\n(LLMs) with retrievals from external knowledge databases, is emerging as a\npopular approach for reliable LLM serving. However, efficient RAG serving\nremains an open challenge due to the rapid emergence of many RAG variants and\nthe substantial differences in workload characteristics across them. In this\npaper, we make three fundamental contributions to advancing RAG serving. First,\nwe introduce RAGSchema, a structured abstraction that captures the wide range\nof RAG algorithms, serving as a foundation for performance optimization.\nSecond, we analyze several representative RAG workloads with distinct\nRAGSchema, revealing significant performance variability across these\nworkloads. Third, to address this variability and meet diverse performance\nrequirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a\nsystem optimization framework for efficient RAG serving. Our evaluation shows\nthat RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in\ntime-to-first-token latency compared to RAG systems built on LLM-system\nextensions."
        ],
        "title": "RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.09319v1": {
        "url": "http://arxiv.org/abs/2505.09319v1",
        "description": "Large Language Model (LLM) inference systems present significant challenges\nin statistical performance characterization due to dynamic workload variations,\ndiverse hardware architectures, and complex interactions between model size,\nbatch processing, and throughput requirements. Accurate statistical\ncharacterization enables better workload scheduling, adaptive resource\nprovisioning, and cost-aware inference optimization, making it crucial for\nimproving efficiency in large-scale AI deployments. Traditional analytical\nmodels provide explainability but cannot cover the vast diversity of real-world\nworkloads, making it impossible to benchmark every scenario in advance. Machine\nlearning (ML) approaches effectively predict performance for non-benchmarked\ncases but struggle when extrapolating beyond their observed training space. To\naddress these limitations for LLM inference systems, we propose an Analytical\nwith Learning Augmentation (ALA) framework that bridges analytical modeling\nwith \\ml for robust statistical prediction and uncertainty estimation in LLM\ninference workloads. Our method employs an analytical throughput model with\nparameters estimated for benchmarked workloads, then extends to unobserved\nconfigurations using \\ml predictions. We enhance this with simulated annealing\nto exploit subsets of the workload data point combinations and develop an error\npredictor. Finally, we quantify uncertainty based on vector space similarity\nbetween new and observed workloads to ensure robust generalization. Through\nextensive experimentation on diverse LLM inference workloads, we demonstrate\nthat our framework achieves low median errors while maintaining adaptability to\nnew inference scenarios.",
        "snippets": [
            "Large Language Model (LLM) inference systems present significant challenges\nin statistical performance characterization due to dynamic workload variations,\ndiverse hardware architectures, and complex interactions between model size,\nbatch processing, and throughput requirements. Accurate statistical\ncharacterization enables better workload scheduling, adaptive resource\nprovisioning, and cost-aware inference optimization, making it crucial for\nimproving efficiency in large-scale AI deployments. Traditional analytical\nmodels provide explainability but cannot cover the vast diversity of real-world\nworkloads, making it impossible to benchmark every scenario in advance. Machine\nlearning (ML) approaches effectively predict performance for non-benchmarked\ncases but struggle when extrapolating beyond their observed training space. To\naddress these limitations for LLM inference systems, we propose an Analytical\nwith Learning Augmentation (ALA) framework that bridges analytical modeling\nwith \\ml for robust statistical prediction and uncertainty estimation in LLM\ninference workloads. Our method employs an analytical throughput model with\nparameters estimated for benchmarked workloads, then extends to unobserved\nconfigurations using \\ml predictions. We enhance this with simulated annealing\nto exploit subsets of the workload data point combinations and develop an error\npredictor. Finally, we quantify uncertainty based on vector space similarity\nbetween new and observed workloads to ensure robust generalization. Through\nextensive experimentation on diverse LLM inference workloads, we demonstrate\nthat our framework achieves low median errors while maintaining adaptability to\nnew inference scenarios."
        ],
        "title": "Statistical Modeling and Uncertainty Estimation of LLM Inference Systems",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2505.24095v1": {
        "url": "http://arxiv.org/abs/2505.24095v1",
        "description": "Serving Large Language Models (LLMs) efficiently in multi-region setups\nremains a challenge. Due to cost and GPU availability concerns, providers\ntypically deploy LLMs in multiple regions using instance with long-term\ncommitments, like reserved instances or on-premise clusters, which are often\nunderutilized due to their region-local traffic handling and diurnal traffic\nvariance. In this paper, we introduce SkyLB, a locality-aware multi-region load\nbalancer for LLM inference that aggregates regional diurnal patterns through\ncross-region traffic handling. By doing so, SkyLB enables providers to reserve\ninstances based on expected global demand, rather than peak demand in each\nindividual region. Meanwhile, SkyLB preserves KV-Cache locality and a balanced\nload, ensuring cost efficiency without sacrificing performance. SkyLB achieves\nthis with a cache-aware cross-region traffic handler and a selective pushing\nload balancing mechanism based on checking pending requests. Our evaluation on\nreal-world workloads shows that it achieves 1.12-2.06x higher throughput and\n1.74-6.30x lower latency compared to existing load balancers, while reducing\ntotal serving cost by 25%.",
        "snippets": [
            "Serving Large Language Models (LLMs) efficiently in multi-region setups\nremains a challenge. Due to cost and GPU availability concerns, providers\ntypically deploy LLMs in multiple regions using instance with long-term\ncommitments, like reserved instances or on-premise clusters, which are often\nunderutilized due to their region-local traffic handling and diurnal traffic\nvariance. In this paper, we introduce SkyLB, a locality-aware multi-region load\nbalancer for LLM inference that aggregates regional diurnal patterns through\ncross-region traffic handling. By doing so, SkyLB enables providers to reserve\ninstances based on expected global demand, rather than peak demand in each\nindividual region. Meanwhile, SkyLB preserves KV-Cache locality and a balanced\nload, ensuring cost efficiency without sacrificing performance. SkyLB achieves\nthis with a cache-aware cross-region traffic handler and a selective pushing\nload balancing mechanism based on checking pending requests. Our evaluation on\nreal-world workloads shows that it achieves 1.12-2.06x higher throughput and\n1.74-6.30x lower latency compared to existing load balancers, while reducing\ntotal serving cost by 25%."
        ],
        "title": "SkyLB: A Locality-Aware Cross-Region Load Balancer for LLM Inference",
        "meta": {
            "query": "KVcache workload patterns in realworld LLM serving scenarios"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2409.12299v1": {
        "url": "http://arxiv.org/abs/2409.12299v1",
        "description": "Web applications, accessible via web browsers over the Internet, facilitate\ncomplex functionalities without local software installation. In the context of\nweb applications, a workload refers to the number of user requests sent by\nusers or applications to the underlying system. Existing studies have leveraged\nweb application workloads to achieve various objectives, such as workload\nprediction and auto-scaling. However, these studies are conducted in an ad hoc\nmanner, lacking a systematic understanding of the characteristics of web\napplication workloads. In this study, we first conduct a systematic literature\nreview to identify and analyze existing studies leveraging web application\nworkloads. Our analysis sheds light on their workload utilization, analysis\ntechniques, and high-level objectives. We further systematically analyze the\ncharacteristics of the web application workloads identified in the literature\nreview. Our analysis centers on characterizing these workloads at two distinct\ntemporal granularities: daily and weekly. We successfully identify and\ncategorize three daily and three weekly patterns within the workloads. By\nproviding a statistical characterization of these workload patterns, our study\nhighlights the uniqueness of each pattern, paving the way for the development\nof realistic workload generation and resource provisioning techniques that can\nbenefit a range of applications and research areas.",
        "snippets": [
            "Web applications, accessible via web browsers over the Internet, facilitate\ncomplex functionalities without local software installation. In the context of\nweb applications, a workload refers to the number of user requests sent by\nusers or applications to the underlying system. Existing studies have leveraged\nweb application workloads to achieve various objectives, such as workload\nprediction and auto-scaling. However, these studies are conducted in an ad hoc\nmanner, lacking a systematic understanding of the characteristics of web\napplication workloads. In this study, we first conduct a systematic literature\nreview to identify and analyze existing studies leveraging web application\nworkloads. Our analysis sheds light on their workload utilization, analysis\ntechniques, and high-level objectives. We further systematically analyze the\ncharacteristics of the web application workloads identified in the literature\nreview. Our analysis centers on characterizing these workloads at two distinct\ntemporal granularities: daily and weekly. We successfully identify and\ncategorize three daily and three weekly patterns within the workloads. By\nproviding a statistical characterization of these workload patterns, our study\nhighlights the uniqueness of each pattern, paving the way for the development\nof realistic workload generation and resource provisioning techniques that can\nbenefit a range of applications and research areas."
        ],
        "title": "Understanding Web Application Workloads and Their Applications: Systematic Literature Review and Characterization",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2403.16288v2": {
        "url": "http://arxiv.org/abs/2403.16288v2",
        "description": "Dragonfly interconnect is a crucial network technology for supercomputers. To\nsupport exascale systems, network resources are shared such that links and\nrouters are not dedicated to any node pair. While link utilization is\nincreased, workload performance is often offset by network contention.\nRecently, intelligent routing built on reinforcement learning demonstrates\nhigher network throughput with lower packet latency. However, its effectiveness\nin reducing workload interference is unknown. In this work, we present\nextensive network simulations to study multi-workload contention under\ndifferent routing mechanisms, intelligent routing and adaptive routing, on a\nlarge-scale Dragonfly system. We develop an enhanced network simulation\ntoolkit, along with a suite of workloads with distinctive communication\npatterns. We also present two metrics to characterize application communication\nintensity. Our analysis focuses on examining how different workloads interfere\nwith each other under different routing mechanisms by inspecting both\napplication-level and network-level metrics. Several key insights are made from\nthe analysis.",
        "snippets": [
            "Dragonfly interconnect is a crucial network technology for supercomputers. To\nsupport exascale systems, network resources are shared such that links and\nrouters are not dedicated to any node pair. While link utilization is\nincreased, workload performance is often offset by network contention.\nRecently, intelligent routing built on reinforcement learning demonstrates\nhigher network throughput with lower packet latency. However, its effectiveness\nin reducing workload interference is unknown. In this work, we present\nextensive network simulations to study multi-workload contention under\ndifferent routing mechanisms, intelligent routing and adaptive routing, on a\nlarge-scale Dragonfly system. We develop an enhanced network simulation\ntoolkit, along with a suite of workloads with distinctive communication\npatterns. We also present two metrics to characterize application communication\nintensity. Our analysis focuses on examining how different workloads interfere\nwith each other under different routing mechanisms by inspecting both\napplication-level and network-level metrics. Several key insights are made from\nthe analysis."
        ],
        "title": "Study of Workload Interference with Intelligent Routing on Dragonfly",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1912.07172v1": {
        "url": "http://arxiv.org/abs/1912.07172v1",
        "description": "The synthetic workload is essential and critical to the performance\nevaluation of database systems. When evaluating the database performance for a\nspecific application, the similarity between synthetic workload and real\napplication workload determines the credibility of evaluation results. However,\nthe workload currently used for performance evaluation is difficult to have the\nsame workload characteristics as the target application, which leads to\ninaccurate evaluation results. To address this problem, we propose a workload\nduplicator (Lauca) that can generate synthetic workloads with highly similar\nperformance metrics for specific applications. To the best of our knowledge,\nLauca is the first application-oriented transactional workload generator. By\ncarefully studying the application-oriented synthetic workload generation\nproblem, we present the key workload characteristics (transaction logic and\ndata access distribution) of online transaction processing (OLTP) applications,\nand propose novel workload characterization and generation algorithms, which\nguarantee the high fidelity of synthetic workloads. We conduct extensive\nexperiments using workloads from TPC-C, SmallBank and micro benchmarks on both\nMySQL and PostgreSQL databases, and experimental results show that Lauca\nconsistently generates high-quality synthetic workloads.",
        "snippets": [
            "The synthetic workload is essential and critical to the performance\nevaluation of database systems. When evaluating the database performance for a\nspecific application, the similarity between synthetic workload and real\napplication workload determines the credibility of evaluation results. However,\nthe workload currently used for performance evaluation is difficult to have the\nsame workload characteristics as the target application, which leads to\ninaccurate evaluation results. To address this problem, we propose a workload\nduplicator (Lauca) that can generate synthetic workloads with highly similar\nperformance metrics for specific applications. To the best of our knowledge,\nLauca is the first application-oriented transactional workload generator. By\ncarefully studying the application-oriented synthetic workload generation\nproblem, we present the key workload characteristics (transaction logic and\ndata access distribution) of online transaction processing (OLTP) applications,\nand propose novel workload characterization and generation algorithms, which\nguarantee the high fidelity of synthetic workloads. We conduct extensive\nexperiments using workloads from TPC-C, SmallBank and micro benchmarks on both\nMySQL and PostgreSQL databases, and experimental results show that Lauca\nconsistently generates high-quality synthetic workloads."
        ],
        "title": "Lauca: Generating Application-Oriented Synthetic Workloads",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1412.2673v2": {
        "url": "http://arxiv.org/abs/1412.2673v2",
        "description": "Since the mid 1990s, grid computing systems have emerged as an analogy for\nmaking computing power as pervasive an easily accessible as an electric power\ngrid. Since then, grid computing systems have been shown to be able to provide\nvery large amounts of storage and computing power to mainly support the\nscientific and engineering research on a wide geographic scale. Understanding\nthe workload characteristics incoming to such systems is a milestone for the\ndesign and the tuning of effective resource management strategies. This is\naccomplished through the workload characterization, where workload\ncharacteristics are analyzed and a possibly realistic model for those is\nobtained. In this paper, we study the workload of some real grid systems by\nusing a data mining approach to build a workload model for job interarrival\ntime and runtime, and a Bayesian approach to capture user correlations and\nusage patterns. The final model is then validated against the workload coming\nfrom a real grid system.",
        "snippets": [
            "Since the mid 1990s, grid computing systems have emerged as an analogy for\nmaking computing power as pervasive an easily accessible as an electric power\ngrid. Since then, grid computing systems have been shown to be able to provide\nvery large amounts of storage and computing power to mainly support the\nscientific and engineering research on a wide geographic scale. Understanding\nthe workload characteristics incoming to such systems is a milestone for the\ndesign and the tuning of effective resource management strategies. This is\naccomplished through the workload characterization, where workload\ncharacteristics are analyzed and a possibly realistic model for those is\nobtained. In this paper, we study the workload of some real grid systems by\nusing a data mining approach to build a workload model for job interarrival\ntime and runtime, and a Bayesian approach to capture user correlations and\nusage patterns. The final model is then validated against the workload coming\nfrom a real grid system."
        ],
        "title": "Mining the Workload of Real Grid Computing Systems",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1611.10316v1": {
        "url": "http://arxiv.org/abs/1611.10316v1",
        "description": "This work studies the behavior of state-of-the-art memory controller designs\nwhen executing scale-out workloads. It considers memory scheduling techniques,\nmemory page management policies, the number of memory channels, and the address\nmapping scheme used. Experimental measurements demonstrate: 1)~Several recently\nproposed memory scheduling policies are not a good match for these scale-out\nworkloads. 2)~The relatively simple First-Ready-First-Come-First-Served\n(FR-FCFS) policy performs consistently better, and 3)~for most of the studied\nworkloads, the even simpler First-Come-First-Served scheduling policy is within\n1\\% of FR-FCFS. 4)~Increasing the number of memory channels offers negligible\nperformance benefits, e.g., performance improves by 1.7\\% on average for\n4-channels vs. 1-channel. 5)~77\\%-90\\% of DRAM rows activations are accessed\nonly once before closure. These observation can guide future development and\noptimization of memory controllers for scale-out workloads.",
        "snippets": [
            "This work studies the behavior of state-of-the-art memory controller designs\nwhen executing scale-out workloads. It considers memory scheduling techniques,\nmemory page management policies, the number of memory channels, and the address\nmapping scheme used. Experimental measurements demonstrate: 1)~Several recently\nproposed memory scheduling policies are not a good match for these scale-out\nworkloads. 2)~The relatively simple First-Ready-First-Come-First-Served\n(FR-FCFS) policy performs consistently better, and 3)~for most of the studied\nworkloads, the even simpler First-Come-First-Served scheduling policy is within\n1\\% of FR-FCFS. 4)~Increasing the number of memory channels offers negligible\nperformance benefits, e.g., performance improves by 1.7\\% on average for\n4-channels vs. 1-channel. 5)~77\\%-90\\% of DRAM rows activations are accessed\nonly once before closure. These observation can guide future development and\noptimization of memory controllers for scale-out workloads."
        ],
        "title": "Memory Controller Design Under Cloud Workloads",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2205.11582v1": {
        "url": "http://arxiv.org/abs/2205.11582v1",
        "description": "In the recent past, characterizing workloads has been attempted to gain a\nfoothold in the emerging serverless cloud market, especially in the large\nproduction cloud clusters of Google, AWS, and so forth. While analyzing and\ncharacterizing real workloads from a large production cloud cluster benefits\ncloud providers, researchers, and daily users, analyzing the workload traces of\nthese clusters has been an arduous task due to the heterogeneous nature of\ndata. This article proposes a scalable infrastructure based on Google's\ndataproc for analyzing the workload traces of cloud environments. We evaluated\nthe functioning of the proposed infrastructure using the workload traces of\nGoogle cloud cluster-usage-traces-v3. We perform the workload characterization\non this dataset, focusing on the heterogeneity of the workload, the variations\nin job durations, aspects of resources consumption, and the overall\navailability of resources provided by the cluster. The findings reported in the\npaper will be beneficial for cloud infrastructure providers and users while\nmanaging the cloud computing resources, especially serverless platforms.",
        "snippets": [
            "In the recent past, characterizing workloads has been attempted to gain a\nfoothold in the emerging serverless cloud market, especially in the large\nproduction cloud clusters of Google, AWS, and so forth. While analyzing and\ncharacterizing real workloads from a large production cloud cluster benefits\ncloud providers, researchers, and daily users, analyzing the workload traces of\nthese clusters has been an arduous task due to the heterogeneous nature of\ndata. This article proposes a scalable infrastructure based on Google's\ndataproc for analyzing the workload traces of cloud environments. We evaluated\nthe functioning of the proposed infrastructure using the workload traces of\nGoogle cloud cluster-usage-traces-v3. We perform the workload characterization\non this dataset, focusing on the heterogeneity of the workload, the variations\nin job durations, aspects of resources consumption, and the overall\navailability of resources provided by the cluster. The findings reported in the\npaper will be beneficial for cloud infrastructure providers and users while\nmanaging the cloud computing resources, especially serverless platforms."
        ],
        "title": "Scalable Infrastructure for Workload Characterization of Cluster Traces",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1801.04306v1": {
        "url": "http://arxiv.org/abs/1801.04306v1",
        "description": "Workload characterization is an integral part of performance analysis of high\nperformance computing (HPC) systems. An understanding of workload properties\nsheds light on resource utilization and can be used to inform performance\noptimization both at the software and system configuration levels. It can\nprovide information on how computational science usage modalities are changing\nthat could potentially aid holistic capacity planning for the wider HPC\necosystem. Here, we report on the results of a detailed workload analysis of\nthe portfolio of supercomputers comprising the NSF Innovative HPC program in\norder to characterize its past and current workload and look for trends to\nunderstand the nature of how the broad portfolio of computational science\nresearch is being supported and how it is changing over time. The workload\nanalysis also sought to illustrate a wide variety of usage patterns and\nperformance requirements for jobs running on these systems. File system\nperformance, memory utilization and the types of parallelism employed by users\n(MPI, threads, etc) were also studied for all systems for which job level\nperformance data was available.",
        "snippets": [
            "Workload characterization is an integral part of performance analysis of high\nperformance computing (HPC) systems. An understanding of workload properties\nsheds light on resource utilization and can be used to inform performance\noptimization both at the software and system configuration levels. It can\nprovide information on how computational science usage modalities are changing\nthat could potentially aid holistic capacity planning for the wider HPC\necosystem. Here, we report on the results of a detailed workload analysis of\nthe portfolio of supercomputers comprising the NSF Innovative HPC program in\norder to characterize its past and current workload and look for trends to\nunderstand the nature of how the broad portfolio of computational science\nresearch is being supported and how it is changing over time. The workload\nanalysis also sought to illustrate a wide variety of usage patterns and\nperformance requirements for jobs running on these systems. File system\nperformance, memory utilization and the types of parallelism employed by users\n(MPI, threads, etc) were also studied for all systems for which job level\nperformance data was available."
        ],
        "title": "A Workload Analysis of NSF's Innovative HPC Resources Using XDMoD",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2405.07250v1": {
        "url": "http://arxiv.org/abs/2405.07250v1",
        "description": "Cloud providers introduce features (e.g., Spot VMs, Harvest VMs, and\nBurstable VMs) and optimizations (e.g., oversubscription, auto-scaling, power\nharvesting, and overclocking) to improve efficiency and reliability. To\neffectively utilize these features, it's crucial to understand the\ncharacteristics of workloads running in the cloud. However, workload\ncharacteristics can be complex and depend on multiple signals, making manual\ncharacterization difficult and unscalable. In this study, we conduct the first\nlarge-scale examination of first-party workloads at Microsoft to understand\ntheir characteristics. Through an empirical study, we aim to answer the\nfollowing questions: (1) What are the critical workload characteristics that\nimpact efficiency and reliability on cloud platforms? (2) How do these\ncharacteristics vary across different workloads? (3) How can cloud platforms\nleverage these insights to efficiently characterize all workloads at scale?\nThis study provides a deeper understanding of workload characteristics and\ntheir impact on cloud performance, which can aid in optimizing cloud services.\nAdditionally, it identifies potential areas for future research.",
        "snippets": [
            "Cloud providers introduce features (e.g., Spot VMs, Harvest VMs, and\nBurstable VMs) and optimizations (e.g., oversubscription, auto-scaling, power\nharvesting, and overclocking) to improve efficiency and reliability. To\neffectively utilize these features, it's crucial to understand the\ncharacteristics of workloads running in the cloud. However, workload\ncharacteristics can be complex and depend on multiple signals, making manual\ncharacterization difficult and unscalable. In this study, we conduct the first\nlarge-scale examination of first-party workloads at Microsoft to understand\ntheir characteristics. Through an empirical study, we aim to answer the\nfollowing questions: (1) What are the critical workload characteristics that\nimpact efficiency and reliability on cloud platforms? (2) How do these\ncharacteristics vary across different workloads? (3) How can cloud platforms\nleverage these insights to efficiently characterize all workloads at scale?\nThis study provides a deeper understanding of workload characteristics and\ntheir impact on cloud performance, which can aid in optimizing cloud services.\nAdditionally, it identifies potential areas for future research."
        ],
        "title": "Towards Cloud Efficiency with Large-scale Workload Characterization",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2407.19697v2": {
        "url": "http://arxiv.org/abs/2407.19697v2",
        "description": "Accurate workload forecasting is critical for efficient resource management\nin cloud computing systems, enabling effective scheduling and autoscaling.\nDespite recent advances with transformer-based forecasting models, challenges\nremain due to the non-stationary, nonlinear characteristics of workload time\nseries and the long-term dependencies. In particular, inconsistent performance\nbetween long-term history and near-term forecasts hinders long-range\npredictions. This paper proposes a novel framework leveraging self-supervised\nmultiscale representation learning to capture both long-term and near-term\nworkload patterns. The long-term history is encoded through multiscale\nrepresentations while the near-term observations are modeled via temporal flow\nfusion. These representations of different scales are fused using an attention\nmechanism and characterized with normalizing flows to handle\nnon-Gaussian/non-linear distributions of time series. Extensive experiments on\n9 benchmarks demonstrate superiority over existing methods.",
        "snippets": [
            "Accurate workload forecasting is critical for efficient resource management\nin cloud computing systems, enabling effective scheduling and autoscaling.\nDespite recent advances with transformer-based forecasting models, challenges\nremain due to the non-stationary, nonlinear characteristics of workload time\nseries and the long-term dependencies. In particular, inconsistent performance\nbetween long-term history and near-term forecasts hinders long-range\npredictions. This paper proposes a novel framework leveraging self-supervised\nmultiscale representation learning to capture both long-term and near-term\nworkload patterns. The long-term history is encoded through multiscale\nrepresentations while the near-term observations are modeled via temporal flow\nfusion. These representations of different scales are fused using an attention\nmechanism and characterized with normalizing flows to handle\nnon-Gaussian/non-linear distributions of time series. Extensive experiments on\n9 benchmarks demonstrate superiority over existing methods."
        ],
        "title": "Multiscale Representation Enhanced Temporal Flow Fusion Model for Long-Term Workload Forecasting",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1307.8013v1": {
        "url": "http://arxiv.org/abs/1307.8013v1",
        "description": "As the amount of data explodes rapidly, more and more corporations are using\ndata centers to make effective decisions and gain a competitive edge. Data\nanalysis applications play a significant role in data centers, and hence it has\nbecame increasingly important to understand their behaviors in order to further\nimprove the performance of data center computer systems. In this paper, after\ninvestigating three most important application domains in terms of page views\nand daily visitors, we choose eleven representative data analysis workloads and\ncharacterize their micro-architectural characteristics by using hardware\nperformance counters, in order to understand the impacts and implications of\ndata analysis workloads on the systems equipped with modern superscalar\nout-of-order processors. Our study on the workloads reveals that data analysis\napplications share many inherent characteristics, which place them in a\ndifferent class from desktop (SPEC CPU2006), HPC (HPCC), and service workloads,\nincluding traditional server workloads (SPECweb2005) and scale-out service\nworkloads (four among six benchmarks in CloudSuite), and accordingly we give\nseveral recommendations for architecture and system optimizations. On the basis\nof our workload characterization work, we released a benchmark suite named\nDCBench for typical datacenter workloads, including data analysis and service\nworkloads, with an open-source license on our project home page on\nhttp://prof.ict.ac.cn/DCBench. We hope that DCBench is helpful for performing\narchitecture and small-to-medium scale system researches for datacenter\ncomputing.",
        "snippets": [
            "As the amount of data explodes rapidly, more and more corporations are using\ndata centers to make effective decisions and gain a competitive edge. Data\nanalysis applications play a significant role in data centers, and hence it has\nbecame increasingly important to understand their behaviors in order to further\nimprove the performance of data center computer systems. In this paper, after\ninvestigating three most important application domains in terms of page views\nand daily visitors, we choose eleven representative data analysis workloads and\ncharacterize their micro-architectural characteristics by using hardware\nperformance counters, in order to understand the impacts and implications of\ndata analysis workloads on the systems equipped with modern superscalar\nout-of-order processors. Our study on the workloads reveals that data analysis\napplications share many inherent characteristics, which place them in a\ndifferent class from desktop (SPEC CPU2006), HPC (HPCC), and service workloads,\nincluding traditional server workloads (SPECweb2005) and scale-out service\nworkloads (four among six benchmarks in CloudSuite), and accordingly we give\nseveral recommendations for architecture and system optimizations. On the basis\nof our workload characterization work, we released a benchmark suite named\nDCBench for typical datacenter workloads, including data analysis and service\nworkloads, with an open-source license on our project home page on\nhttp://prof.ict.ac.cn/DCBench. We hope that DCBench is helpful for performing\narchitecture and small-to-medium scale system researches for datacenter\ncomputing."
        ],
        "title": "Characterizing Data Analysis Workloads in Data Centers",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1808.02919v2": {
        "url": "http://arxiv.org/abs/1808.02919v2",
        "description": "Warehouse-scale cloud datacenters co-locate workloads with different and\noften complementary characteristics for improved resource utilization. To\nbetter understand the challenges in managing such intricate, heterogeneous\nworkloads while providing quality-assured resource orchestration and user\nexperience, we analyze Alibaba's co-located workload trace, the first publicly\navailable dataset with precise information about the category of each job. Two\ntypes of workload---long-running, user-facing, containerized production jobs,\nand transient, highly dynamic, non-containerized, and non-production batch\njobs---are running on a shared cluster of 1313 machines. Our multifaceted\nanalysis reveals insights that we believe are useful for system designers and\nIT practitioners working on cluster management systems.",
        "snippets": [
            "Warehouse-scale cloud datacenters co-locate workloads with different and\noften complementary characteristics for improved resource utilization. To\nbetter understand the challenges in managing such intricate, heterogeneous\nworkloads while providing quality-assured resource orchestration and user\nexperience, we analyze Alibaba's co-located workload trace, the first publicly\navailable dataset with precise information about the category of each job. Two\ntypes of workload---long-running, user-facing, containerized production jobs,\nand transient, highly dynamic, non-containerized, and non-production batch\njobs---are running on a shared cluster of 1313 machines. Our multifaceted\nanalysis reveals insights that we believe are useful for system designers and\nIT practitioners working on cluster management systems."
        ],
        "title": "Characterizing Co-located Datacenter Workloads: An Alibaba Case Study",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1501.02729v1": {
        "url": "http://arxiv.org/abs/1501.02729v1",
        "description": "Our increasing reliance on the cloud has led to the emergence of scale-out\nworkloads. These scale-out workloads are latency-sensitive as they are user\ndriven. In order to meet strict latency constraints, they require massive\ncomputing infrastructure, which consume significant amount of energy and\ncontribute to operational costs. This cost is further aggravated by the lack of\nenergy proportionality in servers. As Internet services become even more\nubiquitous, scale-out workloads will need increasingly larger cluster\ninstallations. As such, we desire an investigation into the energy\nproportionality and the mechanisms to improve the power consumption of\nscale-out workloads.\n  Therefore, in this paper, we study the energy proportionality and power\nconsumption of clusters in the context of scale-out workloads. Towards this\nend, we evaluate the potential of power and resource provisioning to improve\nthe energy proportionality for this class of workloads. Using data serving, web\nsearching and data caching as our representative workloads, we first analyze\nthe component-level power distribution on a cluster. Second, we characterize\nhow these workloads utilize the cluster. Third, we analyze the potential of\npower provisioning techniques (i.e., active low-power, turbo and idle low-power\nmodes) to improve the energy proportionality of scale-out workloads. We then\ndescribe the ability of active low-power modes to provide trade-offs in power\nand latency. Finally, we compare and contrast power provisioning and resource\nprovisioning techniques. Our study reveals various insights which will help\nimprove the energy proportionality and power consumption of scale-out\nworkloads.",
        "snippets": [
            "Our increasing reliance on the cloud has led to the emergence of scale-out\nworkloads. These scale-out workloads are latency-sensitive as they are user\ndriven. In order to meet strict latency constraints, they require massive\ncomputing infrastructure, which consume significant amount of energy and\ncontribute to operational costs. This cost is further aggravated by the lack of\nenergy proportionality in servers. As Internet services become even more\nubiquitous, scale-out workloads will need increasingly larger cluster\ninstallations. As such, we desire an investigation into the energy\nproportionality and the mechanisms to improve the power consumption of\nscale-out workloads.\n  Therefore, in this paper, we study the energy proportionality and power\nconsumption of clusters in the context of scale-out workloads. Towards this\nend, we evaluate the potential of power and resource provisioning to improve\nthe energy proportionality for this class of workloads. Using data serving, web\nsearching and data caching as our representative workloads, we first analyze\nthe component-level power distribution on a cluster. Second, we characterize\nhow these workloads utilize the cluster. Third, we analyze the potential of\npower provisioning techniques (i.e., active low-power, turbo and idle low-power\nmodes) to improve the energy proportionality of scale-out workloads. We then\ndescribe the ability of active low-power modes to provide trade-offs in power\nand latency. Finally, we compare and contrast power provisioning and resource\nprovisioning techniques. Our study reveals various insights which will help\nimprove the energy proportionality and power consumption of scale-out\nworkloads."
        ],
        "title": "On the Energy Proportionality of Scale-Out Workloads",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1811.06901v1": {
        "url": "http://arxiv.org/abs/1811.06901v1",
        "description": "In warehouse-scale cloud datacenters, co-locating online services and offline\nbatch jobs is an efficient approach to improving datacenter utilization. To\nbetter facilitate the understanding of interactions among the co-located\nworkloads and their real-world operational demands, Alibaba recently released a\ncluster usage and co-located workload dataset, which is the first publicly\ndataset with precise information about the category of each job. In this paper,\nwe perform a deep analysis on the released Alibaba workload dataset, from the\nperspective of anomaly analysis and diagnosis. Through data preprocessing, node\nsimilarity analysis based on Dynamic Time Warping (DTW), co-located workloads\ncharacteristics analysis and anomaly analysis based on iForest, we reveals\nseveral insights including: (1) The performance discrepancy of machines in\nAlibaba's production cluster is relatively large, for the distribution and\nresource utilization of co-located workloads is not balanced. For instance, the\nresource utilization (especially memory utilization) of batch jobs is\nfluctuating and not as stable as that of online containers, and the reason is\nthat online containers are long-running jobs with more memory-demanding and\nmost batch jobs are short jobs, (2) Based on the distribution of co-located\nworkload instance numbers, the machines can be classified into 8 workload\ndistribution categories1. And most patterns of machine resource utilization\ncurves are similar in the same workload distribution category. (3) In addition\nto the system failures, unreasonable scheduling and workload imbalance are the\nmain causes of anomalies in Alibaba's cluster.",
        "snippets": [
            "In warehouse-scale cloud datacenters, co-locating online services and offline\nbatch jobs is an efficient approach to improving datacenter utilization. To\nbetter facilitate the understanding of interactions among the co-located\nworkloads and their real-world operational demands, Alibaba recently released a\ncluster usage and co-located workload dataset, which is the first publicly\ndataset with precise information about the category of each job. In this paper,\nwe perform a deep analysis on the released Alibaba workload dataset, from the\nperspective of anomaly analysis and diagnosis. Through data preprocessing, node\nsimilarity analysis based on Dynamic Time Warping (DTW), co-located workloads\ncharacteristics analysis and anomaly analysis based on iForest, we reveals\nseveral insights including: (1) The performance discrepancy of machines in\nAlibaba's production cluster is relatively large, for the distribution and\nresource utilization of co-located workloads is not balanced. For instance, the\nresource utilization (especially memory utilization) of batch jobs is\nfluctuating and not as stable as that of online containers, and the reason is\nthat online containers are long-running jobs with more memory-demanding and\nmost batch jobs are short jobs, (2) Based on the distribution of co-located\nworkload instance numbers, the machines can be classified into 8 workload\ndistribution categories1. And most patterns of machine resource utilization\ncurves are similar in the same workload distribution category. (3) In addition\nto the system failures, unreasonable scheduling and workload imbalance are the\nmain causes of anomalies in Alibaba's cluster."
        ],
        "title": "Anomaly Analysis for Co-located Datacenter Workloads in the Alibaba Cluster",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2309.02680v1": {
        "url": "http://arxiv.org/abs/2309.02680v1",
        "description": "Vector processing has become commonplace in today's CPU microarchitectures.\nVector instructions improve performance and energy which is crucial for\nresource-constraint mobile devices. The research community currently lacks a\ncomprehensive benchmark suite to study the benefits of vector processing for\nmobile devices. This paper presents Swan-an extensive vector processing\nbenchmark suite for mobile applications. Swan consists of a diverse set of\ndata-parallel workloads from four commonly used mobile applications: operating\nsystem, web browser, audio/video messaging application, and PDF rendering\nengine. Using Swan benchmark suite, we conduct a detailed analysis of the\nperformance, power, and energy consumption of vectorized workloads, and show\nthat: (a) Vectorized kernels increase the pressure on cache hierarchy due to\nthe higher rate of memory requests. (b) Vector processing is more beneficial\nfor workloads with lower precision operations and higher cache hit rates. (c)\nLimited Instruction-Level Parallelism and strided memory accesses to\nmulti-dimensional data structures prevent vector processing benefits from\nscaling with more SIMD functional units and wider registers. (d) Despite lower\ncomputation throughput than domain-specific accelerators, such as GPU, vector\nprocessing outperforms these accelerators for kernels with lower operation\ncounts. Finally, we show five common computation patterns in mobile\ndata-parallel workloads that dominate the execution time.",
        "snippets": [
            "Vector processing has become commonplace in today's CPU microarchitectures.\nVector instructions improve performance and energy which is crucial for\nresource-constraint mobile devices. The research community currently lacks a\ncomprehensive benchmark suite to study the benefits of vector processing for\nmobile devices. This paper presents Swan-an extensive vector processing\nbenchmark suite for mobile applications. Swan consists of a diverse set of\ndata-parallel workloads from four commonly used mobile applications: operating\nsystem, web browser, audio/video messaging application, and PDF rendering\nengine. Using Swan benchmark suite, we conduct a detailed analysis of the\nperformance, power, and energy consumption of vectorized workloads, and show\nthat: (a) Vectorized kernels increase the pressure on cache hierarchy due to\nthe higher rate of memory requests. (b) Vector processing is more beneficial\nfor workloads with lower precision operations and higher cache hit rates. (c)\nLimited Instruction-Level Parallelism and strided memory accesses to\nmulti-dimensional data structures prevent vector processing benefits from\nscaling with more SIMD functional units and wider registers. (d) Despite lower\ncomputation throughput than domain-specific accelerators, such as GPU, vector\nprocessing outperforms these accelerators for kernels with lower operation\ncounts. Finally, we show five common computation patterns in mobile\ndata-parallel workloads that dominate the execution time."
        ],
        "title": "Vector-Processing for Mobile Devices: Benchmark and Analysis",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1208.4174v1": {
        "url": "http://arxiv.org/abs/1208.4174v1",
        "description": "Within the past few years, organizations in diverse industries have adopted\nMapReduce-based systems for large-scale data processing. Along with these new\nusers, important new workloads have emerged which feature many small, short,\nand increasingly interactive jobs in addition to the large, long-running batch\njobs for which MapReduce was originally designed. As interactive, large-scale\nquery processing is a strength of the RDBMS community, it is important that\nlessons from that field be carried over and applied where possible in this new\ndomain. However, these new workloads have not yet been described in the\nliterature. We fill this gap with an empirical analysis of MapReduce traces\nfrom six separate business-critical deployments inside Facebook and at Cloudera\ncustomers in e-commerce, telecommunications, media, and retail. Our key\ncontribution is a characterization of new MapReduce workloads which are driven\nin part by interactive analysis, and which make heavy use of query-like\nprogramming frameworks on top of MapReduce. These workloads display diverse\nbehaviors which invalidate prior assumptions about MapReduce such as uniform\ndata access, regular diurnal patterns, and prevalence of large jobs. A\nsecondary contribution is a first step towards creating a TPC-like data\nprocessing benchmark for MapReduce.",
        "snippets": [
            "Within the past few years, organizations in diverse industries have adopted\nMapReduce-based systems for large-scale data processing. Along with these new\nusers, important new workloads have emerged which feature many small, short,\nand increasingly interactive jobs in addition to the large, long-running batch\njobs for which MapReduce was originally designed. As interactive, large-scale\nquery processing is a strength of the RDBMS community, it is important that\nlessons from that field be carried over and applied where possible in this new\ndomain. However, these new workloads have not yet been described in the\nliterature. We fill this gap with an empirical analysis of MapReduce traces\nfrom six separate business-critical deployments inside Facebook and at Cloudera\ncustomers in e-commerce, telecommunications, media, and retail. Our key\ncontribution is a characterization of new MapReduce workloads which are driven\nin part by interactive analysis, and which make heavy use of query-like\nprogramming frameworks on top of MapReduce. These workloads display diverse\nbehaviors which invalidate prior assumptions about MapReduce such as uniform\ndata access, regular diurnal patterns, and prevalence of large jobs. A\nsecondary contribution is a first step towards creating a TPC-like data\nprocessing benchmark for MapReduce."
        ],
        "title": "Interactive Analytical Processing in Big Data Systems: A Cross-Industry Study of MapReduce Workloads",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2406.06448v1": {
        "url": "http://arxiv.org/abs/2406.06448v1",
        "description": "Vertical take-off and landing (VTOL) aircraft do not require a prolonged\nrunway, thus allowing them to land almost anywhere. In recent years, their\nflexibility has made them popular in development, research, and operation. When\ncompared to traditional fixed-wing aircraft and rotorcraft, VTOLs bring unique\nchallenges as they combine many maneuvers from both types of aircraft. Pilot\nworkload is a critical factor for safe and efficient operation of VTOLs. In\nthis work, we conduct a user study to collect multimodal data from 28 pilots\nwhile they perform a variety of VTOL flight tasks. We analyze and interpolate\nbehavioral patterns related to their performance and perceived workload.\nFinally, we build machine learning models to estimate their workload from the\ncollected data. Our results are promising, suggesting that quantitative and\naccurate VTOL pilot workload monitoring is viable. Such assistive tools would\nhelp the research field understand VTOL operations and serve as a stepping\nstone for the industry to ensure VTOL safe operations and further remote\noperations.",
        "snippets": [
            "Vertical take-off and landing (VTOL) aircraft do not require a prolonged\nrunway, thus allowing them to land almost anywhere. In recent years, their\nflexibility has made them popular in development, research, and operation. When\ncompared to traditional fixed-wing aircraft and rotorcraft, VTOLs bring unique\nchallenges as they combine many maneuvers from both types of aircraft. Pilot\nworkload is a critical factor for safe and efficient operation of VTOLs. In\nthis work, we conduct a user study to collect multimodal data from 28 pilots\nwhile they perform a variety of VTOL flight tasks. We analyze and interpolate\nbehavioral patterns related to their performance and perceived workload.\nFinally, we build machine learning models to estimate their workload from the\ncollected data. Our results are promising, suggesting that quantitative and\naccurate VTOL pilot workload monitoring is viable. Such assistive tools would\nhelp the research field understand VTOL operations and serve as a stepping\nstone for the industry to ensure VTOL safe operations and further remote\noperations."
        ],
        "title": "How is the Pilot Doing: VTOL Pilot Workload Estimation by Multimodal Machine Learning on Psycho-physiological Signals",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1708.05584v1": {
        "url": "http://arxiv.org/abs/1708.05584v1",
        "description": "We study a single server FIFO queue that offers general service. Each of n\ncustomers enter the queue at random time epochs that are inde- pendent and\nidentically distributed. We call this the random scattering traffic model, and\nthe queueing model RS/G/1. We study the workload process associated with the\nqueue in two different settings. First, we present Gaussian process\napproximations in a high intensity asymptotic scale and characterize the\ntransient distribution of the approximation. Second, we study the rare event\npaths of the workload by proving a large deviations principle in the same high\nintensity regime. We also obtain exact asymptotics for the Gaussian\napproximations developed prior. This analysis significantly extends and\nsimplifies recent work in [1] on uniform population acceleration asymptotics to\nthe queue length and workload in the RS/G/1 queue.",
        "snippets": [
            "We study a single server FIFO queue that offers general service. Each of n\ncustomers enter the queue at random time epochs that are inde- pendent and\nidentically distributed. We call this the random scattering traffic model, and\nthe queueing model RS/G/1. We study the workload process associated with the\nqueue in two different settings. First, we present Gaussian process\napproximations in a high intensity asymptotic scale and characterize the\ntransient distribution of the approximation. Second, we study the rare event\npaths of the workload by proving a large deviations principle in the same high\nintensity regime. We also obtain exact asymptotics for the Gaussian\napproximations developed prior. This analysis significantly extends and\nsimplifies recent work in  on uniform population acceleration asymptotics to\nthe queue length and workload in the RS/G/1 queue."
        ],
        "title": "On Gaussian Limits and Large Deviations for Queues Fed by High Intensity Randomly Scattered Traffic",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1604.08484v1": {
        "url": "http://arxiv.org/abs/1604.08484v1",
        "description": "While cluster computing frameworks are continuously evolving to provide\nreal-time data analysis capabilities, Apache Spark has managed to be at the\nforefront of big data analytics for being a unified framework for both, batch\nand stream data processing. However, recent studies on micro-architectural\ncharacterization of in-memory data analytics are limited to only batch\nprocessing workloads. We compare micro-architectural performance of batch\nprocessing and stream processing workloads in Apache Spark using hardware\nperformance counters on a dual socket server. In our evaluation experiments, we\nhave found that batch processing are stream processing workloads have similar\nmicro-architectural characteristics and are bounded by the latency of frequent\ndata access to DRAM. For data accesses we have found that simultaneous\nmulti-threading is effective in hiding the data latencies. We have also\nobserved that (i) data locality on NUMA nodes can improve the performance by\n10% on average and(ii) disabling next-line L1-D prefetchers can reduce the\nexecution time by up-to 14\\% and (iii) multiple small executors can provide\nup-to 36\\% speedup over single large executor.",
        "snippets": [
            "While cluster computing frameworks are continuously evolving to provide\nreal-time data analysis capabilities, Apache Spark has managed to be at the\nforefront of big data analytics for being a unified framework for both, batch\nand stream data processing. However, recent studies on micro-architectural\ncharacterization of in-memory data analytics are limited to only batch\nprocessing workloads. We compare micro-architectural performance of batch\nprocessing and stream processing workloads in Apache Spark using hardware\nperformance counters on a dual socket server. In our evaluation experiments, we\nhave found that batch processing are stream processing workloads have similar\nmicro-architectural characteristics and are bounded by the latency of frequent\ndata access to DRAM. For data accesses we have found that simultaneous\nmulti-threading is effective in hiding the data latencies. We have also\nobserved that (i) data locality on NUMA nodes can improve the performance by\n10% on average and(ii) disabling next-line L1-D prefetchers can reduce the\nexecution time by up-to 14\\% and (iii) multiple small executors can provide\nup-to 36\\% speedup over single large executor."
        ],
        "title": "Architectural Impact on Performance of In-memory Data Analytics: Apache Spark Case Study",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1805.00140v1": {
        "url": "http://arxiv.org/abs/1805.00140v1",
        "description": "Solid-State Drives (SSDs) are recently employed in enterprise servers and\nhigh-end storage systems in order to enhance performance of storage subsystem.\nAlthough employing high speed SSDs in the storage subsystems can significantly\nimprove system performance, it comes with significant reliability threat for\nwrite operations upon power failures. In this paper, we present a comprehensive\nanalysis investigating the impact of workload dependent parameters on the\nreliability of SSDs under power failure for variety of SSDs (from top\nmanufacturers). To this end, we first develop a platform to perform two\nimportant features required for study: a) a realistic fault injection into the\nSSD in the computing systems and b) data loss detection mechanism on the SSD\nupon power failure. In the proposed physical fault injection platform, SSDs\nexperience a real discharge phase of Power Supply Unit (PSU) that occurs during\npower failure in data centers which was neglected in previous studies. The\nimpact of workload dependent parameters such as workload Working Set Size\n(WSS), request size, request type, access pattern, and sequence of accesses on\nthe failure of SSDs is carefully studied in the presence of realistic power\nfailures. Experimental results over thousands number of fault injections show\nthat data loss occurs even after completion of the request (up to 700ms) where\nthe failure rate is influenced by the type, size, access pattern, and sequence\nof IO accesses while other parameters such as workload WSS has no impact on the\nfailure of SSDs.",
        "snippets": [
            "Solid-State Drives (SSDs) are recently employed in enterprise servers and\nhigh-end storage systems in order to enhance performance of storage subsystem.\nAlthough employing high speed SSDs in the storage subsystems can significantly\nimprove system performance, it comes with significant reliability threat for\nwrite operations upon power failures. In this paper, we present a comprehensive\nanalysis investigating the impact of workload dependent parameters on the\nreliability of SSDs under power failure for variety of SSDs (from top\nmanufacturers). To this end, we first develop a platform to perform two\nimportant features required for study: a) a realistic fault injection into the\nSSD in the computing systems and b) data loss detection mechanism on the SSD\nupon power failure. In the proposed physical fault injection platform, SSDs\nexperience a real discharge phase of Power Supply Unit (PSU) that occurs during\npower failure in data centers which was neglected in previous studies. The\nimpact of workload dependent parameters such as workload Working Set Size\n(WSS), request size, request type, access pattern, and sequence of accesses on\nthe failure of SSDs is carefully studied in the presence of realistic power\nfailures. Experimental results over thousands number of fault injections show\nthat data loss occurs even after completion of the request (up to 700ms) where\nthe failure rate is influenced by the type, size, access pattern, and sequence\nof IO accesses while other parameters such as workload WSS has no impact on the\nfailure of SSDs."
        ],
        "title": "Investigating Power Outage Effects on Reliability of Solid-State Drives",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/1506.07943v1": {
        "url": "http://arxiv.org/abs/1506.07943v1",
        "description": "Big data areas are expanding in a fast way in terms of increasing workloads\nand runtime systems, and this situation imposes a serious challenge to workload\ncharacterization, which is the foundation of innovative system and architecture\ndesign. The previous major efforts on big data benchmarking either propose a\ncomprehensive but a large amount of workloads, or only select a few workloads\naccording to so-called popularity, which may lead to partial or even biased\nobservations. In this paper, on the basis of a comprehensive big data benchmark\nsuite---BigDataBench, we reduced 77 workloads to 17 representative workloads\nfrom a micro-architectural perspective. On a typical state-of-practice\nplatform---Intel Xeon E5645, we compare the representative big data workloads\nwith SPECINT, SPECCFP, PARSEC, CloudSuite and HPCC. After a comprehensive\nworkload characterization, we have the following observations. First, the big\ndata workloads are data movement dominated computing with more branch\noperations, taking up to 92% percentage in terms of instruction mix, which\nplaces them in a different class from Desktop (SPEC CPU2006), CMP (PARSEC), HPC\n(HPCC) workloads. Second, corroborating the previous work, Hadoop and Spark\nbased big data workloads have higher front-end stalls. Comparing with the\ntraditional workloads i. e. PARSEC, the big data workloads have larger\ninstructions footprint. But we also note that, in addition to varied\ninstruction-level parallelism, there are significant disparities of front-end\nefficiencies among different big data workloads. Third, we found complex\nsoftware stacks that fail to use state-of-practise processors efficiently are\none of the main factors leading to high front-end stalls. For the same\nworkloads, the L1I cache miss rates have one order of magnitude differences\namong diverse implementations with different software stacks.",
        "snippets": [
            "Big data areas are expanding in a fast way in terms of increasing workloads\nand runtime systems, and this situation imposes a serious challenge to workload\ncharacterization, which is the foundation of innovative system and architecture\ndesign. The previous major efforts on big data benchmarking either propose a\ncomprehensive but a large amount of workloads, or only select a few workloads\naccording to so-called popularity, which may lead to partial or even biased\nobservations. In this paper, on the basis of a comprehensive big data benchmark\nsuite---BigDataBench, we reduced 77 workloads to 17 representative workloads\nfrom a micro-architectural perspective. On a typical state-of-practice\nplatform---Intel Xeon E5645, we compare the representative big data workloads\nwith SPECINT, SPECCFP, PARSEC, CloudSuite and HPCC. After a comprehensive\nworkload characterization, we have the following observations. First, the big\ndata workloads are data movement dominated computing with more branch\noperations, taking up to 92% percentage in terms of instruction mix, which\nplaces them in a different class from Desktop (SPEC CPU2006), CMP (PARSEC), HPC\n(HPCC) workloads. Second, corroborating the previous work, Hadoop and Spark\nbased big data workloads have higher front-end stalls. Comparing with the\ntraditional workloads i. e. PARSEC, the big data workloads have larger\ninstructions footprint. But we also note that, in addition to varied\ninstruction-level parallelism, there are significant disparities of front-end\nefficiencies among different big data workloads. Third, we found complex\nsoftware stacks that fail to use state-of-practise processors efficiently are\none of the main factors leading to high front-end stalls. For the same\nworkloads, the L1I cache miss rates have one order of magnitude differences\namong diverse implementations with different software stacks."
        ],
        "title": "Characterization and Architectural Implications of Big Data Workloads",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    },
    "http://arxiv.org/abs/2406.15769v1": {
        "url": "http://arxiv.org/abs/2406.15769v1",
        "description": "An effective auto-scaling framework is essential for microservices to ensure\nperformance stability and resource efficiency under dynamic workloads. As\nrevealed by many prior studies, the key to efficient auto-scaling lies in\naccurately learning performance patterns, i.e., the relationship between\nperformance metrics and workloads in data-driven schemes. However, we notice\nthat there are two significant challenges in characterizing performance\npatterns for large-scale microservices. Firstly, diverse microservices\ndemonstrate varying sensitivities to heterogeneous machines, causing difficulty\nin quantifying the performance difference in a fixed manner. Secondly, frequent\nversion upgrades of microservices result in uncertain changes in performance\npatterns, known as pattern drifts, leading to imprecise resource capacity\nestimation issues. To address these challenges, we propose Humas, a\nheterogeneity- and upgrade-aware auto-scaling framework for large-scale\nmicroservices. Firstly, Humas quantifies the difference in resource efficiency\namong heterogeneous machines for various microservices online and normalizes\ntheir resources in standard units. Additionally, Humas develops a least squares\ndensity-difference (LSDD) based algorithm to identify pattern drifts caused by\nupgrades. Lastly, Humas generates capacity adjustment plans for microservices\nbased on the latest performance patterns and predicted workloads. The\nexperiment results conducted on 50 real microservices with over 11,000\ncontainers demonstrate that Humas improves resource efficiency and performance\nstability by approximately 30.4% and 48.0%, respectively, compared to\nstate-of-the-art approaches.",
        "snippets": [
            "An effective auto-scaling framework is essential for microservices to ensure\nperformance stability and resource efficiency under dynamic workloads. As\nrevealed by many prior studies, the key to efficient auto-scaling lies in\naccurately learning performance patterns, i.e., the relationship between\nperformance metrics and workloads in data-driven schemes. However, we notice\nthat there are two significant challenges in characterizing performance\npatterns for large-scale microservices. Firstly, diverse microservices\ndemonstrate varying sensitivities to heterogeneous machines, causing difficulty\nin quantifying the performance difference in a fixed manner. Secondly, frequent\nversion upgrades of microservices result in uncertain changes in performance\npatterns, known as pattern drifts, leading to imprecise resource capacity\nestimation issues. To address these challenges, we propose Humas, a\nheterogeneity- and upgrade-aware auto-scaling framework for large-scale\nmicroservices. Firstly, Humas quantifies the difference in resource efficiency\namong heterogeneous machines for various microservices online and normalizes\ntheir resources in standard units. Additionally, Humas develops a least squares\ndensity-difference (LSDD) based algorithm to identify pattern drifts caused by\nupgrades. Lastly, Humas generates capacity adjustment plans for microservices\nbased on the latest performance patterns and predicted workloads. The\nexperiment results conducted on 50 real microservices with over 11,000\ncontainers demonstrate that Humas improves resource efficiency and performance\nstability by approximately 30.4% and 48.0%, respectively, compared to\nstate-of-the-art approaches."
        ],
        "title": "Humas: A Heterogeneity- and Upgrade-aware Microservice Auto-scaling Framework in Large-scale Data Centers",
        "meta": {
            "query": "Recent studies on characterizing KVcache workload patterns"
        },
        "citation_uuid": -1
    }
}