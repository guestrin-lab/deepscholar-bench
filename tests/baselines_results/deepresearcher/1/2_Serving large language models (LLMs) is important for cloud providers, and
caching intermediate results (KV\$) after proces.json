{
    "latency": 45.14080307260156,
    "question": "\nYour task is to write a Related Works section for an academic paper given the paper's abstract. Your response should provide the Related Works section and references. Only include references that are published before 2025-06-03T08:51:38+00:00. Mention them in a separate reference list at the end and cite them properly in the Related Works section. Here is the paper abstract:\nServing large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity..\n",
    "messages": [
        {
            "role": "system",
            "content": "## Background information \n* Today is 2025-06-09\n* You are Deep AI Research Assistant\n\nThe question I give you is a complex question that requires a *deep research* to answer.\n\nI will provide you with two tools to help you answer the question:\n* A web search tool to help you perform google search. Tool call format:\n{\"name\": \"web_search\", \"arguments\": {\"query\": [\"<query1>\",\"<query2>\",\"<query3>\"]}}\n* A webpage browsing tool to help you get new page content. Tool call format:\n{\"name\": \"browse_webpage\", \"arguments\": {\"url_list\": [\"<url1>\",\"<url2>\",\"<url3>\"]}}\n\nYou don't have to answer the question now, but you should first think about the research plan or what to search next.\n\nYour output format should be one of the following two formats:\n\n<think>\nYOUR THINKING PROCESS\n</think>\n<answer>\nYOUR ANSWER AFTER GETTING ENOUGH INFORMATION\n</answer>\n\nor\n\n<think>\nYOUR THINKING PROCESS\n</think>\n<tool_call>\nYOUR TOOL CALL WITH CORRECT FORMAT\n</tool_call>\n\nYou should always follow the above two formats strictly. You will be heavily penalized if you do not follow the format strictly.\nOnly output the final answer (in words, numbers or phrase) inside the <answer></answer> tag, without any explanations or extra information. If this is a yes-or-no question, you should only answer yes or no.\n"
        },
        {
            "role": "user",
            "content": "\nYour task is to write a Related Works section for an academic paper given the paper's abstract. Your response should provide the Related Works section and references. Only include references that are published before 2025-06-03T08:51:38+00:00. Mention them in a separate reference list at the end and cite them properly in the Related Works section. Here is the paper abstract:\nServing large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity..\n"
        },
        {
            "role": "assistant",
            "content": "<think>To write a Related Works section for the given paper, I need to understand the context and the research area. The paper focuses on serving large language models (LLMs) and the benefits of caching intermediate results (KV$) for improving serving throughput and latency. The research characterizes KV$ workload patterns from a leading LLM service provider and proposes a workload-aware cache eviction policy.\n\nFirst, I will search for related works on caching in LLM serving, cache eviction policies, and workload characterization.\n\n</think>",
            "tool_calls": [
                {
                    "id": "ee26fc3c-c0d9-445f-a996-0b8f70173a82",
                    "type": "function",
                    "function": {
                        "name": "web_search",
                        "arguments": "{\"query\": [\"caching in LLM serving\", \"cache eviction policies\", \"workload characterization\"]}"
                    }
                }
            ]
        },
        {
            "role": "tool",
            "content": "[{\"search_query\": \"caching in LLM serving\", \"web_page_info_list\": [{\"quick_summary\": \"Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for\\ntask-specific Large Language Model (LLM) applications. For multi-LoRA serving,\\ncaching hot KV caches and LoRA adapters in high bandwidth memory of\\naccelerations can improve inference performance. However, existing Multi-LoRA\\ninference systems fail to optimize serving performance like Time-To-First-Toke\\n(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore\\npropose FASTLIBRA, a Multi-LoRA caching system to optimize the serving\\nperformance. FASTLIBRA comprises a dependency-aware cache manager and a\\nperformance-driven cache swapper. The cache manager maintains the usage\\ndependencies between LoRAs and KV caches during the inference with a unified\\ncaching pool. The cache swapper determines the swap-in or out of LoRAs and KV\\ncaches based on a unified cost model, when the HBM is idle or busy,\\nrespectively. Experimental results show that ELORA reduces the TTFT by 63.4% on\\naverage, compared to state-of-the-art works.\", \"title\": \"Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management\", \"url\": \"http://arxiv.org/abs/2505.03756v1\"}, {\"quick_summary\": \"As large language models (LLMs) become widely used, their environmental\\nimpact$\\\\unicode{x2014}$especially carbon emissions$\\\\unicode{x2014}$has\\nattracted more attention. Prior studies focus on compute-related carbon\\nemissions. In this paper, we find that storage is another key contributor. LLM\\ncaching, which saves and reuses KV caches for repeated context, reduces\\noperational carbon by avoiding redundant computation. However, this benefit\\ncomes at the cost of embodied carbon from high-capacity, high-speed SSDs. As\\nLLMs scale, the embodied carbon of storage grows significantly.\\n  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching\\nframework that selects the optimal cache size for LLM serving. EmbAdvisor\\nprofiles different LLM tasks and uses an Integer Linear Programming (ILP)\\nsolver to select cache sizes that meet SLOs while minimizing total carbon\\nemissions. Overall, EmbAdvisor reduces the average carbon emissions of a\\nLlama-3 70B model by 9.5% under various carbon intensities compared to a\\nnon-adaptive cache scenario, and can save up to 31.2% when the carbon intensity\\nis low.\", \"title\": \"EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving\", \"url\": \"http://arxiv.org/abs/2505.23970v1\"}, {\"quick_summary\": \"Large language model (LLM) inference serving systems are essential to various\\nLLM-based applications. As demand for LLM services continues to grow, scaling\\nthese systems to handle high request rates while meeting latency Service-Level\\nObjectives (SLOs), referred to as effective throughput, becomes critical.\\nHowever, existing systems often struggle to improve effective throughput,\\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\\nattainment. We identify two major causes of this bottleneck: (1)\\nmemory-intensive KV cache that limits batch size expansion under GPU memory\\nconstraints, and (2) rigid batch composition enforced by the default\\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\\nKV cache with a memory-efficient hidden cache for reusable input hidden state\\nvectors, allowing large batch sizes and improving request concurrency. Based on\\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\\nthat dynamically optimizes batch composition. We formally define the adaptive\\nscheduling optimization problem and propose an efficient algorithm with\\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\\nto 8.8x improvement in effective throughput compared to the state-of-the-art\\ninference serving systems.\", \"title\": \"Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving\", \"url\": \"http://arxiv.org/abs/2504.07494v1\"}, {\"quick_summary\": \"Large Language Models (LLMs) are wildly popular today and it is important to\\nserve them efficiently. Existing LLM serving systems are stateless across\\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\\nconversations, a growing log of the conversation history must be processed\\nalongside any request by the serving system at each turn, resulting in repeated\\nprocessing.\\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\\nconversation LLM serving. $Pensieve$ maintains the conversation state across\\nrequests by caching previously processed history to avoid duplicate processing.\\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\\nrecent PagedAttention kernel to support attention between multiple input tokens\\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\\n$Pensieve$ can achieve $1.14$-$3.0\\\\times$ the throughput of vLLM and\\nTensorRT-LLM and significantly reduce latency.\", \"title\": \"Stateful Large Language Model Serving with Pensieve\", \"url\": \"http://arxiv.org/abs/2312.05516v3\"}, {\"quick_summary\": \"Transformer-based large language models (LLMs) demonstrate impressive\\nperformance across various natural language processing tasks. Serving LLM\\ninference for generating long contents, however, poses a challenge due to the\\nenormous memory footprint of the transient state, known as the key-value (KV)\\ncache, which scales with the sequence length and batch size. In this paper, we\\npresent InfiniGen, a novel KV cache management framework tailored for long-text\\ngeneration, which synergistically works with modern offloading-based inference\\nsystems. InfiniGen leverages the key insight that a few important tokens that\\nare essential for computing the subsequent attention layer in the Transformer\\ncan be speculated by performing a minimal rehearsal with the inputs of the\\ncurrent layer and part of the query weight and key cache of the subsequent\\nlayer. This allows us to prefetch only the essential KV cache entries (without\\nfetching them all), thereby mitigating the fetch overhead from the host memory\\nin offloading-based LLM serving systems. Our evaluation on several\\nrepresentative LLMs shows that InfiniGen improves the overall performance of a\\nmodern offloading-based system by up to 3.00x compared to prior KV cache\\nmanagement methods while offering substantially better model accuracy.\", \"title\": \"InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management\", \"url\": \"http://arxiv.org/abs/2406.19707v1\"}, {\"quick_summary\": \"Efficiently serving large language models (LLMs) requires batching of many\\nrequests to reduce the cost per request. Yet, with larger batch sizes and\\nlonger context lengths, the key-value (KV) cache, which stores attention keys\\nand values to avoid re-computations, significantly increases memory demands and\\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\\nof the KV cache causes the computational core to be idle, which limits the\\ninference speed. A straightforward and effective solution to reduce KV cache\\nsize is quantization, which decreases the total bytes taken by KV cache.\\nHowever, there is a lack of in-depth studies that explore the element\\ndistribution of KV cache to understand the hardness and limitation of KV cache\\nquantization. To fill the gap, we conducted a comprehensive study on the\\nelement distribution in KV cache of popular LLMs. Our findings indicate that\\nthe key cache should be quantized per-channel, i.e., group elements along the\\nchannel dimension and quantize them together. In contrast, the value cache\\nshould be quantized per-token. From this analysis, we developed a tuning-free\\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\\nalmost the same quality while using $\\\\mathbf{2.6\\\\times}$ less peak memory\\n(including model weight). This reduction in memory usage enables up to\\n$\\\\mathbf{4\\\\times}$ larger batch size, bringing $\\\\mathbf{2.35\\\\times \\\\sim\\n3.47\\\\times}$ throughput on real LLM inference workload. The source code is\\navailable at https://github.com/jy-yuan/KIVI.\", \"title\": \"KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache\", \"url\": \"http://arxiv.org/abs/2402.02750v2\"}, {\"quick_summary\": \"Large Language Models (LLMs) are increasingly being deployed in applications\\nsuch as chatbots, code editors, and conversational agents. A key feature of\\nLLMs is their ability to engage in multi-turn interactions with humans or\\nexternal tools, enabling a wide range of tasks. Each new request in a\\nmulti-turn interaction depends on the intermediate state, specifically the\\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\\nExisting serving engines either recompute the K,V caches or offload them to\\nmain memory. Profiling reveals that recomputation can result in over 99% of\\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\\nGPU memory makes inference serving stateful, leading to load imbalances across\\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\\nleverages the observation that multi-turn work loads provide additional hints\\nthat allow K,V caches to be migrated off the critical serving path. By\\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\\nfinegrained scheduling of inference requests. Our experiments demonstrate that\\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\\nbaselines, with a similar latency profile.\", \"title\": \"SYMPHONY: Improving Memory Management for LLM Inference Workloads\", \"url\": \"http://arxiv.org/abs/2412.16434v1\"}, {\"quick_summary\": \"Efficiently serving large language models (LLMs) under dynamic and bursty\\nworkloads remains a key challenge for real-world deployment. Existing serving\\nframeworks and static model compression techniques fail to adapt to workload\\nfluctuations, leading to either service-level objective (SLO) violations under\\nfull-precision serving or persistent accuracy degradation with static\\nquantization. We present MorphServe, a dynamic, workload-aware LLM serving\\nframework based on morphological adaptation. MorphServe introduces two\\nasynchronous, token-level runtime mechanisms: quantized layer swapping, which\\nselectively replaces less impactful layers with quantized alternatives during\\nhigh-load periods, and pressure-aware KV cache resizing, which dynamically\\nadjusts KV cache capacity in response to memory pressure. These mechanisms\\nenable state-preserving transitions with minimum runtime overhead and are fully\\ncompatible with modern scheduling and attention techniques. Extensive\\nexperiments on Vicuna and Llama family models with real-world workloads\\ndemonstrate that MorphServe reduces average SLO violations by 92.45 percent and\\nimproves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving,\\nwithout compromising generation quality. These results establish MorphServe as\\na practical and elastic solution for LLM deployment in dynamic environments.\", \"title\": \"Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing\", \"url\": \"http://arxiv.org/abs/2506.02006v1\"}, {\"quick_summary\": \"Processing long contexts has become a critical capability for modern large\\nlanguage models (LLMs). However, serving long-context LLMs comes with\\nsignificant inference costs due to the high memory overhead of the key-value\\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\\ncache selection, which results in a trade-off between accuracy and efficiency.\\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\\npaper presents PSA, a $\\\\underline{P}$rogressive $\\\\underline{S}$parse\\n$\\\\underline{A}$ttention mechanism that integrates algorithmic innovations with\\nsystem co-design to achieve both high inference accuracy and improved\\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\\nbudget of different tokens and layers according to their real attention weight\\ndistributions, rather than relying on a fixed budget $k$. This enables high\\naccuracy while minimizing KV cache usage. To further enhance execution\\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\\ninterleaving and synchronization overhead during PSA computation. Additionally,\\nwe implement unified GPU memory management that optimizes PSA's memory\\nutilization by accounting for uneven memory requirements across different model\\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\\nusage for attention computation by up to 2.4$\\\\times$ and 8.8$\\\\times$, and\\nincreases end-to-end serving throughput by up to 1.4$\\\\times$ and 2.0$\\\\times$,\\ncompared to state-of-the-art DSAes and systems without sparse attention,\\nrespectively.\", \"title\": \"Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving\", \"url\": \"http://arxiv.org/abs/2503.00392v1\"}]}, {\"search_query\": \"cache eviction policies\", \"web_page_info_list\": [{\"quick_summary\": \"The information centric networks (ICN) can be viewed as a network of caches.\\nConversely, ICN type of cache networks has distinctive features e.g, contents\\npopularity, usability time of content and other factors inflicts some diverse\\nrequirements for cache eviction policies. In this paper we defined four\\nimportant characteristics of a suitable eviction policy for ICN. We analysed\\nwell known eviction policies in view of defined characteristics. Based upon\\nanalysis we propose a new eviction scheme which is well suitable for ICN type\\nof cache networks.\", \"title\": \"Time Aware Least Recent Used (TLRU) Cache Management Policy in ICN\", \"url\": \"http://arxiv.org/abs/1801.00390v1\"}, {\"quick_summary\": \"In this paper, we propose PopNetCod, a popularity-based caching policy for\\nnetwork coding enabled Named Data Networking. PopNetCod is a distributed\\ncaching policy, in which each router measures the local popularity of the\\ncontent objects by analyzing the requests that it receives. It then uses this\\ninformation to decide which Data packets to cache or evict from its content\\nstore. Since network coding is used, partial caching of content objects is\\nsupported, which facilitates the management of the content store. The routers\\ndecide the Data packets that they cache or evict in an online manner when they\\nreceive requests for Data packets. This allows the most popular Data packets to\\nbe cached closer to the network edges. The evaluation of PopNetCod shows an\\nimproved cache-hit rate compared to the widely used Leave Copy Everywhere\\nplacement policy and the Least Recently Used eviction policy. The improved\\ncache-hit rate helps the clients to achieve higher goodput, while it also\\nreduces the load on the source servers.\", \"title\": \"PopNetCod: A Popularity-based Caching Policy for Network Coding enabled Named Data Networking\", \"url\": \"http://arxiv.org/abs/1901.01187v1\"}, {\"quick_summary\": \"This paper proposes to use a frequency based cache admission policy in order\\nto boost the effectiveness of caches subject to skewed access distributions.\\nGiven a newly accessed item and an eviction candidate from the cache, our\\nscheme decides, based on the recent access history, whether it is worth\\nadmitting the new item into the cache at the expense of the eviction candidate.\\n  Realizing this concept is enabled through a novel approximate LFU structure\\ncalled TinyLFU, which maintains an approximate representation of the access\\nfrequency of a large sample of recently accessed items. TinyLFU is very compact\\nand light-weight as it builds upon Bloom filter theory.\\n  We study the properties of TinyLFU through simulations of both synthetic\\nworkloads as well as multiple real traces from several sources. These\\nsimulations demonstrate the performance boost obtained by enhancing various\\nreplacement policies with the TinyLFU eviction policy. Also, a new combined\\nreplacement and eviction policy scheme nicknamed W-TinyLFU is presented.\\nW-TinyLFU is demonstrated to obtain equal or better hit-ratios than other state\\nof the art replacement policies on these traces. It is the only scheme to\\nobtain such good results on all traces.\", \"title\": \"TinyLFU: A Highly Efficient Cache Admission Policy\", \"url\": \"http://arxiv.org/abs/1512.00727v2\"}, {\"quick_summary\": \"Caches are used to reduce the speed differential between the CPU and memory\\nto improve the performance of modern processors. However, attackers can use\\ncontention-based cache timing attacks to steal sensitive information from\\nvictim processes through carefully designed cache eviction sets. And L1 data\\ncache attacks are widely exploited and pose a significant privacy and\\nconfidentiality threat. Existing hardware-based countermeasures mainly focus on\\ncache partitioning, randomization, and cache line flushing, which unfortunately\\neither incur high overhead or can be circumvented by sophisticated attacks. In\\nthis paper, we propose a novel hardware-software co-design called BackCache\\nwith the idea of always achieving cache hits instead of cache misses to\\nmitigate contention-based cache timing attacks on the L1 data cache. BackCache\\nplaces the evicted cache lines from the L1 data cache into a fully-associative\\nbackup cache to hide the evictions. To improve the security of BackCache, we\\nintroduce a randomly used replacement policy (RURP) and a dynamic backup cache\\nresizing mechanism. We also present a theoretical security analysis to\\ndemonstrate the effectiveness of BackCache. Our evaluation on the gem5\\nsimulator shows that BackCache can degrade the performance by 2.61%, 2.66%, and\\n3.36% For OS kernel, single-thread, and multi-thread benchmarks.\", \"title\": \"BackCache: Mitigating Contention-Based Cache Timing Attacks by Hiding Cache Line Evictions\", \"url\": \"http://arxiv.org/abs/2304.10268v5\"}, {\"quick_summary\": \"Many Information Centric Networking (ICN) proposals use a network of caches\\nto bring the contents closer to the consumers, reduce the load on producers and\\ndecrease the unnecessary retransmission for ISPs. Nevertheless, the existing\\ncache management scheme for the network of caches obtain poor performance. The\\nmain reason for performance degradation in a network of caches is the filter\\neffect of the replacement policy. A cache serves the requests that generate\\ncache-hits and forwards the requests that generate cache-misses. This filtering\\nchanges the pattern of requests and leads to decreased hit ratios in the\\nsubsequent caches. In this paper, we propose a coordinated caching scheme to\\nsolve the filter effect problem by introducing the selection policy. This\\npolicy manages a cache such that: i) the cache obtains a high hit ratio ii) the\\nmissed requests from the cache can be used by subsequent caches to obtain a\\nhigh hit ratio. Our coordinated selection scheme achieves an overall hit ratio\\nof a network of caches equivalent to that of edge routers with big caches.\\nMoreover, our scheme decreases the average number of evictions per cache slot\\nby four order of magnitude compared to the LRU universal caching.\", \"title\": \"Selection Policy: Fighting against Filter Effect in Network of Caches\", \"url\": \"http://arxiv.org/abs/1310.3584v1\"}, {\"quick_summary\": \"The page cache is a central part of an OS. It reduces repeated accesses to\\nstorage by deciding which pages to retain in memory. As a result, the page\\ncache has a significant impact on the performance of many applications.\\nHowever, its one-size-fits-all eviction policy performs poorly in many\\nworkloads. While the systems community has experimented with a plethora of new\\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\\nCDNs), it is very difficult to implement such policies in the page cache, due\\nto the complexity of modifying kernel code. To address these shortcomings, we\\ndesign a novel eBPF-based framework for the Linux page cache, called\\n$\\\\texttt{cachebpf}$, that allows developers to customize the page cache without\\nmodifying the kernel. $\\\\texttt{cachebpf}$ enables applications to customize the\\npage cache policy for their specific needs, while also ensuring that different\\napplications' policies do not interfere with each other and preserving the page\\ncache's ability to share memory across different processes. We demonstrate the\\nflexibility of $\\\\texttt{cachebpf}$'s interface by using it to implement several\\neviction policies. Our evaluation shows that it is indeed beneficial for\\napplications to customize the page cache to match their workloads' unique\\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\\ntail latency.\", \"title\": \"Cache is King: Smart Page Eviction with eBPF\", \"url\": \"http://arxiv.org/abs/2502.02750v1\"}, {\"quick_summary\": \"To cope with the ongoing changing demands of the internet, 'in-network\\ncaching' has been presented as an application solution for two decades. With\\nthe advent of information-centric network (ICN) architecture, 'in-network\\ncaching' becomes a network level solution. Some unique features of ICNs, e.g.,\\nrapidly changing cache states, higher request arrival rates, smaller cache\\nsizes, and other factors, impose diverse requirements on the content eviction\\npolicies. In particular, eviction policies should be fast and lightweight. In\\nthis study, we propose cache replication and eviction schemes, Conditional\\nLeave Cope Everywhere (CLCE) and Least Frequent Recently Used (LFRU), which are\\nwell suited for the ICN type of cache networks (CNs). The CLCE replication\\nscheme reduces the redundant caching of contents; hence improves the cache\\nspace utilization. LFRU approximates the Least Frequently Used (LFU) scheme\\ncoupled with the Least Recently Used (LRU) scheme and is practically\\nimplementable for rapidly changing cache networks like ICNs.\", \"title\": \"A Cache Management Scheme for Efficient Content Eviction and Replication in Cache Networks\", \"url\": \"http://arxiv.org/abs/1702.04078v4\"}, {\"quick_summary\": \"Caches on the modern commodity CPUs have become one of the major sources of\\nside-channel leakages and been abused as a new attack vector. To thwart the\\ncache-based side-channel attacks, two types of countermeasures have been\\nproposed: detection-based ones that limit the amount of microarchitectural\\ntraces an attacker can leave, and cache prefetching-and-locking techniques that\\nclaim to prevent such leakage by disallowing evictions on sensitive data. In\\nthis paper, we present the Prime+Retouch attack that completely bypasses these\\ndefense schemes by accurately inferring the cache activities with the metadata\\nof the cache replacement policy. Prime+Retouch has three noticeable properties:\\n1) it incurs no eviction on the victim's data, allowing us to bypass the two\\nknown mitigation schemes, 2) it requires minimal synchronization of only one\\nmemory access to the attacker's pre-primed cache lines, and 3) it leaks data\\nvia non-shared memory, yet because underlying eviction metadata is shared.\\n  We demonstrate Prime+Retouch in two architectures: predominant Intel x86 and\\nemerging Apple M1. We elucidate how Prime+Retouch can break the T-table\\nimplementation of AES with robust cache side-channel mitigations such as Cloak,\\nunder both normal and SGX-protected environments. We also manifest feasibility\\nof the Prime+Retouch attack on the M1 platform imposing more restrictions where\\nthe precise measurement tools such as core clock cycle timer and performance\\ncounters are inaccessible to the attacker. Furthermore, we first demystify\\nundisclosed cache architecture and its eviction policy of L1 data cache on\\nApple M1 architecture. We also devise a user-space noise-free cache monitoring\\ntool by repurposing Intel TSX.\", \"title\": \"Prime+Retouch: When Cache is Locked and Leaked\", \"url\": \"http://arxiv.org/abs/2402.15425v1\"}, {\"quick_summary\": \"Caches have become the prime method for unintended information extraction\\nacross logical isolation boundaries. Even Spectre and Meltdown rely on the\\ncache side channel, as it provides great resolution and is widely available on\\nall major CPU platforms. As a consequence, several methods to stop cache\\nattacks by detecting them have been proposed. Detection is strongly aided by\\nthe fact that observing cache activity of co-resident processes is not possible\\nwithout altering the cache state and thereby forcing evictions on the observed\\nprocesses. In this work, we show that this widely held assumption is incorrect.\\nThrough clever usage of the cache replacement policy it is possible to track a\\nvictims process cache accesses without forcing evictions on the victim's data.\\nHence, online detection mechanisms that rely on these evictions can be\\ncircumvented as they do not detect be the introduced RELOAD+REFRESH attack. The\\nattack requires a profound understanding of the cache replacement policy. We\\npresent a methodology to recover the replacement policy and apply it to the\\nlast five generations of Intel processors. We further show empirically that the\\nperformance of RELOAD+REFRESH on cryptographic implementations is comparable to\\nthat of other widely used cache attacks, while its detectability becomes\\nextremely difficult, due to the negligible effect on the victims cache access\\npattern.\", \"title\": \"RELOAD+REFRESH: Abusing Cache Replacement Policies to Perform Stealthy Cache Attacks\", \"url\": \"http://arxiv.org/abs/1904.06278v1\"}, {\"quick_summary\": \"We propose a caching policy that uses a feedforward neural network (FNN) to\\npredict content popularity. Our scheme outperforms popular eviction policies\\nlike LRU or ARC, but also a new policy relying on the more complex recurrent\\nneural networks. At the same time, replacing the FNN predictor with a naive\\nlinear estimator does not degrade caching performance significantly,\\nquestioning then the role of neural networks for these applications.\", \"title\": \"Feedforward Neural Networks for Caching: Enough or Too Much?\", \"url\": \"http://arxiv.org/abs/1810.06930v1\"}]}, {\"search_query\": \"workload characterization\", \"web_page_info_list\": [{\"quick_summary\": \"The synthetic workload is essential and critical to the performance\\nevaluation of database systems. When evaluating the database performance for a\\nspecific application, the similarity between synthetic workload and real\\napplication workload determines the credibility of evaluation results. However,\\nthe workload currently used for performance evaluation is difficult to have the\\nsame workload characteristics as the target application, which leads to\\ninaccurate evaluation results. To address this problem, we propose a workload\\nduplicator (Lauca) that can generate synthetic workloads with highly similar\\nperformance metrics for specific applications. To the best of our knowledge,\\nLauca is the first application-oriented transactional workload generator. By\\ncarefully studying the application-oriented synthetic workload generation\\nproblem, we present the key workload characteristics (transaction logic and\\ndata access distribution) of online transaction processing (OLTP) applications,\\nand propose novel workload characterization and generation algorithms, which\\nguarantee the high fidelity of synthetic workloads. We conduct extensive\\nexperiments using workloads from TPC-C, SmallBank and micro benchmarks on both\\nMySQL and PostgreSQL databases, and experimental results show that Lauca\\nconsistently generates high-quality synthetic workloads.\", \"title\": \"Lauca: Generating Application-Oriented Synthetic Workloads\", \"url\": \"http://arxiv.org/abs/1912.07172v1\"}, {\"quick_summary\": \"Web applications, accessible via web browsers over the Internet, facilitate\\ncomplex functionalities without local software installation. In the context of\\nweb applications, a workload refers to the number of user requests sent by\\nusers or applications to the underlying system. Existing studies have leveraged\\nweb application workloads to achieve various objectives, such as workload\\nprediction and auto-scaling. However, these studies are conducted in an ad hoc\\nmanner, lacking a systematic understanding of the characteristics of web\\napplication workloads. In this study, we first conduct a systematic literature\\nreview to identify and analyze existing studies leveraging web application\\nworkloads. Our analysis sheds light on their workload utilization, analysis\\ntechniques, and high-level objectives. We further systematically analyze the\\ncharacteristics of the web application workloads identified in the literature\\nreview. Our analysis centers on characterizing these workloads at two distinct\\ntemporal granularities: daily and weekly. We successfully identify and\\ncategorize three daily and three weekly patterns within the workloads. By\\nproviding a statistical characterization of these workload patterns, our study\\nhighlights the uniqueness of each pattern, paving the way for the development\\nof realistic workload generation and resource provisioning techniques that can\\nbenefit a range of applications and research areas.\", \"title\": \"Understanding Web Application Workloads and Their Applications: Systematic Literature Review and Characterization\", \"url\": \"http://arxiv.org/abs/2409.12299v1\"}, {\"quick_summary\": \"Big data areas are expanding in a fast way in terms of increasing workloads\\nand runtime systems, and this situation imposes a serious challenge to workload\\ncharacterization, which is the foundation of innovative system and architecture\\ndesign. The previous major efforts on big data benchmarking either propose a\\ncomprehensive but a large amount of workloads, or only select a few workloads\\naccording to so-called popularity, which may lead to partial or even biased\\nobservations. In this paper, on the basis of a comprehensive big data benchmark\\nsuite---BigDataBench, we reduced 77 workloads to 17 representative workloads\\nfrom a micro-architectural perspective. On a typical state-of-practice\\nplatform---Intel Xeon E5645, we compare the representative big data workloads\\nwith SPECINT, SPECCFP, PARSEC, CloudSuite and HPCC. After a comprehensive\\nworkload characterization, we have the following observations. First, the big\\ndata workloads are data movement dominated computing with more branch\\noperations, taking up to 92% percentage in terms of instruction mix, which\\nplaces them in a different class from Desktop (SPEC CPU2006), CMP (PARSEC), HPC\\n(HPCC) workloads. Second, corroborating the previous work, Hadoop and Spark\\nbased big data workloads have higher front-end stalls. Comparing with the\\ntraditional workloads i. e. PARSEC, the big data workloads have larger\\ninstructions footprint. But we also note that, in addition to varied\\ninstruction-level parallelism, there are significant disparities of front-end\\nefficiencies among different big data workloads. Third, we found complex\\nsoftware stacks that fail to use state-of-practise processors efficiently are\\none of the main factors leading to high front-end stalls. For the same\\nworkloads, the L1I cache miss rates have one order of magnitude differences\\namong diverse implementations with different software stacks.\", \"title\": \"Characterization and Architectural Implications of Big Data Workloads\", \"url\": \"http://arxiv.org/abs/1506.07943v1\"}, {\"quick_summary\": \"Finding the similarity between two workload behaviors is helpful in 1.\\ncreating proxy workloads 2. characterizing an unknown workload's behavior by\\nmatching its behavior against known workloads. In this article, we propose a\\nmethod to measure the similarity between two workloads using machine\\nlearning-based analysis of the performance telemetry data collected for the\\nexecution runs of the two workloads. We also demonstrate the accuracy of the\\ntechnique by measuring the similarity between a variety of know benchmark\\nworkloads.\", \"title\": \"Workload Similarity Analysis using Machine Learning Techniques\", \"url\": \"http://arxiv.org/abs/2210.13757v2\"}, {\"quick_summary\": \"This paper presents a proposal aiming at better understanding a workload of\\nSQL queries and detecting coherent explorations hidden within the workload. In\\nparticular, our work investigates SQLShare [11], a database-as-a-service\\nplatform targeting scientists and data scientists with minimal database\\nexperience, whose workload was made available to the research community.\\nAccording to the authors of [11], this workload is the only one containing\\nprimarily ad-hoc hand-written queries over user-uploaded datasets. We analyzed\\nthis workload by extracting features that characterize SQL queries and we show\\nhow to use these features to separate sequences of SQL queries into meaningful\\nexplorations. We ran several tests over various query workloads to validate\\nempirically our approach.\", \"title\": \"Detecting coherent explorations in SQL workloads\", \"url\": \"http://arxiv.org/abs/1907.05618v1\"}, {\"quick_summary\": \"Workloads in modern cloud data centers are becoming increasingly complex. The\\nnumber of workloads running in cloud data centers has been growing\\nexponentially for the last few years, and cloud service providers (CSP) have\\nbeen supporting on-demand services in real-time. Realizing the growing\\ncomplexity of cloud environment and cloud workloads, hardware vendors such as\\nIntel and AMD are increasingly introducing cloud-specific workload acceleration\\nfeatures in their CPU platforms. These features are typically targeted towards\\npopular and commonly-used cloud workloads. Nonetheless, uncommon,\\ncustomer-specific workloads (unknown workloads), if their characteristics are\\ndifferent from common workloads (known workloads), may not realize the\\npotential of the underlying platform. To address this problem of realizing the\\nfull potential of the underlying platform, we develop a machine learning based\\ntechnique to characterize, profile and predict workloads running in the cloud\\nenvironment. Experimental evaluation of our technique demonstrates good\\nprediction performance. We also develop techniques to analyze the performance\\nof the model in a standalone manner.\", \"title\": \"CWD: A Machine Learning based Approach to Detect Unknown Cloud Workloads\", \"url\": \"http://arxiv.org/abs/2211.15739v1\"}, {\"quick_summary\": \"In the recent past, characterizing workloads has been attempted to gain a\\nfoothold in the emerging serverless cloud market, especially in the large\\nproduction cloud clusters of Google, AWS, and so forth. While analyzing and\\ncharacterizing real workloads from a large production cloud cluster benefits\\ncloud providers, researchers, and daily users, analyzing the workload traces of\\nthese clusters has been an arduous task due to the heterogeneous nature of\\ndata. This article proposes a scalable infrastructure based on Google's\\ndataproc for analyzing the workload traces of cloud environments. We evaluated\\nthe functioning of the proposed infrastructure using the workload traces of\\nGoogle cloud cluster-usage-traces-v3. We perform the workload characterization\\non this dataset, focusing on the heterogeneity of the workload, the variations\\nin job durations, aspects of resources consumption, and the overall\\navailability of resources provided by the cluster. The findings reported in the\\npaper will be beneficial for cloud infrastructure providers and users while\\nmanaging the cloud computing resources, especially serverless platforms.\", \"title\": \"Scalable Infrastructure for Workload Characterization of Cluster Traces\", \"url\": \"http://arxiv.org/abs/2205.11582v1\"}, {\"quick_summary\": \"Cloud providers introduce features (e.g., Spot VMs, Harvest VMs, and\\nBurstable VMs) and optimizations (e.g., oversubscription, auto-scaling, power\\nharvesting, and overclocking) to improve efficiency and reliability. To\\neffectively utilize these features, it's crucial to understand the\\ncharacteristics of workloads running in the cloud. However, workload\\ncharacteristics can be complex and depend on multiple signals, making manual\\ncharacterization difficult and unscalable. In this study, we conduct the first\\nlarge-scale examination of first-party workloads at Microsoft to understand\\ntheir characteristics. Through an empirical study, we aim to answer the\\nfollowing questions: (1) What are the critical workload characteristics that\\nimpact efficiency and reliability on cloud platforms? (2) How do these\\ncharacteristics vary across different workloads? (3) How can cloud platforms\\nleverage these insights to efficiently characterize all workloads at scale?\\nThis study provides a deeper understanding of workload characteristics and\\ntheir impact on cloud performance, which can aid in optimizing cloud services.\\nAdditionally, it identifies potential areas for future research.\", \"title\": \"Towards Cloud Efficiency with Large-scale Workload Characterization\", \"url\": \"http://arxiv.org/abs/2405.07250v1\"}, {\"quick_summary\": \"With the widespread adoption of Large Language Models (LLMs), serving LLM\\ninference requests has become an increasingly important task, attracting active\\nresearch advancements. Practical workloads play an essential role in this\\nprocess: they are critical for motivating and benchmarking serving techniques\\nand systems. However, the existing understanding of real-world LLM serving\\nworkloads is limited due to the lack of a comprehensive workload\\ncharacterization. Prior analyses remain insufficient in scale and scope, thus\\nfailing to fully capture intricate workload characteristics.\\n  In this paper, we fill the gap with an in-depth characterization of LLM\\nserving workloads collected from our worldwide cloud inference serving service,\\ncovering not only language models but also emerging multimodal and reasoning\\nmodels, and unveiling important new findings in each case. Moreover, based on\\nour findings, we propose ServeGen, a principled framework for generating\\nrealistic LLM serving workloads by composing them on a per-client basis. A\\npractical use case in production validates that ServeGen avoids 50%\\nunder-provisioning compared to naive workload generation, demonstrating\\nServeGen's advantage in performance benchmarking. ServeGen is available at\\nhttps://github.com/alibaba/ServeGen.\", \"title\": \"ServeGen: Workload Characterization and Generation of Large Language Model Serving in Production\", \"url\": \"http://arxiv.org/abs/2505.09999v2\"}, {\"quick_summary\": \"Smart databases are adopting artificial intelligence (AI) technologies to\\nachieve {\\\\em instance optimality}, and in the future, databases will come with\\nprepackaged AI models within their core components. The reason is that every\\ndatabase runs on different workloads, demands specific resources, and settings\\nto achieve optimal performance. It prompts the necessity to understand\\nworkloads running in the system along with their features comprehensively,\\nwhich we dub as workload characterization.\\n  To address this workload characterization problem, we propose our query plan\\nencoders that learn essential features and their correlations from query plans.\\nOur pretrained encoders capture the {\\\\em structural} and the {\\\\em computational\\nperformance} of queries independently. We show that our pretrained encoders are\\nadaptable to workloads that expedite the transfer learning process. We\\nperformed independent assessments of structural encoder and performance\\nencoders with multiple downstream tasks. For the overall evaluation of our\\nquery plan encoders, we architect two downstream tasks (i) query latency\\nprediction and (ii) query classification. These tasks show the importance of\\nfeature-based workload characterization. We also performed extensive\\nexperiments on individual encoders to verify the effectiveness of\\nrepresentation learning and domain adaptability.\", \"title\": \"Database Workload Characterization with Query Plan Encoders\", \"url\": \"http://arxiv.org/abs/2105.12287v1\"}]}]"
        },
        {
            "role": "assistant",
            "content": "<Related Works>\nLarge language models (LLMs) have become crucial in cloud computing, and efficiently serving them is essential. Caching intermediate results (KV$) has been shown to improve serving throughput and latency. Several studies have explored caching in LLM serving. For instance, FASTLIBRA, a Multi-LoRA caching system, optimizes serving performance by caching hot KV caches and LoRA adapters in high-bandwidth memory, reducing Time-To-First-Token (TTFT) by 63.4% on average compared to state-of-the-art works [1]. EmbAdvisor, a carbon-aware caching framework, selects the optimal cache size for LLM serving, reducing average carbon emissions by 9.5% under various carbon intensities [2]. Apt-Serve, a scalable framework, features a hybrid cache scheme and adaptive runtime scheduling, achieving up to 8.8x improvement in effective throughput compared to state-of-the-art inference serving systems [3]. Pensieve, a system optimized for multi-turn conversation LLM serving, maintains conversation state across requests by caching previously processed history, achieving 1.14-3.0x the throughput of vLLM and TensorRT-LLM and significantly reducing latency [4]. InfiniGen, a KV cache management framework, synergistically works with modern offloading-based inference systems, improving overall performance by up to 3.00x compared to prior KV cache management methods [5]. \n\nKIVI, a 2-bit KV cache quantization algorithm, enables Llama, Falcon, and Mistral models to maintain almost the same quality while using 2.6x less peak memory, bringing 2.35x-3.47x throughput on real LLM inference workload [6]. SYMPHONY, a system leveraging multi-turn workloads to migrate K,V caches off the critical serving path, handles over 8x the number of requests compared to state-of-the-art baselines, with a similar latency profile [7]. MorphServe, a dynamic, workload-aware LLM serving framework, reduces average SLO violations by 92.45 percent and improves P95 TTFT latency by 2.2x-3.9x compared to full-precision serving [8]. PSA, a progressive sparse attention mechanism, reduces KV cache usage for attention computation by up to 2.4x and increases end-to-end serving throughput by up to 1.4x and 2.0x, compared to state-of-the-art DSAes and systems without sparse attention, respectively [9].\n\nCache eviction policies have also been studied extensively. TLRU, a time-aware least recent used cache management policy, is suitable for ICN-type cache networks [10]. PopNetCod, a popularity-based caching policy, improves cache-hit rate compared to widely used Leave Copy Everywhere placement policy and Least Recently Used eviction policy [11]. TinyLFU, a highly efficient cache admission policy, boosts the effectiveness of caches subject to skewed access distributions [12]. BackCache, a novel hardware-software co-design, mitigates contention-based cache timing attacks on the L1 data cache [13]. \n\nWorkload characterization is essential for understanding the behavior of different workloads. Lauca, a workload duplicator, generates synthetic workloads with highly similar performance metrics for specific applications [14]. A systematic literature review and characterization of web application workloads identify daily and weekly patterns within the workloads [15]. Big data workload characterization reveals that big data workloads are data movement dominated computing with more branch operations [16]. \n\nOur work contributes to the understanding of KV$ workload patterns from a leading LLM service provider and proposes a workload-aware cache eviction policy that improves serving performance under real-world traces.\n\n</Related Works>\n\n<references>\n[1] http://arxiv.org/abs/2505.03756v1\n[2] http://arxiv.org/abs/2505.23970v1\n[3] http://arxiv.org/abs/2504.07494v1\n[4] http://arxiv.org/abs/2312.05516v3\n[5] http://arxiv.org/abs/2406.19707v1\n[6] http://arxiv.org/abs/2402.02750v2\n[7] http://arxiv.org/abs/2412.16434v1\n[8] http://arxiv.org/abs/2506.02006v1\n[9] http://arxiv.org/abs/2503.00392v1\n[10] http://arxiv.org/abs/1801.00390v1\n[11] http://arxiv.org/abs/1901.01187v1\n[12] http://arxiv.org/abs/1512.00727v2\n[13] http://arxiv.org/abs/2304.10268v5\n[14] http://arxiv.org/abs/1912.07172v1\n[15] http://arxiv.org/abs/2409.12299v1\n"
        }
    ],
    "abstract": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity..\n",
    "token_count": 10443,
    "tool_calls": [],
    "tool_response": [
        [
            {
                "search_query": "caching in LLM serving",
                "web_page_info_list": [
                    {
                        "quick_summary": "Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for\ntask-specific Large Language Model (LLM) applications. For multi-LoRA serving,\ncaching hot KV caches and LoRA adapters in high bandwidth memory of\naccelerations can improve inference performance. However, existing Multi-LoRA\ninference systems fail to optimize serving performance like Time-To-First-Toke\n(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore\npropose FASTLIBRA, a Multi-LoRA caching system to optimize the serving\nperformance. FASTLIBRA comprises a dependency-aware cache manager and a\nperformance-driven cache swapper. The cache manager maintains the usage\ndependencies between LoRAs and KV caches during the inference with a unified\ncaching pool. The cache swapper determines the swap-in or out of LoRAs and KV\ncaches based on a unified cost model, when the HBM is idle or busy,\nrespectively. Experimental results show that ELORA reduces the TTFT by 63.4% on\naverage, compared to state-of-the-art works.",
                        "title": "Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management",
                        "url": "http://arxiv.org/abs/2505.03756v1"
                    },
                    {
                        "quick_summary": "As large language models (LLMs) become widely used, their environmental\nimpact$\\unicode{x2014}$especially carbon emissions$\\unicode{x2014}$has\nattracted more attention. Prior studies focus on compute-related carbon\nemissions. In this paper, we find that storage is another key contributor. LLM\ncaching, which saves and reuses KV caches for repeated context, reduces\noperational carbon by avoiding redundant computation. However, this benefit\ncomes at the cost of embodied carbon from high-capacity, high-speed SSDs. As\nLLMs scale, the embodied carbon of storage grows significantly.\n  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching\nframework that selects the optimal cache size for LLM serving. EmbAdvisor\nprofiles different LLM tasks and uses an Integer Linear Programming (ILP)\nsolver to select cache sizes that meet SLOs while minimizing total carbon\nemissions. Overall, EmbAdvisor reduces the average carbon emissions of a\nLlama-3 70B model by 9.5% under various carbon intensities compared to a\nnon-adaptive cache scenario, and can save up to 31.2% when the carbon intensity\nis low.",
                        "title": "EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving",
                        "url": "http://arxiv.org/abs/2505.23970v1"
                    },
                    {
                        "quick_summary": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
                        "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving",
                        "url": "http://arxiv.org/abs/2504.07494v1"
                    },
                    {
                        "quick_summary": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.",
                        "title": "Stateful Large Language Model Serving with Pensieve",
                        "url": "http://arxiv.org/abs/2312.05516v3"
                    },
                    {
                        "quick_summary": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance across various natural language processing tasks. Serving LLM\ninference for generating long contents, however, poses a challenge due to the\nenormous memory footprint of the transient state, known as the key-value (KV)\ncache, which scales with the sequence length and batch size. In this paper, we\npresent InfiniGen, a novel KV cache management framework tailored for long-text\ngeneration, which synergistically works with modern offloading-based inference\nsystems. InfiniGen leverages the key insight that a few important tokens that\nare essential for computing the subsequent attention layer in the Transformer\ncan be speculated by performing a minimal rehearsal with the inputs of the\ncurrent layer and part of the query weight and key cache of the subsequent\nlayer. This allows us to prefetch only the essential KV cache entries (without\nfetching them all), thereby mitigating the fetch overhead from the host memory\nin offloading-based LLM serving systems. Our evaluation on several\nrepresentative LLMs shows that InfiniGen improves the overall performance of a\nmodern offloading-based system by up to 3.00x compared to prior KV cache\nmanagement methods while offering substantially better model accuracy.",
                        "title": "InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management",
                        "url": "http://arxiv.org/abs/2406.19707v1"
                    },
                    {
                        "quick_summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                        "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                        "url": "http://arxiv.org/abs/2402.02750v2"
                    },
                    {
                        "quick_summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                        "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                        "url": "http://arxiv.org/abs/2412.16434v1"
                    },
                    {
                        "quick_summary": "Efficiently serving large language models (LLMs) under dynamic and bursty\nworkloads remains a key challenge for real-world deployment. Existing serving\nframeworks and static model compression techniques fail to adapt to workload\nfluctuations, leading to either service-level objective (SLO) violations under\nfull-precision serving or persistent accuracy degradation with static\nquantization. We present MorphServe, a dynamic, workload-aware LLM serving\nframework based on morphological adaptation. MorphServe introduces two\nasynchronous, token-level runtime mechanisms: quantized layer swapping, which\nselectively replaces less impactful layers with quantized alternatives during\nhigh-load periods, and pressure-aware KV cache resizing, which dynamically\nadjusts KV cache capacity in response to memory pressure. These mechanisms\nenable state-preserving transitions with minimum runtime overhead and are fully\ncompatible with modern scheduling and attention techniques. Extensive\nexperiments on Vicuna and Llama family models with real-world workloads\ndemonstrate that MorphServe reduces average SLO violations by 92.45 percent and\nimproves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving,\nwithout compromising generation quality. These results establish MorphServe as\na practical and elastic solution for LLM deployment in dynamic environments.",
                        "title": "Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing",
                        "url": "http://arxiv.org/abs/2506.02006v1"
                    },
                    {
                        "quick_summary": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively.",
                        "title": "Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving",
                        "url": "http://arxiv.org/abs/2503.00392v1"
                    }
                ]
            },
            {
                "search_query": "cache eviction policies",
                "web_page_info_list": [
                    {
                        "quick_summary": "The information centric networks (ICN) can be viewed as a network of caches.\nConversely, ICN type of cache networks has distinctive features e.g, contents\npopularity, usability time of content and other factors inflicts some diverse\nrequirements for cache eviction policies. In this paper we defined four\nimportant characteristics of a suitable eviction policy for ICN. We analysed\nwell known eviction policies in view of defined characteristics. Based upon\nanalysis we propose a new eviction scheme which is well suitable for ICN type\nof cache networks.",
                        "title": "Time Aware Least Recent Used (TLRU) Cache Management Policy in ICN",
                        "url": "http://arxiv.org/abs/1801.00390v1"
                    },
                    {
                        "quick_summary": "In this paper, we propose PopNetCod, a popularity-based caching policy for\nnetwork coding enabled Named Data Networking. PopNetCod is a distributed\ncaching policy, in which each router measures the local popularity of the\ncontent objects by analyzing the requests that it receives. It then uses this\ninformation to decide which Data packets to cache or evict from its content\nstore. Since network coding is used, partial caching of content objects is\nsupported, which facilitates the management of the content store. The routers\ndecide the Data packets that they cache or evict in an online manner when they\nreceive requests for Data packets. This allows the most popular Data packets to\nbe cached closer to the network edges. The evaluation of PopNetCod shows an\nimproved cache-hit rate compared to the widely used Leave Copy Everywhere\nplacement policy and the Least Recently Used eviction policy. The improved\ncache-hit rate helps the clients to achieve higher goodput, while it also\nreduces the load on the source servers.",
                        "title": "PopNetCod: A Popularity-based Caching Policy for Network Coding enabled Named Data Networking",
                        "url": "http://arxiv.org/abs/1901.01187v1"
                    },
                    {
                        "quick_summary": "This paper proposes to use a frequency based cache admission policy in order\nto boost the effectiveness of caches subject to skewed access distributions.\nGiven a newly accessed item and an eviction candidate from the cache, our\nscheme decides, based on the recent access history, whether it is worth\nadmitting the new item into the cache at the expense of the eviction candidate.\n  Realizing this concept is enabled through a novel approximate LFU structure\ncalled TinyLFU, which maintains an approximate representation of the access\nfrequency of a large sample of recently accessed items. TinyLFU is very compact\nand light-weight as it builds upon Bloom filter theory.\n  We study the properties of TinyLFU through simulations of both synthetic\nworkloads as well as multiple real traces from several sources. These\nsimulations demonstrate the performance boost obtained by enhancing various\nreplacement policies with the TinyLFU eviction policy. Also, a new combined\nreplacement and eviction policy scheme nicknamed W-TinyLFU is presented.\nW-TinyLFU is demonstrated to obtain equal or better hit-ratios than other state\nof the art replacement policies on these traces. It is the only scheme to\nobtain such good results on all traces.",
                        "title": "TinyLFU: A Highly Efficient Cache Admission Policy",
                        "url": "http://arxiv.org/abs/1512.00727v2"
                    },
                    {
                        "quick_summary": "Caches are used to reduce the speed differential between the CPU and memory\nto improve the performance of modern processors. However, attackers can use\ncontention-based cache timing attacks to steal sensitive information from\nvictim processes through carefully designed cache eviction sets. And L1 data\ncache attacks are widely exploited and pose a significant privacy and\nconfidentiality threat. Existing hardware-based countermeasures mainly focus on\ncache partitioning, randomization, and cache line flushing, which unfortunately\neither incur high overhead or can be circumvented by sophisticated attacks. In\nthis paper, we propose a novel hardware-software co-design called BackCache\nwith the idea of always achieving cache hits instead of cache misses to\nmitigate contention-based cache timing attacks on the L1 data cache. BackCache\nplaces the evicted cache lines from the L1 data cache into a fully-associative\nbackup cache to hide the evictions. To improve the security of BackCache, we\nintroduce a randomly used replacement policy (RURP) and a dynamic backup cache\nresizing mechanism. We also present a theoretical security analysis to\ndemonstrate the effectiveness of BackCache. Our evaluation on the gem5\nsimulator shows that BackCache can degrade the performance by 2.61%, 2.66%, and\n3.36% For OS kernel, single-thread, and multi-thread benchmarks.",
                        "title": "BackCache: Mitigating Contention-Based Cache Timing Attacks by Hiding Cache Line Evictions",
                        "url": "http://arxiv.org/abs/2304.10268v5"
                    },
                    {
                        "quick_summary": "Many Information Centric Networking (ICN) proposals use a network of caches\nto bring the contents closer to the consumers, reduce the load on producers and\ndecrease the unnecessary retransmission for ISPs. Nevertheless, the existing\ncache management scheme for the network of caches obtain poor performance. The\nmain reason for performance degradation in a network of caches is the filter\neffect of the replacement policy. A cache serves the requests that generate\ncache-hits and forwards the requests that generate cache-misses. This filtering\nchanges the pattern of requests and leads to decreased hit ratios in the\nsubsequent caches. In this paper, we propose a coordinated caching scheme to\nsolve the filter effect problem by introducing the selection policy. This\npolicy manages a cache such that: i) the cache obtains a high hit ratio ii) the\nmissed requests from the cache can be used by subsequent caches to obtain a\nhigh hit ratio. Our coordinated selection scheme achieves an overall hit ratio\nof a network of caches equivalent to that of edge routers with big caches.\nMoreover, our scheme decreases the average number of evictions per cache slot\nby four order of magnitude compared to the LRU universal caching.",
                        "title": "Selection Policy: Fighting against Filter Effect in Network of Caches",
                        "url": "http://arxiv.org/abs/1310.3584v1"
                    },
                    {
                        "quick_summary": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
                        "title": "Cache is King: Smart Page Eviction with eBPF",
                        "url": "http://arxiv.org/abs/2502.02750v1"
                    },
                    {
                        "quick_summary": "To cope with the ongoing changing demands of the internet, 'in-network\ncaching' has been presented as an application solution for two decades. With\nthe advent of information-centric network (ICN) architecture, 'in-network\ncaching' becomes a network level solution. Some unique features of ICNs, e.g.,\nrapidly changing cache states, higher request arrival rates, smaller cache\nsizes, and other factors, impose diverse requirements on the content eviction\npolicies. In particular, eviction policies should be fast and lightweight. In\nthis study, we propose cache replication and eviction schemes, Conditional\nLeave Cope Everywhere (CLCE) and Least Frequent Recently Used (LFRU), which are\nwell suited for the ICN type of cache networks (CNs). The CLCE replication\nscheme reduces the redundant caching of contents; hence improves the cache\nspace utilization. LFRU approximates the Least Frequently Used (LFU) scheme\ncoupled with the Least Recently Used (LRU) scheme and is practically\nimplementable for rapidly changing cache networks like ICNs.",
                        "title": "A Cache Management Scheme for Efficient Content Eviction and Replication in Cache Networks",
                        "url": "http://arxiv.org/abs/1702.04078v4"
                    },
                    {
                        "quick_summary": "Caches on the modern commodity CPUs have become one of the major sources of\nside-channel leakages and been abused as a new attack vector. To thwart the\ncache-based side-channel attacks, two types of countermeasures have been\nproposed: detection-based ones that limit the amount of microarchitectural\ntraces an attacker can leave, and cache prefetching-and-locking techniques that\nclaim to prevent such leakage by disallowing evictions on sensitive data. In\nthis paper, we present the Prime+Retouch attack that completely bypasses these\ndefense schemes by accurately inferring the cache activities with the metadata\nof the cache replacement policy. Prime+Retouch has three noticeable properties:\n1) it incurs no eviction on the victim's data, allowing us to bypass the two\nknown mitigation schemes, 2) it requires minimal synchronization of only one\nmemory access to the attacker's pre-primed cache lines, and 3) it leaks data\nvia non-shared memory, yet because underlying eviction metadata is shared.\n  We demonstrate Prime+Retouch in two architectures: predominant Intel x86 and\nemerging Apple M1. We elucidate how Prime+Retouch can break the T-table\nimplementation of AES with robust cache side-channel mitigations such as Cloak,\nunder both normal and SGX-protected environments. We also manifest feasibility\nof the Prime+Retouch attack on the M1 platform imposing more restrictions where\nthe precise measurement tools such as core clock cycle timer and performance\ncounters are inaccessible to the attacker. Furthermore, we first demystify\nundisclosed cache architecture and its eviction policy of L1 data cache on\nApple M1 architecture. We also devise a user-space noise-free cache monitoring\ntool by repurposing Intel TSX.",
                        "title": "Prime+Retouch: When Cache is Locked and Leaked",
                        "url": "http://arxiv.org/abs/2402.15425v1"
                    },
                    {
                        "quick_summary": "Caches have become the prime method for unintended information extraction\nacross logical isolation boundaries. Even Spectre and Meltdown rely on the\ncache side channel, as it provides great resolution and is widely available on\nall major CPU platforms. As a consequence, several methods to stop cache\nattacks by detecting them have been proposed. Detection is strongly aided by\nthe fact that observing cache activity of co-resident processes is not possible\nwithout altering the cache state and thereby forcing evictions on the observed\nprocesses. In this work, we show that this widely held assumption is incorrect.\nThrough clever usage of the cache replacement policy it is possible to track a\nvictims process cache accesses without forcing evictions on the victim's data.\nHence, online detection mechanisms that rely on these evictions can be\ncircumvented as they do not detect be the introduced RELOAD+REFRESH attack. The\nattack requires a profound understanding of the cache replacement policy. We\npresent a methodology to recover the replacement policy and apply it to the\nlast five generations of Intel processors. We further show empirically that the\nperformance of RELOAD+REFRESH on cryptographic implementations is comparable to\nthat of other widely used cache attacks, while its detectability becomes\nextremely difficult, due to the negligible effect on the victims cache access\npattern.",
                        "title": "RELOAD+REFRESH: Abusing Cache Replacement Policies to Perform Stealthy Cache Attacks",
                        "url": "http://arxiv.org/abs/1904.06278v1"
                    },
                    {
                        "quick_summary": "We propose a caching policy that uses a feedforward neural network (FNN) to\npredict content popularity. Our scheme outperforms popular eviction policies\nlike LRU or ARC, but also a new policy relying on the more complex recurrent\nneural networks. At the same time, replacing the FNN predictor with a naive\nlinear estimator does not degrade caching performance significantly,\nquestioning then the role of neural networks for these applications.",
                        "title": "Feedforward Neural Networks for Caching: Enough or Too Much?",
                        "url": "http://arxiv.org/abs/1810.06930v1"
                    }
                ]
            },
            {
                "search_query": "workload characterization",
                "web_page_info_list": [
                    {
                        "quick_summary": "The synthetic workload is essential and critical to the performance\nevaluation of database systems. When evaluating the database performance for a\nspecific application, the similarity between synthetic workload and real\napplication workload determines the credibility of evaluation results. However,\nthe workload currently used for performance evaluation is difficult to have the\nsame workload characteristics as the target application, which leads to\ninaccurate evaluation results. To address this problem, we propose a workload\nduplicator (Lauca) that can generate synthetic workloads with highly similar\nperformance metrics for specific applications. To the best of our knowledge,\nLauca is the first application-oriented transactional workload generator. By\ncarefully studying the application-oriented synthetic workload generation\nproblem, we present the key workload characteristics (transaction logic and\ndata access distribution) of online transaction processing (OLTP) applications,\nand propose novel workload characterization and generation algorithms, which\nguarantee the high fidelity of synthetic workloads. We conduct extensive\nexperiments using workloads from TPC-C, SmallBank and micro benchmarks on both\nMySQL and PostgreSQL databases, and experimental results show that Lauca\nconsistently generates high-quality synthetic workloads.",
                        "title": "Lauca: Generating Application-Oriented Synthetic Workloads",
                        "url": "http://arxiv.org/abs/1912.07172v1"
                    },
                    {
                        "quick_summary": "Web applications, accessible via web browsers over the Internet, facilitate\ncomplex functionalities without local software installation. In the context of\nweb applications, a workload refers to the number of user requests sent by\nusers or applications to the underlying system. Existing studies have leveraged\nweb application workloads to achieve various objectives, such as workload\nprediction and auto-scaling. However, these studies are conducted in an ad hoc\nmanner, lacking a systematic understanding of the characteristics of web\napplication workloads. In this study, we first conduct a systematic literature\nreview to identify and analyze existing studies leveraging web application\nworkloads. Our analysis sheds light on their workload utilization, analysis\ntechniques, and high-level objectives. We further systematically analyze the\ncharacteristics of the web application workloads identified in the literature\nreview. Our analysis centers on characterizing these workloads at two distinct\ntemporal granularities: daily and weekly. We successfully identify and\ncategorize three daily and three weekly patterns within the workloads. By\nproviding a statistical characterization of these workload patterns, our study\nhighlights the uniqueness of each pattern, paving the way for the development\nof realistic workload generation and resource provisioning techniques that can\nbenefit a range of applications and research areas.",
                        "title": "Understanding Web Application Workloads and Their Applications: Systematic Literature Review and Characterization",
                        "url": "http://arxiv.org/abs/2409.12299v1"
                    },
                    {
                        "quick_summary": "Big data areas are expanding in a fast way in terms of increasing workloads\nand runtime systems, and this situation imposes a serious challenge to workload\ncharacterization, which is the foundation of innovative system and architecture\ndesign. The previous major efforts on big data benchmarking either propose a\ncomprehensive but a large amount of workloads, or only select a few workloads\naccording to so-called popularity, which may lead to partial or even biased\nobservations. In this paper, on the basis of a comprehensive big data benchmark\nsuite---BigDataBench, we reduced 77 workloads to 17 representative workloads\nfrom a micro-architectural perspective. On a typical state-of-practice\nplatform---Intel Xeon E5645, we compare the representative big data workloads\nwith SPECINT, SPECCFP, PARSEC, CloudSuite and HPCC. After a comprehensive\nworkload characterization, we have the following observations. First, the big\ndata workloads are data movement dominated computing with more branch\noperations, taking up to 92% percentage in terms of instruction mix, which\nplaces them in a different class from Desktop (SPEC CPU2006), CMP (PARSEC), HPC\n(HPCC) workloads. Second, corroborating the previous work, Hadoop and Spark\nbased big data workloads have higher front-end stalls. Comparing with the\ntraditional workloads i. e. PARSEC, the big data workloads have larger\ninstructions footprint. But we also note that, in addition to varied\ninstruction-level parallelism, there are significant disparities of front-end\nefficiencies among different big data workloads. Third, we found complex\nsoftware stacks that fail to use state-of-practise processors efficiently are\none of the main factors leading to high front-end stalls. For the same\nworkloads, the L1I cache miss rates have one order of magnitude differences\namong diverse implementations with different software stacks.",
                        "title": "Characterization and Architectural Implications of Big Data Workloads",
                        "url": "http://arxiv.org/abs/1506.07943v1"
                    },
                    {
                        "quick_summary": "Finding the similarity between two workload behaviors is helpful in 1.\ncreating proxy workloads 2. characterizing an unknown workload's behavior by\nmatching its behavior against known workloads. In this article, we propose a\nmethod to measure the similarity between two workloads using machine\nlearning-based analysis of the performance telemetry data collected for the\nexecution runs of the two workloads. We also demonstrate the accuracy of the\ntechnique by measuring the similarity between a variety of know benchmark\nworkloads.",
                        "title": "Workload Similarity Analysis using Machine Learning Techniques",
                        "url": "http://arxiv.org/abs/2210.13757v2"
                    },
                    {
                        "quick_summary": "This paper presents a proposal aiming at better understanding a workload of\nSQL queries and detecting coherent explorations hidden within the workload. In\nparticular, our work investigates SQLShare [11], a database-as-a-service\nplatform targeting scientists and data scientists with minimal database\nexperience, whose workload was made available to the research community.\nAccording to the authors of [11], this workload is the only one containing\nprimarily ad-hoc hand-written queries over user-uploaded datasets. We analyzed\nthis workload by extracting features that characterize SQL queries and we show\nhow to use these features to separate sequences of SQL queries into meaningful\nexplorations. We ran several tests over various query workloads to validate\nempirically our approach.",
                        "title": "Detecting coherent explorations in SQL workloads",
                        "url": "http://arxiv.org/abs/1907.05618v1"
                    },
                    {
                        "quick_summary": "Workloads in modern cloud data centers are becoming increasingly complex. The\nnumber of workloads running in cloud data centers has been growing\nexponentially for the last few years, and cloud service providers (CSP) have\nbeen supporting on-demand services in real-time. Realizing the growing\ncomplexity of cloud environment and cloud workloads, hardware vendors such as\nIntel and AMD are increasingly introducing cloud-specific workload acceleration\nfeatures in their CPU platforms. These features are typically targeted towards\npopular and commonly-used cloud workloads. Nonetheless, uncommon,\ncustomer-specific workloads (unknown workloads), if their characteristics are\ndifferent from common workloads (known workloads), may not realize the\npotential of the underlying platform. To address this problem of realizing the\nfull potential of the underlying platform, we develop a machine learning based\ntechnique to characterize, profile and predict workloads running in the cloud\nenvironment. Experimental evaluation of our technique demonstrates good\nprediction performance. We also develop techniques to analyze the performance\nof the model in a standalone manner.",
                        "title": "CWD: A Machine Learning based Approach to Detect Unknown Cloud Workloads",
                        "url": "http://arxiv.org/abs/2211.15739v1"
                    },
                    {
                        "quick_summary": "In the recent past, characterizing workloads has been attempted to gain a\nfoothold in the emerging serverless cloud market, especially in the large\nproduction cloud clusters of Google, AWS, and so forth. While analyzing and\ncharacterizing real workloads from a large production cloud cluster benefits\ncloud providers, researchers, and daily users, analyzing the workload traces of\nthese clusters has been an arduous task due to the heterogeneous nature of\ndata. This article proposes a scalable infrastructure based on Google's\ndataproc for analyzing the workload traces of cloud environments. We evaluated\nthe functioning of the proposed infrastructure using the workload traces of\nGoogle cloud cluster-usage-traces-v3. We perform the workload characterization\non this dataset, focusing on the heterogeneity of the workload, the variations\nin job durations, aspects of resources consumption, and the overall\navailability of resources provided by the cluster. The findings reported in the\npaper will be beneficial for cloud infrastructure providers and users while\nmanaging the cloud computing resources, especially serverless platforms.",
                        "title": "Scalable Infrastructure for Workload Characterization of Cluster Traces",
                        "url": "http://arxiv.org/abs/2205.11582v1"
                    },
                    {
                        "quick_summary": "Cloud providers introduce features (e.g., Spot VMs, Harvest VMs, and\nBurstable VMs) and optimizations (e.g., oversubscription, auto-scaling, power\nharvesting, and overclocking) to improve efficiency and reliability. To\neffectively utilize these features, it's crucial to understand the\ncharacteristics of workloads running in the cloud. However, workload\ncharacteristics can be complex and depend on multiple signals, making manual\ncharacterization difficult and unscalable. In this study, we conduct the first\nlarge-scale examination of first-party workloads at Microsoft to understand\ntheir characteristics. Through an empirical study, we aim to answer the\nfollowing questions: (1) What are the critical workload characteristics that\nimpact efficiency and reliability on cloud platforms? (2) How do these\ncharacteristics vary across different workloads? (3) How can cloud platforms\nleverage these insights to efficiently characterize all workloads at scale?\nThis study provides a deeper understanding of workload characteristics and\ntheir impact on cloud performance, which can aid in optimizing cloud services.\nAdditionally, it identifies potential areas for future research.",
                        "title": "Towards Cloud Efficiency with Large-scale Workload Characterization",
                        "url": "http://arxiv.org/abs/2405.07250v1"
                    },
                    {
                        "quick_summary": "With the widespread adoption of Large Language Models (LLMs), serving LLM\ninference requests has become an increasingly important task, attracting active\nresearch advancements. Practical workloads play an essential role in this\nprocess: they are critical for motivating and benchmarking serving techniques\nand systems. However, the existing understanding of real-world LLM serving\nworkloads is limited due to the lack of a comprehensive workload\ncharacterization. Prior analyses remain insufficient in scale and scope, thus\nfailing to fully capture intricate workload characteristics.\n  In this paper, we fill the gap with an in-depth characterization of LLM\nserving workloads collected from our worldwide cloud inference serving service,\ncovering not only language models but also emerging multimodal and reasoning\nmodels, and unveiling important new findings in each case. Moreover, based on\nour findings, we propose ServeGen, a principled framework for generating\nrealistic LLM serving workloads by composing them on a per-client basis. A\npractical use case in production validates that ServeGen avoids 50%\nunder-provisioning compared to naive workload generation, demonstrating\nServeGen's advantage in performance benchmarking. ServeGen is available at\nhttps://github.com/alibaba/ServeGen.",
                        "title": "ServeGen: Workload Characterization and Generation of Large Language Model Serving in Production",
                        "url": "http://arxiv.org/abs/2505.09999v2"
                    },
                    {
                        "quick_summary": "Smart databases are adopting artificial intelligence (AI) technologies to\nachieve {\\em instance optimality}, and in the future, databases will come with\nprepackaged AI models within their core components. The reason is that every\ndatabase runs on different workloads, demands specific resources, and settings\nto achieve optimal performance. It prompts the necessity to understand\nworkloads running in the system along with their features comprehensively,\nwhich we dub as workload characterization.\n  To address this workload characterization problem, we propose our query plan\nencoders that learn essential features and their correlations from query plans.\nOur pretrained encoders capture the {\\em structural} and the {\\em computational\nperformance} of queries independently. We show that our pretrained encoders are\nadaptable to workloads that expedite the transfer learning process. We\nperformed independent assessments of structural encoder and performance\nencoders with multiple downstream tasks. For the overall evaluation of our\nquery plan encoders, we architect two downstream tasks (i) query latency\nprediction and (ii) query classification. These tasks show the importance of\nfeature-based workload characterization. We also performed extensive\nexperiments on individual encoders to verify the effectiveness of\nrepresentation learning and domain adaptability.",
                        "title": "Database Workload Characterization with Query Plan Encoders",
                        "url": "http://arxiv.org/abs/2105.12287v1"
                    }
                ]
            }
        ]
    ],
    "ctxs": [
        {
            "search_query": "caching in LLM serving",
            "title": "Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management",
            "text": "Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for\ntask-specific Large Language Model (LLM) applications. For multi-LoRA serving,\ncaching hot KV caches and LoRA adapters in high bandwidth memory of\naccelerations can improve inference performance. However, existing Multi-LoRA\ninference systems fail to optimize serving performance like Time-To-First-Toke\n(TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore\npropose FASTLIBRA, a Multi-LoRA caching system to optimize the serving\nperformance. FASTLIBRA comprises a dependency-aware cache manager and a\nperformance-driven cache swapper. The cache manager maintains the usage\ndependencies between LoRAs and KV caches during the inference with a unified\ncaching pool. The cache swapper determines the swap-in or out of LoRAs and KV\ncaches based on a unified cost model, when the HBM is idle or busy,\nrespectively. Experimental results show that ELORA reduces the TTFT by 63.4% on\naverage, compared to state-of-the-art works.",
            "url": "http://arxiv.org/abs/2505.03756v1",
            "id": "http://arxiv.org/abs/2505.03756v1"
        },
        {
            "search_query": "caching in LLM serving",
            "title": "EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving",
            "text": "As large language models (LLMs) become widely used, their environmental\nimpact$\\unicode{x2014}$especially carbon emissions$\\unicode{x2014}$has\nattracted more attention. Prior studies focus on compute-related carbon\nemissions. In this paper, we find that storage is another key contributor. LLM\ncaching, which saves and reuses KV caches for repeated context, reduces\noperational carbon by avoiding redundant computation. However, this benefit\ncomes at the cost of embodied carbon from high-capacity, high-speed SSDs. As\nLLMs scale, the embodied carbon of storage grows significantly.\n  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching\nframework that selects the optimal cache size for LLM serving. EmbAdvisor\nprofiles different LLM tasks and uses an Integer Linear Programming (ILP)\nsolver to select cache sizes that meet SLOs while minimizing total carbon\nemissions. Overall, EmbAdvisor reduces the average carbon emissions of a\nLlama-3 70B model by 9.5% under various carbon intensities compared to a\nnon-adaptive cache scenario, and can save up to 31.2% when the carbon intensity\nis low.",
            "url": "http://arxiv.org/abs/2505.23970v1",
            "id": "http://arxiv.org/abs/2505.23970v1"
        },
        {
            "search_query": "caching in LLM serving",
            "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving",
            "text": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
            "url": "http://arxiv.org/abs/2504.07494v1",
            "id": "http://arxiv.org/abs/2504.07494v1"
        },
        {
            "search_query": "caching in LLM serving",
            "title": "Stateful Large Language Model Serving with Pensieve",
            "text": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.",
            "url": "http://arxiv.org/abs/2312.05516v3",
            "id": "http://arxiv.org/abs/2312.05516v3"
        },
        {
            "search_query": "caching in LLM serving",
            "title": "InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management",
            "text": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance across various natural language processing tasks. Serving LLM\ninference for generating long contents, however, poses a challenge due to the\nenormous memory footprint of the transient state, known as the key-value (KV)\ncache, which scales with the sequence length and batch size. In this paper, we\npresent InfiniGen, a novel KV cache management framework tailored for long-text\ngeneration, which synergistically works with modern offloading-based inference\nsystems. InfiniGen leverages the key insight that a few important tokens that\nare essential for computing the subsequent attention layer in the Transformer\ncan be speculated by performing a minimal rehearsal with the inputs of the\ncurrent layer and part of the query weight and key cache of the subsequent\nlayer. This allows us to prefetch only the essential KV cache entries (without\nfetching them all), thereby mitigating the fetch overhead from the host memory\nin offloading-based LLM serving systems. Our evaluation on several\nrepresentative LLMs shows that InfiniGen improves the overall performance of a\nmodern offloading-based system by up to 3.00x compared to prior KV cache\nmanagement methods while offering substantially better model accuracy.",
            "url": "http://arxiv.org/abs/2406.19707v1",
            "id": "http://arxiv.org/abs/2406.19707v1"
        },
        {
            "search_query": "caching in LLM serving",
            "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
            "text": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
            "url": "http://arxiv.org/abs/2402.02750v2",
            "id": "http://arxiv.org/abs/2402.02750v2"
        },
        {
            "search_query": "caching in LLM serving",
            "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
            "text": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
            "url": "http://arxiv.org/abs/2412.16434v1",
            "id": "http://arxiv.org/abs/2412.16434v1"
        },
        {
            "search_query": "caching in LLM serving",
            "title": "Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing",
            "text": "Efficiently serving large language models (LLMs) under dynamic and bursty\nworkloads remains a key challenge for real-world deployment. Existing serving\nframeworks and static model compression techniques fail to adapt to workload\nfluctuations, leading to either service-level objective (SLO) violations under\nfull-precision serving or persistent accuracy degradation with static\nquantization. We present MorphServe, a dynamic, workload-aware LLM serving\nframework based on morphological adaptation. MorphServe introduces two\nasynchronous, token-level runtime mechanisms: quantized layer swapping, which\nselectively replaces less impactful layers with quantized alternatives during\nhigh-load periods, and pressure-aware KV cache resizing, which dynamically\nadjusts KV cache capacity in response to memory pressure. These mechanisms\nenable state-preserving transitions with minimum runtime overhead and are fully\ncompatible with modern scheduling and attention techniques. Extensive\nexperiments on Vicuna and Llama family models with real-world workloads\ndemonstrate that MorphServe reduces average SLO violations by 92.45 percent and\nimproves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving,\nwithout compromising generation quality. These results establish MorphServe as\na practical and elastic solution for LLM deployment in dynamic environments.",
            "url": "http://arxiv.org/abs/2506.02006v1",
            "id": "http://arxiv.org/abs/2506.02006v1"
        },
        {
            "search_query": "caching in LLM serving",
            "title": "Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving",
            "text": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively.",
            "url": "http://arxiv.org/abs/2503.00392v1",
            "id": "http://arxiv.org/abs/2503.00392v1"
        },
        {
            "search_query": "cache eviction policies",
            "title": "Time Aware Least Recent Used (TLRU) Cache Management Policy in ICN",
            "text": "The information centric networks (ICN) can be viewed as a network of caches.\nConversely, ICN type of cache networks has distinctive features e.g, contents\npopularity, usability time of content and other factors inflicts some diverse\nrequirements for cache eviction policies. In this paper we defined four\nimportant characteristics of a suitable eviction policy for ICN. We analysed\nwell known eviction policies in view of defined characteristics. Based upon\nanalysis we propose a new eviction scheme which is well suitable for ICN type\nof cache networks.",
            "url": "http://arxiv.org/abs/1801.00390v1",
            "id": "http://arxiv.org/abs/1801.00390v1"
        },
        {
            "search_query": "cache eviction policies",
            "title": "PopNetCod: A Popularity-based Caching Policy for Network Coding enabled Named Data Networking",
            "text": "In this paper, we propose PopNetCod, a popularity-based caching policy for\nnetwork coding enabled Named Data Networking. PopNetCod is a distributed\ncaching policy, in which each router measures the local popularity of the\ncontent objects by analyzing the requests that it receives. It then uses this\ninformation to decide which Data packets to cache or evict from its content\nstore. Since network coding is used, partial caching of content objects is\nsupported, which facilitates the management of the content store. The routers\ndecide the Data packets that they cache or evict in an online manner when they\nreceive requests for Data packets. This allows the most popular Data packets to\nbe cached closer to the network edges. The evaluation of PopNetCod shows an\nimproved cache-hit rate compared to the widely used Leave Copy Everywhere\nplacement policy and the Least Recently Used eviction policy. The improved\ncache-hit rate helps the clients to achieve higher goodput, while it also\nreduces the load on the source servers.",
            "url": "http://arxiv.org/abs/1901.01187v1",
            "id": "http://arxiv.org/abs/1901.01187v1"
        },
        {
            "search_query": "cache eviction policies",
            "title": "TinyLFU: A Highly Efficient Cache Admission Policy",
            "text": "This paper proposes to use a frequency based cache admission policy in order\nto boost the effectiveness of caches subject to skewed access distributions.\nGiven a newly accessed item and an eviction candidate from the cache, our\nscheme decides, based on the recent access history, whether it is worth\nadmitting the new item into the cache at the expense of the eviction candidate.\n  Realizing this concept is enabled through a novel approximate LFU structure\ncalled TinyLFU, which maintains an approximate representation of the access\nfrequency of a large sample of recently accessed items. TinyLFU is very compact\nand light-weight as it builds upon Bloom filter theory.\n  We study the properties of TinyLFU through simulations of both synthetic\nworkloads as well as multiple real traces from several sources. These\nsimulations demonstrate the performance boost obtained by enhancing various\nreplacement policies with the TinyLFU eviction policy. Also, a new combined\nreplacement and eviction policy scheme nicknamed W-TinyLFU is presented.\nW-TinyLFU is demonstrated to obtain equal or better hit-ratios than other state\nof the art replacement policies on these traces. It is the only scheme to\nobtain such good results on all traces.",
            "url": "http://arxiv.org/abs/1512.00727v2",
            "id": "http://arxiv.org/abs/1512.00727v2"
        },
        {
            "search_query": "cache eviction policies",
            "title": "BackCache: Mitigating Contention-Based Cache Timing Attacks by Hiding Cache Line Evictions",
            "text": "Caches are used to reduce the speed differential between the CPU and memory\nto improve the performance of modern processors. However, attackers can use\ncontention-based cache timing attacks to steal sensitive information from\nvictim processes through carefully designed cache eviction sets. And L1 data\ncache attacks are widely exploited and pose a significant privacy and\nconfidentiality threat. Existing hardware-based countermeasures mainly focus on\ncache partitioning, randomization, and cache line flushing, which unfortunately\neither incur high overhead or can be circumvented by sophisticated attacks. In\nthis paper, we propose a novel hardware-software co-design called BackCache\nwith the idea of always achieving cache hits instead of cache misses to\nmitigate contention-based cache timing attacks on the L1 data cache. BackCache\nplaces the evicted cache lines from the L1 data cache into a fully-associative\nbackup cache to hide the evictions. To improve the security of BackCache, we\nintroduce a randomly used replacement policy (RURP) and a dynamic backup cache\nresizing mechanism. We also present a theoretical security analysis to\ndemonstrate the effectiveness of BackCache. Our evaluation on the gem5\nsimulator shows that BackCache can degrade the performance by 2.61%, 2.66%, and\n3.36% For OS kernel, single-thread, and multi-thread benchmarks.",
            "url": "http://arxiv.org/abs/2304.10268v5",
            "id": "http://arxiv.org/abs/2304.10268v5"
        },
        {
            "search_query": "cache eviction policies",
            "title": "Selection Policy: Fighting against Filter Effect in Network of Caches",
            "text": "Many Information Centric Networking (ICN) proposals use a network of caches\nto bring the contents closer to the consumers, reduce the load on producers and\ndecrease the unnecessary retransmission for ISPs. Nevertheless, the existing\ncache management scheme for the network of caches obtain poor performance. The\nmain reason for performance degradation in a network of caches is the filter\neffect of the replacement policy. A cache serves the requests that generate\ncache-hits and forwards the requests that generate cache-misses. This filtering\nchanges the pattern of requests and leads to decreased hit ratios in the\nsubsequent caches. In this paper, we propose a coordinated caching scheme to\nsolve the filter effect problem by introducing the selection policy. This\npolicy manages a cache such that: i) the cache obtains a high hit ratio ii) the\nmissed requests from the cache can be used by subsequent caches to obtain a\nhigh hit ratio. Our coordinated selection scheme achieves an overall hit ratio\nof a network of caches equivalent to that of edge routers with big caches.\nMoreover, our scheme decreases the average number of evictions per cache slot\nby four order of magnitude compared to the LRU universal caching.",
            "url": "http://arxiv.org/abs/1310.3584v1",
            "id": "http://arxiv.org/abs/1310.3584v1"
        },
        {
            "search_query": "cache eviction policies",
            "title": "Cache is King: Smart Page Eviction with eBPF",
            "text": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
            "url": "http://arxiv.org/abs/2502.02750v1",
            "id": "http://arxiv.org/abs/2502.02750v1"
        },
        {
            "search_query": "cache eviction policies",
            "title": "A Cache Management Scheme for Efficient Content Eviction and Replication in Cache Networks",
            "text": "To cope with the ongoing changing demands of the internet, 'in-network\ncaching' has been presented as an application solution for two decades. With\nthe advent of information-centric network (ICN) architecture, 'in-network\ncaching' becomes a network level solution. Some unique features of ICNs, e.g.,\nrapidly changing cache states, higher request arrival rates, smaller cache\nsizes, and other factors, impose diverse requirements on the content eviction\npolicies. In particular, eviction policies should be fast and lightweight. In\nthis study, we propose cache replication and eviction schemes, Conditional\nLeave Cope Everywhere (CLCE) and Least Frequent Recently Used (LFRU), which are\nwell suited for the ICN type of cache networks (CNs). The CLCE replication\nscheme reduces the redundant caching of contents; hence improves the cache\nspace utilization. LFRU approximates the Least Frequently Used (LFU) scheme\ncoupled with the Least Recently Used (LRU) scheme and is practically\nimplementable for rapidly changing cache networks like ICNs.",
            "url": "http://arxiv.org/abs/1702.04078v4",
            "id": "http://arxiv.org/abs/1702.04078v4"
        },
        {
            "search_query": "cache eviction policies",
            "title": "Prime+Retouch: When Cache is Locked and Leaked",
            "text": "Caches on the modern commodity CPUs have become one of the major sources of\nside-channel leakages and been abused as a new attack vector. To thwart the\ncache-based side-channel attacks, two types of countermeasures have been\nproposed: detection-based ones that limit the amount of microarchitectural\ntraces an attacker can leave, and cache prefetching-and-locking techniques that\nclaim to prevent such leakage by disallowing evictions on sensitive data. In\nthis paper, we present the Prime+Retouch attack that completely bypasses these\ndefense schemes by accurately inferring the cache activities with the metadata\nof the cache replacement policy. Prime+Retouch has three noticeable properties:\n1) it incurs no eviction on the victim's data, allowing us to bypass the two\nknown mitigation schemes, 2) it requires minimal synchronization of only one\nmemory access to the attacker's pre-primed cache lines, and 3) it leaks data\nvia non-shared memory, yet because underlying eviction metadata is shared.\n  We demonstrate Prime+Retouch in two architectures: predominant Intel x86 and\nemerging Apple M1. We elucidate how Prime+Retouch can break the T-table\nimplementation of AES with robust cache side-channel mitigations such as Cloak,\nunder both normal and SGX-protected environments. We also manifest feasibility\nof the Prime+Retouch attack on the M1 platform imposing more restrictions where\nthe precise measurement tools such as core clock cycle timer and performance\ncounters are inaccessible to the attacker. Furthermore, we first demystify\nundisclosed cache architecture and its eviction policy of L1 data cache on\nApple M1 architecture. We also devise a user-space noise-free cache monitoring\ntool by repurposing Intel TSX.",
            "url": "http://arxiv.org/abs/2402.15425v1",
            "id": "http://arxiv.org/abs/2402.15425v1"
        },
        {
            "search_query": "cache eviction policies",
            "title": "RELOAD+REFRESH: Abusing Cache Replacement Policies to Perform Stealthy Cache Attacks",
            "text": "Caches have become the prime method for unintended information extraction\nacross logical isolation boundaries. Even Spectre and Meltdown rely on the\ncache side channel, as it provides great resolution and is widely available on\nall major CPU platforms. As a consequence, several methods to stop cache\nattacks by detecting them have been proposed. Detection is strongly aided by\nthe fact that observing cache activity of co-resident processes is not possible\nwithout altering the cache state and thereby forcing evictions on the observed\nprocesses. In this work, we show that this widely held assumption is incorrect.\nThrough clever usage of the cache replacement policy it is possible to track a\nvictims process cache accesses without forcing evictions on the victim's data.\nHence, online detection mechanisms that rely on these evictions can be\ncircumvented as they do not detect be the introduced RELOAD+REFRESH attack. The\nattack requires a profound understanding of the cache replacement policy. We\npresent a methodology to recover the replacement policy and apply it to the\nlast five generations of Intel processors. We further show empirically that the\nperformance of RELOAD+REFRESH on cryptographic implementations is comparable to\nthat of other widely used cache attacks, while its detectability becomes\nextremely difficult, due to the negligible effect on the victims cache access\npattern.",
            "url": "http://arxiv.org/abs/1904.06278v1",
            "id": "http://arxiv.org/abs/1904.06278v1"
        },
        {
            "search_query": "cache eviction policies",
            "title": "Feedforward Neural Networks for Caching: Enough or Too Much?",
            "text": "We propose a caching policy that uses a feedforward neural network (FNN) to\npredict content popularity. Our scheme outperforms popular eviction policies\nlike LRU or ARC, but also a new policy relying on the more complex recurrent\nneural networks. At the same time, replacing the FNN predictor with a naive\nlinear estimator does not degrade caching performance significantly,\nquestioning then the role of neural networks for these applications.",
            "url": "http://arxiv.org/abs/1810.06930v1",
            "id": "http://arxiv.org/abs/1810.06930v1"
        },
        {
            "search_query": "workload characterization",
            "title": "Lauca: Generating Application-Oriented Synthetic Workloads",
            "text": "The synthetic workload is essential and critical to the performance\nevaluation of database systems. When evaluating the database performance for a\nspecific application, the similarity between synthetic workload and real\napplication workload determines the credibility of evaluation results. However,\nthe workload currently used for performance evaluation is difficult to have the\nsame workload characteristics as the target application, which leads to\ninaccurate evaluation results. To address this problem, we propose a workload\nduplicator (Lauca) that can generate synthetic workloads with highly similar\nperformance metrics for specific applications. To the best of our knowledge,\nLauca is the first application-oriented transactional workload generator. By\ncarefully studying the application-oriented synthetic workload generation\nproblem, we present the key workload characteristics (transaction logic and\ndata access distribution) of online transaction processing (OLTP) applications,\nand propose novel workload characterization and generation algorithms, which\nguarantee the high fidelity of synthetic workloads. We conduct extensive\nexperiments using workloads from TPC-C, SmallBank and micro benchmarks on both\nMySQL and PostgreSQL databases, and experimental results show that Lauca\nconsistently generates high-quality synthetic workloads.",
            "url": "http://arxiv.org/abs/1912.07172v1",
            "id": "http://arxiv.org/abs/1912.07172v1"
        },
        {
            "search_query": "workload characterization",
            "title": "Understanding Web Application Workloads and Their Applications: Systematic Literature Review and Characterization",
            "text": "Web applications, accessible via web browsers over the Internet, facilitate\ncomplex functionalities without local software installation. In the context of\nweb applications, a workload refers to the number of user requests sent by\nusers or applications to the underlying system. Existing studies have leveraged\nweb application workloads to achieve various objectives, such as workload\nprediction and auto-scaling. However, these studies are conducted in an ad hoc\nmanner, lacking a systematic understanding of the characteristics of web\napplication workloads. In this study, we first conduct a systematic literature\nreview to identify and analyze existing studies leveraging web application\nworkloads. Our analysis sheds light on their workload utilization, analysis\ntechniques, and high-level objectives. We further systematically analyze the\ncharacteristics of the web application workloads identified in the literature\nreview. Our analysis centers on characterizing these workloads at two distinct\ntemporal granularities: daily and weekly. We successfully identify and\ncategorize three daily and three weekly patterns within the workloads. By\nproviding a statistical characterization of these workload patterns, our study\nhighlights the uniqueness of each pattern, paving the way for the development\nof realistic workload generation and resource provisioning techniques that can\nbenefit a range of applications and research areas.",
            "url": "http://arxiv.org/abs/2409.12299v1",
            "id": "http://arxiv.org/abs/2409.12299v1"
        },
        {
            "search_query": "workload characterization",
            "title": "Characterization and Architectural Implications of Big Data Workloads",
            "text": "Big data areas are expanding in a fast way in terms of increasing workloads\nand runtime systems, and this situation imposes a serious challenge to workload\ncharacterization, which is the foundation of innovative system and architecture\ndesign. The previous major efforts on big data benchmarking either propose a\ncomprehensive but a large amount of workloads, or only select a few workloads\naccording to so-called popularity, which may lead to partial or even biased\nobservations. In this paper, on the basis of a comprehensive big data benchmark\nsuite---BigDataBench, we reduced 77 workloads to 17 representative workloads\nfrom a micro-architectural perspective. On a typical state-of-practice\nplatform---Intel Xeon E5645, we compare the representative big data workloads\nwith SPECINT, SPECCFP, PARSEC, CloudSuite and HPCC. After a comprehensive\nworkload characterization, we have the following observations. First, the big\ndata workloads are data movement dominated computing with more branch\noperations, taking up to 92% percentage in terms of instruction mix, which\nplaces them in a different class from Desktop (SPEC CPU2006), CMP (PARSEC), HPC\n(HPCC) workloads. Second, corroborating the previous work, Hadoop and Spark\nbased big data workloads have higher front-end stalls. Comparing with the\ntraditional workloads i. e. PARSEC, the big data workloads have larger\ninstructions footprint. But we also note that, in addition to varied\ninstruction-level parallelism, there are significant disparities of front-end\nefficiencies among different big data workloads. Third, we found complex\nsoftware stacks that fail to use state-of-practise processors efficiently are\none of the main factors leading to high front-end stalls. For the same\nworkloads, the L1I cache miss rates have one order of magnitude differences\namong diverse implementations with different software stacks.",
            "url": "http://arxiv.org/abs/1506.07943v1",
            "id": "http://arxiv.org/abs/1506.07943v1"
        },
        {
            "search_query": "workload characterization",
            "title": "Workload Similarity Analysis using Machine Learning Techniques",
            "text": "Finding the similarity between two workload behaviors is helpful in 1.\ncreating proxy workloads 2. characterizing an unknown workload's behavior by\nmatching its behavior against known workloads. In this article, we propose a\nmethod to measure the similarity between two workloads using machine\nlearning-based analysis of the performance telemetry data collected for the\nexecution runs of the two workloads. We also demonstrate the accuracy of the\ntechnique by measuring the similarity between a variety of know benchmark\nworkloads.",
            "url": "http://arxiv.org/abs/2210.13757v2",
            "id": "http://arxiv.org/abs/2210.13757v2"
        },
        {
            "search_query": "workload characterization",
            "title": "Detecting coherent explorations in SQL workloads",
            "text": "This paper presents a proposal aiming at better understanding a workload of\nSQL queries and detecting coherent explorations hidden within the workload. In\nparticular, our work investigates SQLShare [11], a database-as-a-service\nplatform targeting scientists and data scientists with minimal database\nexperience, whose workload was made available to the research community.\nAccording to the authors of [11], this workload is the only one containing\nprimarily ad-hoc hand-written queries over user-uploaded datasets. We analyzed\nthis workload by extracting features that characterize SQL queries and we show\nhow to use these features to separate sequences of SQL queries into meaningful\nexplorations. We ran several tests over various query workloads to validate\nempirically our approach.",
            "url": "http://arxiv.org/abs/1907.05618v1",
            "id": "http://arxiv.org/abs/1907.05618v1"
        },
        {
            "search_query": "workload characterization",
            "title": "CWD: A Machine Learning based Approach to Detect Unknown Cloud Workloads",
            "text": "Workloads in modern cloud data centers are becoming increasingly complex. The\nnumber of workloads running in cloud data centers has been growing\nexponentially for the last few years, and cloud service providers (CSP) have\nbeen supporting on-demand services in real-time. Realizing the growing\ncomplexity of cloud environment and cloud workloads, hardware vendors such as\nIntel and AMD are increasingly introducing cloud-specific workload acceleration\nfeatures in their CPU platforms. These features are typically targeted towards\npopular and commonly-used cloud workloads. Nonetheless, uncommon,\ncustomer-specific workloads (unknown workloads), if their characteristics are\ndifferent from common workloads (known workloads), may not realize the\npotential of the underlying platform. To address this problem of realizing the\nfull potential of the underlying platform, we develop a machine learning based\ntechnique to characterize, profile and predict workloads running in the cloud\nenvironment. Experimental evaluation of our technique demonstrates good\nprediction performance. We also develop techniques to analyze the performance\nof the model in a standalone manner.",
            "url": "http://arxiv.org/abs/2211.15739v1",
            "id": "http://arxiv.org/abs/2211.15739v1"
        },
        {
            "search_query": "workload characterization",
            "title": "Scalable Infrastructure for Workload Characterization of Cluster Traces",
            "text": "In the recent past, characterizing workloads has been attempted to gain a\nfoothold in the emerging serverless cloud market, especially in the large\nproduction cloud clusters of Google, AWS, and so forth. While analyzing and\ncharacterizing real workloads from a large production cloud cluster benefits\ncloud providers, researchers, and daily users, analyzing the workload traces of\nthese clusters has been an arduous task due to the heterogeneous nature of\ndata. This article proposes a scalable infrastructure based on Google's\ndataproc for analyzing the workload traces of cloud environments. We evaluated\nthe functioning of the proposed infrastructure using the workload traces of\nGoogle cloud cluster-usage-traces-v3. We perform the workload characterization\non this dataset, focusing on the heterogeneity of the workload, the variations\nin job durations, aspects of resources consumption, and the overall\navailability of resources provided by the cluster. The findings reported in the\npaper will be beneficial for cloud infrastructure providers and users while\nmanaging the cloud computing resources, especially serverless platforms.",
            "url": "http://arxiv.org/abs/2205.11582v1",
            "id": "http://arxiv.org/abs/2205.11582v1"
        },
        {
            "search_query": "workload characterization",
            "title": "Towards Cloud Efficiency with Large-scale Workload Characterization",
            "text": "Cloud providers introduce features (e.g., Spot VMs, Harvest VMs, and\nBurstable VMs) and optimizations (e.g., oversubscription, auto-scaling, power\nharvesting, and overclocking) to improve efficiency and reliability. To\neffectively utilize these features, it's crucial to understand the\ncharacteristics of workloads running in the cloud. However, workload\ncharacteristics can be complex and depend on multiple signals, making manual\ncharacterization difficult and unscalable. In this study, we conduct the first\nlarge-scale examination of first-party workloads at Microsoft to understand\ntheir characteristics. Through an empirical study, we aim to answer the\nfollowing questions: (1) What are the critical workload characteristics that\nimpact efficiency and reliability on cloud platforms? (2) How do these\ncharacteristics vary across different workloads? (3) How can cloud platforms\nleverage these insights to efficiently characterize all workloads at scale?\nThis study provides a deeper understanding of workload characteristics and\ntheir impact on cloud performance, which can aid in optimizing cloud services.\nAdditionally, it identifies potential areas for future research.",
            "url": "http://arxiv.org/abs/2405.07250v1",
            "id": "http://arxiv.org/abs/2405.07250v1"
        },
        {
            "search_query": "workload characterization",
            "title": "ServeGen: Workload Characterization and Generation of Large Language Model Serving in Production",
            "text": "With the widespread adoption of Large Language Models (LLMs), serving LLM\ninference requests has become an increasingly important task, attracting active\nresearch advancements. Practical workloads play an essential role in this\nprocess: they are critical for motivating and benchmarking serving techniques\nand systems. However, the existing understanding of real-world LLM serving\nworkloads is limited due to the lack of a comprehensive workload\ncharacterization. Prior analyses remain insufficient in scale and scope, thus\nfailing to fully capture intricate workload characteristics.\n  In this paper, we fill the gap with an in-depth characterization of LLM\nserving workloads collected from our worldwide cloud inference serving service,\ncovering not only language models but also emerging multimodal and reasoning\nmodels, and unveiling important new findings in each case. Moreover, based on\nour findings, we propose ServeGen, a principled framework for generating\nrealistic LLM serving workloads by composing them on a per-client basis. A\npractical use case in production validates that ServeGen avoids 50%\nunder-provisioning compared to naive workload generation, demonstrating\nServeGen's advantage in performance benchmarking. ServeGen is available at\nhttps://github.com/alibaba/ServeGen.",
            "url": "http://arxiv.org/abs/2505.09999v2",
            "id": "http://arxiv.org/abs/2505.09999v2"
        },
        {
            "search_query": "workload characterization",
            "title": "Database Workload Characterization with Query Plan Encoders",
            "text": "Smart databases are adopting artificial intelligence (AI) technologies to\nachieve {\\em instance optimality}, and in the future, databases will come with\nprepackaged AI models within their core components. The reason is that every\ndatabase runs on different workloads, demands specific resources, and settings\nto achieve optimal performance. It prompts the necessity to understand\nworkloads running in the system along with their features comprehensively,\nwhich we dub as workload characterization.\n  To address this workload characterization problem, we propose our query plan\nencoders that learn essential features and their correlations from query plans.\nOur pretrained encoders capture the {\\em structural} and the {\\em computational\nperformance} of queries independently. We show that our pretrained encoders are\nadaptable to workloads that expedite the transfer learning process. We\nperformed independent assessments of structural encoder and performance\nencoders with multiple downstream tasks. For the overall evaluation of our\nquery plan encoders, we architect two downstream tasks (i) query latency\nprediction and (ii) query classification. These tasks show the importance of\nfeature-based workload characterization. We also performed extensive\nexperiments on individual encoders to verify the effectiveness of\nrepresentation learning and domain adaptability.",
            "url": "http://arxiv.org/abs/2105.12287v1",
            "id": "http://arxiv.org/abs/2105.12287v1"
        }
    ],
    "output": "<Related Works>\nLarge language models (LLMs) have become crucial in cloud computing, and efficiently serving them is essential. Caching intermediate results (KV$) has been shown to improve serving throughput and latency. Several studies have explored caching in LLM serving. For instance, FASTLIBRA, a Multi-LoRA caching system, optimizes serving performance by caching hot KV caches and LoRA adapters in high-bandwidth memory, reducing Time-To-First-Token (TTFT) by 63.4% on average compared to state-of-the-art works [1]. EmbAdvisor, a carbon-aware caching framework, selects the optimal cache size for LLM serving, reducing average carbon emissions by 9.5% under various carbon intensities [2]. Apt-Serve, a scalable framework, features a hybrid cache scheme and adaptive runtime scheduling, achieving up to 8.8x improvement in effective throughput compared to state-of-the-art inference serving systems [3]. Pensieve, a system optimized for multi-turn conversation LLM serving, maintains conversation state across requests by caching previously processed history, achieving 1.14-3.0x the throughput of vLLM and TensorRT-LLM and significantly reducing latency [4]. InfiniGen, a KV cache management framework, synergistically works with modern offloading-based inference systems, improving overall performance by up to 3.00x compared to prior KV cache management methods [5]. \n\nKIVI, a 2-bit KV cache quantization algorithm, enables Llama, Falcon, and Mistral models to maintain almost the same quality while using 2.6x less peak memory, bringing 2.35x-3.47x throughput on real LLM inference workload [6]. SYMPHONY, a system leveraging multi-turn workloads to migrate K,V caches off the critical serving path, handles over 8x the number of requests compared to state-of-the-art baselines, with a similar latency profile [7]. MorphServe, a dynamic, workload-aware LLM serving framework, reduces average SLO violations by 92.45 percent and improves P95 TTFT latency by 2.2x-3.9x compared to full-precision serving [8]. PSA, a progressive sparse attention mechanism, reduces KV cache usage for attention computation by up to 2.4x and increases end-to-end serving throughput by up to 1.4x and 2.0x, compared to state-of-the-art DSAes and systems without sparse attention, respectively [9].\n\nCache eviction policies have also been studied extensively. TLRU, a time-aware least recent used cache management policy, is suitable for ICN-type cache networks [10]. PopNetCod, a popularity-based caching policy, improves cache-hit rate compared to widely used Leave Copy Everywhere placement policy and Least Recently Used eviction policy [11]. TinyLFU, a highly efficient cache admission policy, boosts the effectiveness of caches subject to skewed access distributions [12]. BackCache, a novel hardware-software co-design, mitigates contention-based cache timing attacks on the L1 data cache [13]. \n\nWorkload characterization is essential for understanding the behavior of different workloads. Lauca, a workload duplicator, generates synthetic workloads with highly similar performance metrics for specific applications [14]. A systematic literature review and characterization of web application workloads identify daily and weekly patterns within the workloads [15]. Big data workload characterization reveals that big data workloads are data movement dominated computing with more branch operations [16]. \n\nOur work contributes to the understanding of KV$ workload patterns from a leading LLM service provider and proposes a workload-aware cache eviction policy that improves serving performance under real-world traces.\n\n</Related Works>\n\n<references>\n[1] http://arxiv.org/abs/2505.03756v1\n[2] http://arxiv.org/abs/2505.23970v1\n[3] http://arxiv.org/abs/2504.07494v1\n[4] http://arxiv.org/abs/2312.05516v3\n[5] http://arxiv.org/abs/2406.19707v1\n[6] http://arxiv.org/abs/2402.02750v2\n[7] http://arxiv.org/abs/2412.16434v1\n[8] http://arxiv.org/abs/2506.02006v1\n[9] http://arxiv.org/abs/2503.00392v1\n[10] http://arxiv.org/abs/1801.00390v1\n[11] http://arxiv.org/abs/1901.01187v1\n[12] http://arxiv.org/abs/1512.00727v2\n[13] http://arxiv.org/abs/2304.10268v5\n[14] http://arxiv.org/abs/1912.07172v1\n[15] http://arxiv.org/abs/2409.12299v1\n"
}