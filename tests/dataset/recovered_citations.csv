parent_paper_title,parent_paper_arxiv_id,citation_shorthand,raw_citation_text,cited_paper_title,cited_paper_arxiv_link,cited_paper_abstract,has_metadata,is_arxiv_paper,bib_paper_authors,bib_paper_year,bib_paper_month,bib_paper_url,bib_paper_doi,bib_paper_journal,original_title,search_res_title,search_res_url,search_res_content
TaxAgent: How Large Language Model Designs Fiscal Policy,2506.02838v1,NBERw21340,\cite{NBERw21340},Effective Policy for Reducing Inequality? The Earned Income Tax Credit and the Distribution of Income,,,True,False,"Hoynes, Hilary W and Patel, Ankur J",2015.0,July,http://www.nber.org/papers/w21340,10.3386/w21340,,Effective Policy for Reducing Inequality? The Earned Income Tax Credit and the Distribution of Income,Effective Policy for Reducing Inequality? The Earned Income Tax Credit ...,https://www.nber.org/papers/w21340,"We use a quasi-experiment approach, using variation in generosity due to policy expansions across tax years and family sizes. Our results show that a policy-induced $1000 increase in the EITC leads to a 7.3 percentage point increase in employment and a 9.4 percentage point reduction in the share of families with after-tax and transfer income"
TaxAgent: How Large Language Model Designs Fiscal Policy,2506.02838v1,NBERw21211,\cite{NBERw21211},The Earned Income Tax Credit (EITC),,,True,False,"Nichols, Austin and Rothstein, Jesse",2015.0,May,http://www.nber.org/papers/w21211,10.3386/w21211,,The Earned Income Tax Credit (EITC),Earned Income Tax Credit (EITC) | Internal Revenue Service,https://www.irs.gov/credits-deductions/individuals/earned-income-tax-credit-eitc,"The Earned Income Tax Credit (EITC) helps low- to moderate-income workers and families get a tax break. If you qualify, you can use the credit to reduce the taxes you owe - and maybe increase your refund. Who qualifies. You may claim the EITC if your income is low- to moderate. The amount of your credit may change if you have children"
TaxAgent: How Large Language Model Designs Fiscal Policy,2506.02838v1,Foo2019ProcessAC,\cite{Foo2019ProcessAC},Process and Critical Approaches to Solving the Systemic Climate Change Governance Problem,,,True,False,Check Woo Foo,2019.0,,https://api.semanticscholar.org/CorpusID:235319207,,Politics \& Energy eJournal,Process and Critical Approaches to Solving the Systemic Climate Change Governance Problem,Process and Critical Approaches to Solving the Systemic Climate Change ...,https://www.researchgate.net/publication/349946409_Process_and_Critical_Approaches_to_Solving_the_Systemic_Climate_Change_Governance_Problem,Evolution of climate change governance hewed closely to non-legal critical success factors of clear and updated science and iterative approach to policy and legislation but was constrained by
TaxAgent: How Large Language Model Designs Fiscal Policy,2506.02838v1,Patjoshi2015DesignAD,\cite{Patjoshi2015DesignAD},Design and Development of Advanced Control strategies for Power Quality Enhancement at Distribution Level,,,True,False,Rajesh Kumar Patjoshi,2015.0,,https://api.semanticscholar.org/CorpusID:112918597,,,Design and Development of Advanced Control strategies for Power Quality Enhancement at Distribution Level,Design and Development of Advanced Control strategies for Power Quality ...,http://ethesis.nitrkl.ac.in/6971/,"Therefore, the main focus behind this thesis is to develop advanced control strategies that improve the compensation capability of the UPQC so that power quality issues of distribution network are efficiently improved. Firstly, the current harmonics are considered and are compensated by using the shunt active power filter (SAPF)."
TaxAgent: How Large Language Model Designs Fiscal Policy,2506.02838v1,10.1257/jep.25.4.165,\cite{10.1257/jep.25.4.165},The Case for a Progressive Tax: From Basic Research to Policy Recommendations,,,True,False,"Diamond, Peter and Saez, Emmanuel",2011.0,December,https://www.aeaweb.org/articles?id=10.1257/jep.25.4.165,10.1257/jep.25.4.165,Journal of Economic Perspectives,The Case for a Progressive Tax: From Basic Research to Policy Recommendations,The Case for a Progressive Tax: From Basic Research to Policy ...,https://www.aeaweb.org/articles?id=10.1257/jep.25.4.165,"The Case for a Progressive Tax: From Basic Research to Policy Recommendations by Peter Diamond and Emmanuel Saez. Published in volume 25, issue 4, pages 165-90 of Journal of Economic Perspectives, Fall 2011, Abstract: This paper presents the case for tax progressivity based on recent results in opti"
TaxAgent: How Large Language Model Designs Fiscal Policy,2506.02838v1,10.2307/2296779,\cite{10.2307/2296779},An Exploration in the Theory of Optimum Income Taxation12,,,True,False,"Mirrlees, J. A.",1971.0,04,https://doi.org/10.2307/2296779,10.2307/2296779,The Review of Economic Studies,An Exploration in the Theory of Optimum Income Taxation12,An Exploration in the Theory of Optimum Income Taxation12,https://academic.oup.com/restud/article-abstract/38/2/175/1527903,"J. A. Mirrlees; An Exploration in the Theory of Optimum Income Taxation12, The Review of Economic Studies, Volume 38, Issue 2, 1 April 1971, Pages 175-208,"
TaxAgent: How Large Language Model Designs Fiscal Policy,2506.02838v1,RePEc:aea:aecrev:v:61:y:1971:i:1:p:8-27,\cite{RePEc:aea:aecrev:v:61:y:1971:i:1:p:8-27},Optimal Taxation and Public Production: I--Production Efficiency,,,True,False,"Diamond, Peter and Mirrlees, James",1971.0,,https://EconPapers.repec.org/RePEc:aea:aecrev:v:61:y:1971:i:1:p:8-27,,American Economic Review,Optimal Taxation and Public Production: I--Production Efficiency,Optimal Taxation and Public Production: I--Production Efficiency,https://www.researchgate.net/publication/4727502_Optimal_Taxation_and_Public_Production_I--Production_Efficiency,"PDF | On Feb 1, 1971, Peter A. Diamond and others published Optimal Taxation and Public Production: I--Production Efficiency | Find, read and cite all the research you need on ResearchGate"
TaxAgent: How Large Language Model Designs Fiscal Policy,2506.02838v1,10.1111/1467-937X.00166,\cite{10.1111/1467-937X.00166},Using Elasticities to Derive Optimal Income Tax Rates,,,True,False,"Saez, Emmanuel",2001.0,01,https://doi.org/10.1111/1467-937X.00166,10.1111/1467-937X.00166,The Review of Economic Studies,Using Elasticities to Derive Optimal Income Tax Rates,PDF,https://eml.berkeley.edu/~saez/derive.pdf,"This paper derives optimal income tax formulas using compensated and uncompensated elasticities of earnings with respect to tax rates. It shows how the shape of the income distribution, the substitution and income effects, and the heterogeneity of preferences affect the equity-efficiency tradeoff in optimal taxation."
TaxAgent: How Large Language Model Designs Fiscal Policy,2506.02838v1,10.1257/pol.6.1.230,\cite{10.1257/pol.6.1.230},Optimal Taxation of Top Labor Incomes: A Tale of Three Elasticities,,,True,False,"Piketty, Thomas and Saez, Emmanuel and Stantcheva, Stefanie",2014.0,February,https://www.aeaweb.org/articles?id=10.1257/pol.6.1.230,10.1257/pol.6.1.230,American Economic Journal: Economic Policy,Optimal Taxation of Top Labor Incomes: A Tale of Three Elasticities,Optimal Taxation of Top Labor Incomes: A Tale of Three Elasticities,https://www.nber.org/papers/w17616,We derive the optimal top tax rate formula as a function of the three corresponding behavioral elasticities. The first elasticity (labor supply) is the sole real factor limiting optimal top tax rates. The optimal tax system should be designed to minimize the second elasticity (avoidance) through tax enforcement and tax neutrality across income
TaxAgent: How Large Language Model Designs Fiscal Policy,2506.02838v1,10.1257/pol.20180033,\cite{10.1257/pol.20180033},Optimal Income Taxation with Unemployment and Wage Responses: A Sufficient Statistics Approach,,,True,False,"Kroft, Kory and Kucko, Kavan and Lehmann, Etienne and Schmieder, Johannes",2020.0,February,https://www.aeaweb.org/articles?id=10.1257/pol.20180033,10.1257/pol.20180033,American Economic Journal: Economic Policy,Optimal Income Taxation with Unemployment and Wage Responses: A Sufficient Statistics Approach,Optimal Income Taxation with Unemployment and Wage Responses: A ...,https://www.aeaweb.org/articles?id=10.1257/pol.20180033,"(February 2020) - We derive a sufficient statistics tax formula in a model that incorporates unemployment and endogenous wages to study the shape of the optimal income tax. Key sufficient statistics are the macro employment response to taxation, the micro and macro participation response to taxation, and the wage-moderating effect of tax"
TaxAgent: How Large Language Model Designs Fiscal Policy,2506.02838v1,NBERc14009,\cite{NBERc14009},The Impact of Machine Learning on Economics,,,True,False,Susan Athey,2018.0,January,http://www.nber.org/chapters/c14009,,,The Impact of Machine Learning on Economics,The Impact of Machine Learning on Economics | NBER,https://www.nber.org/books-and-chapters/economics-artificial-intelligence-agenda/impact-machine-learning-economics,"The Impact of Machine Learning on Economics | NBER Development Economics Economics of Health Training Program in Aging and Health Economics The Impact of Machine Learning on Economics Finally, we overview a set of broader predictions about the future impact of machine learning on economics, including its impacts on the nature of collaboration, funding, research tools, and research questions. Comment on ""The Impact of Machine Learning on Economics"" In addition to working papers, the NBER disseminates affiliates’ latest findings through a range of free periodicals — the NBER Reporter, the NBER Digest, the Bulletin on Retirement and Disability, the Bulletin on Health, and the Bulletin on Entrepreneurship — as well as online conference reports, video lectures, and interviews. National Bureau of Economic Research"
TaxAgent: How Large Language Model Designs Fiscal Policy,2506.02838v1,AxtellFarmer2022,\cite{AxtellFarmer2022},"Agent Based Modeling in Economics and Finance: Past, Present, and Future",,,True,False,"Axtell, R. and Farmer, J.",2022.0,,,,Journal of Economic Literature,"Agent Based Modeling in Economics and Finance: Past, Present, and Future","Agent-Based Modeling in Economics and Finance: Past, Present, and ...",https://www.aeaweb.org/articles?id=10.1257/jel.20221319,"Agent-Based Modeling in Economics and Finance: Past, Present, and Future by Robert L. Axtell and J. Doyne Farmer. Published in volume 63, issue 1, pages 197-287 of Journal of Economic Literature, March 2025, Abstract: Agent-based modeling (ABM) is a novel computational methodology for representing t"
TaxAgent: How Large Language Model Designs Fiscal Policy,2506.02838v1,DelliGatti2018,\cite{DelliGatti2018},Contents,,,True,False,"Delli Gatti, Domenico and Fagiolo, Giorgio and Gallegati, Mauro and Richiardi, Matteo and Russo, Alberto",2018.0,,,,,Contents,Content vs. Contents: What's the Difference? - Grammarly,https://www.grammarly.com/commonly-confused-words/content-vs-contents,"Learn how to use content and contents correctly in different contexts. Content refers to the ideas or subject matter of a medium, while contents refers to the individual items or components of a container."
TaxAgent: How Large Language Model Designs Fiscal Policy,2506.02838v1,nie2024surveylargelanguagemodels,\cite{nie2024surveylargelanguagemodels},"A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges",,,True,False,Yuqi Nie and Yaxuan Kong and Xiaowen Dong and John M. Mulvey and H. Vincent Poor and Qingsong Wen and Stefan Zohren,2024.0,,https://arxiv.org/abs/2406.11903,,,"A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges",A Survey of Large Language Models for Financial Applications: Progress ...,https://arxiv.org/abs/2406.11903,"Recent advances in large language models (LLMs) have unlocked novel opportunities for machine learning applications in the financial domain. These models have demonstrated remarkable capabilities in understanding context, processing vast amounts of data, and generating human-preferred contents. In this survey, we explore the application of LLMs on various financial tasks, focusing on their"
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,cachedattention,\cite{cachedattention},"Cost-Efficient Large Language Model Serving for Multi-turn Conversations
               with CachedAttention",,,True,False,"Bin Gao and
               Zhuomin He and
               Puru Sharma and
               Qingxuan Kang and
               Djordje Jevdjic and
               Junbo Deng and
               Xingkun Yang and
               Zhou Yu and
               Pengfei Zuo",2024.0,,https://www.usenix.org/conference/atc24/presentation/gao-bin-cost,,,"Cost-Efficient Large Language Model Serving for Multi-turn Conversations
               with CachedAttention",Cost-Efficient Large Language Model Serving for Multi-turn ...,https://arxiv.org/abs/2403.19708,"Interacting with humans through multi-turn conversations is a fundamental feature of large language models (LLMs). However, existing LLM serving engines executing multi-turn conversations are inefficient due to the need to repeatedly compute the key-value (KV) caches of historical tokens, incurring high serving costs. To address the problem, this paper proposes CachedAttention, a new attention"
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,sglang,\cite{sglang},Efficiently Programming Large Language Models using SGLang,,,True,False,"Lianmin Zheng and
                Liangsheng Yin and
                Zhiqiang Xie and
                Jeff Huang and
                Chuyue Sun and
                Cody Hao Yu and
                Shiyi Cao and
                Christos Kozyrakis and
                Ion Stoica and
                Joseph E. Gonzalez and
                Clark W. Barrett and
                Ying Sheng",2023.0,,https://doi.org/10.48550/arXiv.2312.07104,10.48550/ARXIV.2312.07104,CoRR,Efficiently Programming Large Language Models using SGLang,Efficiently Programming Large Language Models using SGLang,https://openreview.net/forum?id=RSvd7Wfto6,"Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs."
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,openaiapi,\cite{openaiapi},OpenAI developer platform,,,True,False,OpenAI,,,,,,OpenAI developer platform,API Platform | OpenAI,https://openai.com/api/,"OpenAI API Platform provides access to powerful and versatile models for text and vision tasks, as well as tools for fine-tuning, customization, and agent building. Explore the features, pricing, and use cases of the platform and sign up for a developer account."
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,genimiapi,\cite{genimiapi},Gemini API,,,True,False,Google,2025.0,,,,,Gemini API,Gemini API | Google AI for Developers,https://ai.google.dev/gemini-api/docs,"Gemini API | Google AI for Developers Image 1: Google AI for Developers Gemini Gemini API Google AI Studio Google AI Edge Gemini API Developer Competition Google AI Forum Image 2: Google AI for Developers Gemini API docs Google AI Studio quickstart VertexAI Gemini API Gemini  Gemini API Google AI Studio Google AI Edge Gemini API Developer Competition Google AI Forum Gemini API Gemini Developer API Get a Gemini API Key response = client.models.generate_content( model=""gemini-2.0-flash"", const response = await ai.models.generateContent({ model: ""gemini-2.0-flash"", genai.Text(""Explain how AI works in a few words""), import com.google.genai.Client; Use Gemini in Google AI Studio Input millions of tokens to Gemini models and derive understanding from unstructured images, videos, and documents. ### Start building with the Gemini API"
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,claudeapi,\cite{claudeapi},Claude API,,,True,False,Anthropic,2025.0,,,,,Claude API,Build with Claude \ Anthropic,https://www.anthropic.com/api,"Build with Claude \ Anthropic Skip to main contentSkip to footer Claude API Solutions Research Commitments Learn News Try Claude Build with Claude Create user-facing experiences, new products, and new ways to work with the most advanced AI models on the market. Start buildingDeveloper docs Get started Self-serve Launch your own generative AI solution with: Access to all Claude models Usage-based tiers Automatically increasing rate limits Simple pay-as-you-go pricing Self-serve deployment on workbench Prompting guides & developer documentation Start building Need additional support? Light & fast Haiku Our fastest model that can execute lightweight actions, with industry-leading speed. Hard-working Sonnet Our best combination of performance and speed for efficient, high-throughput tasks. Powerful Opus Our highest-performing model, which can handle complex analysis, longer tasks with many steps, and higher-order math and coding tasks."
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,mooncake,\cite{mooncake},Mooncake Trace,,,True,False,,2025.0,,,,,Mooncake Trace,Welcome to Mooncake — Mooncake - kvcache-ai.github.io,https://kvcache-ai.github.io/Mooncake/,"Dec 16, 2024: vLLM officially supports Mooncake Transfer Engine for disaggregated prefilling and KV cache transfer. Nov 28, 2024: We open sourced the Transfer Engine, the central component of Mooncake. We also provide two demonstrations of Transfer Engine: a P2P Store and vLLM integration. July 9, 2024: We open sourced the trace as a jsonl file."
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,hu2024epic,\cite{hu2024epic},EPIC: Efficient Position-Independent Context Caching for Serving Large Language Models,,,True,False,Junhao Hu and Wenrui Huang and Haoyi Wang and Weidong Wang and Tiancheng Hu and Qin Zhang and Hao Feng and Xusheng Chen and Yizhou Shan and Tao Xie,2024.0,,https://arxiv.org/abs/2410.15332,,,EPIC: Efficient Position-Independent Context Caching for Serving Large Language Models,EPIC: Efficient Position-Independent Caching for Serving Large Language ...,https://arxiv.org/abs/2410.15332,"Large Language Models (LLMs) show great capabilities in a wide range of applications, but serving them efficiently becomes increasingly challenging as requests (prompts) become more complex. Context caching improves serving performance by reusing Key-Value (KV) vectors, the intermediate representations of tokens that are repeated across requests. However, existing context caching requires"
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,h2o,\cite{h2o},"{H2O:} Heavy-Hitter Oracle for Efficient Generative Inference of Large
               Language Models",,,True,False,"Zhenyu Zhang and
               Ying Sheng and
               Tianyi Zhou and
               Tianlong Chen and
               Lianmin Zheng and
               Ruisi Cai and
               Zhao Song and
               Yuandong Tian and
               Christopher R{\'{e}} and
               Clark W. Barrett and
               Zhangyang Wang and
               Beidi Chen",2023.0,,http://papers.nips.cc/paper\_files/paper/2023/hash/6ceefa7b15572587b78ecfcebb2827f8-Abstract-Conference.html,,,"{H2O:} Heavy-Hitter Oracle for Efficient Generative Inference of Large
               Language Models",H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of ...,https://arxiv.org/abs/2306.14048,Abstract page for arXiv paper 2306.14048: H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models.
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,lruk,\cite{lruk},The {LRU-K} Page Replacement Algorithm For Database Disk Buffering,,,True,False,"Elizabeth J. O'Neil and
               Patrick E. O'Neil and
               Gerhard Weikum",1993.0,,https://doi.org/10.1145/170035.170081,10.1145/170035.170081,,The {LRU-K} Page Replacement Algorithm For Database Disk Buffering,The LRU-K page replacement algorithm for database disk buffering,https://dl.acm.org/doi/10.1145/170036.170081,"This paper introduces a new approach to database disk buffering, called the LRU-K method. The basic idea of LRU-K is to keep track of the times of the last K references to popular database pages, using this information to statistically estimate the interarrival times of references on a page by page basis. Although the LRU-K approach performs optimal statistical inference under relatively"
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,slru,\cite{slru},Caching Strategies to Improve Disk System Performance,,,True,False,"Ramakrishna Karedla and
               J. Spencer Love and
               Bradley G. Wherry",1994.0,,https://doi.org/10.1109/2.268884,10.1109/2.268884,Computer,Caching Strategies to Improve Disk System Performance,Caching strategies to improve disk system performance,https://ieeexplore.ieee.org/document/268884,"In this article, we examine the use of caching as a means to increase system response time and improve the data throughput of the disk subsystem. Caching can help to alleviate I/O subsystem bottlenecks caused by mechanical latencies. This article describes a caching strategy that offers the performance of caches twice its size."
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,twoq,\cite{twoq},"2Q: {A} Low Overhead High Performance Buffer Management Replacement
                  Algorithm",,,True,False,"Theodore Johnson and
                  Dennis E. Shasha",1994.0,,http://www.vldb.org/conf/1994/P439.PDF,,,"2Q: {A} Low Overhead High Performance Buffer Management Replacement
                  Algorithm",2Q | Proceedings of the 20th International Conference on Very Large ...,https://dl.acm.org/doi/10.5555/645920.672996,"2Q: A Low Overhead High Performance Buffer Management Replacement Algorithm. Authors: Theodore Johnson, Dennis Shasha Authors Info & Claims. ... A Low Overhead High Performance Buffer Management Replacement Algorithm. Pages 439 - 450. PREVIOUS CHAPTER. Dual-Buffering Strategies in Object Bases. Previous."
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,eelru,\cite{eelru},{EELRU:} Simple and Effective Adaptive Page Replacement,,,True,False,"Yannis Smaragdakis and
                  Scott F. Kaplan and
                  Paul R. Wilson",1999.0,,https://doi.org/10.1145/301453.301486,10.1145/301453.301486,,{EELRU:} Simple and Effective Adaptive Page Replacement,EELRU: Simple and Effective Adaptive Page Replacement - ResearchGate,https://www.researchgate.net/publication/2822757_EELRU_Simple_and_Effective_Adaptive_Page_Replacement,"EELRU is a simple adaptive replacement algorithm, which uses only the kind of information needed by LRU---how recently each page has been touched relative to the others."
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,lrfu,\cite{lrfu},"{LRFU:} {A} Spectrum of Policies that Subsumes the Least Recently
                  Used and Least Frequently Used Policies",,,True,False,"Donghee Lee and
                  Jongmoo Choi and
                  Jong{-}Hun Kim and
                  Sam H. Noh and
                  Sang Lyul Min and
                  Yookun Cho and
                  Chong{-}Sang Kim",2001.0,,https://doi.org/10.1109/TC.2001.970573,10.1109/TC.2001.970573,{IEEE} Trans. Computers,"{LRFU:} {A} Spectrum of Policies that Subsumes the Least Recently
                  Used and Least Frequently Used Policies",LRFU: A Spectrum of Policies that Subsumes the Least Recently ...,https://www.researchgate.net/publication/220328843_LRFU_A_Spectrum_of_Policies_that_Subsumes_the_Least_Recently_Used_and_Least_Frequently_Used_Policies,The spectrum is called the LRFU (Least Recently/Frequently Used) policy and is formed by how much more weight we give to the recent history than to the older
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,lirs,\cite{lirs},"{LIRS:} an efficient low inter-reference recency set replacement policy
               to improve buffer cache performance",,,True,False,"Song Jiang and
               Xiaodong Zhang",2002.0,,https://doi.org/10.1145/511334.511340,10.1145/511334.511340,,"{LIRS:} an efficient low inter-reference recency set replacement policy
               to improve buffer cache performance",LIRS: an efficient low inter-reference recency set replacement policy ...,https://dl.acm.org/doi/10.1145/511399.511340,"LIRS: an efficient low inter-reference recency set replacement policy to improve buffer cache performance Authors : Song Jiang , Xiaodong Zhang Authors Info & Claims ACM SIGMETRICS Performance Evaluation Review , Volume 30 , Issue 1"
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,arc,\cite{arc},"{ARC:} {A} Self-Tuning, Low Overhead Replacement Cache",,,True,False,"Nimrod Megiddo and
               Dharmendra S. Modha",2003.0,,http://www.usenix.org/events/fast03/tech/megiddo.html,,,"{ARC:} {A} Self-Tuning, Low Overhead Replacement Cache","ARC: A Self-Tuning, Low Overhead Replacement Cache",https://www.usenix.org/conference/fast-03/arc-self-tuning-low-overhead-replacement-cache,"ARC: A Self-Tuning, Low Overhead Replacement Cache | USENIX ARC: A Self-Tuning, Low Overhead Replacement Cache We propose a new cache management policy, namely, Adaptive Replacement Cache (ARC), that has several advantages. The policy ARC is empirically universal, that is, it empirically performs as well as a certain fixed replacement policy—even when the latter uses the best workload-specific tuning parameter that was selected in an offline fashion. Consequently, ARC works uniformly well across varied workloads and cache sizes without any need for workload specific a priori knowledge or tuning. Nimrod Megiddo, IBM Almaden Research Center Modha, IBM Almaden Research Center title = {{ARC}: A {Self-Tuning}, Low Overhead Replacement Cache}, url = {https://www.usenix.org/conference/fast-03/arc-self-tuning-low-overhead-replacement-cache}, http://www.usenix.org/events/fast03/tech/full_papers/megiddo/megiddo.pdf"
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,mq,\cite{mq},Second-Level Buffer Cache Management,,,True,False,"Yuanyuan Zhou and
                  Zhifeng Chen and
                  Kai Li",2004.0,,https://doi.org/10.1109/TPDS.2004.13,10.1109/TPDS.2004.13,{IEEE} Trans. Parallel Distributed Syst.,Second-Level Buffer Cache Management,Second-level buffer cache management - IEEE Xplore,https://ieeexplore.ieee.org/document/1291820,"Buffer caches are commonly used in servers to reduce the number of slow disk accesses or network messages. These buffer caches form a multilevel buffer cache hierarchy. In such a hierarchy, second-level buffer caches have different access patterns from first-level buffer caches because accesses to a second-level are actually misses from a first-level. Therefore, commonly used cache management"
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,car,\cite{car},{CAR:} Clock with Adaptive Replacement,,,True,False,"Sorav Bansal and
                  Dharmendra S. Modha",2004.0,,http://www.usenix.org/events/fast04/tech/bansal.html,,,{CAR:} Clock with Adaptive Replacement,[1503.07624] Analyzing Adaptive Cache Replacement Strategies - arXiv.org,https://arxiv.org/abs/1503.07624,"Adaptive Replacement Cache (ARC) and CLOCK with Adaptive Replacement (CAR) are state-of-the- art ""adaptive"" cache replacement algorithms invented to improve on the shortcomings of classical cache replacement policies such as LRU, LFU and CLOCK. By separating out items that have been accessed only once and items that have been accessed more frequently, both ARC and CAR are able to control the"
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,clockpro,\cite{clockpro},CLOCK-Pro: An Effective Improvement of the {CLOCK} Replacement,,,True,False,"Song Jiang and
                  Feng Chen and
                  Xiaodong Zhang",2005.0,,http://www.usenix.org/events/usenix05/tech/general/jiang.html,,,CLOCK-Pro: An Effective Improvement of the {CLOCK} Replacement,Paper Title: CLOCK-Pro: An Effective Improvement of the CLOCK Replacement,http://oss.csie.fju.edu.tw/~an96/nov05.htm,"Paper Title: CLOCK-Pro: An Effective Improvement of the CLOCK Replacement Date: November 5, 2007 ... CLOCK-Pro is a low cost page replacement algorithm that can be easily accepted by current systems. 2. CLOCK-pro provides a systematic solution to address the CLOCK problems. 3. CLOCK-pro can fully adaptive to strong or weak access patterns"
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,lhd,\cite{lhd},{LHD:} Improving Cache Hit Rate by Maximizing Hit Density,,,True,False,"Nathan Beckmann and
               Haoxian Chen and
               Asaf Cidon",2018.0,,https://www.usenix.org/conference/nsdi18/presentation/beckmann,,,{LHD:} Improving Cache Hit Rate by Maximizing Hit Density,LHD: Improving Cache Hit Rate by Maximizing Hit Density,https://www.usenix.org/conference/nsdi18/presentation/beckmann,"To make LHD practical, we design and implement RankCache, an efficient key-value cache based on memcached. We evaluate RankCache and LHD on commercial memcached and enterprise storage traces, where LHD consistently achieves better hit rates than prior policies. LHD requires much less space than prior policies to match their hit rate, on average"
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,cacheus,\cite{cacheus},Learning Cache Replacement with {CACHEUS},,,True,False,"Liana V. Rodriguez and
               Farzana Beente Yusuf and
               Steven Lyons and
               Eysler Paz and
               Raju Rangaswami and
               Jason Liu and
               Ming Zhao and
               Giri Narasimhan",2021.0,,https://www.usenix.org/conference/fast21/presentation/rodriguez,,,Learning Cache Replacement with {CACHEUS},GitHub - sylab/cacheus: The design and algorithms used in Cacheus are ...,https://github.com/sylab/cacheus,"The relevant paper to cite for follow-up or related work on Cacheus is: @inproceedings {cacheus-fast21, author = {Liana V. Rodriguez and Farzana Yusuf and Steven Lyons and Eysler Paz and Raju Rangaswami and Jason Liu and Ming Zhao and Giri Narasimhan}, title = {Learning Cache Replacement with {CACHEUS}}, booktitle = {19th {USENIX} Conference on File and Storage Technologies ({FAST} 21)}, year"
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,sieve,\cite{sieve},"{SIEVE} is Simpler than {LRU:} an Efficient Turn-Key Eviction Algorithm
               for Web Caches",,,True,False,"Yazhuo Zhang and
               Juncheng Yang and
               Yao Yue and
               Ymir Vigfusson and
               K. V. Rashmi",2024.0,,https://www.usenix.org/conference/nsdi24/presentation/zhang-yazhuo,,,"{SIEVE} is Simpler than {LRU:} an Efficient Turn-Key Eviction Algorithm
               for Web Caches",,,
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,cherkasova1998improving,\cite{cherkasova1998improving},Improving WWW proxies performance with greedy-dual-size-frequency caching policy,,,True,False,"Cherkasova, Ludmila",1998.0,,,,,Improving WWW proxies performance with greedy-dual-size-frequency caching policy,Greedy-Dual-Size-Frequency Caching Policy - shiftleft.com,http://shiftleft.com/mirrors/www.hpl.hp.com/personal/Lucy_Cherkasova/projects/gdfs.html,"We introduce Greedy-Dual-Size-Frequency caching policy to maximize hit and byte hit rates for WWW proxies. Proposed caching strategy incorporates in a simple way the most important characterizations of the file and its accesses such as file size, file access frequency and recentness of the last access. ... L. Cherkasova: Improving WWW Proxies"
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,yang2020twemcache,\cite{yang2020twemcache},A large scale analysis of hundreds of in-memory cache clusters at Twitter,,,True,False,Juncheng Yang and Yao Yue and K. V. Rashmi,2020.0,,https://www.usenix.org/conference/osdi20/presentation/yang,,,A large scale analysis of hundreds of in-memory cache clusters at Twitter,Reading Group. A large scale analysis of hundreds of in-memory cache ...,https://charap.co/reading-group-a-large-scale-analysis-of-hundreds-of-in-memory-cache-clusters-at-twitter/,"In the 41st distributed systems reading group meeting, we have looked at in-memory caches through the lens of yet another OSDI20 paper: ""A large scale analysis of hundreds of in-memory cache clusters at Twitter. "" This paper explores various cache usages at Twitter and distills the findings into a digestible set of figures."
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,berg2020cachelib,\cite{berg2020cachelib},The {CacheLib} Caching Engine: Design and Experiences at Scale,,,True,False,Benjamin Berg and Daniel S. Berger and Sara McAllister and Isaac Grosof and Sathya Gunasekar and Jimmy Lu and Michael Uhlar and Jim Carrig and Nathan Beckmann and Mor Harchol-Balter and Gregory R. Ganger,2020.0,,https://www.usenix.org/conference/osdi20/presentation/berg,,,The {CacheLib} Caching Engine: Design and Experiences at Scale,The CacheLib Caching Engine: Design and Experiences at Scale,https://www.usenix.org/conference/osdi20/presentation/berg,"CacheLib is a general-purpose caching engine, designed based on experiences with a range of caching use cases at Facebook, that facilitates the easy development and maintenance of caches. CacheLib was first deployed at Facebook in 2017 and today powers over 70 services including CDN, storage, and application-data caches."
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,icebreaker,\cite{icebreaker},IceBreaker: warming serverless functions better with heterogeneity,,,True,False,"Rohan Basu Roy and
               Tirthak Patel and
               Devesh Tiwari",2022.0,,https://doi.org/10.1145/3503222.3507750,10.1145/3503222.3507750,,IceBreaker: warming serverless functions better with heterogeneity,IceBreaker: warming serverless functions better with heterogeneity ...,https://dl.acm.org/doi/10.1145/3503222.3507750,"By employing heterogeneity, IceBreaker allows for more number of nodes under the same cost budget and hence, keeps more number of functions warm and reduces the wait time during high load. Our real-system evaluation confirms that IceBreaker reduces the overall keep-alive cost by 45% and execution time by 27% using representative serverless"
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,fasscache,\cite{fasscache},FaasCache: keeping serverless computing alive with greedy-dual caching,,,True,False,"Alexander Fuerst and
               Prateek Sharma",2021.0,,https://doi.org/10.1145/3445814.3446757,10.1145/3445814.3446757,,FaasCache: keeping serverless computing alive with greedy-dual caching,FaasCache: keeping serverless computing alive with greedy-dual caching ...,https://dl.acm.org/doi/abs/10.1145/3445814.3446757,"Our caching-inspired Greedy-Dual keep-alive policy can be effective in reducing the cold-start overhead by more than 3× compared to current approaches. Caching concepts such as reuse distances and hit-ratio curves can also be used for auto-scaled server resource provisioning, which can reduce the resource requirement of FaaS providers by 30%"
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,DBLP:conf/osdi/YuJKKC22,\cite{DBLP:conf/osdi/YuJKKC22},"Orca: {A} Distributed Serving System for Transformer-Based Generative
               Models",,,True,False,"Gyeong{-}In Yu and
               Joo Seong Jeong and
               Geon{-}Woo Kim and
               Soojeong Kim and
               Byung{-}Gon Chun",2022.0,,https://www.usenix.org/conference/osdi22/presentation/yu,,,"Orca: {A} Distributed Serving System for Transformer-Based Generative
               Models",js0522 - GitHub,https://github.com/js0522/,"Orca: A Distributed Serving System for Transformer-Based Generative Models . Introduction. Transformer-based models are increased in parameter size from GPT-1 100 million to GPT-3 nearly 200 billion, such large-scale model emphasized the need for system support. Current Orca system analyzed the model inferencing process which generate outputs"
"KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache
  at a Large Cloud Provider",2506.02634v1,298501,\cite{298501},{Cost-Efficient} Large Language Model Serving for Multi-turn Conversations with {CachedAttention},,,True,False,Bin Gao and Zhuomin He and Puru Sharma and Qingxuan Kang and Djordje Jevdjic and Junbo Deng and Xingkun Yang and Zhou Yu and Pengfei Zuo,2024.0,,https://www.usenix.org/conference/atc24/presentation/gao-bin-cost,,,{Cost-Efficient} Large Language Model Serving for Multi-turn Conversations with {CachedAttention},降低大模型推理87%时延!华为云论文入选顶会usenix Atc'24,https://bbs.huaweicloud.com/blogs/431038,"降低大模型推理87%时延！华为云论文入选顶会USENIX ATC'24-云社区-华为云 冒泡提示 云社区 博客 降低大模型推理87%时延！华为云论文入选顶会USENIX ATC'24  微信 微博 分享文章到微博 复制链接 复制链接到剪贴板  降低大模型推理87%时延！华为云论文入选顶会USENIX ATC'24 举报 Image 1华为云头条 发表于 2024/07/17 19:37:46 2024/07/17 【摘要】 显著减少重复计算开销从而提升推理性能  近日，计算机系统领域顶会USENIX ATC 2024在美国加州圣克拉拉召开，华为云EMS团队的论文《Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention》被该顶会收录！ CachedAttention论文提出全球首个面向大模型推理的多级KV Cache缓存系统，称作AttentionStore，基于AttentionStore实现了大模型推理过程中的KV Cache复用，降低了高达87%的首Token时延（TTFT, Time to First Token）。 USENIX ATC (Annual Technical Conference) 会议创办于1992年，距今有32年的历史，是由美国高等计算系统协会 (USENIX) 组织的聚焦计算机系统领域的顶级国际会议。2024年的USENIX ATC会议收到488篇投稿，录用77篇，录用率仅15.8%。 Image 2: 1.PNG ▶CachedAttention论文介绍 通过多轮对话与人类互动是大型语言模型（LLM）的一个基本特征。然而，现有的LLM服务引擎执行多轮对话时，需要反复计算历史Token的Key-Value (KV) Cache，从而导致效率低下，产生高昂的推理服务成本。 为了解决这个问题，本论文提出了CachedAttention，一种新的注意力机制允许在多轮对话中重用KV Cache，显著减少重复计算开销从而提升推理性能。 CachedAttention维护一个分层的KV Cache存储系统（称作AttentionStore），利用经济高效的DRAM和SSD介质来保存请求的KV Cache。具体而言，为了减少从慢速介质上访问KV Cache的开销，CachedAttention采用了分层预加载和异步保存方法将KV Cache访问与NPU计算重叠。 其次，为了确保要访问的KV Cache总是放置在最快的存储层次，CachedAttention采用了一个调度程序感知的Fetch和Evict方法，有意识地根据推理作业调度程序的提示，在不同层中进行KV Cache的放置。 最后，为了避免由于LLM上下文窗口溢出而导致保存的KV Cache失效，CachedAttention通过解耦位置编码并有效截断KV Cache，使保存的KV Cache依然有效。 Image 3: 2.png CachedAttention论文信息：Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic, Junbo Deng, Xingkun Yang, Zhou Yu, Pengfei Zuo, “Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention”, in Proceedings of the 2024 USENIX Annual Technical Conference (USENIX ATC), 2024. Image 4: EMS (003).png Image 6"
"Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning
  Nonverbal Cues from Video-Grounded Dialogues",2506.00958v1,bai2023:qwen,\cite{bai2023:qwen},"Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond",,,True,False,"Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",2023.0,,,,,"Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond",Qwen-VL: A Versatile Vision-Language Model for Understanding ...,https://arxiv.org/abs/2308.12966,"arXiv smileybones ## arXiv Is Hiring a DevOps Engineer arxiv logo arXiv logo Cornell University Logo | Cite as: | arXiv:2308.12966 [cs.CV] | |  | (or  arXiv:2308.12966v3 [cs.CV] for this version) | |  |  Focus to learn more  arXiv-issued DOI via DataCite | # Bibliographic and Citation Tools arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? arXiv Operational Status   "
"Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning
  Nonverbal Cues from Video-Grounded Dialogues",2506.00958v1,zhang2023:video,\cite{zhang2023:video},Video-llama: An instruction-tuned audio-visual language model for video understanding,,,True,False,"Zhang, Hang and Li, Xin and Bing, Lidong",2023.0,,,,arXiv preprint arXiv:2306.02858,Video-llama: An instruction-tuned audio-visual language model for video understanding,Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video ...,https://aclanthology.org/2023.emnlp-demo.49/,Video-LLaMA is a multi-modal framework that empowers Large Language Models with the capability of understanding both visual and auditory content in the video. It bootstraps cross-modal training from pre-trained encoders and tunes the model with visual-instruction datasets.
"Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning
  Nonverbal Cues from Video-Grounded Dialogues",2506.00958v1,achiam2023:gpt,\cite{achiam2023:gpt},Gpt-4 technical report,,,True,False,"Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",2023.0,,,,arXiv preprint arXiv:2303.08774,Gpt-4 technical report,GPT-4 Technical Report - Papers With Code,https://paperswithcode.com/paper/gpt-4-technical-report-1,"Add a new dataset here Save GPT-4 Technical Report Preprint 2023·OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, Barret Zoph· Edit social preview We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4."
"Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning
  Nonverbal Cues from Video-Grounded Dialogues",2506.00958v1,busso2008:iemocap,\cite{busso2008:iemocap},IEMOCAP: Interactive emotional dyadic motion capture database,,,True,False,"Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S",2008.0,,,,Language resources and evaluation,IEMOCAP: Interactive emotional dyadic motion capture database,IEMOCAP: interactive emotional dyadic motion capture database,https://openreview.net/forum?id=5ZcVy7PTFz,"Since emotions are expressed through a combination of verbal and non-verbal channels, a joint analysis of speech and gestures is required to understand expressive human communication. To facilitate such investigations, this paper describes a new corpus named the ""interactive emotional dyadic motion capture database"" (IEMOCAP), collected by the Speech Analysis and Interpretation Laboratory"
"Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning
  Nonverbal Cues from Video-Grounded Dialogues",2506.00958v1,zadeh2018:multimodal,\cite{zadeh2018:multimodal},Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph,,,True,False,"Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe",2018.0,,,,,Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph,CMU-MOSEI Dataset - Papers With Code,https://paperswithcode.com/dataset/cmu-mosei,Introduced by Zadeh et al. in Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph CMU Multimodal Opinion Sentiment and Emotion Intensity ( CMU-MOSEI ) is the largest dataset of sentence-level sentiment analysis and emotion recognition in online videos.
"Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning
  Nonverbal Cues from Video-Grounded Dialogues",2506.00958v1,shafique2023:nonverbal,\cite{shafique2023:nonverbal},Nonverbal Communication Cue Recognition: A Pathway to More Accessible Communication,,,True,False,"Shafique, Zoya and Wang, Haiyan and Tian, Yingli",2023.0,,,,,Nonverbal Communication Cue Recognition: A Pathway to More Accessible Communication,Nonverbal Communication Cue Recognition: A Pathway to More Accessible ...,https://ieeexplore.ieee.org/document/10208387,"Nonverbal communication, such as body language, facial expressions, and hand gestures, is crucial to human communication as it conveys more information about emotions and attitudes than spoken words. However, individuals who are blind or have low-vision (BLV) may not have access to this method of communication, leading to asymmetry in conversations. Developing systems to recognize nonverbal"
"Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning
  Nonverbal Cues from Video-Grounded Dialogues",2506.00958v1,wu2024:motionllm,\cite{wu2024:motionllm},MotionLLM: Multimodal Motion-Language Learning with Large Language Models,,,True,False,"Wu, Qi and Zhao, Yubo and Wang, Yifan and Tai, Yu-Wing and Tang, Chi-Keung",2024.0,,,,arXiv preprint arXiv:2405.17013,MotionLLM: Multimodal Motion-Language Learning with Large Language Models,MotionLLM: Multimodal Motion-Language Learning with Large Language Models,https://arxiv.org/html/2405.17013v2,"Large Language Models (LLMs) have attracted great attention in both industry and academia. Many LLMs, such as GPT-4 [1], LLaMA [35], Gemma [32], have shown their advanced capabilities in robustness and generalization across various downstream tasks.These progresses have motivated researchers to explore the application of LLMs in multimodal tasks, such as integrating them with modalities like"
"Counterfactual Activation Editing for Post-hoc Prosody and
  Mispronunciation Correction in TTS Models",2506.00832v1,strom2006expressive,\cite{strom2006expressive},Expressive prosody for unit-selection speech synthesis.,,,True,False,"Strom, Volker and Clark, Robert AJ and King, Simon",2006.0,,,,,Expressive prosody for unit-selection speech synthesis.,PDF,https://www.isca-archive.org/interspeech_2006/strom06_interspeech.pdf,"The Festival unit selection speech synthesis system, Multisyn [1], achieves highly natural synthetic speech by avoiding use of an ex-plicit model of prosody in terms of F0 and duration. Instead, large amounts of speech are recorded, so that each diphone is available in a variety of prosodic contexts. Of course, this means that there"
"Counterfactual Activation Editing for Post-hoc Prosody and
  Mispronunciation Correction in TTS Models",2506.00832v1,mohan2021ctrl,\cite{mohan2021ctrl},Ctrl-P: Temporal control of prosodic variation for speech synthesis,,,True,False,"Mohan, Devang S Ram and Hu, Vivian and Teh, Tian Huey and Torresquintero, Alexandra and Wallis, Christopher GR and Staib, Marlene and Foglianti, Lorenzo and Gao, Jiameng and King, Simon",2021.0,,,,arXiv preprint arXiv:2106.08352,Ctrl-P: Temporal control of prosodic variation for speech synthesis,Ctrl-P: Temporal Control of Prosodic Variation for Speech Synthesis,https://arxiv.org/pdf/2106.08352,"Ctrl-P: Temporal Control of Prosodic Variation for Speech Synthesis Devang S Ram Mohan 1*, Vivian Hu , Tian Huey Teh , Alexandra Torresquintero1, Christopher G. R. Wallis 1, Marlene Staib , Lorenzo Foglianti , Jiameng Gao1, Simon King1;2 1Papercup Technologies Ltd., 2University of Edinburgh fdevang,vivian,tiang@papercup.com Abstract Text does not fully specify the spoken form, so text-to-speech"
"Counterfactual Activation Editing for Post-hoc Prosody and
  Mispronunciation Correction in TTS Models",2506.00832v1,lenglet2022speaking,\cite{lenglet2022speaking},Speaking Rate Control of end-to-end TTS Models by Direct Manipulation of the Encoder's Output Embeddings,,,True,False,"Lenglet, Martin and Perrotin, Olivier and Bailly, G{\'e}rard",2022.0,,,,,Speaking Rate Control of end-to-end TTS Models by Direct Manipulation of the Encoder's Output Embeddings,Speaking Rate Control of end-to-end TTS Models by Direct Manipulation ...,https://www.isca-archive.org/interspeech_2022/lenglet22_interspeech.html,"Experimental results show that the control provided by embeddings reproduces a behaviour closer to natural speech data. doi: 10.21437/Interspeech.2022-759 Cite as: Lenglet, M., Perrotin, O., Bailly, G. (2022) Speaking Rate Control of end-to-end TTS Models by Direct Manipulation of the Encoder's Output Embeddings. Proc."
"Counterfactual Activation Editing for Post-hoc Prosody and
  Mispronunciation Correction in TTS Models",2506.00832v1,fong2022speech,\cite{fong2022speech},Speech Audio Corrector: using speech from non-target speakers for one-off correction of mispronunciations in grapheme-input text-to-speech,,,True,False,"Fong, Jason and Lyth, Daniel and Henter, Gustav Eje and Tang, Hao and King, Simon",2022.0,,,,,Speech Audio Corrector: using speech from non-target speakers for one-off correction of mispronunciations in grapheme-input text-to-speech,Speech Audio Corrector: using speech from non-target speakers for one ...,https://www.isca-archive.org/interspeech_2022/fong22_interspeech.html,"Speech Audio Corrector: using speech from non-target speakers for one-off correction of mispronunciations in grapheme-input text-to-speech Jason Fong, Daniel Lyth, Gustav Eje Henter, Hao Tang, Simon King ... Experimental results show that our proposed system successfully enables one-off correction of mispronunciations in grapheme-based TTS with"
"Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding
  based on Guided Space Transformation",2505.24754v1,pennington-etal-2014-glove,\cite{pennington-etal-2014-glove},Glove: Global Vectors for Word Representation,,,True,False,"Jeffrey Pennington and
                  Richard Socher and
                  Christopher D. Manning",2014.0,,https://doi.org/10.3115/v1/d14-1162,10.3115/V1/D14-1162,,Glove: Global Vectors for Word Representation,GloVe: Global Vectors for Word Representation - Stanford University,https://nlp.stanford.edu/projects/glove/,"GloVe: Global Vectors for Word Representation GloVe: Global Vectors for Word RepresentationJeffrey Pennington, Richard Socher, Christopher D. GloVe: Global Vectors for Word Representation. GloVe is designed in order that such vector differences capture as much as possible the meaning specified by the juxtaposition of two words. The GloVe model is trained on the non-zero entries of a global word-word co-occurrence matrix, which tabulates how frequently words co-occur with one another in a given corpus. The training objective of GloVe is to learn word vectors such that their dot product equals the logarithm of the words' probability of co-occurrence. This feature is not unique to GloVe -- in fact, I'm unaware of any model for word vector learning that avoids this issue."
"Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding
  based on Guided Space Transformation",2505.24754v1,cer-etal-2018-universal,\cite{cer-etal-2018-universal},Universal Sentence Encoder for English,,,True,False,"Daniel Cer and
                  Yinfei Yang and
                  Sheng{-}yi Kong and
                  Nan Hua and
                  Nicole Limtiaco and
                  Rhomni St. John and
                  Noah Constant and
                  Mario Guajardo{-}Cespedes and
                  Steve Yuan and
                  Chris Tar and
                  Brian Strope and
                  Ray Kurzweil",2018.0,,https://doi.org/10.18653/v1/d18-2029,10.18653/V1/D18-2029,,Universal Sentence Encoder for English,Universal Sentence Encoder for English - ACL Anthology,https://aclanthology.org/D18-2029/,"Universal Sentence Encoder for English. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 169-174, Brussels, Belgium. Association for Computational Linguistics. Cite (Informal): Universal Sentence Encoder for English (Cer et al., EMNLP 2018) Copy Citation:"
"Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding
  based on Guided Space Transformation",2505.24754v1,zhuo-etal-2023-whitenedcse,\cite{zhuo-etal-2023-whitenedcse},WhitenedCSE: Whitening-based Contrastive Learning of Sentence Embeddings,,,True,False,"Wenjie Zhuo and
                  Yifan Sun and
                  Xiaohan Wang and
                  Linchao Zhu and
                  Yi Yang",2023.0,,https://doi.org/10.18653/v1/2023.acl-long.677,10.18653/V1/2023.ACL-LONG.677,,WhitenedCSE: Whitening-based Contrastive Learning of Sentence Embeddings,WhitenedCSE: Whitening-based Contrastive Learning of Sentence Embeddings,https://aclanthology.org/2023.acl-long.677/,"This paper presents a whitening-based contrastive learning method for sentence embedding learning (WhitenedCSE), which combines contrastive learning with a novel shuffled group whitening. Generally, contrastive learning pulls distortions of a single sample (i.e., positive samples) close and push negative samples far away, correspondingly"
"Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding
  based on Guided Space Transformation",2505.24754v1,li-li-2024-aoe,\cite{li-li-2024-aoe},AoE: Angle-optimized Embeddings for Semantic Textual Similarity,,,True,False,"Xianming Li and
                  Jing Li",2024.0,,https://doi.org/10.18653/v1/2024.acl-long.101,10.18653/V1/2024.ACL-LONG.101,,AoE: Angle-optimized Embeddings for Semantic Textual Similarity,AoE: Angle-optimized Embeddings for Semantic Textual Similarity,https://aclanthology.org/2024.acl-long.101/,"AoE: Angle-optimized Embeddings for Semantic Textual Similarity. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1825-1839, Bangkok, Thailand."
NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization,2505.24575v1,pu-etal-2022-two,\cite{pu-etal-2022-two},Two-Stage Movie Script Summarization: An Efficient Method For Low-Resource Long Document Summarization,,,True,False,"Liu, Dongqi  and
      Hong, Xudong  and
      Lin, Pin-Jie  and
      Chang, Ernie  and
      Demberg, Vera",2022.0,,https://aclanthology.org/2022.creativesumm-1.9/,,,Two-Stage Movie Script Summarization: An Efficient Method For Low-Resource Long Document Summarization,T4S: Two-Stage Screenplay Synopsis Summary Generation with ... - Springer,https://link.springer.com/chapter/10.1007/978-3-031-51671-9_6,"Therefore, this paper proposes a two-stage method T4S for generating plot summaries of movie scripts. First, the GraphTP model is employed as an extractor to extract key turning scenes from the scene text sequences. ... Two-stage movie script summarization: an efficient method for low-resource long document summarization. In: Mckeown, K. (ed"
NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization,2505.24575v1,gorinski-lapata-2015-movie,\cite{gorinski-lapata-2015-movie},Movie Script Summarization as Graph-based Scene Extraction,,,True,False,"Gorinski, Philip John  and
      Lapata, Mirella",2015.0,,https://aclanthology.org/N15-1113/,10.3115/v1/N15-1113,,Movie Script Summarization as Graph-based Scene Extraction,Movie Script Summarization as Graph-based Scene Extraction,https://aclanthology.org/N15-1113/,"gorinski-lapata-2015-movie Cite (ACL): Philip John Gorinski and Mirella Lapata. 2015. Movie Script Summarization as Graph-based Scene Extraction. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1066-1076, Denver, Colorado. Association for"
NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization,2505.24575v1,zaheer2020bigbird,\cite{zaheer2020bigbird},"Advances in Neural Information Processing Systems 33, NeurIPS 2020",,,True,False,"Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr",2020.0,,https://proceedings.neurips.cc/paper_files/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf,,,"Advances in Neural Information Processing Systems 33, NeurIPS 2020",Book - NeurIPS,https://proceedings.neurips.cc/paper_files/paper/2020,"NeurIPS Proceedings. Search. Advances in Neural Information Processing Systems 33 (NeurIPS 2020) Edited by: H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin. ... Scalable Multi-Agent Reinforcement Learning for Networked Systems with Average Reward Guannan Qu, Yiheng Lin, Adam Wierman, Na Li;"
NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization,2505.24575v1,guo-etal-2022-longt5,\cite{guo-etal-2022-longt5},{L}ong{T}5: {E}fficient Text-To-Text Transformer for Long Sequences,,,True,False,"Guo, Mandy  and
      Ainslie, Joshua  and
      Uthus, David  and
      Ontanon, Santiago  and
      Ni, Jianmo  and
      Sung, Yun-Hsuan  and
      Yang, Yinfei",2022.0,,https://aclanthology.org/2022.findings-naacl.55/,10.18653/v1/2022.findings-naacl.55,,{L}ong{T}5: {E}fficient Text-To-Text Transformer for Long Sequences,GitHub - google-research/longt5,https://github.com/google-research/longt5,"LongT5: Efficient Text-To-Text Transformer for Long Sequences LongT5 is an extension of the T5 model that handles long sequence inputs more efficiently. We integrated attention ideas from long-input transformers ETC ,and adopted pre-training strategies from summarization pre-training PEGASUS into the scalable T5 architecture."
NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization,2505.24575v1,gpt4_technical,\cite{gpt4_technical},GPT-4 Technical Report,,,True,False,OpenAI,2023.0,,,,arXiv preprint arXiv:2303.08774,GPT-4 Technical Report,GPT-4 Technical Report - Papers With Code,https://paperswithcode.com/paper/gpt-4-technical-report-1,"Add a new dataset here Save GPT-4 Technical Report Preprint 2023·OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, Barret Zoph· Edit social preview We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4."
NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization,2505.24575v1,mistralai2024large,\cite{mistralai2024large},Large Enough,,,True,False,{Mistral AI},2024.0,,https://mistral.ai/news/mistral-large-2407/,,,Large Enough,a large enough | English examples in context | Ludwig,https://ludwig.guru/s/a+large+enough,"""a large enough"" is a perfectly acceptable phrase in written English You can use it to express a comparison between two items in terms of size. For example: ""He bought a large enough truck to fit all the equipment he needed."" Exact (60) It's probably not a large enough sample. 1. Listen. Like."
NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization,2505.24575v1,NEURIPS2020_rag,\cite{NEURIPS2020_rag},Advances in Neural Information Processing Systems,,,True,False,"Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\""{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\""{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe",2020.0,,https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf,,,Advances in Neural Information Processing Systems,List of Proceedings,https://papers.nips.cc/,"From 2022 on, the Datasets and Benchmarks papers are in the main NeurIPS proceedings. Advances in Neural Information Processing Systems Datasets and Benchmarks, 2021 Advances in Neural Information Processing Systems 37 (NeurIPS 2024) Advances in Neural Information Processing Systems 36 (NeurIPS 2023) Advances in Neural Information Processing Systems 35 (NeurIPS 2022) Advances in Neural Information Processing Systems 34 (NeurIPS 2021) Advances in Neural Information Processing Systems 33 (NeurIPS 2020) Advances in Neural Information Processing Systems 32 (NeurIPS 2019) Advances in Neural Information Processing Systems 31 (NeurIPS 2018) Requests for name changes in the electronic proceedings will be accepted with no questions asked. Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings."
NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization,2505.24575v1,geng-etal-2022-improving-abstractive,\cite{geng-etal-2022-improving-abstractive},Improving Abstractive Dialogue Summarization with Speaker-Aware Supervised Contrastive Learning,,,True,False,"Geng, Zhichao  and
      Zhong, Ming  and
      Yin, Zhangyue  and
      Qiu, Xipeng  and
      Huang, Xuanjing",2022.0,,https://aclanthology.org/2022.coling-1.569/,,,Improving Abstractive Dialogue Summarization with Speaker-Aware Supervised Contrastive Learning,Improving Abstractive Dialogue Summarization with Speaker-Aware ...,https://aclanthology.org/2022.coling-1.569/,"@inproceedings{geng-etal-2022-improving-abstractive, title = ""Improving Abstractive Dialogue Summarization with Speaker-Aware Supervised Contrastive Learning"", author = ""Geng, Zhichao and Zhong, Ming and Yin, Zhangyue and Qiu, Xipeng and Huang, Xuanjing"", editor = ""Calzolari, Nicoletta and Huang, Chu-Ren and Kim, Hansaem and Pustejovsky, James and Wanner, Leo and Choi, Key-Sun and Ryu, Pum-Mo"
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,ouyang2022traininglanguagemodelsfollow,\cite{ouyang2022traininglanguagemodelsfollow},"Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022",,,True,False,"Long Ouyang and
Jeffrey Wu and
Xu Jiang and
Diogo Almeida and
Carroll L. Wainwright and
Pamela Mishkin and
Chong Zhang and
Sandhini Agarwal and
Katarina Slama and
Alex Ray and
John Schulman and
Jacob Hilton and
Fraser Kelton and
Luke Miller and
Maddie Simens and
Amanda Askell and
Peter Welinder and
Paul F. Christiano and
Jan Leike and
Ryan Lowe",2022.0,,https://dblp.org/rec/conf/nips/Ouyang0JAWMZASR22.bib,,,"Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022","""Advances in Neural Information Processing Systems 35: Annual ... - dblp",https://dblp.org/rec/conf/nips/2022,"Bibliographic details on Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. dblp. Blog; Statistics; Update feed; XML dump; ... New Orleans, LA, USA, November 28 - December 9, 2022. 2022, ISBN 9781713871088"
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,qi2023finetuningalignedlanguagemodels,\cite{qi2023finetuningalignedlanguagemodels},"The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024",,,True,False,"Xiangyu Qi and
Yi Zeng and
Tinghao Xie and
Pin{-}Yu Chen and
Ruoxi Jia and
Prateek Mittal and
Peter Henderson",2024.0,,https://dblp.org/rec/conf/iclr/Qi0XC0M024.bib,,,"The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024",,,
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,zou2023universaltransferableadversarialattacks,\cite{zou2023universaltransferableadversarialattacks},Universal and Transferable Adversarial Attacks on Aligned Language Models,,,True,False,Andy Zou and Zifan Wang and Nicholas Carlini and Milad Nasr and J. Zico Kolter and Matt Fredrikson,2023.0,,https://arxiv.org/abs/2307.15043,,ArXiv preprint,Universal and Transferable Adversarial Attacks on Aligned Language Models,Universal and Transferable Attacks on Aligned Language Models,https://llm-attacks.org/,"This web page presents a paper that demonstrates how to construct universal and transferable adversarial attacks on large language models (LLMs) like ChatGPT, Bard, or Claude. The attacks can cause the LLMs to produce harmful content even if they are fine-tuned to avoid it."
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,chao2024jailbreakingblackboxlarge,\cite{chao2024jailbreakingblackboxlarge},Jailbreaking Black Box Large Language Models in Twenty Queries,,,True,False,Patrick Chao and Alexander Robey and Edgar Dobriban and Hamed Hassani and George J. Pappas and Eric Wong,2023.0,,https://arxiv.org/abs/2310.08419,,ArXiv preprint,Jailbreaking Black Box Large Language Models in Twenty Queries,Jailbreaking Black Box Large Language Models in Twenty Queries - arXiv.org,https://arxiv.org/html/2310.08419,A paper that proposes a new algorithm for generating semantic jailbreaks for LLMs with only black-box access. PAIR uses an attacker LLM to iteratively refine candidate prompts that elicit objectionable content from the target LLM in fewer than twenty queries.
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,arditi2024refusallanguagemodelsmediated,\cite{arditi2024refusallanguagemodelsmediated},"Advances in Neural Information Processing Systems 38: Annual Conference
on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver,
BC, Canada, December 10 - 15, 2024",,,True,False,"Andy Arditi and
Oscar Obeso and
Aaquib Syed and
Daniel Paleka and
Nina Panickssery and
Wes Gurnee and
Neel Nanda",2024.0,,https://dblp.org/rec/conf/nips/ArditiOSPPGN24.bib,,,"Advances in Neural Information Processing Systems 38: Annual Conference
on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver,
BC, Canada, December 10 - 15, 2024",2024 Conference - NeurIPS,https://neurips.cc/Conferences/2024,"2024 Conference Skip to yearly menu bar Skip to main content Main Navigation NeurIPS Help/FAQ Contact NeurIPS Code of Ethics Code of Conduct Create Profile Journal To Conference Track Diversity & Inclusion Proceedings Future Meetings Press Exhibitor Information Privacy Policy Downloads My Stuff Login Select Year: (2024) 2025 2023 2024 2022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2008 2007 2006 Past Conferences Dates Submit Call for Papers Camera Ready Call For Tutorials Call For Datasets Benchmarks Call for Workshops Call for Competitions Call For Ethics Reviewers Call for High School Projects Call for Affinity Events Call for Socials Call for Creative AI Call for Expo Call for Education Proposals Author Resources Poster Information Reviewer Integrity Reviewer Resources Reviewer Guidelines Area Chair Guidelines Senior Area Chair Guidelines Ethics Reviewer Guidelines Attend Visa Information Hotels Financial Assistance and Volunteers At the Conference Attending with Children Visiting Vancouver After Dark Networking Event Organizers NeurIPS Board Organizing Committee Program Committee NeurIPS Foundation Exhibitors 2024 Exhibitors Information FAQ Conference Site NeurIPS 2024 The Thirty-Eighth Annual Conference on Neural Information Processing Systems Vancouver Convention Center Tuesday Dec 10 through Sunday Dec 15 firstbacksecondback 2024 Conference Start Here Schedule Invited Talks Orals Spotlights Posters Paper Visualization Tutorials Affinity Events Socials Town Hall Competitions Workshops Daily Schedule - Mobile View Exhibitors Registration Registration 2024 Pricing » Cancellation Policy » Certificate of Attendance Announcements The NeurIPS 2025 Call For Papers is now available Browse main conference papers with the Paper Visualization. About the Conference The conference was founded in 1987 and is now a multi-track interdisciplinary annual meeting that includes invited talks, demonstrations, symposia, and oral and poster presentations of refereed papers. Along with the conference is a professional exposition focusing on machine learning in practice, a series of tutorials, and topical workshops that provide a less formal setting for the exchange of ideas."
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,Spectralediting,\cite{Spectralediting},"Advances in Neural Information Processing Systems 38: Annual Conference
on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver,
BC, Canada, December 10 - 15, 2024",,,True,False,"Yifu Qiu and
Zheng Zhao and
Yftah Ziser and
Anna Korhonen and
Edoardo Maria Ponti and
Shay B. Cohen",2024.0,,https://dblp.org/rec/conf/nips/Qiu0ZKPC24.bib,,,"Advances in Neural Information Processing Systems 38: Annual Conference
on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver,
BC, Canada, December 10 - 15, 2024",2024 Conference - NeurIPS,https://neurips.cc/Conferences/2024,"2024 Conference Skip to yearly menu bar Skip to main content Main Navigation NeurIPS Help/FAQ Contact NeurIPS Code of Ethics Code of Conduct Create Profile Journal To Conference Track Diversity & Inclusion Proceedings Future Meetings Press Exhibitor Information Privacy Policy Downloads My Stuff Login Select Year: (2024) 2025 2023 2024 2022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2008 2007 2006 Past Conferences Dates Submit Call for Papers Camera Ready Call For Tutorials Call For Datasets Benchmarks Call for Workshops Call for Competitions Call For Ethics Reviewers Call for High School Projects Call for Affinity Events Call for Socials Call for Creative AI Call for Expo Call for Education Proposals Author Resources Poster Information Reviewer Integrity Reviewer Resources Reviewer Guidelines Area Chair Guidelines Senior Area Chair Guidelines Ethics Reviewer Guidelines Attend Visa Information Hotels Financial Assistance and Volunteers At the Conference Attending with Children Visiting Vancouver After Dark Networking Event Organizers NeurIPS Board Organizing Committee Program Committee NeurIPS Foundation Exhibitors 2024 Exhibitors Information FAQ Conference Site NeurIPS 2024 The Thirty-Eighth Annual Conference on Neural Information Processing Systems Vancouver Convention Center Tuesday Dec 10 through Sunday Dec 15 firstbacksecondback 2024 Conference Start Here Schedule Invited Talks Orals Spotlights Posters Paper Visualization Tutorials Affinity Events Socials Town Hall Competitions Workshops Daily Schedule - Mobile View Exhibitors Registration Registration 2024 Pricing » Cancellation Policy » Certificate of Attendance Announcements The NeurIPS 2025 Call For Papers is now available Browse main conference papers with the Paper Visualization. About the Conference The conference was founded in 1987 and is now a multi-track interdisciplinary annual meeting that includes invited talks, demonstrations, symposia, and oral and poster presentations of refereed papers. Along with the conference is a professional exposition focusing on machine learning in practice, a series of tutorials, and topical workshops that provide a less formal setting for the exchange of ideas."
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,uppaal2025profs,\cite{uppaal2025profs},The Thirteenth International Conference on Learning Representations 2025,,,True,False,"Uppaal, Rheeya and Dey, Apratim and He, Yiting and Zhong, Yiqiao and Hu, Junjie",2025.0,,,,,The Thirteenth International Conference on Learning Representations 2025,The International Conference on Learning Representations (ICLR),https://europe.naverlabs.com/updates/iclr/,"The Thirteenth International Conference on Learning Representations (ICLR) 2025. Singapore, 24 th-28 th April 2025. ... Our research combines expertise in visual representation learning, self-supervised learning and human behaviour understanding to build AI components that help robots understand and navigate in their 3D environment, detect and"
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,burns2024discoveringlatentknowledgelanguage,\cite{burns2024discoveringlatentknowledgelanguage},"The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023",,,True,False,"Collin Burns and
Haotian Ye and
Dan Klein and
Jacob Steinhardt",2023.0,,https://dblp.org/rec/conf/iclr/BurnsYKS23.bib,,,"The Eleventh International Conference on Learning Representations,
{ICLR} 2023, Kigali, Rwanda, May 1-5, 2023",2023 Conference - ICLR 2025,https://iclr.cc/Conferences/2023,ICLR is the premier gathering of professionals dedicated to the advancement of the branch of artificial intelligence called representation learning.
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,haghighatkhah2022betterhitnailhead,\cite{haghighatkhah2022betterhitnailhead},Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,,,True,False,"Haghighatkhah, Pantea  and
Fokkens, Antske  and
Sommerauer, Pia  and
Speckmann, Bettina  and
Verbeek, Kevin",2022.0,,https://aclanthology.org/2022.emnlp-main.575,10.18653/v1/2022.emnlp-main.575,,Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,Proceedings of the 2022 Conference on Empirical Methods in Natural ...,https://aclanthology.org/2022.emnlp-main.0/,"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates. Cite (Informal): Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (Goldberg et al., EMNLP 2022) Copy Citation:"
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,ravfogel2020nulloutguardingprotected,\cite{ravfogel2020nulloutguardingprotected},Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,,,True,False,"Ravfogel, Shauli  and
Elazar, Yanai  and
Gonen, Hila  and
Twiton, Michael  and
Goldberg, Yoav",2020.0,,https://aclanthology.org/2020.acl-main.647,10.18653/v1/2020.acl-main.647,,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,PDF,https://www.proceedings.com/content/055/055162webtoc.pdf,"Browse the online version of the 58th Annual Meeting of the Association for Computational Linguistics (ACL 2020) proceedings, covering topics such as dialogue, discourse, and cross-modal language generation. See the list of papers, authors, and abstracts for each paper."
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,belrose2023leaceperfectlinearconcept,\cite{belrose2023leaceperfectlinearconcept},"Advances in Neural Information Processing Systems 36: Annual Conference
on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023",,,True,False,"Nora Belrose and
David Schneider{-}Joseph and
Shauli Ravfogel and
Ryan Cotterell and
Edward Raff and
Stella Biderman",2023.0,,https://dblp.org/rec/conf/nips/BelroseSRCRB23.bib,,,"Advances in Neural Information Processing Systems 36: Annual Conference
on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023","""Advances in Neural Information Processing Systems 36: Annual ... - dblp",https://dblp.org/rec/conf/nips/2023,"Bibliographic details on Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023."
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,bolukbasi2016man,\cite{bolukbasi2016man},"Advances in Neural Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain",,,True,False,"Tolga Bolukbasi and
Kai{-}Wei Chang and
James Y. Zou and
Venkatesh Saligrama and
Adam Tauman Kalai",2016.0,,https://dblp.org/rec/conf/nips/BolukbasiCZSK16.bib,,,"Advances in Neural Information Processing Systems 29: Annual Conference
on Neural Information Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain","""Advances in Neural Information Processing Systems 29: Annual ... - dblp",https://dblp.org/rec/conf/nips/2016,"Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain. 2016 manage site settings To protect your privacy, all features that rely on external API calls from your browser are turned off by default ."
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,park2024linearrepresentationhypothesisgeometry,\cite{park2024linearrepresentationhypothesisgeometry},"Forty-first International Conference on Machine Learning, {ICML} 2024,
Vienna, Austria, July 21-27, 2024",,,True,False,"Kiho Park and
Yo Joong Choe and
Victor Veitch",2024.0,,https://dblp.org/rec/conf/icml/ParkCV24.bib,,,"Forty-first International Conference on Machine Learning, {ICML} 2024,
Vienna, Austria, July 21-27, 2024",dblp: ICML 2024,https://dblp.org/db/conf/icml/icml2024,"Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net 2024. Accept (Oral) view. electronic edition @ openreview.net (open access) ... Position: Opportunities Exist for Machine Learning in Magnetic Fusion Energy. view."
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,mikolov2013linguistic,\cite{mikolov2013linguistic},Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,,,True,False,"Mikolov, Tomas  and
Yih, Wen-tau  and
Zweig, Geoffrey",2013.0,,https://aclanthology.org/N13-1090,,,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,North American Chapter of the Association for Computational Linguistics ...,https://aclanthology.org/events/naacl-2013/,Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies 141 papers; Proceedings of the 2013 NAACL HLT Student Research Workshop 14 papers; Proceedings of the 2013 NAACL HLT Demonstration Session 10 papers; NAACL HLT 2013 Tutorial Abstracts 7 papers
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,nanda2023emergentlinearrepresentationsworld,\cite{nanda2023emergentlinearrepresentationsworld},Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP,,,True,False,"Nanda, Neel  and
Lee, Andrew  and
Wattenberg, Martin",2023.0,,https://aclanthology.org/2023.blackboxnlp-1.2,10.18653/v1/2023.blackboxnlp-1.2,,Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP,PDF,https://aclanthology.org/2023.blackboxnlp-1.0.pdf,"BlackboxNLP, now in its sixth iteration, has played an important role in bringing together scholars from a diverse range of backgrounds in order to rigorously study the behavior, representations, and computations of black-box neural network models. Our workshop showcases original, cutting-edge research on topics including but not limited to:"
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,hernandez2021lowdimensionallineargeometrycontextualized,\cite{hernandez2021lowdimensionallineargeometrycontextualized},Proceedings of the 25th Conference on Computational Natural Language Learning,,,True,False,"Hernandez, Evan  and
Andreas, Jacob",2021.0,,https://aclanthology.org/2021.conll-1.7,10.18653/v1/2021.conll-1.7,,Proceedings of the 25th Conference on Computational Natural Language Learning,Proceedings of the 25th Conference on Computational Natural Language ...,https://aclanthology.org/2021.conll-1.0/,"Proceedings of the 25th Conference on Computational Natural Language Learning. Association for Computational Linguistics, Online. Cite (Informal): Proceedings of the 25th Conference on Computational Natural Language Learning (Bisazza & Abend, CoNLL 2021) Copy Citation: BibTeX Markdown MODS XML Endnote More options… PDF:"
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,bricken2023monosemanticity,\cite{bricken2023monosemanticity},Towards Monosemanticity: Decomposing Language Models With Dictionary Learning,,,True,False,"Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher",2023.0,,,,Transformer Circuits Thread,Towards Monosemanticity: Decomposing Language Models With Dictionary Learning,Towards Monosemanticity: A Step Towards Understanding Large Language Models,https://towardsdatascience.com/towards-monosemanticity-a-step-towards-understanding-large-language-models-e7b88380d7b3/,"Anthropic is one of the companies that has made great strides in understanding these large models. The main question is how these models work apart from a mathematical point of view. In Oct '23, they published this paper: Towards Monosemanticity: Decomposing Language models with dictionary learning . This paper aims to solve this problem and"
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,templeton2024scaling,\cite{templeton2024scaling},Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet,,,True,False,"Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom",2024.0,,https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html,,Transformer Circuits Thread,Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet,"Understanding the ""Scaling of Monosemanticity"" in AI ... - Medium",https://medium.com/thedeephub/understanding-the-scaling-of-monosemanticity-in-ai-models-a-comprehensive-analysis-f72818fa44ca,"The research from ""Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet, Templeton et al. demonstrates that sparse autoencoders can scale to larger models like Claude"
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,cunningham2023sparseautoencodershighlyinterpretable,\cite{cunningham2023sparseautoencodershighlyinterpretable},"The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024",,,True,False,"Robert Huben and
Hoagy Cunningham and
Logan Riggs and
Aidan Ewart and
Lee Sharkey",2024.0,,https://dblp.org/rec/conf/iclr/HubenCRES24.bib,,,"The Twelfth International Conference on Learning Representations,
{ICLR} 2024, Vienna, Austria, May 7-11, 2024",,,
COSMIC: Generalized Refusal Direction Identification in LLM Activations,2506.00085v1,elhage2021mathematical,\cite{elhage2021mathematical},A Mathematical Framework for Transformer Circuits,,,True,False,"Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris",2021.0,,,,Transformer Circuits Thread,A Mathematical Framework for Transformer Circuits,A Mathematical Framework for Transformer Circuits,https://ckodser.github.io/summaries/A_Mathematical_Framework_for_Transformer_Circuits/,A Mathematical Framework for Transformer Circuits. In Transformers residual stream is the main object and layers read and write from/to it.
"Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for
  LLMs",2505.23996v1,open-llm-leaderboard-v2,\cite{open-llm-leaderboard-v2},Open LLM Leaderboard v2,,,True,False,Clémentine Fourrier and Nathan Habib and Alina Lozovskaya and Konrad Szafer and Thomas Wolf,2024.0,,,,,Open LLM Leaderboard v2,Hugging Face Upgrades Open LLM Leaderboard v2 for Enhanced AI Model ...,https://www.infoq.com/news/2024/10/open-llm-leaderboard-v2-launch/,"Scaling Large Language Model Serving Infrastructure at Meta/presentations/llm-meta/en/smallimage/ye-charlotte-qi-thumbnail-1747727365712.jpg) She explains how traditional product management principles remain crucial while highlighting the nuances of working with LLMs. Learn about prompt engineering, data-driven development lifecycles, model selection criteria, and critical risk assessment for trust, safety, legal, and privacy in GenAI. Hugging Face Upgrades Open LLM Leaderboard v2 for Enhanced AI Model Comparison # Hugging Face Upgrades Open LLM Leaderboard v2 for Enhanced AI Model Comparison Hugging Face has recently released Open LLM Leaderboard v2, an upgraded version of their popular benchmarking platform for large language models. InfoQ spoke to Alina Lozovskaia, one of the Leaderboard maintainers at Hugging Face, to learn more about the motivation behind this update and its implications for the AI community."
"Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for
  LLMs",2505.23996v1,pant-dadu-2022-incorporating,\cite{pant-dadu-2022-incorporating},Incorporating Subjectivity into Gendered Ambiguous Pronoun ({GAP}) Resolution using Style Transfer,,,True,False,"Pant, Kartikey  and
      Dadu, Tanvi",2022.0,,,,,Incorporating Subjectivity into Gendered Ambiguous Pronoun ({GAP}) Resolution using Style Transfer,‪Tanvi Dadu‬ - ‪Google Scholar‬,https://scholar.google.com/citations?user=vz6lRPoAAAAJ&hl=en,"Towards Detection of Subjective Bias using Contextualized Word Embeddings. K Pant, T Dadu, R Mamidi ... Incorporating Subjectivity into Gendered Ambiguous Pronoun (GAP) Resolution using Style Transfer. K Pant, T Dadu. Proceedings of the 4th Workshop on Gender Bias in Natural Language"
"Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for
  LLMs",2505.23996v1,fadeeva2023lm,\cite{fadeeva2023lm},LM-polygraph: Uncertainty estimation for language models,,,True,False,"Fadeeva, Ekaterina and Vashurin, Roman and Tsvigun, Akim and Vazhentsev, Artem and Petrakov, Sergey and Fedyanin, Kirill and Vasilev, Daniil and Goncharova, Elizaveta and Panchenko, Alexander and Panov, Maxim and others",2023.0,,,,,LM-polygraph: Uncertainty estimation for language models,GitHub - IINemo/lm-polygraph,https://github.com/IINemo/lm-polygraph,"@inproceedings{fadeeva-etal-2023-lm, title = ""{LM}-Polygraph: Uncertainty Estimation for Language Models"", author = ""Fadeeva, Ekaterina and Vashurin, Roman and Tsvigun, Akim and Vazhentsev, Artem and Petrakov, Sergey and Fedyanin, Kirill and Vasilev, Daniil and Goncharova, Elizaveta and Panchenko, Alexander and Panov, Maxim and Baldwin, Timothy and Shelmanov, Artem"", editor = ""Feng, Yansong"
"Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for
  LLMs",2505.23996v1,bridle1990probabilistic,\cite{bridle1990probabilistic},"Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition",,,True,False,"Bridle, John S",1990.0,,,,,"Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition",Probabilistic Interpretation of Feedforward Classification Network ...,https://link.springer.com/chapter/10.1007/978-3-642-76153-9_28,"Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition. Conference paper; pp 227-236; ... J.S. (1990). Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition. In: Soulié, F.F., Hérault, J"
"Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for
  LLMs",2505.23996v1,jurafsky2000speech,\cite{jurafsky2000speech},"Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition",,,True,False,"Jurafsky, Daniel and Martin, James H",2000.0,,,,,"Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition",[PDF] Speech and Language Processing - Stanford University,https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf,"Page 1. Speech and Language Processing. An Introduction to Natural Language Processing,. Computational Linguistics, and Speech Recognition with Language Models."
"Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for
  LLMs",2505.23996v1,vovk2005algorithmic,\cite{vovk2005algorithmic},Algorithmic learning in a random world,,,True,False,"Vovk, Vladimir and Gammerman, Alexander and Shafer, Glenn",2005.0,,,,,Algorithmic learning in a random world,"Vovk, Gammerman and Shafer ""Algorithmic learning in a random world""",https://alrw.net/,"This book introduces conformal prediction, a method of machine learning that provides reliable measures of accuracy and reliability. It also covers topics such as algorithmic randomness, probability forecasting, causal networks, and online compression modelling."
"Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for
  LLMs",2505.23996v1,yu2022learning,\cite{yu2022learning},Learning Uncertainty for Unknown Domains with Zero-Target-Assumption,,,True,False,"Yu, Yu and Sajjad, Hassan and Xu, Jia",2022.0,,,,,Learning Uncertainty for Unknown Domains with Zero-Target-Assumption,Publications - Jia Xu,http://www.jiaxu.org/publications.html,"Learning Uncertainty for Unknown Domains with Zero-Target-Assumption. Yu Yu, Hassan Sajjad, and Jia Xu. 2023, in Proceedings of ICLR. Probabilistic Robustness for Data Filtering. Yu Yu, Abdul Rafae Khan, Shahram Khadivi, and Jia Xu. 2023, in Proceedings of EACL. Fluent Translation Built on Giant Pre-trained Models"
"Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for
  LLMs",2505.23996v1,vashurin2025benchmarking,\cite{vashurin2025benchmarking},Benchmarking uncertainty quantification methods for large language models with lm-polygraph,,,True,False,"Vashurin, Roman and Fadeeva, Ekaterina and Vazhentsev, Artem and Rvanova, Lyudmila and Vasilev, Daniil and Tsvigun, Akim and Petrakov, Sergey and Xing, Rui and Sadallah, Abdelrahman and Grishchenkov, Kirill and others",2025.0,,,,Transactions of the Association for Computational Linguistics,Benchmarking uncertainty quantification methods for large language models with lm-polygraph,Benchmarking Uncertainty Quantification Methods for Large Language ...,https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00737/128713/Benchmarking-Uncertainty-Quantification-Methods,"The benchmark is based on the LM-Polygraph framework (Fadeeva et al., 2023), which implements state-of-the-art UQ baselines in a unified way, enabling a large-scale, consistent comparison of methods developed in recent work. It includes the tasks of selective question-answering (QA), selective generation (machine translation [MT] and text"
"Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for
  LLMs",2505.23996v1,santilli2024spurious,\cite{santilli2024spurious},On a spurious interaction between uncertainty scores and answer evaluation metrics in generative qa tasks,,,True,False,"Santilli, Andrea and Xiong, Miao and Kirchhof, Michael and Rodriguez, Pau and Danieli, Federico and Suau, Xavier and Zappella, Luca and Williamson, Sinead and Golinski, Adam",2024.0,,,,,On a spurious interaction between uncertainty scores and answer evaluation metrics in generative qa tasks,On a Spurious Interaction between Uncertainty Scores & Answer ...,https://openreview.net/pdf?id=jGtL0JFdeD,"On a Spurious Interaction between Uncertainty Scores & Answer Evaluation Metrics in Generative QA Tasks ... i.e., the numerical score that judges how well an LLM answer matches the reference answer. If the answer evaluation metric is based on a substring-overlap metric, such as Rouge-L [22] or SQuAD [31], or a fixed-length learned"
"Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for
  LLMs",2505.23996v1,kuzmin-etal-2023-uncertainty,\cite{kuzmin-etal-2023-uncertainty},Uncertainty Estimation for Debiased Models: Does Fairness Hurt Reliability?,,,True,False,"Kuzmin, Gleb  and
      Vazhentsev, Artem  and
      Shelmanov, Artem  and
      Han, Xudong  and
      Suster, Simon  and
      Panov, Maxim  and
      Panchenko, Alexander  and
      Baldwin, Timothy",2023.0,,https://aclanthology.org/2023.ijcnlp-main.48/,10.18653/v1/2023.ijcnlp-main.48,,Uncertainty Estimation for Debiased Models: Does Fairness Hurt Reliability?,A project that aims to maximize both fairness and safety of models,https://github.com/mbzuai-nlp/fairlib_uncertainty,"This repository contains the implementation for the ""Uncertainty Estimation for Debiased Models: Does Fairness Hurt Reliability?"" paper from IJCNLP-AACL2023. The code provides a variety of methods for debiasing and scenarios for its joint usage with Uncertainty Estimation (UE) methods for neural network models."
"Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for
  LLMs",2505.23996v1,kuzucu2023uncertainty,\cite{kuzucu2023uncertainty},Uncertainty as a Fairness Measure,,,True,False,"Kuzucu, Selim and Cheong, Jiaee and Gunes, Hatice and Kalkan, Sinan",2023.0,,,,arXiv preprint arXiv:2312.11299,Uncertainty as a Fairness Measure,Uncertainty as a Fairness Measure - ACM Digital Library,https://dl.acm.org/doi/pdf/10.1613/jair.1.16041,"Figure 1:Existing fairness measures utilize point predictions for quantifying fairness, which ignores the uncertainty (variance) of the predictions (a-b). We fill this gap by using uncertainty instead for measuring fairness (c-d). It has been identified in the literature that fairness is a multi-faceted concept, which has"
"Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for
  LLMs",2505.23996v1,kaiser2022uncertainty,\cite{kaiser2022uncertainty},Uncertainty-aware predictive modeling for fair data-driven decisions,,,True,False,"Kaiser, Patrick and Kern, Christoph and R{\""u}gamer, David",2022.0,,,,arXiv preprint arXiv:2211.02730,Uncertainty-aware predictive modeling for fair data-driven decisions,Uncertainty-aware predictive modeling for fair data-driven decisions,https://www.researchgate.net/publication/365189617_Uncertainty-aware_predictive_modeling_for_fair_data-driven_decisions,"We posit that a fair model needs to be an uncertainty-aware model, e.g. by drawing on distributional regression. For fair decisions, we argue that a safe fail option should be used for individuals"
"Synthetic Generation and Latent Projection Denoising of Rim Lesions in
  Multiple Sclerosis",2505.23353v1,Mcal,\cite{Mcal},Synthetic quantitative MRI through relaxometry modelling,,,True,False,"Callaghan, Martina F. and Mohammadi, Siawoosh and Weiskopf, Nikolaus",2016.0,,https://dx.doi.org/10.1002/nbm.3658,10.1002/nbm.3658,NMR in Biomedicine,Synthetic quantitative MRI through relaxometry modelling,Synthetic quantitative MRI through relaxometry modelling,https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/10.1002/nbm.3658,Quantitative MRI (qMRI) provides standardized measures of specific physical parameters that are sensitive to the underlying tissue microstructure and are a first step towards achieving maps of biologically relevant metrics through in vivo histology using MRI. Recently proposed models have described the interdependence of qMRI parameters.
"Synthetic Generation and Latent Projection Denoising of Rim Lesions in
  Multiple Sclerosis",2505.23353v1,Jand,\cite{Jand},Synthetic MRI for stroke: a qualitative and quantitative pilot study,,,True,False,"André, Joachim and Barrit, Sami and Jissendi, Patrice",2022.0,,,10.1038/s41598-022-15204-8,Scientific Reports,Synthetic MRI for stroke: a qualitative and quantitative pilot study,Synthetic MRI for stroke: a qualitative and quantitative pilot study ...,https://pubmed.ncbi.nlm.nih.gov/35798771/,Synthetic MR provides qualitative and quantitative multi-parametric data about tissue properties in a single acquisition. Its use in stroke imaging is not yet established. We compared synthetic and conventional image quality and studied synthetic relaxometry of acute and chronic ischemic lesions to investigate its interest for stroke imaging.
"Synthetic Generation and Latent Projection Denoising of Rim Lesions in
  Multiple Sclerosis",2505.23353v1,Emoy,\cite{Emoy},A deep learning approach for synthetic MRI based on two routine sequences and training with synthetic data,,,True,False,"Moya-Sáez, Elisa and Peña-Nogales, Óscar and Luis-García, Rodrigo de and Alberola-López, Carlos",2021.0,,https://www.sciencedirect.com/science/article/pii/S0169260721004454,https://doi.org/10.1016/j.cmpb.2021.106371,Computer Methods and Programs in Biomedicine,A deep learning approach for synthetic MRI based on two routine sequences and training with synthetic data,A deep learning approach for synthetic MRI based on two routine ...,https://www.sciencedirect.com/science/article/pii/S0169260721004454,"Methods: Our approach is based on a convolutional neural network (CNN) trained with synthetic data; specifically, a synthetic dataset with 120 volumes was constructed from the anatomical brain model of the BrainWeb tool and served as the training set. The CNN learns an end-to-end mapping function to transform the input T1- and T2-weighted images to their underlying T1, T2, and PD parametric maps."
"Synthetic Generation and Latent Projection Denoising of Rim Lesions in
  Multiple Sclerosis",2505.23353v1,Kgop,\cite{Kgop},"Synthetic data in generalizable, learning-based neuroimaging",,,True,False,"Gopinath, Karthik and Hoopes, Andrew and Alexander, Daniel C. and Arnold, Steven E. and Balbastre, Yael and Billot, Benjamin and Casamitjana, Adrià and Cheng, You and Chua, Russ Yue Zhi and Edlow, Brian L. and Fischl, Bruce and Gazula, Harshvardhan and Hoffmann, Malte and Keene, C. Dirk and Kim, Seunghoi and Kimberly, W. Taylor and Laguna, Sonia and Larson, Kathleen E. and Van Leemput, Koen and Puonti, Oula and Rodrigues, Livia M. and Rosen, Matthew S. and Tregidgo, Henry F. J. and Varadarajan, Divya and Young, Sean I. and Dalca, Adrian V. and Iglesias, Juan Eugenio",2024.0,11,https://doi.org/10.1162/imag\_a\_00337,10.1162/imag_a_00337,Imaging Neuroscience,"Synthetic data in generalizable, learning-based neuroimaging","Synthetic data in generalizable, learning-based neuroimaging",https://www.research-collection.ethz.ch/handle/20.500.11850/713322,"This retrospective paper reviews a family o f recently proposed methods, based on synthetic data, for generalizable machine learning in brain MRI analysis. Central to this framework is the concept of domain randomization, which involves training neural networks on a vastly diverse array of synthetically generated images with random contrast"
"Synthetic Generation and Latent Projection Denoising of Rim Lesions in
  Multiple Sclerosis",2505.23353v1,Jigl,\cite{Jigl},SynthSR: A public AI tool to turn heterogeneous clinical brain scans into high-resolution T1-weighted images for 3D morphometry,,,True,False,Juan E. Iglesias  and Benjamin Billot  and Yaël Balbastre  and Colin Magdamo  and Steven E. Arnold  and Sudeshna Das  and Brian L. Edlow  and Daniel C. Alexander  and Polina Golland  and Bruce Fischl,2023.0,,https://www.science.org/doi/abs/10.1126/sciadv.add3607,10.1126/sciadv.add3607,Science Advances,SynthSR: A public AI tool to turn heterogeneous clinical brain scans into high-resolution T1-weighted images for 3D morphometry,SynthSR - Free Surfer Wiki,https://surfer.nmr.mgh.harvard.edu/fswiki/SynthSR,"SynthSR: a public AI tool to turn heterogeneous clinical brain scans into high-resolution T1-weighted images for 3D morphometry. JE Iglesias, B Billot, Y Balbastre, C Magdamo, S Arnold, S Das, B Edlow, D Alexander, P Golland, B Fischl. Science Advances, 9(5), eadd3607 (2023). If you use the Hyperfine version, please cite this paper as well:"
"Synthetic Generation and Latent Projection Denoising of Rim Lesions in
  Multiple Sclerosis",2505.23353v1,weli,\cite{weli},Detecting Alzheimer's Disease on Small Dataset: A Knowledge Transfer Perspective,,,True,False,"Li, Wei and Zhao, Yifei and Chen, Xi and Xiao, Yang and Qin, Yuanyuan",2019.0,,,10.1109/JBHI.2018.2839771,IEEE Journal of Biomedical and Health Informatics,Detecting Alzheimer's Disease on Small Dataset: A Knowledge Transfer Perspective,Detecting Alzheimer's Disease on Small Dataset: A Knowledge Transfer ...,https://pubmed.ncbi.nlm.nih.gov/29994324/,Computer-aided diagnosis (CAD) is an attractive topic in Alzheimer's disease (AD) research. Many algorithms are based on a relatively large training dataset. ... Detecting Alzheimer's Disease on Small Dataset: A Knowledge Transfer Perspective IEEE J Biomed Health Inform. 2019 May;23(3) :1234
"Synthetic Generation and Latent Projection Denoising of Rim Lesions in
  Multiple Sclerosis",2505.23353v1,smat,\cite{smat},Employing deep learning and transfer learning for accurate brain tumor detection,,,True,False,"Mathivanan, Sandeep Kumar and Sonaimuthu, Sridevi and Murugesan, Sankar and Rajadurai, Hariharan and Shivahare, Basu Dev and Shah, Mohd Asif",2024.0,,,10.1038/s41598-024-57970-7,Scientific Reports,Employing deep learning and transfer learning for accurate brain tumor detection,Employing deep learning and transfer learning for accurate brain tumor ...,https://pubmed.ncbi.nlm.nih.gov/38538708/,"Despite this, brain tumor diagnosis remains a challenging endeavour due to the intricate structure of the brain. This study delves into the potential of deep transfer learning architectures to elevate the accuracy of brain tumor diagnosis. Transfer learning is a machine learning technique that allows us to repurpose pre-trained models on new tasks."
"Synthetic Generation and Latent Projection Denoising of Rim Lesions in
  Multiple Sclerosis",2505.23353v1,Vtha,\cite{Vtha},SinGAN-Seg: Synthetic training data generation for medical image segmentation,,,True,False,"Thambawita, Vajira AND Salehi, Pegah AND Sheshkal, Sajad Amouei AND Hicks, Steven A. AND Hammer, Hugo L. AND Parasa, Sravanthi AND Lange, Thomas de AND Halvorsen, Pål AND Riegler, Michael A.",2022.0,05,https://doi.org/10.1371/journal.pone.0267976,10.1371/journal.pone.0267976,PLOS ONE,SinGAN-Seg: Synthetic training data generation for medical image segmentation,SinGAN-Seg: Synthetic training data generation for medical image ...,https://arxiv.org/abs/2107.00471,"Abstract page for arXiv paper 2107.00471: SinGAN-Seg: Synthetic training data generation for medical image segmentation Analyzing medical data to find abnormalities is a time-consuming and costly task, particularly for rare abnormalities, requiring tremendous efforts from medical experts."
"Synthetic Generation and Latent Projection Denoising of Rim Lesions in
  Multiple Sclerosis",2505.23353v1,Bahm,\cite{Bahm},Brain Tumor Classification Using a Combination of Variational Autoencoders and Generative Adversarial Networks,,,True,False,"Ahmad, Bilal and Sun, Jun and You, Qi and Palade, Vasile and Mao, Zhongjie",2022.0,,https://www.mdpi.com/2227-9059/10/2/223,,Biomedicines,Brain Tumor Classification Using a Combination of Variational Autoencoders and Generative Adversarial Networks,PDF,https://pure.coventry.ac.uk/ws/portalfiles/portal/51980749/Published.pdf,"Palade, V.; Mao, Z. Brain Tumor. Classification Using a Combination. artificiallygenerated images could solve the limitation of small medical datasets up to a reasonable. of Variational Autoencoders and extent and help the deep learning models perform acceptably. We used the ResNet50 as a classifier, Generative Adversarial Networks."
"Synthetic Generation and Latent Projection Denoising of Rim Lesions in
  Multiple Sclerosis",2505.23353v1,Hzha,\cite{Hzha},QSMRim-Net: Imbalance-aware learning for identification of chronic active multiple sclerosis lesions on quantitative susceptibility maps,,,True,False,"Zhang, Hang and Nguyen, Thanh D. and Zhang, Jinwei and Marcille, Melanie and Spincemaille, Pascal and Wang, Yi and Gauthier, Susan A. and Sweeney, Elizabeth M.",2022.0,,https://www.sciencedirect.com/science/article/pii/S2213158222000444,https://doi.org/10.1016/j.nicl.2022.102979,NeuroImage: Clinical,QSMRim-Net: Imbalance-aware learning for identification of chronic active multiple sclerosis lesions on quantitative susceptibility maps,QSMRim-Net: Imbalance-aware learning for identification of chronic ...,https://pubmed.ncbi.nlm.nih.gov/35247730/,"Quantitative susceptibility mapping (QSM) is an MRI technique that is sensitive to chronic active lesions, termed rim + lesions on the QSM. We present QSMRim-Net, a data imbalance-aware deep neural network that fuses lesion-level radiomic and convolutional image features for automated identification of rim + lesions on QSM."
Pre-Training Curriculum for Multi-Token Prediction in Language Models,2505.22757v1,bengio2009curriculum,\cite{bengio2009curriculum},Curriculum learning,,,True,False,"Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason",2009.0,,https://doi.org/10.1145/1553374.1553380,10.1145/1553374.1553380,,Curriculum learning,Curriculum learning - Wikipedia,https://en.wikipedia.org/wiki/Curriculum_learning,"Curriculum learning - Wikipedia Curriculum learning Curriculum learning is a technique in machine learning in which a model is trained on examples of increasing difficulty, where the definition of ""difficulty"" may be provided externally or discovered as part of the training process. Most generally, curriculum learning is the technique of successively increasing the difficulty of examples in the training set that is presented to a model over multiple training iterations. Simple approaches may use a fixed schedule, such as training on easy examples for half of the available iterations and then all examples for the second half.[3] Other approaches use self-paced learning to increase the difficulty in proportion to the performance of the model on the current set.[10] ""Curriculum Learning"". ""Curriculum Learning"". Curriculum learning"
Pre-Training Curriculum for Multi-Token Prediction in Language Models,2505.22757v1,cl_nlu,\cite{cl_nlu},Curriculum Learning for Natural Language Understanding,,,True,False,"Xu, Benfeng  and
      Zhang, Licheng  and
      Mao, Zhendong  and
      Wang, Quan  and
      Xie, Hongtao  and
      Zhang, Yongdong",2020.0,,https://aclanthology.org/2020.acl-main.542,10.18653/v1/2020.acl-main.542,,Curriculum Learning for Natural Language Understanding,[2108.02170] Curriculum learning for language modeling - arXiv.org,https://arxiv.org/abs/2108.02170,"Language Models like ELMo and BERT have provided robust representations of natural language, which serve as the language understanding component for a diverse range of downstream tasks.Curriculum learning is a method that employs a structured training regime instead, which has been leveraged in computer vision and machine translation to improve model training speed and model performance. While"
Pre-Training Curriculum for Multi-Token Prediction in Language Models,2505.22757v1,cl_bert,\cite{cl_bert},Pre-training a {BERT} with Curriculum Learning by Increasing Block-Size of Input Text,,,True,False,"Nagatsuka, Koichi  and
      Broni-Bediako, Clifford  and
      Atsumi, Masayasu",2021.0,,https://aclanthology.org/2021.ranlp-1.112,,,Pre-training a {BERT} with Curriculum Learning by Increasing Block-Size of Input Text,Length-Based Curriculum Learning for Efficient Pre-training of Language ...,https://dl.acm.org/doi/10.1007/s00354-022-00198-8,"Nagatsuka, K., Broni-Bediako, C., Atsumi, M.: Pre-training a BERT with curriculum learning by increasing block-size of input text. In: Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021), pp. 989-996 (2021) Google Scholar"
Pre-Training Curriculum for Multi-Token Prediction in Language Models,2505.22757v1,bert_lrc,\cite{bert_lrc},Modeling Easiness for Training Transformers with Curriculum Learning,,,True,False,"Ranaldi, Leonardo  and
      Pucci, Giulia  and
      Zanzotto, Fabio Massimo",2023.0,,https://aclanthology.org/2023.ranlp-1.101,,,Modeling Easiness for Training Transformers with Curriculum Learning,acl-bg.org,https://acl-bg.org/proceedings/2023/RANLP+2023/bib/2023.ranlp-1.101.bib,"@InProceedings{ranaldi-pucci-zanzotto:2023:RANLP, author = {Ranaldi, Leonardo and Pucci, Giulia and Zanzotto, Fabio Massimo}, title = {Modeling Easiness for Training Transformers with Curriculum Learning}, booktitle = {Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing}, month = {September}, year = {2023}, address = {Varna, Bulgaria}, publisher"
Pre-Training Curriculum for Multi-Token Prediction in Language Models,2505.22757v1,babylm_2023,\cite{babylm_2023},Findings of the {B}aby{LM} Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora,,,True,False,"Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",2023.0,,https://aclanthology.org/2023.conll-babylm.1,10.18653/v1/2023.conll-babylm.1,,Findings of the {B}aby{LM} Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora,GitHub Pages,https://babylm.github.io/,"Sample-efficient pretraining on a developmentally plausible corpus . Overview • Guidelines • Timeline • FAQs • Previous papers . Summary: The 3rd BabyLM Challenge will be held as a workshop for EMNLP 2025! The overarching goals of the challenge remain the same, however, some of the rules are different for this year. ... • We introduce"
Pre-Training Curriculum for Multi-Token Prediction in Language Models,2505.22757v1,prophetnet,\cite{prophetnet},{P}rophet{N}et: Predicting Future N-gram for Sequence-to-{S}equence{P}re-training,,,True,False,"Qi, Weizhen  and
      Yan, Yu  and
      Gong, Yeyun  and
      Liu, Dayiheng  and
      Duan, Nan  and
      Chen, Jiusheng  and
      Zhang, Ruofei  and
      Zhou, Ming",2020.0,,https://aclanthology.org/2020.findings-emnlp.217,10.18653/v1/2020.findings-emnlp.217,,{P}rophet{N}et: Predicting Future N-gram for Sequence-to-{S}equence{P}re-training,ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,https://arxiv.org/abs/2001.04063,"This paper presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of optimizing one-step-ahead prediction in the traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction that predicts the next n"
Pre-Training Curriculum for Multi-Token Prediction in Language Models,2505.22757v1,blockwise_parallel_decoding,\cite{blockwise_parallel_decoding},Advances in Neural Information Processing Systems,,,True,False,"Stern, Mitchell and Shazeer, Noam and Uszkoreit, Jakob",2018.0,,https://proceedings.neurips.cc/paper_files/paper/2018/file/c4127b9194fe8562c64dc0f5bf2c93bc-Paper.pdf,,,Advances in Neural Information Processing Systems,List of Proceedings,https://papers.nips.cc/,"From 2022 on, the Datasets and Benchmarks papers are in the main NeurIPS proceedings. Advances in Neural Information Processing Systems Datasets and Benchmarks, 2021 Advances in Neural Information Processing Systems 37 (NeurIPS 2024) Advances in Neural Information Processing Systems 36 (NeurIPS 2023) Advances in Neural Information Processing Systems 35 (NeurIPS 2022) Advances in Neural Information Processing Systems 34 (NeurIPS 2021) Advances in Neural Information Processing Systems 33 (NeurIPS 2020) Advances in Neural Information Processing Systems 32 (NeurIPS 2019) Advances in Neural Information Processing Systems 31 (NeurIPS 2018) Requests for name changes in the electronic proceedings will be accepted with no questions asked. Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings."
Pre-Training Curriculum for Multi-Token Prediction in Language Models,2505.22757v1,layerskip,\cite{layerskip},{L}ayer{S}kip: Enabling Early Exit Inference and Self-Speculative Decoding,,,True,False,"Elhoushi, Mostafa  and
      Shrivastava, Akshat  and
      Liskovich, Diana  and
      Hosmer, Basil  and
      Wasti, Bram  and
      Lai, Liangzhen  and
      Mahmoud, Anas  and
      Acun, Bilge  and
      Agarwal, Saurabh  and
      Roman, Ahmed  and
      Aly, Ahmed  and
      Chen, Beidi  and
      Wu, Carole-Jean",2024.0,,https://aclanthology.org/2024.acl-long.681,10.18653/v1/2024.acl-long.681,,{L}ayer{S}kip: Enabling Early Exit Inference and Self-Speculative Decoding,LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding,https://arxiv.org/abs/2404.16710,"We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, we show that this training recipe increases the accuracy of early"
"Learning Pyramid-structured Long-range Dependencies for 3D Human Pose
  Estimation",2506.02853v1,lee1985determination,\cite{lee1985determination},Determination of {3D} human body postures from a single view,,,True,False,"Lee, Hsi-Jian and Chen, Zen",1985.0,,,,"Computer Vision, Graphics, and Image Processing",Determination of {3D} human body postures from a single view,Determination of 3D human body postures from a single view,https://www.sciencedirect.com/science/article/pii/0734189X85900945,"In this paper a method is proposed to recover and interpret the 3D body structures of a person from a single view, provided that (1) at least six feature points on the head and a set of body joints are available on the image plane, and (2) the geometry of head and lengths of body segments formed by joints are known."
"Learning Pyramid-structured Long-range Dependencies for 3D Human Pose
  Estimation",2506.02853v1,mehta2017monocular,\cite{mehta2017monocular},Monocular {3D} human pose estimation in the wild using improved cnn supervision,,,True,False,"Mehta, Dushyant and Rhodin, Helge and Casas, Dan and Fua, Pascal and Sotnychenko, Oleksandr and Xu, Weipeng and Theobalt, Christian",2017.0,,,,,Monocular {3D} human pose estimation in the wild using improved cnn supervision,Monocular 3D Human Pose Estimation in the Wild Using Improved CNN ...,https://ieeexplore.ieee.org/document/8374605,"We propose a CNN-based approach for 3D human body pose estimation from single RGB images that addresses the issue of limited generalizability of models trained solely on the starkly limited publicly available 3D pose data. Using only the existing 3D pose data and 2D pose data, we show state-of-the-art performance on established benchmarks through transfer of learned features, while also"
"Learning Pyramid-structured Long-range Dependencies for 3D Human Pose
  Estimation",2506.02853v1,cai2019exploiting,\cite{cai2019exploiting},Exploiting spatial-temporal relationships for {3D} pose estimation via graph convolutional networks,,,True,False,"Cai, Yujun and Ge, Liuhao and Liu, Jun and Cai, Jianfei and Cham, Tat-Jen and Yuan, Junsong and Thalmann, Nadia Magnenat",2019.0,,,,,Exploiting spatial-temporal relationships for {3D} pose estimation via graph convolutional networks,PDF,https://openaccess.thecvf.com/content_ICCV_2019/papers/Cai_Exploiting_Spatial-Temporal_Relationships_for_3D_Pose_Estimation_via_Graph_Convolutional_ICCV_2019_paper.pdf,"• By treating a sequence of skeletons as a spatial-temporal graph, we propose to use GCN to effectively exploit the spatial conﬁgurations and temporal consis-tencies for 3D pose estimation, both of which are sig-niﬁcant for improving the 3D pose estimation accu-racy. • We design a local-to-global network architecture,"
"Learning Pyramid-structured Long-range Dependencies for 3D Human Pose
  Estimation",2506.02853v1,zhao2019semantic,\cite{zhao2019semantic},{Semantic Graph Convolutional Networks for 3D Human Pose Regression},,,True,False,"Zhao, Long and Peng, Xi and Tian, Yu and Kapadia, Mubbasir and Metaxas, Dimitris N",2019.0,,,,,{Semantic Graph Convolutional Networks for 3D Human Pose Regression},Semantic Graph Convolutional Networks for 3D Human Pose Regression,https://arxiv.org/abs/1904.03345,"In this paper, we study the problem of learning Graph Convolutional Networks (GCNs) for regression. Current architectures of GCNs are limited to the small receptive field of convolution filters and shared transformation matrix for each node. To address these limitations, we propose Semantic Graph Convolutional Networks (SemGCN), a novel neural network architecture that operates on regression"
"Learning Pyramid-structured Long-range Dependencies for 3D Human Pose
  Estimation",2506.02853v1,zou2021modulated,\cite{zou2021modulated},Modulated graph convolutional network for {3D} human pose estimation,,,True,False,"Zou, Zhiming and Tang, Wei",2021.0,,,,,Modulated graph convolutional network for {3D} human pose estimation,Modulated Graph Convolutional Network for 3D Human Pose Estimation ...,https://github.com/ZhimingZo/Modulated-GCN,"The proposed Modulated Graph Convolutional Network (Modulated GCN) is a graph convolutional network architecture that operates on regression tasks with graph-structured data. We evaluate our model for 3D human pose estimation on the Human3.6M Dataset. In this repository, only 2D joints of the human pose are exploited as inputs."
"Learning Pyramid-structured Long-range Dependencies for 3D Human Pose
  Estimation",2506.02853v1,zhao2022graformer,\cite{zhao2022graformer},{GraFormer: Graph-oriented Transformer for {3D} Pose Estimation},,,True,False,"Zhao, Weixi and Wang, Weiqiang and Tian, Yunjie",2022.0,,,,,{GraFormer: Graph-oriented Transformer for {3D} Pose Estimation},GraFormer: Graph Convolution Transformer for 3D Pose Estimation,https://arxiv.org/abs/2109.08364,"Exploiting relations among 2D joints plays a crucial role yet remains semi-developed in 2D-to-3D pose estimation. To alleviate this issue, we propose GraFormer, a novel transformer architecture combined with graph convolution for 3D pose estimation. The proposed GraFormer comprises two repeatedly stacked core modules, GraAttention and ChebGConv block. GraAttention enables all 2D joints to"
"Learning Pyramid-structured Long-range Dependencies for 3D Human Pose
  Estimation",2506.02853v1,ZhongTMM2024,\cite{ZhongTMM2024},{Frame-Padded Multiscale Transformer for Monocular {3D} Human Pose Estimation},,,True,False,"Zhong, Yuanhong and Yang, Guangxia and Zhong, Daidi and Yang, Xun and Wang, Shanshan",2024.0,,,10.1109/TMM.2023.3347095,IEEE Transactions on Multimedia,{Frame-Padded Multiscale Transformer for Monocular {3D} Human Pose Estimation},Frame-Padded Multiscale Transformer for Monocular 3D Human Pose Estimation,https://ieeexplore.ieee.org/document/10374279,"Monocular 3D human pose estimation is an ill-posed problem in computer vision due to its depth ambiguity. Most existing works supplement the depth information by extracting temporal pose features from video frames, and they have made notable progress. However, these approaches divide a long sequence of video frames into multiple short sequences for separate processing, which leads to the loss"
"Learning Pyramid-structured Long-range Dependencies for 3D Human Pose
  Estimation",2506.02853v1,WangTMM2024,\cite{WangTMM2024},{Exploiting Temporal Correlations for {3D} Human Pose Estimation},,,True,False,"Wang, Ruibin and Ying, Xianghua and Xing, Bowei",2024.0,,,10.1109/TMM.2023.3323874,IEEE Transactions on Multimedia,{Exploiting Temporal Correlations for {3D} Human Pose Estimation},Exploiting Temporal Correlations for 3D Human Pose Estimation,https://ieeexplore.ieee.org/document/10278485,"Exploiting the rich temporal information in human pose sequences to facilitate 3D pose estimation has garnered particular attention. While various learning architectures have been designed for temporal exploiting, these architectures are usually trained via the 3D pose loss independently imposed on every single frame, without explicit temporal signals introduced for supervision. This"
"Learning Pyramid-structured Long-range Dependencies for 3D Human Pose
  Estimation",2506.02853v1,tang20233d,\cite{tang20233d},{3D} human pose estimation with spatio-temporal criss-cross attention,,,True,False,"Tang, Zhenhua and Qiu, Zhaofan and Hao, Yanbin and Hong, Richang and Yao, Ting",2023.0,,,,,{3D} human pose estimation with spatio-temporal criss-cross attention,PDF,https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_3D_Human_Pose_Estimation_With_Spatio-Temporal_Criss-Cross_Attention_CVPR_2023_paper.pdf,"Figure 1. Modeling spatio-temporal correlation for 3D human pose estimation by (a) utilizing spatio-temporal attention on all joints in the entire video, (b) separating the framework into two steps that respectively capture spatial and temporal context, and (c) our Spatio-Temporal Criss-cross attention (STC), i.e., a two-"
"Learning Pyramid-structured Long-range Dependencies for 3D Human Pose
  Estimation",2506.02853v1,ci2019optimizing,\cite{ci2019optimizing},Optimizing network structure for {3D} human pose estimation,,,True,False,"Ci, Hai and Wang, Chunyu and Ma, Xiaoxuan and Wang, Yizhou",2019.0,,,,,Optimizing network structure for {3D} human pose estimation,Optimizing Network Structure for 3D Human Pose Estimation,https://ieeexplore.ieee.org/document/9010332,"A human pose is naturally represented as a graph where the joints are the nodes and the bones are the edges. So it is natural to apply Graph Convolutional Network (GCN) to estimate 3D poses from 2D poses. In this work, we propose a generic formulation where both GCN and Fully Connected Network (FCN) are its special cases. From this formulation, we discover that GCN has limited representation"
"Learning Pyramid-structured Long-range Dependencies for 3D Human Pose
  Estimation",2506.02853v1,liu2020comprehensive,\cite{liu2020comprehensive},A comprehensive study of weight sharing in graph networks for {3D} human pose estimation,,,True,False,"Liu, Kenkun and Ding, Rongqi and Zou, Zhiming and Wang, Le and Tang, Wei",2020.0,,,,,A comprehensive study of weight sharing in graph networks for {3D} human pose estimation,KenkunLiu (Kenkun Liu) - GitHub,https://github.com/KenkunLiu/,"Weight-Sharing-in-Graph-Networks-for-3D-HPE Weight-Sharing-in-Graph-Networks-for-3D-HPE Public. The codes in this repository is for the ECCV2020 paper A Comprehensive Study of Weight Sharing in Graph Networks for 3D Human Pose Estimation. ... [NeurIPS 2023] Official implementation of the paper ""A Comprehensive Benchmark for Neural Human"
"Learning Pyramid-structured Long-range Dependencies for 3D Human Pose
  Estimation",2506.02853v1,he2021db,\cite{he2021db},{DB-LSTM: Densely-connected Bi-directional LSTM for human action recognition},,,True,False,"He, Jun-Yan and Wu, Xiao and Cheng, Zhi-Qi and Yuan, Zhaoquan and Jiang, Yu-Gang",2021.0,,,,Neurocomputing,{DB-LSTM: Densely-connected Bi-directional LSTM for human action recognition},DB-LSTM: Densely-connected Bi-directional LSTM for human action ...,https://www.sciencedirect.com/science/article/pii/S0925231220317859,A Densely-connected Bi-directional LSTM (DB-LSTM) network is novelly proposed to capture the long-range temporal properties and to alleviate the problem of gradient vanishing. ... The results also show that the fusion strategy is effective in improving the accuracy of human action recognition. TSN and TS-LSTM are also two-stream structure
"Learning Pyramid-structured Long-range Dependencies for 3D Human Pose
  Estimation",2506.02853v1,zhang2023learning,\cite{zhang2023learning},Learning Enriched Hop-Aware Correlation for Robust {3D} Human Pose Estimation,,,True,False,"Zhang, Shengping and Wang, Chenyang and Nie, Liqiang and Yao, Hongxun and Huang, Qingming and Tian, Qi",2023.0,,,,International Journal of Computer Vision,Learning Enriched Hop-Aware Correlation for Robust {3D} Human Pose Estimation,Learning Enriched Hop-Aware Correlation for Robust 3D Human Pose ...,https://link.springer.com/article/10.1007/s11263-023-01770-5,"Graph convolution networks (GCNs) based methods for 3D human pose estimation usually aggregate immediate features of single-hop nodes, which are unaware of the correlation of multi-hop nodes and therefore neglect long-range dependency for predicting complex poses. In addition, they typically operate either on single-scale or sequential down-sampled multi-scale graph representations, resulting"
"Learning Pyramid-structured Long-range Dependencies for 3D Human Pose
  Estimation",2506.02853v1,xue2022boosting,\cite{xue2022boosting},Boosting monocular {3D} human pose estimation with part aware attention,,,True,False,"Xue, Youze and Chen, Jiansheng and Gu, Xiangming and Ma, Huimin and Ma, Hongbing",2022.0,,,,IEEE Transactions on Image Processing,Boosting monocular {3D} human pose estimation with part aware attention,GitHub - thuxyz19/3D-HPE-PAA,https://github.com/thuxyz19/3D-HPE-PAA,Boosting Monocular 3D Human Pose Estimation with Part Aware Attention. ... ./data data_2d_h36m_cpn_ft_h36m_dbb.npz data_2d_h36m_gt.npz data_3d_h36m.npz Download the checkpoints ... Part of the code is borrowed from Poseformer and VideoPose3D. We thank the authors for releasing their codes.
"Learning Pyramid-structured Long-range Dependencies for 3D Human Pose
  Estimation",2506.02853v1,wu2022hpgcn,\cite{wu2022hpgcn},{HPGCN: Hierarchical poselet-guided graph convolutional network for {3D} pose estimation},,,True,False,"Wu, Yongpeng and Kong, Dehui and Wang, Shaofan and Li, Jinghua and Yin, Baocai",2022.0,,,,Neurocomputing,{HPGCN: Hierarchical poselet-guided graph convolutional network for {3D} pose estimation},HPGCN: Hierarchical poselet-guided graph convolutional network for 3D ...,https://www.sciencedirect.com/science/article/pii/S0925231221016817,"By observing that human movements occur due to part of human body (i.e. related skeletons and body components, known as the poselet) and those poselets contribute to each movement in a hierarchical fashion, we propose a hierarchical poselet-guided graph convolutional network (HPGCN) for 3D pose estimation from 2D poses."
"Learning Pyramid-structured Long-range Dependencies for 3D Human Pose
  Estimation",2506.02853v1,hua2022unet,\cite{hua2022unet},Weakly-supervised {3D} human pose estimation with cross-view U-shaped graph convolutional network,,,True,False,"Hua, Guoliang and Liu, Hong and Li, Wenhao and Zhang, Qian and Ding, Runwei and Xu, Xin",2022.0,,,,IEEE Transactions on Multimedia,Weakly-supervised {3D} human pose estimation with cross-view U-shaped graph convolutional network,Weakly-supervised Cross-view 3D Human Pose Estimation - ResearchGate,https://www.researchgate.net/publication/351841471_Weakly-supervised_Cross-view_3D_Human_Pose_Estimation,"cross-view 2D joint detections and then reﬁnes the pose with a reﬁnement model in a weakly-supervised manner. In the reﬁnement progress, a lightweight cross-view U- shaped graph"
Probabilistic Online Event Downsampling,2506.02547v1,cohen2018spatial,\cite{cohen2018spatial},Spatial and temporal downsampling in event-based visual classification,,,True,False,"Cohen, Gregory and Afshar, Saeed and Orchard, Garrick and Tapson, Jonathan and Benosman, Ryad and van Schaik, Andre",2018.0,,,,IEEE Transactions on Neural Networks and Learning Systems,Spatial and temporal downsampling in event-based visual classification,Spatial and Temporal Downsampling in Event-Based Visual Classification,https://www.researchgate.net/publication/322566649_Spatial_and_Temporal_Downsampling_in_Event-Based_Visual_Classification,"For instance, Cohen et al. (2018) [37] demonstrated the advantages of spatial and temporal down-sampling in event-based visual classification, while Kang et al. (2020) [38] examined the effects of"
Probabilistic Online Event Downsampling,2506.02547v1,ghoshevdownsampling,\cite{ghoshevdownsampling},EvDownsampling: a robust method for downsampling event camera data,,,True,False,"Ghosh, Anindya and Nowotny, Thomas and Knight, James",,,,,,EvDownsampling: a robust method for downsampling event camera data,Neuromorphic Downsampling of Event-Based Camera Output,https://dl.acm.org/doi/10.1145/3584954.3584962,Ghosh A Nowotny T Knight J (2025) EvDownsampling: A Robust Method for Downsampling Event Camera Data Computer Vision - ECCV 2024 Workshops 10.1007/978-3-031-92460-6_22 (377-390) ... EvDownsampling: A Robust Method for Downsampling Event Camera Data. Computer Vision - ECCV 2024 Workshops .
Probabilistic Online Event Downsampling,2506.02547v1,barrios2018less,\cite{barrios2018less},Less data same information for event-based sensors: A bioinspired filtering and data reduction algorithm,,,True,False,"Barrios-Avil{\'e}s, Juan and Rosado-Mu{\~n}oz, Alfredo and Medus, Leandro D and Bataller-Mompe{\'a}n, Manuel and Guerrero-Mart{\'\i}nez, Juan F",2018.0,,,,Sensors,Less data same information for event-based sensors: A bioinspired filtering and data reduction algorithm,Less Data Same Information for Event-Based Sensors: A Bioinspired ...,https://pmc.ncbi.nlm.nih.gov/articles/PMC6308842/,The reduction of generated data implies the possibility of lower hardware requirements and less power consumption for the hardware devices. This work proposes a filtering algorithm (LDSI—Less Data Same Information) which reduces the generated data from event-based sensors without loss of relevant information.
Probabilistic Online Event Downsampling,2506.02547v1,Gruel_2023_WACV,\cite{Gruel_2023_WACV},Performance Comparison of DVS Data Spatial Downscaling Methods Using Spiking Neural Networks,,,True,False,"Gruel, Am\'elie and Martinet, Jean and Linares-Barranco, Bernab\'e and Serrano-Gotarredona, Teresa",2023.0,January,,,,Performance Comparison of DVS Data Spatial Downscaling Methods Using Spiking Neural Networks,WACV 2023 - Performance comparison of real time DVS data spatial ...,https://www.youtube.com/watch?v=PQ9LlvLzOVM,"Presentation of the article ""Performance comparison of real time DVS data spatial downscaling methods using Spiking Neural Networks""Authors : Amélie Gruel¹,"
Probabilistic Online Event Downsampling,2506.02547v1,ghosh2023insect,\cite{ghosh2023insect},Insect-inspired Spatio-temporal Downsampling of Event-based Input,,,True,False,"Ghosh, Anindya and Nowotny, Thomas and Knight, James C",2023.0,,,,,Insect-inspired Spatio-temporal Downsampling of Event-based Input,Insect-inspired Spatio-temporal Downsampling of Event-based Input ...,https://www.semanticscholar.org/paper/Insect-inspired-Spatio-temporal-Downsampling-of-Ghosh-Nowotny/9543cbb0c01863495b70a852fe2c1f3a0582cc91,"This work extends previous work and presents a novel biologically-inspired process that can adeptly downsample event streams by factors of up to 16 times compared to the original resolution and massively reduces the number of spikes that downstream neuromorphic processors have to handle. As vision sensors for autonomous systems, event based cameras provide numerous benefits over conventional"
Probabilistic Online Event Downsampling,2506.02547v1,rizzo2023neuromorphic,\cite{rizzo2023neuromorphic},Neuromorphic downsampling of event-based camera output,,,True,False,"Rizzo, Charles P and Schuman, Catherine D and Plank, James S",2023.0,,,,,Neuromorphic downsampling of event-based camera output,EvDownsampling: A Robust Method for Downsampling Event Camera Data,https://link.springer.com/chapter/10.1007/978-3-031-92460-6_22,"This has been shown across a range of tasks including optical flow and camera pose tracking . Neuromorphic systems would seem well-suited for the deployment of event-based vision applications. ... Schuman, C.D., Plank, J.S.: Neuromorphic downsampling of event-based camera output. In: Proceedings of the 2023 Annual Neuro-Inspired Computational"
Probabilistic Online Event Downsampling,2506.02547v1,gruel2023frugal,\cite{gruel2023frugal},Frugal event data: how small is too small? A human performance assessment with shrinking data,,,True,False,"Gruel, Am{\'e}lie and Carreras, Luc{\'\i}a Trillo and Garc{\'\i}a, Marina Bueno and Kupczyk, Ewa and Martinet, Jean",2023.0,,,,,Frugal event data: how small is too small? A human performance assessment with shrinking data,Frugal event data: how small is too small? A human performance ...,https://ieeexplore.ieee.org/document/10208584,"Frugal event data: how small is too small? A human performance assessment with shrinking data Abstract: When designing embedded computer vision systems with limited computational budget, one often needs to take care of the size of input data. In recent years, however, event cameras have shown increasingly large sensor sizes."
"Efficient Test-time Adaptive Object Detection via Sensitivity-Guided
  Pruning",2506.02462v1,deng2024balanced,\cite{deng2024balanced},Balanced Teacher for Source-free Object Detection,,,True,False,"Deng, Jinhong and Li, Wen and Duan, Lixin",2024.0,,,,IEEE Transactions on Circuits and Systems for Video Technology,Balanced Teacher for Source-free Object Detection,Balanced Teacher for Source-Free Object Detection,https://ieeexplore.ieee.org/document/10466767,"We study a practical domain adaptation task, named source-free object detection (SFOD), which aims to adapt a pre-trained source detector to an unlabeled target domain without access to the original labeled source domain samples. In this paper, we design a new self-training approach for SFOD called Balance Teacher based on the mean teacher model. We target two key issues when using self"
"Efficient Test-time Adaptive Object Detection via Sensitivity-Guided
  Pruning",2506.02462v1,yoon2024enhancing,\cite{yoon2024enhancing},Enhancing Source-Free Domain Adaptive Object Detection with Low-confidence Pseudo Label Distillation,,,True,False,"Yoon, Ilhoon and Kwon, Hyeongjun and Kim, Jin and Park, Junyoung and Jang, Hyunsung and Sohn, Kwanghoon",2024.0,,,,arXiv preprint arXiv:2407.13524,Enhancing Source-Free Domain Adaptive Object Detection with Low-confidence Pseudo Label Distillation,ECVA | European Computer Vision Association,https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/11254_ECCV_2024_paper.php,"Enhancing Source-Free Domain Adaptive Object Detection with Low-confidence Pseudo Label Distillation. Ilhoon Yoon, Hyeongjun Kwon, Jin Kim, ... Abstract ""Source-Free domain adaptive Object Detection (SFOD) is a promising strategy for deploying trained detectors to new, unlabeled domains without accessing source data, addressing significant"
"Efficient Test-time Adaptive Object Detection via Sensitivity-Guided
  Pruning",2506.02462v1,zhao2024multi,\cite{zhao2024multi},Multi-Source-Free Domain Adaptive Object Detection,,,True,False,"Zhao, Sicheng and Yao, Huizai and Lin, Chuang and Gao, Yue and Ding, Guiguang",2024.0,,,,International Journal of Computer Vision,Multi-Source-Free Domain Adaptive Object Detection,Multi-source-free Domain Adaptive Object Detection,https://link.springer.com/article/10.1007/s11263-024-02170-z,"To enhance the transferability of object detection models in real-world scenarios where data is sampled from disparate distributions, considerable attention has been devoted to domain adaptive object detection (DAOD). Researchers have also investigated multi-source DAOD to confront the challenges posed by training samples originating from different source domains. However, existing methods"
"Efficient Test-time Adaptive Object Detection via Sensitivity-Guided
  Pruning",2506.02462v1,ruan2024fully,\cite{ruan2024fully},Fully Test-time Adaptation for Object Detection,,,True,False,"Ruan, Xiaoqian and Tang, Wei",2024.0,,,,,Fully Test-time Adaptation for Object Detection,Fully Test-time Adaptation for Object Detection - IEEE Xplore,https://ieeexplore.ieee.org/document/10677998,"Though the object detection performance on standard benchmarks has been improved drastically in the last decade, ... To close this gap, this paper investigates fully test-time adaptation for object detection. It means to update a trained object detector on a single testing image before making a prediction, without access to the training data."
"Efficient Test-time Adaptive Object Detection via Sensitivity-Guided
  Pruning",2506.02462v1,cao2024exploring,\cite{cao2024exploring},Exploring Test-Time Adaptation for Object Detection in Continually Changing Environments,,,True,False,"Cao, Shilei and Liu, Yan and Zheng, Juepeng and Li, Weijia and Dong, Runmin and Fu, Haohuan",2024.0,,,,arXiv preprint arXiv:2406.16439,Exploring Test-Time Adaptation for Object Detection in Continually Changing Environments,Exploring Test-Time Adaptation for Object Detection in Continually ...,https://paperswithcode.com/paper/exploring-test-time-adaptation-for-object,"Exploring Test-Time Adaptation for Object Detection in Continually Changing Environments 24 Jun 2024 ... (CTTA) has recently emerged as a promising technique to gradually adapt a source-trained model to continually changing target domains. Despite recent advancements in addressing CTTA, two critical issues remain: 1) Fixed thresholds for pseudo"
"Efficient Test-time Adaptive Object Detection via Sensitivity-Guided
  Pruning",2506.02462v1,cheng2024survey,\cite{cheng2024survey},"A survey on deep neural network pruning: Taxonomy, comparison, analysis, and recommendations",,,True,False,"Cheng, Hongrong and Zhang, Miao and Shi, Javen Qinfeng",2024.0,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence,"A survey on deep neural network pruning: Taxonomy, comparison, analysis, and recommendations","A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis ...",https://arxiv.org/abs/2308.06767,"This paper provides a comprehensive review of existing research works on deep neural network pruning in a taxonomy of four aspects: speedup, timing, method, and fusion. It also compares, analyzes, and recommends pruning methods and applications, and builds a repository of datasets and evaluations."
"Efficient Test-time Adaptive Object Detection via Sensitivity-Guided
  Pruning",2506.02462v1,lin2017runtime,\cite{lin2017runtime},Runtime neural pruning,,,True,False,"Lin, Ji and Rao, Yongming and Lu, Jiwen and Zhou, Jie",2017.0,,,,Advances in neural information processing systems,Runtime neural pruning,Runtime neural pruning | Proceedings of the 31st International ...,https://dl.acm.org/doi/10.5555/3294771.3294979,"In this paper, we propose a Runtime Neural Pruning (RNP) framework which prunes the deep neural network dynamically at the runtime. Unlike existing neural pruning methods which produce a fixed pruned model for deployment, our method preserves the full ability of the original network and conducts pruning according to the input image and current feature maps adaptively."
"A 2-Stage Model for Vehicle Class and Orientation Detection with
  Photo-Realistic Image Generation",2506.01338v1,dataset1,\cite{dataset1},Citywide reconstruction of cross-sectional traffic flow from moving camera videos,,,True,False,"Kumar, Ashutosh and Kashiyama, Takehiro and Maeda, Hiroya and Sekimoto, Yoshihide",2021.0,,,,,Citywide reconstruction of cross-sectional traffic flow from moving camera videos,Citywide reconstruction of cross-sectional traffic flow from moving ...,https://ieeexplore.ieee.org/document/9671751,Citywide reconstruction of cross-sectional traffic flow from moving camera videos ... This research proposes a novel cross-sectional traffic fl ow es timation al gorithm to re construct tr affic volume from moving camera videos. We develop a vehicle detection dataset with more than one million annotations of vehicles with orientation and train
"A 2-Stage Model for Vehicle Class and Orientation Detection with
  Photo-Realistic Image Generation",2506.01338v1,bigcup,\cite{bigcup},Citywide reconstruction of traffic flow using the vehicle-mounted moving camera in the CARLA driving simulator,,,True,False,"Kumar, Ashutosh and Kashiyama, Takehiro and Maeda, Hiroya and Omata, Hiroshi and Sekimoto, Yoshihide",2022.0,,,,,Citywide reconstruction of traffic flow using the vehicle-mounted moving camera in the CARLA driving simulator,Citywide reconstruction of traffic flow using the vehicle-mounted ...,https://ieeexplore.ieee.org/document/9921927,"Estimating traffic flow is essential in planning road development, routing, navigation, autonomous driving, and other applications in road infrastructure management. Recently, cameras and portable devices mounted on moving vehicles have been proposed to estimate citywide traffic flow. Nevertheless, the efficacy of such algorithms has not been proven on a wide scale under varying environmental"
"A 2-Stage Model for Vehicle Class and Orientation Detection with
  Photo-Realistic Image Generation",2506.01338v1,bigcup2,\cite{bigcup2},Real-time citywide reconstruction of traffic flow from moving cameras on lightweight edge devices,,,True,False,"Kumar, Ashutosh and Kashiyama, Takehiro and Maeda, Hiroya and Omata, Hiroshi and Sekimoto, Yoshihide",2022.0,,,,ISPRS Journal of Photogrammetry and Remote Sensing,Real-time citywide reconstruction of traffic flow from moving cameras on lightweight edge devices,Real-time citywide reconstruction of traffic flow from moving cameras ...,https://ui.adsabs.harvard.edu/abs/2022JPRS..192..115K/abstract,"We obtain a traffic flow reconstruction accuracy ranging from 73.1% to 80.8% evaluated using ground truth data. With the proliferation of moving cameras in vehicles (dash cams, stereo cams, etc.) and inexpensive edge devices, we expect our real-time traffic flow estimation algorithm to have a very promising future."
"A 2-Stage Model for Vehicle Class and Orientation Detection with
  Photo-Realistic Image Generation",2506.01338v1,fastrcnn,\cite{fastrcnn},Fast r-cnn,,,True,False,"Girshick, Ross",2015.0,,,,,Fast r-cnn,[1504.08083] Fast R-CNN - arXiv.org,https://arxiv.org/abs/1504.08083,"Fast R-CNN is a method for efficiently classifying object proposals using deep convolutional networks. It improves training and testing speed, and achieves higher accuracy than previous work on PASCAL VOC 2012 dataset."
"A 2-Stage Model for Vehicle Class and Orientation Detection with
  Photo-Realistic Image Generation",2506.01338v1,faster,\cite{faster},Faster r-cnn: Towards real-time object detection with region proposal networks,,,True,False,"Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian",2015.0,,,,Advances in neural information processing systems,Faster r-cnn: Towards real-time object detection with region proposal networks,Faster R-CNN: Towards Real-Time Object Detection with Region Proposal ...,https://arxiv.org/abs/1506.01497,"[1506.01497] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks Title:Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks View a PDF of the paper titled Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, by Shaoqing Ren and 3 other authors (or arXiv:1506.01497v3 [cs.CV] for this version) View a PDF of the paper titled Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, by Shaoqing Ren and 3 other authors [x] Bibliographic Explorer Toggle  [x] Connected Papers Toggle  [x] Litmaps Toggle  [x] alphaXiv Toggle  [x] Links to Code Toggle  [x] DagsHub Toggle  [x] GotitPub Toggle  [x] Huggingface Toggle  [x] Links to Code Toggle  [x] ScienceCast Toggle  [x] Replicate Toggle  [x] Spaces Toggle  [x] Spaces Toggle  [x] Core recommender toggle "
Aligned Contrastive Loss for Long-Tailed Recognition,2506.01071v1,drummond2003c4,\cite{drummond2003c4},"C4. 5, class imbalance, and cost sensitivity: why under-sampling beats over-sampling",,,True,False,"Drummond, Chris and Holte, Robert C and others",2003.0,,,,,"C4. 5, class imbalance, and cost sensitivity: why under-sampling beats over-sampling",Cluster-based under-sampling approaches for imbalanced data ...,https://www.sciencedirect.com/science/article/pii/S0957417408003527,"C4.5, class imbalance, and cost sensitivity: Why under-sampling beats over-sampling. In Proceedings of the ICML'03 workshop on learning from imbalanced datasets . Google Scholar"
Aligned Contrastive Loss for Long-Tailed Recognition,2506.01071v1,pouyanfar2018dynamic,\cite{pouyanfar2018dynamic},Dynamic sampling in convolutional neural networks for imbalanced data classification,,,True,False,"Pouyanfar, Samira and Tao, Yudong and Mohan, Anup and Tian, Haiman and Kaseb, Ahmed S and Gauen, Kent and Dailey, Ryan and Aghajanzadeh, Sarah and Lu, Yung-Hsiang and Chen, Shu-Ching and others",2018.0,,,,,Dynamic sampling in convolutional neural networks for imbalanced data classification,Dynamic Sampling in Convolutional Neural Networks for Imbalanced Data ...,https://ieeexplore.ieee.org/document/8396983,"Many multimedia systems stream real-time visual data continuously for a wide variety of applications. These systems can produce vast amounts of data, but few studies take advantage of the versatile and real-time data. This paper presents a novel model based on the Convolutional Neural Networks (CNNs) to handle such imbalanced and heterogeneous data and successfully identifies the semantic"
Aligned Contrastive Loss for Long-Tailed Recognition,2506.01071v1,huang2016learning,\cite{huang2016learning},Learning deep representation for imbalanced classification,,,True,False,"Huang, Chen and Li, Yining and Loy, Chen Change and Tang, Xiaoou",2016.0,,,,,Learning deep representation for imbalanced classification,Learning Deep Representation for Imbalanced Classification,https://ieeexplore.ieee.org/document/7780949,"Learning Deep Representation for Imbalanced Classification Abstract: Data in vision domain often exhibit highly-skewed class distribution, i.e., most data belong to a few majority classes, while the minority classes only contain a scarce amount of instances. To mitigate this issue, contemporary classification methods based on deep convolutional"
Aligned Contrastive Loss for Long-Tailed Recognition,2506.01071v1,wang2017learning,\cite{wang2017learning},Learning to model the tail,,,True,False,"Wang, Yu-Xiong and Ramanan, Deva and Hebert, Martial",2017.0,,,,Advances in neural information processing systems,Learning to model the tail,Learning to Model the Tail - NeurIPS,https://proceedings.neurips.cc/paper/2017/hash/147ebe637038ca50a1265abac8dea181-Abstract.html,"Here, the challenge is to learn accurate ""few-shot'' models for classes in the tail of the class distribution, for which little data is available. We cast this problem as transfer learning, where knowledge from the data-rich classes in the head of the distribution is transferred to the data-poor classes in the tail. Our key insights are as follows."
Aligned Contrastive Loss for Long-Tailed Recognition,2506.01071v1,shu2019meta,\cite{shu2019meta},Meta-weight-net: Learning an explicit mapping for sample weighting,,,True,False,"Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu",2019.0,,,,Advances in neural information processing systems,Meta-weight-net: Learning an explicit mapping for sample weighting,Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting,https://arxiv.org/abs/1902.07379,"The paper proposes a method to learn an explicit weighting function for sample re-weighting from data, which can alleviate the overfitting issue of deep neural networks (DNNs) on biased or imbalanced data. The method is based on a universal approximator, a meta-weight-net, and a small amount of unbiased meta-data."
Aligned Contrastive Loss for Long-Tailed Recognition,2506.01071v1,chen2020simple,\cite{chen2020simple},A simple framework for contrastive learning of visual representations,,,True,False,"Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey",2020.0,,,,,A simple framework for contrastive learning of visual representations,A Simple Framework for Contrastive Learning of Visual Representations,https://arxiv.org/abs/2002.05709,"[**View Jobs**](https://info.arxiv.org/hiring/index.html) [View Jobs](https://info.arxiv.org/hiring/index.html) [](https://arxiv.org/IgnoreMe) [Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced) *   [Login](https://arxiv.org/login) *   [Help Pages](https://info.arxiv.org/help) *   [About](https://info.arxiv.org/about) [View PDF](https://arxiv.org/pdf/2002.05709) Cite as:[arXiv:2002.05709](https://arxiv.org/abs/2002.05709) [cs.LG] (or [arXiv:2002.05709v3](https://arxiv.org/abs/2002.05709v3) [cs.LG] for this version) **[[v1]](https://arxiv.org/abs/2002.05709v1)** Thu, 13 Feb 2020 18:50:45 UTC (5,093 KB) **[[v2]](https://arxiv.org/abs/2002.05709v2)** Mon, 30 Mar 2020 15:32:51 UTC (5,047 KB) *   [View PDF](https://arxiv.org/pdf/2002.05709) *   [Other Formats](https://arxiv.org/format/2002.05709) [cs](https://arxiv.org/abs/2002.05709?context=cs) [cs.CV](https://arxiv.org/abs/2002.05709?context=cs.CV) [stat](https://arxiv.org/abs/2002.05709?context=stat) [stat.ML](https://arxiv.org/abs/2002.05709?context=stat.ML) ### [15 blog links](https://arxiv.org/tb/2002.05709) ([what is this?](https://info.arxiv.org/help/trackback.html))  [a](https://arxiv.org/static/browse/0.3.4/css/cite.css)export BibTeX citation Loading... [![Image 7: BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2002.05709&description=A%20Simple%20Framework%20for%20Contrastive%20Learning%20of%20Visual%20Representations ""Bookmark on BibSonomy"")[![Image 8: Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2002.05709&title=A%20Simple%20Framework%20for%20Contrastive%20Learning%20of%20Visual%20Representations ""Bookmark on Reddit"") Bibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_ [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html). [Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2002.05709) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html))  *   [About](https://info.arxiv.org/about) *   [Help](https://info.arxiv.org/help) *   [Contact](https://info.arxiv.org/help/contact.html) *   [Subscribe](https://info.arxiv.org/help/subscribe) *   [Copyright](https://info.arxiv.org/help/license/index.html) *   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html) *   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)"
Aligned Contrastive Loss for Long-Tailed Recognition,2506.01071v1,zhang2022fairness,\cite{zhang2022fairness},Fairness-aware contrastive learning with partially annotated sensitive attributes,,,True,False,"Zhang, Fengda and Kuang, Kun and Chen, Long and Liu, Yuxuan and Wu, Chao and Xiao, Jun",2022.0,,,,,Fairness-aware contrastive learning with partially annotated sensitive attributes,Publications | Long Chen @ HKUST - GitHub Pages,https://zjuchenlong.github.io/Publications/,"Fairness-aware Contrastive Learning with Partially Annotated Sensitive Attributes. Fengda Zhang, Kun Kuang, Long Chen, Yuxuan Liu, Chao Wu, and Jun Xiao . International Conference on Learning Representations (ICLR), 2023 . TPAMI. Counterfactual Samples Synthesizing and Training for Robust Visual Question Answering."
Aligned Contrastive Loss for Long-Tailed Recognition,2506.01071v1,kang2020exploring,\cite{kang2020exploring},Exploring balanced feature spaces for representation learning,,,True,False,"Kang, Bingyi and Li, Yu and Xie, Sa and Yuan, Zehuan and Feng, Jiashi",2020.0,,,,,Exploring balanced feature spaces for representation learning,Exploring Balanced Feature Spaces for Representation Learning,https://github.com/bingykang/BalFeat,This repository contains the PyTorch implementation of the paper Exploring Balanced Feature Spaces for Representation Learning by Bingyi Kang et al. The paper studies the effect of different feature spaces on representation learning and long-tailed recognition.
"Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world
  Video Super-resolution",2506.01037v1,chan2022basicvsr++,\cite{chan2022basicvsr++},Basicvsr++: Improving video super-resolution with enhanced propagation and alignment,,,True,False,"Chan, Kelvin CK and Zhou, Shangchen and Xu, Xiangyu and Loy, Chen Change",2022.0,,,,,Basicvsr++: Improving video super-resolution with enhanced propagation and alignment,BasicVSR++: Improving Video Super-Resolution with Enhanced Propagation ...,https://arxiv.org/abs/2104.13371,BasicVSR++ is a recurrent framework that improves video super-resolution with enhanced propagation and alignment. It outperforms BasicVSR and wins several challenges in NTIRE 2021.
"Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world
  Video Super-resolution",2506.01037v1,jo2018deep,\cite{jo2018deep},Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation,,,True,False,"Jo, Younghyun and Oh, Seoung Wug and Kang, Jaeyeon and Kim, Seon Joo",2018.0,,,,,Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation,GitHub - yhjo09/VSR-DUF,https://github.com/yhjo09/VSR-DUF,"Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation. ... Younghyun and Oh, Seoung Wug and Kang, Jaeyeon and Kim, Seon Joo}, title = {Deep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion Compensation}, booktitle = {The IEEE Conference on Computer Vision"
"Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world
  Video Super-resolution",2506.01037v1,liu2013bayesian,\cite{liu2013bayesian},On Bayesian adaptive video super resolution,,,True,False,"Liu, Ce and Sun, Deqing",2013.0,,,,IEEE transactions on pattern analysis and machine intelligence,On Bayesian adaptive video super resolution,PDF,https://people.csail.mit.edu/celiu/pdfs/VideoSR.pdf,"In this paper, we propose a Bayesian framework for adaptive video super resolution that incorporates high-res image reconstruction, optical ﬂow, noise level and blur ker-nelestimation. Usingasparsitypriorforthehigh-resimage, ﬂow ﬁelds and blur kernel, we show that super resolution computation is reduced to each component problem when"
"Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world
  Video Super-resolution",2506.01037v1,nah2019ntire,\cite{nah2019ntire},Ntire 2019 challenge on video deblurring and super-resolution: Dataset and study,,,True,False,"Nah, Seungjun and Baik, Sungyong and Hong, Seokil and Moon, Gyeongsik and Son, Sanghyun and Timofte, Radu and Mu Lee, Kyoung",2019.0,,,,,Ntire 2019 challenge on video deblurring and super-resolution: Dataset and study,PDF,https://junchenglee.com/other/NTIRE/2019/Video_deblurring_Data.pdf,PDF-1.3 1 0 obj /Kids [ 3 0 R 4 0 R 5 0 R 6 0 R 7 0 R 8 0 R 9 0 R 10 0 R 11 0 R 12 0 R ] /Type /Pages /Count 10 >> endobj 2 0 obj /Title (NTIRE 2019 Challenge on Video Deblurring and Super\055Resolution\072 Dataset and Study) /Producer (PyPDF2) /Author (Seungjun Nah\054 Sungyong Baik\054 Seokil Hong\054 Gyeongsik Moon\054 Sanghyun Son\054
"Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world
  Video Super-resolution",2506.01037v1,yi2019progressive,\cite{yi2019progressive},Progressive fusion video super-resolution network via exploiting non-local spatio-temporal correlations,,,True,False,"Yi, Peng and Wang, Zhongyuan and Jiang, Kui and Jiang, Junjun and Ma, Jiayi",2019.0,,,,,Progressive fusion video super-resolution network via exploiting non-local spatio-temporal correlations,Progressive Fusion Video Super-Resolution Network via Exploiting Non ...,https://github.com/psychopa4/PFNL,"Progressive Fusion Video Super-Resolution Network via Exploiting Non-Local Spatio-Temporal Correlations This work is based on Wang et al and Tao et al . This work has tried to rebuild various state-of-the-art video SR methods, including VESPCN , RVSR-LTD , MCResNet , DRVSR , FRVSR , DUFVSR and PFNL ."
"Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world
  Video Super-resolution",2506.01037v1,realvsr,\cite{realvsr},Real-world video super-resolution: A benchmark dataset and a decomposition based learning scheme,,,True,False,"Yang, Xi and Xiang, Wangmeng and Zeng, Hui and Zhang, Lei",2021.0,,,,,Real-world video super-resolution: A benchmark dataset and a decomposition based learning scheme,Real-world Video Super-resolution: A Benchmark Dataset and A ...,https://ieeexplore.ieee.org/document/9710765,"Real-world Video Super-resolution: A Benchmark Dataset and A Decomposition based Learning Scheme Abstract: Video super-resolution (VSR) aims to improve the spatial resolution of low-resolution (LR) videos. Existing VSR methods are mostly trained and evaluated on synthetic datasets, where the LR videos are uniformly downsampled from their high"
"Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world
  Video Super-resolution",2506.01037v1,variant3,\cite{variant3},Selective structured state-spaces for long-form video understanding,,,True,False,"Wang, Jue and Zhu, Wentao and Wang, Pichao and Yu, Xiang and Liu, Linda and Omar, Mohamed and Hamid, Raffay",2023.0,,,,,Selective structured state-spaces for long-form video understanding,Selective Structured State-Spaces for Long-Form Video Understanding - ar5iv,https://ar5iv.labs.arxiv.org/html/2303.14526,A novel approach to model complex spatiotemporal dependencies in long-form videos using a selective structured state-space sequence (S5) model and a long-short masked contrastive learning (LSMCL) method. The S5 model adaptively selects informative image tokens based on S4 features and the LSMCL method improves the robustness and the temporal horizon of the model.
"Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world
  Video Super-resolution",2506.01037v1,domain1,\cite{domain1},Mamba-nd: Selective state space modeling for multi-dimensional data,,,True,False,"Li, Shufan and Singh, Harkanwar and Grover, Aditya",2024.0,,,,arXiv preprint arXiv:2402.05892,Mamba-nd: Selective state space modeling for multi-dimensional data,Mamba-ND: Selective State Space Modeling for Multi-dimensional Data,https://link.springer.com/chapter/10.1007/978-3-031-73414-4_5,"Early works such as S4 [] and S4ND [] assume linear time invariance (LTI).This constraint makes it possible to solve the above difference equation using a global convolution kernel. Selective state space models, i.e., Mamba [], introduce time-varying parameters that do not follow the LTI assumption.In particular, \(\varDelta \), B, and C become functions of the input signal x(t)."
"Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world
  Video Super-resolution",2506.01037v1,self-supervised_task3,\cite{self-supervised_task3},A simple framework for contrastive learning of visual representations,,,True,False,"Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey",2020.0,,,,,A simple framework for contrastive learning of visual representations,A Simple Framework for Contrastive Learning of Visual Representations,https://arxiv.org/abs/2002.05709,"[**View Jobs**](https://info.arxiv.org/hiring/index.html) [View Jobs](https://info.arxiv.org/hiring/index.html) [](https://arxiv.org/IgnoreMe) [Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced) *   [Login](https://arxiv.org/login) *   [Help Pages](https://info.arxiv.org/help) *   [About](https://info.arxiv.org/about) [View PDF](https://arxiv.org/pdf/2002.05709) Cite as:[arXiv:2002.05709](https://arxiv.org/abs/2002.05709) [cs.LG] (or [arXiv:2002.05709v3](https://arxiv.org/abs/2002.05709v3) [cs.LG] for this version) **[[v1]](https://arxiv.org/abs/2002.05709v1)** Thu, 13 Feb 2020 18:50:45 UTC (5,093 KB) **[[v2]](https://arxiv.org/abs/2002.05709v2)** Mon, 30 Mar 2020 15:32:51 UTC (5,047 KB) *   [View PDF](https://arxiv.org/pdf/2002.05709) *   [Other Formats](https://arxiv.org/format/2002.05709) [cs](https://arxiv.org/abs/2002.05709?context=cs) [cs.CV](https://arxiv.org/abs/2002.05709?context=cs.CV) [stat](https://arxiv.org/abs/2002.05709?context=stat) [stat.ML](https://arxiv.org/abs/2002.05709?context=stat.ML) ### [15 blog links](https://arxiv.org/tb/2002.05709) ([what is this?](https://info.arxiv.org/help/trackback.html))  [a](https://arxiv.org/static/browse/0.3.4/css/cite.css)export BibTeX citation Loading... [![Image 7: BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2002.05709&description=A%20Simple%20Framework%20for%20Contrastive%20Learning%20of%20Visual%20Representations ""Bookmark on BibSonomy"")[![Image 8: Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2002.05709&title=A%20Simple%20Framework%20for%20Contrastive%20Learning%20of%20Visual%20Representations ""Bookmark on Reddit"") Bibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_ [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html). [Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2002.05709) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html))  *   [About](https://info.arxiv.org/about) *   [Help](https://info.arxiv.org/help) *   [Contact](https://info.arxiv.org/help/contact.html) *   [Subscribe](https://info.arxiv.org/help/subscribe) *   [Copyright](https://info.arxiv.org/help/license/index.html) *   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html) *   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)"
"Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world
  Video Super-resolution",2506.01037v1,self-supervised_task5,\cite{self-supervised_task5},Self-supervised representation learning with cross-context learning between global and hypercolumn features,,,True,False,"Gao, Zheng and Feng, Chen and Patras, Ioannis",2024.0,,,,,Self-supervised representation learning with cross-context learning between global and hypercolumn features,Self-Supervised Representation Learning with Cross-Context Learning ...,https://ieeexplore.ieee.org/document/10484066,"Specifically, we stack the intermediate feature maps to construct a ""hypercolumn"" representation so that we can measure instance relations using two contexts (hypercolumn and global feature) separately, and then use the relations of one context to guide the learning of the other. This cross-context learning allows the model to learn from the"
"Efficient 3D Brain Tumor Segmentation with Axial-Coronal-Sagittal
  Embedding",2506.00434v1,ronneberger_unet_miccai_2015,\cite{ronneberger_unet_miccai_2015},U-net: Convolutional networks for biomedical image segmentation,,,True,False,"Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas",2015.0,,,,,U-net: Convolutional networks for biomedical image segmentation,U-Net: Convolutional Networks for Biomedical Image Segmentation - Springer,https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28,"# U-Net: Convolutional Networks for Biomedical Image Segmentation  We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neu-ronal structures in electron microscopic stacks. DOI: 10.1007/978-3-319-24574-4 _28 U-Net: Convolutional Networks for Biomedical Image Segmentation 235 The network does not have any fully connected layers and only uses the valid part of each convolution, i.e., the segmentation map only contains the pixels, for which the full context is available in the input image. U-Net: Convolutional Networks for Biomedical Image Segmentation 237  U-Net: Convolutional Networks for Biomedical Image Segmentation 239                              U-Net: Convolutional Networks for Biomedical Image Segmentation 241 "
"Efficient 3D Brain Tumor Segmentation with Axial-Coronal-Sagittal
  Embedding",2506.00434v1,menze_tmi_2015,\cite{menze_tmi_2015},The {Multimodal} {Brain} {Tumor} {Image} {Segmentation} {Benchmark} ({BRATS}),,,True,False,"Menze, Bjoern H and Jakab, Andras and Bauer, Stefan and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Kirby, Justin and Burren, Yuliya and Porz, Nicole and Slotboom, Johannes and Wiest, Roland and others",2015.0,,,,IEEE TMI,The {Multimodal} {Brain} {Tumor} {Image} {Segmentation} {Benchmark} ({BRATS}),The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS),https://ieeexplore.ieee.org/document/6975210,In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients—manually annotated by up to four raters—and to 65 comparable
"Efficient 3D Brain Tumor Segmentation with Axial-Coronal-Sagittal
  Embedding",2506.00434v1,jiang_cascaded_unet_miccai_2020,\cite{jiang_cascaded_unet_miccai_2020},Two-stage cascaded u-net: 1st place solution to brats challenge 2019 segmentation task,,,True,False,"Jiang, Zeyu and Ding, Changxing and Liu, Minfeng and Tao, Dacheng",2020.0,,,,,Two-stage cascaded u-net: 1st place solution to brats challenge 2019 segmentation task,Multimodal Brain Tumor Segmentation Challenge 2019: Rankings,https://www.med.upenn.edu/cbica/brats2019/rankings.html,"Rankings of Segmentation Task. 1 st place: Team Name: Questionmarks Institution(s): South China University of Technology Accompanying paper: Two-Stage Cascaded U-Net: 1st Place Solution to BraTS Challenge 2019 Segmentation Task Team Members: Zeyu Jiang, Changxing Ding, Minfeng Liu, Dacheng Tao. 2 nd place: Team Name: zyx Institution(s): Chinese Academy of Sciences"
"Efficient 3D Brain Tumor Segmentation with Axial-Coronal-Sagittal
  Embedding",2506.00434v1,isensee_nnunet_nature_2021,\cite{isensee_nnunet_nature_2021},nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation,,,True,False,"Isensee, Fabian and Jaeger, Paul F and Kohl, Simon AA and Petersen, Jens and Maier-Hein, Klaus H",2021.0,,,,Nature methods,nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation,nnU-Net: a self-configuring method for deep learning-based biomedical ...,https://www.nature.com/articles/s41592-020-01008-z,nnU-Net is a deep learning-based image segmentation method that automatically configures itself for diverse biological and medical image segmentation tasks. nnU-Net offers state-of-the-art
Test-time Vocabulary Adaptation for Language-driven Object Detection,2506.00333v1,deng2009imagenet,\cite{deng2009imagenet},{ImageNet: a Large-Scale Hierarchical Image Database},,,True,False,"Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li",2009.0,,,,,{ImageNet: a Large-Scale Hierarchical Image Database},ImageNet,https://image-net.org/,"ImageNet is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images. The project has been instrumental in advancing computer vision and deep learning research. The data is available for free to researchers for non-commercial use."
"Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised
  Learning with Outliers",2505.24443v1,ssl_2,\cite{ssl_2},Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks,,,True,False,"Lee, Dong-Hyun",2013.0,,,,,Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks,Pseudo-Label The Simple and Efficient Semi-Supervised Learning Method ...,https://github.com/iBelieveCJM/pseudo_label-pytorch,"The repository implement a semi-supervised method for Deep Neural Networks, the Pseudo Label. More details for the method please refer to Pseudo-Label The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks."
"Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised
  Learning with Outliers",2505.24443v1,ssl_9,\cite{ssl_9},Semi-supervised Learning by Entropy Minimization,,,True,False,"Yves Grandvalet and
                  Yoshua Bengio",2004.0,,,,,Semi-supervised Learning by Entropy Minimization,Semi-supervised Learning by Entropy Minimization - NeurIPS,https://proceedings.neurips.cc/paper/2004/hash/96f2b50b5d3613adf9c27049b2a888c7-Abstract.html,A paper that proposes a regularization criterion based on minimum entropy to incorporate unlabeled data in supervised learning. The method is applied to various classification problems and compared to other approaches such as mixture models and manifold learning.
"Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised
  Learning with Outliers",2505.24443v1,tnnls_3,\cite{tnnls_3},Graph-Based Semi-Supervised Learning: {A} Comprehensive Review,,,True,False,"Zixing Song and
                  Xiangli Yang and
                  Zenglin Xu and
                  Irwin King",2023.0,,,,{IEEE} Trans. on Neural Networks and Learning Systems,Graph-Based Semi-Supervised Learning: {A} Comprehensive Review,Graph-Based Semi-Supervised Learning: A Comprehensive Review,https://ieeexplore.ieee.org/document/9737635,"Abstract: Semi-supervised learning (SSL) has tremendous value in practice due to the utilization of both labeled and unlabelled data. An essential class of SSL methods, referred to as graph-based semi-supervised learning (GSSL) methods in the literature, is to first represent each sample as a node in an affinity graph, and then, the label information of unlabeled samples can be inferred based"
"Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised
  Learning with Outliers",2505.24443v1,ossl_2,\cite{ossl_2},Semi-Supervised Learning under Class Distribution Mismatch,,,True,False,"Yanbei Chen and
                  Xiatian Zhu and
                  Wei Li and
                  Shaogang Gong",2020.0,,,,,Semi-Supervised Learning under Class Distribution Mismatch,Semi-Supervised Learning under Class Distribution Mismatch,https://ojs.aaai.org/index.php/AAAI/article/view/5763,"Semi-supervised learning (SSL) aims to avoid the need for collecting prohibitively expensive labelled training data. Whilst demonstrating impressive performance boost, existing SSL methods artificially assume that small labelled data and large unlabelled data are drawn from the same class distribution. In a more realistic scenario with class distribution mismatch between the two sets, they"
"Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised
  Learning with Outliers",2505.24443v1,ossl_12,\cite{ossl_12},Rethinking safe semi-supervised learning: Transferring the open-set problem to a close-set one,,,True,False,"Ma, Qiankun and Gao, Jiyao and Zhan, Bo and Guo, Yunpeng and Zhou, Jiliu and Wang, Yan",2023.0,,,,,Rethinking safe semi-supervised learning: Transferring the open-set problem to a close-set one,Rethinking Safe Semi-supervised Learning: Transferring the Open-set ...,https://ieeexplore.ieee.org/document/10376679,"Conventional semi-supervised learning (SSL) lies in the close-set assumption that the labeled and unlabeled sets contain data with the same seen classes, called in-distribution (ID) data. In contrast, safe SSL investigates a more challenging open-set problem where unlabeled set may involve some out-of-distribution (OOD) data with unseen classes, which could harm the performance of SSL. When we"
"Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised
  Learning with Outliers",2505.24443v1,ossl_5,\cite{ossl_5},"Safe-Student for Safe Deep Semi-Supervised Learning with Unseen-Class
                  Unlabeled Data",,,True,False,"Rundong He and
                  Zhongyi Han and
                  Xiankai Lu and
                  Yilong Yin",2022.0,,,,,"Safe-Student for Safe Deep Semi-Supervised Learning with Unseen-Class
                  Unlabeled Data",Safe-Student for Safe Deep Semi-Supervised Learning with Unseen-Class ...,https://ieeexplore.ieee.org/document/9880080,"Deep semi-supervised learning (SSL) methods aim to take advantage of abundant unlabeled data to improve the algorithm performance. In this paper, we consider the problem of safe SSL scenario where unseen-class instances appear in the unlabeled data. This setting is essential and commonly appears in a variety of real applications. One intuitive solution is removing these unseen-class instances"
"Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised
  Learning with Outliers",2505.24443v1,ossl_6,\cite{ossl_6},"{SAFER-STUDENT} for Safe Deep Semi-Supervised Learning With Unseen-Class
                  Unlabeled Data",,,True,False,"Rundong He and
                  Zhongyi Han and
                  Xiankai Lu and
                  Yilong Yin",2024.0,,,,{IEEE} Trans. on Knowledge and Data Engineering,"{SAFER-STUDENT} for Safe Deep Semi-Supervised Learning With Unseen-Class
                  Unlabeled Data",SAFER-STUDENT for Safe Deep Semi-Supervised Learning With ...,https://www.researchgate.net/publication/371000311_SAFER-STUDENT_for_Safe_Deep_Semi-Supervised_Learning_With_Unseen-Class_Unlabeled_Data,Deep semi-supervised learning (SSL) methods aim to utilize abundant unlabeled data to improve the seen-class classification.
"Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised
  Learning with Outliers",2505.24443v1,ossl_8,\cite{ossl_8},"Unknown-Aware Graph Regularization for Robust Semi-supervised Learning
                  from Uncurated Data",,,True,False,"Heejo Kong and
                  Suneung Kim and
                  Ho{-}Joong Kim and
                  Seong{-}Whan Lee",2024.0,,,,,"Unknown-Aware Graph Regularization for Robust Semi-supervised Learning
                  from Uncurated Data",Unknown-Aware Graph Regularization for Robust Semi-supervised Learning ...,https://ojs.aaai.org/index.php/AAAI/article/view/29227,"Recent advances in semi-supervised learning (SSL) have relied on the optimistic assumption that labeled and unlabeled data share the same class distribution. However, this assumption is often violated in real-world scenarios, where unlabeled data may contain out-of-class samples. SSL with such uncurated unlabeled data leads training models to be corrupted."
"Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised
  Learning with Outliers",2505.24443v1,ossl_1,\cite{ossl_1},Safe Deep Semi-Supervised Learning for Unseen-Class Unlabeled Data,,,True,False,"Lan{-}Zhe Guo and
                  Zhenyu Zhang and
                  Yuan Jiang and
                  Yufeng Li and
                  Zhi{-}Hua Zhou",2020.0,,,,,Safe Deep Semi-Supervised Learning for Unseen-Class Unlabeled Data,PDF,https://openaccess.thecvf.com/content/CVPR2022/papers/He_Safe-Student_for_Safe_Deep_Semi-Supervised_Learning_With_Unseen-Class_Unlabeled_Data_CVPR_2022_paper.pdf,"SAFE-STUDENT is a novel framework for safe deep semi-supervised learning (SSL) with unseen-class unlabeled data. It proposes a new scoring function, a label distribution learning module, and an iterative optimization strategy to improve unseen-class identification and seen-class classification."
"Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised
  Learning with Outliers",2505.24443v1,ossl_13,\cite{ossl_13},Binary Decomposition: A Problem Transformation Perspective for Open-Set Semi-Supervised Learning,,,True,False,"Hang, Jun-Yi and Zhang, Min-Ling",2024.0,,,,,Binary Decomposition: A Problem Transformation Perspective for Open-Set Semi-Supervised Learning,Min-Ling Zhang's Publication - Southeast University,https://palm.seu.edu.cn/zhangml/Publication.htm,"Binary decomposition: A problem transformation perspective for open-set semi-supervised learning. In: Proceedings of the 41st International Conference on Machine Learning (ICML'24), Vienna, Austria, 2024, 17505-17518. Y.-F. Zhang, M.-L. Zhang. Generalization analysis for multi-label learning."
"KairosAD: A SAM-Based Model for Industrial Anomaly Detection on Embedded
  Devices",2505.24334v1,bergmann2019mvtec,\cite{bergmann2019mvtec},Conference on Computer Vision and Pattern Recognition (CVPR),,,True,False,"Bergmann, Paul and Fauser, Michael and Sattlegger, David and Steger, Carsten",2019.0,,,10.1007/978-3-031-20056-4_23,,Conference on Computer Vision and Pattern Recognition (CVPR),CVPR 2025 - Computer Vision and Pattern Recognition Conference,https://cvpr.thecvf.com/Conferences/2025,Main Navigation CVPR 2025 - Computer Vision and Pattern Recognition Conference The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) is the premier annual computer vision event comprising the main conference and several co-located workshops and short courses. Important Dates All dates | Timezone: |  Timezone: Russian Invasion of Ukraine CVPR condemns in the strongest possible terms the actions of the Russian Federation government in invading the sovereign state of Ukraine and engaging in war against the Ukrainian people. We express our solidarity and support for the people of Ukraine and for all those who have been adversely affected by this war. The CVPR Logo above may be used on presentations. | IEEE Computer Society | The Computer Vision Foundation
"KairosAD: A SAM-Based Model for Industrial Anomaly Detection on Embedded
  Devices",2505.24334v1,bergmann2018improving,\cite{bergmann2018improving},"Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications",,,True,False,"Bergmann, Paul and Löwe, Sindy and Fauser, Michael and Sattlegger, David and Steger, Carsten",2019.0,,,,,"Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications",Proceedings of the 14th International Joint Conference on Computer ...,https://www.sciencegate.app/source/477396760,"Find the latest published papers in Proceedings of the 14th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications + Top authors, related hot topics, the most cited papers, and related journals"
"KairosAD: A SAM-Based Model for Industrial Anomaly Detection on Embedded
  Devices",2505.24334v1,roth2022towards,\cite{roth2022towards},Conference on Computer Vision and Pattern Recognition (CVPR),,,True,False,"Roth, Karsten and Pemula, Latha and Zepeda, Joaquin and Scholkopf, Bernhard and Brox, Thomas and Gehler, Peter",2022.0,,,10.1109/cvpr52688.2022.00951,,Conference on Computer Vision and Pattern Recognition (CVPR),CVPR 2025 - Computer Vision and Pattern Recognition Conference,https://cvpr.thecvf.com/Conferences/2025,Main Navigation CVPR 2025 - Computer Vision and Pattern Recognition Conference The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR) is the premier annual computer vision event comprising the main conference and several co-located workshops and short courses. Important Dates All dates | Timezone: |  Timezone: Russian Invasion of Ukraine CVPR condemns in the strongest possible terms the actions of the Russian Federation government in invading the sovereign state of Ukraine and engaging in war against the Ukrainian people. We express our solidarity and support for the people of Ukraine and for all those who have been adversely affected by this war. The CVPR Logo above may be used on presentations. | IEEE Computer Society | The Computer Vision Foundation
"Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven
  Facial Animation",2505.23290v1,TaylorKYMKRHM17,\cite{TaylorKYMKRHM17},A deep learning approach for generalized speech animation,,,True,False,"Sarah L. Taylor and
                  Taehwan Kim and
                  Yisong Yue and
                  Moshe Mahler and
                  James Krahe and
                  Anastasio Garcia Rodriguez and
                  Jessica K. Hodgins and
                  Iain A. Matthews",2017.0,,,,TOG,A deep learning approach for generalized speech animation,A deep learning approach for generalized speech animation,https://dl.acm.org/doi/10.1145/3072959.3073699,"Our deep learning approach enjoys several attractive properties: it runs in real-time, requires minimal parameter tuning, generalizes well to novel input speech sequences, is easily edited to create stylized and emotional speech, and is compatible with existing animation retargeting approaches."
"Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven
  Facial Animation",2505.23290v1,li2023mask,\cite{li2023mask},Mask-fpan: Semi-supervised face parsing in the wild with de-occlusion and uv gan,,,True,False,"Li, Lei and Zhang, Tianfang and Kang, Zhongfeng and Jiang, Xikun",2023.0,,,,Computers \& Graphics,Mask-fpan: Semi-supervised face parsing in the wild with de-occlusion and uv gan,PDF,https://scispace.com/pdf/mask-fpan-semi-supervised-face-parsing-in-the-wild-with-de-56ywgch2tq.pdf,"called Mask-FPAN. Our framework includes a de-occlusion module that learns to parse occluded faces in a semi-supervised manner, taking into account face landmark localization, face occlusionstimations, and detected head poses. Additionally, we improve the robustness of 2D face parsing by combining a 3D morphable face model with the UV GAN."
"Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven
  Facial Animation",2505.23290v1,LG-LDM,\cite{LG-LDM},Expressive 3D Facial Animation Generation Based on Local-to-global Latent Diffusion,,,True,False,"Song, Wenfeng and Wang, Xuan and Jiang, Yiming and Li, Shuai and Hao, Aimin and Hou, Xia and Qin, Hong",2024.0,,,,TVCG,Expressive 3D Facial Animation Generation Based on Local-to-global Latent Diffusion,Wenfeng Song - Homepage,https://buaaswf.github.io/,"Expressive 3D Facial Animation Generation Based on Local-to-global Latent Diffusion. Wenfeng Song, Xuan Wang, Yiming Jiang, Shuai Li*, Aimin Hao, Xia Hou, Hong Qin. IEEE Transactions on Visualization and Computer Graphics (TVCG), 2024, 30(11): 7397-7407. Github"
"Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven
  Facial Animation",2505.23290v1,DBLP:conf/bmvc/ChenLLYW21,\cite{DBLP:conf/bmvc/ChenLLYW21},"Talking Head Generation with Audio and Speech Related Facial Action
                  Units",,,True,False,"Sen Chen and
                  Zhilei Liu and
                  Jiaxing Liu and
                  Zhengxiang Yan and
                  Longbiao Wang",2021.0,,,,,"Talking Head Generation with Audio and Speech Related Facial Action
                  Units",Talking Head Generation with Audio and Speech Related Facial Action Units,https://ar5iv.labs.arxiv.org/html/2110.09951,"In this paper, we propose a novel talking head generation system, which uses both audio and speech-related facial action units (AUs) as driving information. The proposed Audio-to-AU module is used to obtain speech-related AU information. AU classifier is used to supervise that the generated frames contain correct AU information."
"Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven
  Facial Animation",2505.23290v1,ElizaldeZR19,\cite{ElizaldeZR19},"Cross Modal Audio Search and Retrieval with Joint Embeddings Based
                  on Text and Audio",,,True,False,"Benjamin Elizalde and
                  Shuayb Zarar and
                  Bhiksha Raj",2019.0,,,,,"Cross Modal Audio Search and Retrieval with Joint Embeddings Based
                  on Text and Audio",Cross Modal Audio Search and Retrieval with Joint Embeddings Based on ...,https://ieeexplore.ieee.org/document/8682632,"Existing audio search engines use one of two approaches: matching text-text or audio-audio pairs. In the former, text queries are matched to semantically similar words in an index of audio metadata to retrieve corresponding audio clips or segments, while in the latter, audio signals are directly used to retrieve acoustically-similar recordings from an audio database. However, independent"
"Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven
  Facial Animation",2505.23290v1,Yu0L19,\cite{Yu0L19},"Mining Audio, Text and Visual Information for Talking Face Generation",,,True,False,"Lingyun Yu and
                  Jun Yu and
                  Qiang Ling",2019.0,,,,,"Mining Audio, Text and Visual Information for Talking Face Generation","Mining Audio, Text and Visual Information for Talking Face Generation ...",https://ieeexplore.ieee.org/document/8970886,"Providing methods to support audio-visual interaction with growing volumes of video data is an increasingly important challenge for data mining. To this end, there has been some success in speech-driven lip motion generation or talking face generation. Among them, talking face generation aims to generate realistic talking heads synchronized with the audio or text input. This task requires"
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,wang2023full,\cite{wang2023full},Full-resolution and full-dynamic-range coded aperture compressive temporal imaging,,,True,False,"Wang, Ping and Wang, Lishun and Qiao, Mu and Yuan, Xin",2023.0,,,,Optics Letters,Full-resolution and full-dynamic-range coded aperture compressive temporal imaging,Full-resolution and full-dynamic-range coded aperture compressive ...,https://opg.optica.org/ol/abstract.cfm?doi=10.1364/OL.499735,"Coded aperture compressive temporal imaging (CACTI) aims to capture a sequence of video frames in a single shot, using an off-the-shelf 2D sensor. This approach effectively increases the frame rate of the sensor while reducing data throughput requirements. However, previous CACTI systems have encountered challenges such as limited spatial resolution and a narrow dynamic range, primarily"
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,figueiredo2007gradient,\cite{figueiredo2007gradient},Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems,,,True,False,"Figueiredo, M{\'a}rio AT and Nowak, Robert D and Wright, Stephen J",2007.0,,,,IEEE Journal of Selected Topics in Signal Processing,Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems,Gradient Projection for Sparse Reconstruction: Application to ...,https://xplorestaging.ieee.org/document/4407762/,"Many problems in signal processing and statistical inference involve finding sparse solutions to under-determined, or ill-conditioned, ... Gradient Projection for Sparse Reconstruction: Application to Compressed Sensing and Other Inverse Problems"
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,4587391,\cite{4587391},Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),,,True,False,"Shiqian Ma and Wotao Yin and Yin Zhang and Chakraborty, Amit",2008.0,,,,,Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),CVPR 2024 Open Access Repository,https://openaccess.thecvf.com/CVPR2024?day=all,"@InProceedings{Zhou_2024_CVPR, author = {Zhou, Jiazhou and Zheng, Xu and Lyu, Yuanhuiyi and Wang, Lin}, title = {ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month"
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,he2009exploiting,\cite{he2009exploiting},Exploiting structure in wavelet-based Bayesian compressive sensing,,,True,False,"He, Lihan and Carin, Lawrence",2009.0,,,,IEEE Transactions on Signal Processing,Exploiting structure in wavelet-based Bayesian compressive sensing,Exploiting Structure in Wavelet-Based Bayesian Compressive Sensing ...,https://ieeexplore.ieee.org/document/4907073,"Bayesian compressive sensing (CS) is considered for signals and images that are sparse in a wavelet basis. The statistical structure of the wavelet coefficients is exploited explicitly in the proposed model, and, therefore, this framework goes beyond simply assuming that the data are compressible in a wavelet basis. The structure exploited within the wavelet coefficients is consistent with"
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,beck2009fast,\cite{beck2009fast},A fast iterative shrinkage-thresholding algorithm for linear inverse problems,,,True,False,"Beck, Amir and Teboulle, Marc",2009.0,,,,SIAM Journal on Imaging Sciences,A fast iterative shrinkage-thresholding algorithm for linear inverse problems,A fast Iterative Shrinkage-Thresholding Algorithm with application to ...,https://ieeexplore.ieee.org/document/4959678,"Abstract: We consider the class of Iterative Shrinkage-Thresholding Algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods is attractive due to its simplicity, however, they are also known to converge quite slowly. In this paper we present a Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) which preserves the computational"
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,kim2010compressed,\cite{kim2010compressed},Compressed sensing using a Gaussian scale mixtures model in wavelet domain,,,True,False,"Kim, Yookyung and Nadar, Mariappan S and Bilgin, Ali",2010.0,,,,,Compressed sensing using a Gaussian scale mixtures model in wavelet domain,Compressed sensing using a gaussian scale mixtures model in wavelet ...,https://experts.arizona.edu/en/publications/compressed-sensing-using-a-gaussian-scale-mixtures-model-in-wavel,"T1 - Compressed sensing using a gaussian scale mixtures model in wavelet domain. AU - Kim, Yookyung. AU - Nadar, Mariappan S. AU - Bilgin, Ali. PY - 2010. Y1 - 2010. N2 - Compressed Sensing (CS) theory has gained attention recently as an alternative to the current paradigm of sampling followed by compression."
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,yang2011alternating,\cite{yang2011alternating},Alternating Direction Algorithms for {$\ell_{1}$}-Problems in Compressive Sensing,,,True,False,"Yang, Junfeng and Zhang, Yin",2011.0,,,,SIAM Journal on Scientific Computing,Alternating Direction Algorithms for {$\ell_{1}$}-Problems in Compressive Sensing,Alternating Direction Algorithms for $\\ell_1$-Problems in Compressive ...,https://arxiv.org/abs/0912.1185,"In this paper, we propose and study the use of alternating direction algorithms for several $\\ell_1$-norm minimization problems arising from sparse solution recovery in compressive sensing, including the basis pursuit problem, the basis-pursuit denoising problems of both unconstrained and constrained forms, as well as others. We present and investigate two classes of algorithms derived from"
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,dong2014compressive,\cite{dong2014compressive},Compressive sensing via nonlocal low-rank regularization,,,True,False,"Dong, Weisheng and Shi, Guangming and Li, Xin and Ma, Yi and Huang, Feng",2014.0,,,,IEEE Transactions on Image Processing,Compressive sensing via nonlocal low-rank regularization,Compressive Sensing via Nonlocal Low-Rank Regularization,https://ieeexplore.ieee.org/abstract/document/6827224,"Sparsity has been widely exploited for exact reconstruction of a signal from a small number of random measurements. Recent advances have suggested that structured or group sparsity often leads to more powerful signal reconstruction techniques in various compressed sensing (CS) studies. In this paper, we propose a nonlocal low-rank regularization (NLR) approach toward exploiting structured"
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,zhang2021plug,\cite{zhang2021plug},Plug-and-play image restoration with deep denoiser prior,,,True,False,"Zhang, Kai and Li, Yawei and Zuo, Wangmeng and Zhang, Lei and Van Gool, Luc and Timofte, Radu",2021.0,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence,Plug-and-play image restoration with deep denoiser prior,Plug-and-Play Image Restoration with Deep Denoiser Prior,https://arxiv.org/pdf/2008.13751,"A novel method to solve various image restoration problems by plugging a deep CNN denoiser as a prior into a half quadratic splitting algorithm. The method combines the flexibility of model-based methods and the effectiveness of learning-based methods, and outperforms state-of-the-art methods on deblurring, super-resolution and demosaicing tasks."
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,hurault2022proximal,\cite{hurault2022proximal},Proximal denoiser for convergent plug-and-play optimization with nonconvex regularization,,,True,False,"Hurault, Samuel and Leclaire, Arthur and Papadakis, Nicolas",2022.0,,,,,Proximal denoiser for convergent plug-and-play optimization with nonconvex regularization,GitHub - samuro95/Prox-PnP,https://github.com/samuro95/Prox-PnP,"Prox-PnP is a Python implementation of the proximal denoiser for plug-and-play optimization with nonconvex regularization, presented at ICML 2022. It includes code for denoising, deblurring and super-resolution tasks, as well as pretrained checkpoints and datasets."
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,shi2019image,\cite{shi2019image},Image compressed sensing using convolutional neural network,,,True,False,"Shi, Wuzhen and Jiang, Feng and Liu, Shaohui and Zhao, Debin",2019.0,,,,IEEE Transactions on Image Processing,Image compressed sensing using convolutional neural network,Image Compressed Sensing Using Convolutional Neural Network,https://ieeexplore.ieee.org/document/8765626,"A paper that proposes an image CS framework using convolutional neural network (CSNet) that includes a sampling network and a reconstruction network, which are optimized jointly. The paper claims that CSNet offers state-of-the-art reconstruction quality, while achieving fast running speed and using learned sampling matrices."
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,shi2019scalable,\cite{shi2019scalable},Scalable convolutional neural network for image compressed sensing,,,True,False,"Shi, Wuzhen and Jiang, Feng and Liu, Shaohui and Zhao, Debin",2019.0,,,,,Scalable convolutional neural network for image compressed sensing,GitHub - WenxueCui/Deep-Compressed-Sensing: Deep Learning/Deep neural ...,https://github.com/WenxueCui/Deep-Compressed-Sensing,"Scalable Compressed Sensing Network (SCSNet) [Matconvnet] W. Shi et al., Scalable Convolutional Neural Network for Image Compressed Sensing, CVPR 2019. DoC-DCS [MatcovnNet] T. N. Canh and B. Jeon, ""Difference of Convolution for Deep Compressive Sensing,"" IEEE International Conference on Imave Processing (ICIP), 2019."
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,yao2019dr2,\cite{yao2019dr2},Dr2-net: Deep residual reconstruction network for image compressive sensing,,,True,False,"Yao, Hantao and Dai, Feng and Zhang, Shiliang and Zhang, Yongdong and Tian, Qi and Xu, Changsheng",2019.0,,,,Neurocomputing,Dr2-net: Deep residual reconstruction network for image compressive sensing,DR2-Net: Deep Residual Reconstruction Network for image compressive sensing,https://www.sciencedirect.com/science/article/pii/S0925231219306162,"This paper proposes a Deep Residual Reconstruction Network (DR 2-Net), which further boosts the reconstruction quality from Compressively Sensed (CS) measurements with fast speed.The DR 2-Net takes the CS measurement of an image patch as input and consists of two components: linear mapping network and residual network.The linear mapping network is first used to obtain a preliminary"
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,metzler2017learned,\cite{metzler2017learned},Advances in Neural Information Processing Systems (NeurIPS),,,True,False,"Metzler, Chris and Mousavi, Ali and Baraniuk, Richard",2017.0,,,,,Advances in Neural Information Processing Systems (NeurIPS),NeurIPS - List of Proceedings,https://proceedings.neurips.cc/,"Advances in Neural Information Processing Systems Datasets and Benchmarks, 2021. Advances in Neural Information Processing Systems 37 (NeurIPS 2024) Advances in Neural Information Processing Systems 36 (NeurIPS 2023) Advances in Neural Information Processing Systems 35 (NeurIPS 2022) Advances in Neural Information Processing Systems 34 (NeurIPS"
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,zhang2018ista,\cite{zhang2018ista},ISTA-Net: Interpretable optimization-inspired deep network for image compressive sensing,,,True,False,"Zhang, Jian and Ghanem, Bernard",2018.0,,,,,ISTA-Net: Interpretable optimization-inspired deep network for image compressive sensing,GitHub - jianzhangcs/ISTA-Net: ISTA-Net: Interpretable Optimization ...,https://github.com/jianzhangcs/ISTA-Net,"ISTA-Net is a novel structured deep network for image compressive sensing, inspired by the Iterative Shrinkage-Thresholding Algorithm. The GitHub repository provides the paper, training data, and PyTorch version of the code, tested on Windows and Linux environments."
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,yang2018admm,\cite{yang2018admm},ADMM-CSNet: A deep learning approach for image compressive sensing,,,True,False,"Yang, Yan and Sun, Jian and Li, Huibin and Xu, Zongben",2018.0,,,,IEEE Transactions on Pattern Analysis and Machine Intelligence,ADMM-CSNet: A deep learning approach for image compressive sensing,ADMM-CSNet: A Deep Learning Approach for Image Compressive Sensing,https://ieeexplore.ieee.org/document/8550778,"This paper proposes a novel deep learning architecture, ADMM-CSNet, for image reconstruction from sparsely sampled measurements. It combines the traditional model-based CS method and the ADMM algorithm, and achieves fast and accurate results for complex-valued and real-valued images."
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,zhang2020optimization,\cite{zhang2020optimization},Optimization-inspired compact deep compressive sensing,,,True,False,"Zhang, Jian and Zhao, Chen and Gao, Wen",2020.0,,,,IEEE Journal of Selected Topics in Signal Processing,Optimization-inspired compact deep compressive sensing,Optimization-Inspired Compact Deep Compressive Sensing,https://ieeexplore.ieee.org/document/9019857/citations?tabFilter=papers,"Optimization-Inspired Compact Deep Compressive Sensing Abstract: In order to improve CS performance of natural images, in this paper, we propose a novel framework to design an OPtimization-INspired Explicable deep Network, dubbed OPINENet, for adaptive sampling and recovery. Both orthogonal and binary constraints of sampling matrix are"
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,zhang2020amp,\cite{zhang2020amp},AMP-Net: Denoising-based deep unfolding for compressive image sensing,,,True,False,"Zhang, Zhonghao and Liu, Yipeng and Liu, Jiani and Wen, Fei and Zhu, Ce",2020.0,,,,IEEE Transactions on Image Processing,AMP-Net: Denoising-based deep unfolding for compressive image sensing,AMP-Net: Denoising-Based Deep Unfolding for Compressive Image Sensing,https://pubmed.ncbi.nlm.nih.gov/33338019/,"Most compressive sensing (CS) reconstruction methods can be divided into two categories, i.e. model-based methods and classical deep network methods. ... AMP-Net: Denoising-Based Deep Unfolding for Compressive Image Sensing IEEE Trans Image Process. 2021:30:1487-1500. doi: 10.1109/TIP.2020.3044472. Epub 2020 Dec 31. Authors Zhonghao Zhang"
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,shen2022transcs,\cite{shen2022transcs},TransCS: a transformer-based hybrid architecture for image compressed sensing,,,True,False,"Shen, Minghe and Gan, Hongping and Ning, Chao and Hua, Yi and Zhang, Tao",2022.0,,,,IEEE Transactions on Image Processing,TransCS: a transformer-based hybrid architecture for image compressed sensing,TransCS: A Transformer-Based Hybrid Architecture for Image Compressed ...,https://ieeexplore.ieee.org/document/9934025,"Well-known compressed sensing (CS) is widely used in image acquisition and reconstruction. However, accurately reconstructing images from measurements at low sampling rates remains a considerable challenge. In this paper, we propose a novel Transformer-based hybrid architecture (dubbed TransCS) to achieve high-quality image CS. In the sampling module, TransCS adopts a trainable sensing matrix"
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,wang2023saunet,\cite{wang2023saunet},Saunet: Spatial-attention unfolding network for image compressive sensing,,,True,False,"Wang, Ping and Yuan, Xin",2023.0,,,,,Saunet: Spatial-attention unfolding network for image compressive sensing,"[ACM MM 2023] Source code and pre-trained models for ""SAUNet: Spatial ...",https://github.com/pwangcs/SAUNet,"@inproceedings{wang2023saunet, title={SAUNet: Spatial-Attention Unfolding Network for Image Compressive Sensing}, author={Wang, Ping and Yuan, Xin}, booktitle={Proceedings of the 31st ACM International Conference on Multimedia}, pages={5099--5108}, year={2023} }"
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,wang2024ufc,\cite{wang2024ufc},UFC-Net: Unrolling Fixed-point Continuous Network for Deep Compressive Sensing,,,True,False,"Wang, Xiaoyang and Gan, Hongping",2024.0,,,,,UFC-Net: Unrolling Fixed-point Continuous Network for Deep Compressive Sensing,PDF,https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_UFC-Net_Unrolling_Fixed-point_Continuous_Network_for_Deep_Compressive_Sensing_CVPR_2024_paper.pdf,"UFC-Net is a deep compressive sensing (CS) method that unrolls the fixed-point continuation algorithm into a neural network. It introduces Convolution-guided Attention Module (CAM) and other components to enhance feature extraction and reconstruction, and outperforms state-of-the-art methods on image CS and CS-MRI tasks."
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,guo2024cpp,\cite{guo2024cpp},CPP-Net: Embracing Multi-Scale Feature Fusion into Deep Unfolding CP-PPA Network for Compressive Sensing,,,True,False,"Guo, Zhen and Gan, Hongping",2024.0,,,,,CPP-Net: Embracing Multi-Scale Feature Fusion into Deep Unfolding CP-PPA Network for Compressive Sensing,PDF,https://openaccess.thecvf.com/content/CVPR2024/papers/Guo_CPP-Net_Embracing_Multi-Scale_Feature_Fusion_into_Deep_Unfolding_CP-PPA_Network_CVPR_2024_paper.pdf,"CPP-Net: Embracing Multi-Scale Feature Fusion into Deep Unfolding CP-PPA Network for Compressive Sensing Zhen Guo Northwestern Polytechnical University Xi'an 710072, China guozhen2022@mail.nwpu.edu.cn Hongping Gan* Northwestern Polytechnical University Xi'an 710072, China ganhongping@nwpu.edu.cn Abstract In the domain of compressive sensing"
"Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction
  Networks for Single-Pixel Imaging",2505.23180v1,geman1995nonlinear,\cite{geman1995nonlinear},Nonlinear image recovery with half-quadratic regularization,,,True,False,"Geman, Donald and Yang, Chengda",1995.0,,,,IEEE transactions on Image Processing,Nonlinear image recovery with half-quadratic regularization,Nonlinear image recovery with half-quadratic regularization,https://ieeexplore.ieee.org/document/392335,Nonlinear image recovery with half-quadratic regularization Abstract: One popular method for the recovery of an ideal intensity image from corrupted or indirect measurements is regularization: minimize an objective function that enforces a roughness penalty in addition to coherence with the data. Linear estimates are relatively easy to compute
"PS4PRO: Pixel-to-pixel Supervision for Photorealistic Rendering and
  Optimization",2505.22616v1,choi2007motion,\cite{choi2007motion},Motion-compensated frame interpolation using bilateral motion estimation and adaptive overlapped block motion compensation,,,True,False,"Choi, Byeong-Doo and Han, Jong-Woo and Kim, Chang-Su and Ko, Sung-Jea",2007.0,,,,IEEE Transactions on Circuits and Systems for Video Technology,Motion-compensated frame interpolation using bilateral motion estimation and adaptive overlapped block motion compensation,Motion-Compensated Frame Interpolation Using Bilateral Motion ...,https://ieeexplore.ieee.org/document/4162523,"In this work, we develop a new motion-compe (MC) interpolation algorithm to enhance the temporal resolution of video sequences. First, we propose the bilateral motion estimation scheme to obtain the motion field of an interpolated frame without yielding the hole and overlapping problems. Then, we partition a frame into several object regions by clustering motion vectors. We apply the variable"
"PS4PRO: Pixel-to-pixel Supervision for Photorealistic Rendering and
  Optimization",2505.22616v1,3dgsEh,\cite{3dgsEh},3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors,,,True,False,"Liu, Xi and Zhou, Chaoyi and Huang, Siyu",2024.0,,,,arXiv preprint arXiv:2410.16266,3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors,GitHub - xiliu8006/3DGS-Enhancer,https://github.com/xiliu8006/3DGS-Enhancer,"3DGS-Enhancer is a novel pipeline for enhancing 3D Gaussian splatting rendering quality with view-consistent 2D diffusion priors. The paper and code are available on GitHub, along with a dataset generator and a spatial-temporal decoder."
Universal Domain Adaptation for Semantic Segmentation,2505.22458v1,zhao2018psanet,\cite{zhao2018psanet},Psanet: Point-wise spatial attention network for scene parsing,,,True,False,"Zhao, Hengshuang and Zhang, Yi and Liu, Shu and Shi, Jianping and Loy, Chen Change and Lin, Dahua and Jia, Jiaya",2018.0,,,,,Psanet: Point-wise spatial attention network for scene parsing,PSANet: Point-wise Spatial Attention Network for Scene Parsing - Springer,https://link.springer.com/chapter/10.1007/978-3-030-01240-3_17,"PSANet is a convolutional neural network that uses point-wise spatial attention and bi-directional information flow to aggregate contextual information for scene parsing. The paper presents the network architecture, experimental results and analysis on three datasets: ADE20K, PASCAL VOC 2012 and Cityscapes."
Universal Domain Adaptation for Semantic Segmentation,2505.22458v1,hong2018conditional,\cite{hong2018conditional},Conditional generative adversarial network for structured domain adaptation,,,True,False,"Hong, Weixiang and Wang, Zhenzhen and Yang, Ming and Yuan, Junsong",2018.0,,,,,Conditional generative adversarial network for structured domain adaptation,Conditional Generative Adversarial Network for Structured Domain Adaptation,https://ieeexplore.ieee.org/document/8578243,"Conditional Generative Adversarial Network for Structured Domain Adaptation Abstract: ... the domain mismatch between real images and synthetic ones is the major challenge against harnessing the generated data and labels. In this paper, we propose a principled way to conduct structured domain adaption for semantic segmentation, i.e"
Universal Domain Adaptation for Semantic Segmentation,2505.22458v1,du2019ssf,\cite{du2019ssf},Ssf-dan: Separated semantic feature based domain adaptation network for semantic segmentation,,,True,False,"Du, Liang and Tan, Jingang and Yang, Hongye and Feng, Jianfeng and Xue, Xiangyang and Zheng, Qibao and Ye, Xiaoqing and Zhang, Xiaolin",2019.0,,,,,Ssf-dan: Separated semantic feature based domain adaptation network for semantic segmentation,SSF-DAN: Separated Semantic Feature Based Domain Adaptation Network for ...,https://ieeexplore.ieee.org/document/9010735,"In this work, we propose a Separated Semantic Feature based domain adaptation network, named SSF-DAN, for semantic segmentation. First, a Semantic-wise Separable Discriminator (SS-D) is designed to independently adapt semantic features across the target and source domains, which addresses the inconsistent adaptation issue in the class-wise"
Universal Domain Adaptation for Semantic Segmentation,2505.22458v1,zou2018unsupervised,\cite{zou2018unsupervised},Unsupervised domain adaptation for semantic segmentation via class-balanced self-training,,,True,False,"Zou, Yang and Yu, Zhiding and Kumar, BVK and Wang, Jinsong",2018.0,,,,,Unsupervised domain adaptation for semantic segmentation via class-balanced self-training,PDF,https://openaccess.thecvf.com/content_ECCV_2018/papers/Yang_Zou_Unsupervised_Domain_Adaptation_ECCV_2018_paper.pdf,This paper proposes a novel framework for unsupervised domain adaptation in semantic segmentation based on iterative self-training and class-balanced self-training. The method aims to reduce the domain gap between source and target data by minimizing a unified loss that combines feature alignment and task-specific loss.
Universal Domain Adaptation for Semantic Segmentation,2505.22458v1,chen2019domain,\cite{chen2019domain},Domain adaptation for semantic segmentation with maximum squares loss,,,True,False,"Chen, Minghao and Xue, Hongyang and Cai, Deng",2019.0,,,,,Domain adaptation for semantic segmentation with maximum squares loss,Domain Adaptation for Semantic Segmentation with Maximum Squares Loss,https://arxiv.org/abs/1909.13589,"Deep neural networks for semantic segmentation always require a large number of samples with pixel-level labels, which becomes the major difficulty in their real-world applications. To reduce the labeling cost, unsupervised domain adaptation (UDA) approaches are proposed to transfer knowledge from labeled synthesized datasets to unlabeled real-world datasets. Recently, some semi-supervised"
Universal Domain Adaptation for Semantic Segmentation,2505.22458v1,wang2021domain,\cite{wang2021domain},Domain adaptive semantic segmentation with self-supervised depth estimation,,,True,False,"Wang, Qin and Dai, Dengxin and Hoyer, Lukas and Van Gool, Luc and Fink, Olga",2021.0,,,,,Domain adaptive semantic segmentation with self-supervised depth estimation,PDF,https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Domain_Adaptive_Semantic_Segmentation_With_Self-Supervised_Depth_Estimation_ICCV_2021_paper.pdf,"Domain Adaptive Semantic Segmentation with Self-Supervised Depth Estimation Qin Wang1 Dengxin Dai1,2* Lukas Hoyer1 Luc Van Gool1,3 Olga Fink1 1ETH Zurich, Switzerland 2MPI for Informatics, Germany 3KU Lueven, Belgium {qwang,lhoyer,ofink}@ethz.ch {dai,vangool}@vision.ee.ethz.ch Abstract Domain adaptation for semantic segmentation aims to improve the model performance in the presence of a distri-bution shift between source and target domain. We propose to use self-supervised depth estima-tion (green) to improve semantic segmentation performance under the unsupervised domain adaptation setup. The additional self-supervised depth estimation can fa-cilitate us to explicitly learn the correlation between tasks to 1 8515 improve the ﬁnal semantic segmentation performance. By exploit-ing the supervision from self-supervised depth estimation and learning the correlation between semantics and depth, the proposed method achieves 55.0% mIoU (stereo depth) on this task."
Universal Domain Adaptation for Semantic Segmentation,2505.22458v1,wang2021uncertainty,\cite{wang2021uncertainty},Uncertainty-aware pseudo label refinery for domain adaptive semantic segmentation,,,True,False,"Wang, Yuxi and Peng, Junran and Zhang, ZhaoXiang",2021.0,,,,,Uncertainty-aware pseudo label refinery for domain adaptive semantic segmentation,Uncertainty-aware Pseudo Label Refinery for Domain Adaptive Semantic ...,https://ieeexplore.ieee.org/document/9710267,"Unsupervised domain adaptation for semantic segmentation aims to assign the pixel-level labels for unlabeled target domain by transferring knowledge from the labeled source domain. A typical self-supervised learning approach generates pseudo labels from the source model and then re-trains the model to fit the target distribution. However, it suffers from noisy pseudo labels due to the"
Universal Domain Adaptation for Semantic Segmentation,2505.22458v1,fu2020learning,\cite{fu2020learning},Learning to detect open classes for universal domain adaptation,,,True,False,"Fu, Bo and Cao, Zhangjie and Long, Mingsheng and Wang, Jianmin",2020.0,,,,,Learning to detect open classes for universal domain adaptation,Learning to Detect Open Classes for Universal Domain Adaptation - Springer,https://link.springer.com/chapter/10.1007/978-3-030-58555-6_34,"The latter two shed light on practical domain adaptation. Universal Domain Adaptation (UniDA) is the most general setting of domain adaptation, which removes all constraints and includes all the previous adaptation settings. It introduces new challenges to detect open classes in target data even with private classes in the source domain."
RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network,2505.22427v1,sugimoto2004obstacle,\cite{sugimoto2004obstacle},Obstacle detection using millimeter-wave radar and its visualization on image sequence,,,True,False,"Sugimoto, Shigeki and Tateda, Hayato and Takahashi, Hidekazu and Okutomi, Masatoshi",2004.0,,,,,Obstacle detection using millimeter-wave radar and its visualization on image sequence,Obstacle detection using millimeter-wave radar and its visualization on ...,https://ieeexplore.ieee.org/document/1334537,"Sensor fusion of millimeter-wave radar and a camera is beneficial for advanced driver assistance functions such as obstacle avoidance and stop&go. However, millimeter-wave radar has low directional resolution, which engenders low measurement accuracy of object position and difficulty of calibration between radar and camera. In this paper, we first propose a calibration method between"
RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network,2505.22427v1,wang2011integrating,\cite{wang2011integrating},Integrating millimeter wave radar with a monocular vision sensor for on-road obstacle detection applications,,,True,False,"Wang, Tao and Zheng, Nanning and Xin, Jingmin and Ma, Zheng",2011.0,,,,Sensors,Integrating millimeter wave radar with a monocular vision sensor for on-road obstacle detection applications,Integrating millimeter wave radar with a monocular vision sensor for on ...,https://pubmed.ncbi.nlm.nih.gov/22164117/,"This paper presents a systematic scheme for fusing millimeter wave (MMW) radar and a monocular vision sensor for on-road obstacle detection. As a whole, a three-level fusion strategy based on visual attention mechanism and driver's visual consciousness is provided for MMW radar and monocular vision fusion so as to obtain better comprehensive performance."
RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network,2505.22427v1,kim2014data,\cite{kim2014data},Data fusion of radar and image measurements for multi-object tracking via Kalman filtering,,,True,False,"Kim, Du Yong and Jeon, Moongu",2014.0,,,,Information Sciences,Data fusion of radar and image measurements for multi-object tracking via Kalman filtering,Data fusion of radar and image measurements for multi-object tracking ...,https://www.sciencedirect.com/science/article/pii/S0020025514003715,To achieve the multi-object tracking we combine the proposed data fusion method with the integrated probability data association (IPDA) technique underlying the multiple-Kalman filter framework. The proposed complementary system based on the radar and CCD camera is experimentally evaluated through a multi-person tracking scenario.
RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network,2505.22427v1,kim2018radar,\cite{kim2018radar},Radar and vision sensor fusion for object detection in autonomous vehicle surroundings,,,True,False,"Kim, Jihun and Han, Dong Seog and Senouci, Benaoumeur",2018.0,,,,,Radar and vision sensor fusion for object detection in autonomous vehicle surroundings,MmWave Radar and Vision Fusion for Object Detection in Autonomous ...,https://www.mdpi.com/1424-8220/22/7/2542,"With autonomous driving developing in a booming stage, accurate object detection in complex scenarios attract wide attention to ensure the safety of autonomous driving. Millimeter wave (mmWave) radar and vision fusion is a mainstream solution for accurate obstacle detection. This article presents a detailed survey on mmWave radar and vision fusion based obstacle detection methods. First, we"
RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network,2505.22427v1,kim2017comparative,\cite{kim2017comparative},Comparative analysis of RADAR-IR sensor fusion methods for object detection,,,True,False,"Kim, Taehwan and Kim, Sungho and Lee, Eunryung and Park, Miryong",2017.0,,,,,Comparative analysis of RADAR-IR sensor fusion methods for object detection,A Comparative Analysis of Camera-Radar Fusion Models and ... - Springer,https://link.springer.com/chapter/10.1007/978-981-97-8605-3_15,"Diverse methods have been investigated, employing distinct sensor modalities and algorithms to accomplish object detection through diverse sensor integration in automobiles. For several decades, research on perception systems has been ongoing, with a major emphasis on creating reliable and efficient implementations for applications involving"
RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network,2505.22427v1,el2015radar,\cite{el2015radar},Radar and vision sensors calibration for outdoor 3D reconstruction,,,True,False,"El Natour, Ghina and Aider, Omar Ait and Rouveure, Raphael and Berry, Fran{\c{c}}ois and Faure, Patrice",2015.0,,,,,Radar and vision sensors calibration for outdoor 3D reconstruction,Radar and vision sensors calibration for outdoor 3D reconstruction ...,https://ieeexplore.ieee.org/abstract/document/7139473,"In this paper we introduce a new geometric calibration algorithm, and a geometric method of 3D reconstruction using a panoramic microwave radar and a camera. These two sensors are complementary, considering the robustness to environmental conditions and depth detection ability of the radar on one hand, and the high spatial resolution of a vision sensor on the other hand. This makes the"
RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network,2505.22427v1,li2023automatic,\cite{li2023automatic},Automatic targetless LiDAR--camera calibration: a survey,,,True,False,"Li, Xingchen and Xiao, Yuxuan and Wang, Beibei and Ren, Haojie and Zhang, Yanyong and Ji, Jianmin",2023.0,,,,Artificial Intelligence Review,Automatic targetless LiDAR--camera calibration: a survey,Automatic Targetless LiDAR-Camera Calibration: A Survey,https://www.researchgate.net/publication/363244175_Automatic_Targetless_LiDAR-Camera_Calibration_A_Survey,"This paper presents a thorough review of these automatic targetless LiDAR-camera calibration methods. Specifically, based on how the potential cues in the environment are retrieved and utilized in"
RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network,2505.22427v1,pandey2012automatic,\cite{pandey2012automatic},Automatic targetless extrinsic calibration of a 3d lidar and camera by maximizing mutual information,,,True,False,"Pandey, Gaurav and McBride, James and Savarese, Silvio and Eustice, Ryan",2012.0,,,,,Automatic targetless extrinsic calibration of a 3d lidar and camera by maximizing mutual information,Automatic Targetless Lidar Camera Calibration - GitHub,https://github.com/xmba15/automatic_lidar_camera_calibration,Auto-calibration of lidar and camera based on maximization of intensity mutual information. This is the reimplementation of the paper: Automatic Targetless Extrinsic Calibration of a 3D Lidar and Camera by Maximizing Mutual Information
RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network,2505.22427v1,taylor2015motion,\cite{taylor2015motion},Motion-based calibration of multimodal sensor arrays,,,True,False,"Taylor, Zachary and Nieto, Juan",2015.0,,,,,Motion-based calibration of multimodal sensor arrays,Motion-based calibration of multimodal sensor arrays,https://ieeexplore.ieee.org/document/7139872,"This paper formulates a new pipeline for automated extrinsic calibration of multi-sensor mobile platforms. The new method can operate on any combination of cameras, navigation sensors and 3D lidars. Current methods for extrinsic calibration are either based on special markers and/or chequerboards, or they require a precise parameters initialisation for the calibration to converge. These two"
RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network,2505.22427v1,levinson2013automatic,\cite{levinson2013automatic},Automatic online calibration of cameras and lasers.,,,True,False,"Levinson, Jesse and Thrun, Sebastian",2013.0,,,,,Automatic online calibration of cameras and lasers.,Automatic Online Calibration of Cameras and Lasers - ResearchGate,https://www.researchgate.net/publication/305298924_Automatic_Online_Calibration_of_Cameras_and_Lasers,"Download Citation | On Jun 23, 2013, Jesse Levinson and others published Automatic Online Calibration of Cameras and Lasers | Find, read and cite all the research you need on ResearchGate"
RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network,2505.22427v1,shi2020calibrcnn,\cite{shi2020calibrcnn},Calibrcnn: Calibrating camera and lidar by recurrent convolutional neural network and geometric constraints,,,True,False,"Shi, Jieying and Zhu, Ziheng and Zhang, Jianhua and Liu, Ruyu and Wang, Zhenhua and Chen, Shengyong and Liu, Honghai",2020.0,,,,,Calibrcnn: Calibrating camera and lidar by recurrent convolutional neural network and geometric constraints,CalibRCNN: Calibrating Camera and LiDAR by Recurrent Convolutional ...,https://ieeexplore.ieee.org/document/9341147,"In this paper, we present Calibration Recurrent Convolutional Neural Network (CalibRCNN) to infer a 6 degrees of freedom (DOF) rigid body transformation between 3D LiDAR and 2D camera. Different from the existing methods, our 3D-2D CalibRCNN not only uses the LSTM network to extract the temporal features between 3D point clouds and RGB images of consecutive frames, but also uses the geometric"
RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network,2505.22427v1,sak2014long,\cite{sak2014long},Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition,,,True,False,"Sak, Ha{\c{s}}im and Senior, Andrew and Beaufays, Fran{\c{c}}oise",2014.0,,,,arXiv preprint arXiv:1402.1128,Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition,Long Short-Term Memory Based Recurrent Neural Network Architectures for ...,https://arxiv.org/abs/1402.1128,"Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting"
RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network,2505.22427v1,pervsic2021online,\cite{pervsic2021online},Online multi-sensor calibration based on moving object tracking,,,True,False,"Per{\v{s}}i{\'c}, Juraj and Petrovi{\'c}, Luka and Markovi{\'c}, Ivan and Petrovi{\'c}, Ivan",2021.0,,,,Advanced Robotics,Online multi-sensor calibration based on moving object tracking,Online multi-sensor calibration based on moving object tracking,https://www.tandfonline.com/doi/full/10.1080/01691864.2020.1819874,"Contrary to currently common online approach of using ego-motion estimation, we propose an online calibration method based on detection and tracking of moving objects. Our motivation comes from the practical perspective that many perception sensors of an autonomous system are part of the pipeline for detection and tracking of moving objects."
"Q-VDiT: Towards Accurate Quantization and Distillation of
  Video-Generation Diffusion Transformers",2505.22167v1,rombach2022ldm,\cite{rombach2022ldm},High-resolution image synthesis with latent diffusion models,,,True,False,"Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\""o}rn",2022.0,,,,,High-resolution image synthesis with latent diffusion models,High-Resolution Image Synthesis With Latent Diffusion Models,https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html,"The paper presents a novel approach to use diffusion models in the latent space of pretrained autoencoders for high-resolution image synthesis. It shows that this method achieves state-of-the-art results on various tasks, such as inpainting, text-to-image, and super-resolution, while reducing computational costs."
"Q-VDiT: Towards Accurate Quantization and Distillation of
  Video-Generation Diffusion Transformers",2505.22167v1,li2024qdm,\cite{li2024qdm},Q-dm: An efficient low-bit quantized diffusion model,,,True,False,"Li, Yanjing and Xu, Sheng and Cao, Xianbin and Sun, Xiao and Zhang, Baochang",2024.0,,,,Advances in Neural Information Processing Systems,Q-dm: An efficient low-bit quantized diffusion model,Q-DM: An Efficient Low-bit Quantized Diffusion Model,https://openreview.net/forum?id=sFGkL5BsPi,"Denoising diffusion generative models are capable of generating high-quality data, but suffers from the computation-costly generation process, due to a iterative noise estimation using full-precision networks. As an intuitive solution, quantization can significantly reduce the computational and memory consumption by low-bit parameters and operations."
"Q-VDiT: Towards Accurate Quantization and Distillation of
  Video-Generation Diffusion Transformers",2505.22167v1,zheng2024binarydm,\cite{zheng2024binarydm},Binarydm: Towards accurate binarization of diffusion model,,,True,False,"Zheng, Xingyu and Qin, Haotong and Ma, Xudong and Zhang, Mingyuan and Hao, Haojie and Wang, Jiakai and Zhao, Zixiang and Guo, Jinyang and Liu, Xianglong",2024.0,,,,arXiv preprint arXiv:2404.05662,Binarydm: Towards accurate binarization of diffusion model,BinaryDM: Towards Accurate Binarization of Diffusion Model,https://openreview.net/forum?id=meohWhEgjA,"With 1.1-bit weight and 4-bit activation (W1.1A4), BinaryDM achieves as low as 7.11 FID and saves the performance from collapse (baseline FID 39.69). As the first binarization method for diffusion models, W1.1A4 BinaryDM achieves impressive 9.3 times OPs and 24.8 times model size savings, showcasing its substantial potential for edge deployment."
"Q-VDiT: Towards Accurate Quantization and Distillation of
  Video-Generation Diffusion Transformers",2505.22167v1,huang2024tfmq,\cite{huang2024tfmq},Tfmq-dm: Temporal feature maintenance quantization for diffusion models,,,True,False,"Huang, Yushi and Gong, Ruihao and Liu, Jing and Chen, Tianlong and Liu, Xianglong",2024.0,,,,,Tfmq-dm: Temporal feature maintenance quantization for diffusion models,TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models,https://github.com/ModelTC/TFMQ-DM,"This is the official implementation of our paper TFMQ-DM, a novel training-free framework that achieves a new state-of-the-art result in PTQ of diffusion models, especially under 4-bit weight quantization, and significantly accelerates quantization time. For some hard tasks, e.g., CelebA-HQ, our"
"Q-VDiT: Towards Accurate Quantization and Distillation of
  Video-Generation Diffusion Transformers",2505.22167v1,chen2024qdit,\cite{chen2024qdit},Q-dit: Accurate post-training quantization for diffusion transformers,,,True,False,"Chen, Lei and Meng, Yuan and Tang, Chen and Ma, Xinzhu and Jiang, Jingyan and Wang, Xin and Wang, Zhi and Zhu, Wenwu",2024.0,,,,arXiv preprint arXiv:2406.17343,Q-dit: Accurate post-training quantization for diffusion transformers,Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers,https://arxiv.org/abs/2406.17343,"Q-DiT is a post-training quantization framework that improves the quality and scalability of image and video generation by diffusion transformers. It adapts to the spatial and temporal variance of weights and activations across channels and timesteps, and achieves state-of-the-art results on ImageNet and VBench."
"Q-VDiT: Towards Accurate Quantization and Distillation of
  Video-Generation Diffusion Transformers",2505.22167v1,li2024svdqunat,\cite{li2024svdqunat},Svdqunat: Absorbing outliers by low-rank components for 4-bit diffusion models,,,True,False,"Li, Muyang and Lin, Yujun and Zhang, Zhekai and Cai, Tianle and Li, Xiuyu and Guo, Junxian and Xie, Enze and Meng, Chenlin and Zhu, Jun-Yan and Han, Song",2024.0,,,,arXiv preprint arXiv:2411.05007,Svdqunat: Absorbing outliers by low-rank components for 4-bit diffusion models,GitHub - mit-han-lab/nunchaku: [ICLR2025 Spotlight] SVDQuant: Absorbing ...,https://github.com/mit-han-lab/nunchaku,"SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models Muyang Li*, Yujun Lin*, Zhekai Zhang*, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han MIT, NVIDIA, CMU, Princeton, UC Berkeley, SJTU, and Pika Labs. demo.mov"
"Q-VDiT: Towards Accurate Quantization and Distillation of
  Video-Generation Diffusion Transformers",2505.22167v1,zhao2024vidit,\cite{zhao2024vidit},ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation,,,True,False,"Zhao, Tianchen and Fang, Tongcheng and Liu, Enshu and Rui, Wan and Soedarmadji, Widyadewi and Li, Shiyao and Lin, Zinan and Dai, Guohao and Yan, Shengen and Yang, Huazhong and others",2024.0,,,,arXiv preprint arXiv:2406.02540,ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation,[ICLR'25] ViDiT-Q: Efficient and Accurate Quantization of Diffusion ...,https://github.com/thu-nics/ViDiT-Q,"This repo contains the official code of our ICLR'25 paper: ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation. We introduce ViDiT-Q, a quantization method specialized for diffusion transformers. For popular large-scale models (e.g., open-sora, Latte"
"ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation
  with Lightweight Specialized LLM",2505.22552v1,faviq,\cite{faviq},{F}a{VIQ}: {FA}ct Verification from Information-seeking Questions,,,True,False,"Park, Jungsoo  and
      Min, Sewon  and
      Kang, Jaewoo  and
      Zettlemoyer, Luke  and
      Hajishirzi, Hannaneh",2022.0,,https://aclanthology.org/2022.acl-long.354/,10.18653/v1/2022.acl-long.354,,{F}a{VIQ}: {FA}ct Verification from Information-seeking Questions,FaVIQ,https://faviq.github.io/,"We introduce FaVIQ (Fact Verification from Information-seeking Questions), a challenging and realistic fact verification dataset that reflects confusions raised by real users. We use the ambiguity in information-seeking questions and their disambiguation, and automatically convert them to true and false claims."
"ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation
  with Lightweight Specialized LLM",2505.22552v1,rat-sql,\cite{rat-sql},{RAT-SQL}: Relation-Aware Schema Encoding and Linking for Text-to-{SQL} Parsers,,,True,False,"Wang, Bailin  and
      Shin, Richard  and
      Liu, Xiaodong  and
      Polozov, Oleksandr  and
      Richardson, Matthew",2020.0,,https://aclanthology.org/2020.acl-main.677/,10.18653/v1/2020.acl-main.677,,{RAT-SQL}: Relation-Aware Schema Encoding and Linking for Text-to-{SQL} Parsers,GitHub - microsoft/rat-sql: A relation-aware semantic parsing model ...,https://github.com/microsoft/rat-sql,"This repository contains code for the ACL 2020 paper ""RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers"". If you use RAT-SQL in your work, please cite it as follows: ... {rat-sql, title = "" {RAT-SQL}: Relation-Aware Schema Encoding and Linking for Text-to-{SQL} Parsers "", author = "" Wang, Bailin and Shin, Richard and"
"ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation
  with Lightweight Specialized LLM",2505.22552v1,kg_gpt,\cite{kg_gpt},"{KG-GPT:} {A} General Framework for Reasoning on Knowledge Graphs
                  Using Large Language Models",,,True,False,"Jiho Kim and
                  Yeonsu Kwon and
                  Yohan Jo and
                  Edward Choi",2023.0,,https://doi.org/10.18653/v1/2023.findings-emnlp.631,10.18653/V1/2023.FINDINGS-EMNLP.631,,"{KG-GPT:} {A} General Framework for Reasoning on Knowledge Graphs
                  Using Large Language Models",,,
"ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation
  with Lightweight Specialized LLM",2505.22552v1,struct-gpt,\cite{struct-gpt},{S}truct{GPT}: A General Framework for Large Language Model to Reason over Structured Data,,,True,False,"Jiang, Jinhao  and
      Zhou, Kun  and
      Dong, Zican  and
      Ye, Keming  and
      Zhao, Xin  and
      Wen, Ji-Rong",2023.0,,https://aclanthology.org/2023.emnlp-main.574/,10.18653/v1/2023.emnlp-main.574,,{S}truct{GPT}: A General Framework for Large Language Model to Reason over Structured Data,StructGPT: A General Framework for Large Language Model to Reason over ...,https://openreview.net/forum?id=R635gF7lXD,"In this paper, we aim to improve the reasoning ability of large language models (LLMs) over structured data in a unified way. Inspired by the studies on tool augmentation for LLMs, we develop an Iterative Reading-then-Reasoning (IRR) framework to solve question answering tasks based on structured data, called StructGPT. In this framework, we construct the specialized interfaces to collect"
"VDDP: Verifiable Distributed Differential Privacy under the
  Client-Server-Verifier Setup",2504.21752v1,dprio,\cite{dprio},DPrio: Efficient Differential Privacy with High Utility for Prio,,,True,False,"Dana Keeler and
                  Chelsea Komlo and
                  Emily Lepert and
                  Shannon Veitch and
                  Xi He",2023.0,,https://doi.org/10.56553/popets-2023-0086,10.56553/POPETS-2023-0086,Proc. Priv. Enhancing Technol.,DPrio: Efficient Differential Privacy with High Utility for Prio,DPrio: Efficient Differential Privacy with High Utility for Prio,https://www.semanticscholar.org/paper/DPrio:-Efficient-Differential-Privacy-with-High-for-Keeler-Komlo/ae1b2a4e5beaaa850183ad37e0880bb70ae34f4e/figure/0,This work presents a lightweight method that is called DPrio to augment Prio and related systems with differential privacy assurances while ensuring higher data utility than existing noise generation protocols.
"VDDP: Verifiable Distributed Differential Privacy under the
  Client-Server-Verifier Setup",2504.21752v1,DBLP:conf/iclr/ShamsabadiTCBHP24,\cite{DBLP:conf/iclr/ShamsabadiTCBHP24},"Confidential-DPproof: Confidential Proof of Differentially Private
                  Training",,,True,False,"Ali Shahin Shamsabadi and
                  Gefei Tan and
                  Tudor Cebere and
                  Aur{\'{e}}lien Bellet and
                  Hamed Haddadi and
                  Nicolas Papernot and
                  Xiao Wang and
                  Adrian Weller",2024.0,,https://openreview.net/forum?id=PQY2v6VtGe,,,"Confidential-DPproof: Confidential Proof of Differentially Private
                  Training",Conﬁdential-DPproof :CONFIDENTIAL PROOF OF DIFFERENTIALLY PRIVATETRAINING,https://par.nsf.gov/servlets/purl/10567174,"we design a customized zero-knowledge proof protocol tailored to the requirements introduced by differentially private training, including random noise addition and privacy ampliﬁcation by subsampling. In experiments on CIFAR-10, Conﬁdential-DPproof trainsamodelachievingstate-of-the-art91%testaccuracywithacertiﬁed"
"VDDP: Verifiable Distributed Differential Privacy under the
  Client-Server-Verifier Setup",2504.21752v1,BC23,\cite{BC23},Interactive Proofs For Differentially Private Counting,,,True,False,"Ari Biswas and
                  Graham Cormode",2023.0,,https://doi.org/10.1145/3576915.3616681,10.1145/3576915.3616681,,Interactive Proofs For Differentially Private Counting,Interactive Proofs For Differentially Private Counting,https://dl.acm.org/doi/10.1145/3576915.3616681,"Interactive Proofs For Differentially Private Counting. Authors: Ari Biswas, Graham Cormode Authors Info & Claims. ... Interactive Proofs For Differentially Private Counting. Security and privacy. Recommendations. A Differentially Private Encryption Scheme. Information Security . Abstract."
"VDDP: Verifiable Distributed Differential Privacy under the
  Client-Server-Verifier Setup",2504.21752v1,DBLP:conf/ccs/BellBGL020,\cite{DBLP:conf/ccs/BellBGL020},Secure Single-Server Aggregation with (Poly)Logarithmic Overhead,,,True,False,"James Henry Bell and
                  Kallista A. Bonawitz and
                  Adri{\`{a}} Gasc{\'{o}}n and
                  Tancr{\`{e}}de Lepoint and
                  Mariana Raykova",2020.0,,https://doi.org/10.1145/3372297.3417885,10.1145/3372297.3417885,,Secure Single-Server Aggregation with (Poly)Logarithmic Overhead,Secure Single-Server Aggregation with (Poly)Logarithmic Overhead ...,https://dl.acm.org/doi/10.1145/3372297.3417885,"James Bell, Keith Bonawitz, Adrià Gascó n, Tancrè de Lepoint, and Mariana Raykova. 2020. Secure Single-Server Aggregation with (Poly)Logarithmic Overhead. IACR Cryptol. ePrint Arch., Vol. 2020 (2020), 704. ... This paper introduces LERNA, a new framework for single-server secure aggregation. Our protocols are tailored to the setting where"
"VDDP: Verifiable Distributed Differential Privacy under the
  Client-Server-Verifier Setup",2504.21752v1,DBLP:conf/eurocrypt/DworkKMMN06,\cite{DBLP:conf/eurocrypt/DworkKMMN06},"Our Data, Ourselves: Privacy Via Distributed Noise Generation",,,True,False,"Cynthia Dwork and
                  Krishnaram Kenthapadi and
                  Frank McSherry and
                  Ilya Mironov and
                  Moni Naor",2006.0,,https://doi.org/10.1007/11761679\_29,10.1007/11761679\_29,,"Our Data, Ourselves: Privacy Via Distributed Noise Generation","Our Data, Ourselves: Privacy Via Distributed Noise Generation - Springer",https://link.springer.com/chapter/10.1007/11761679_29,"This paper presents efficient distributed protocols for generating shares of random noise, secure against malicious participants. The noise generation is used to create privacy-preserving statistical databases that perturb the true answer to a query by adding noise."
"VDDP: Verifiable Distributed Differential Privacy under the
  Client-Server-Verifier Setup",2504.21752v1,DBLP:conf/ccs/ChampionSU19,\cite{DBLP:conf/ccs/ChampionSU19},Securely Sampling Biased Coins with Applications to Differential Privacy,,,True,False,"Jeffrey Champion and
                  Abhi Shelat and
                  Jonathan R. Ullman",2019.0,,https://doi.org/10.1145/3319535.3354256,10.1145/3319535.3354256,,Securely Sampling Biased Coins with Applications to Differential Privacy,Secure Multiparty Sampling of a Biased Coin for Differential Privacy,https://link.springer.com/chapter/10.1007/978-3-031-54204-6_19,Sampling a biased coin is a key primitive in designing secure multiparty computation (MPC) for differentially private mechanisms. We explore privately sampling a biased coin from l unbiased coins and offer an unconditionally secure MPC protocol for this task that can be implemented using either \(7.5l - 4\) (when l is even) or \(7.5l - 1.5\) (when l is odd) multiplications in 7 rounds.
"VDDP: Verifiable Distributed Differential Privacy under the
  Client-Server-Verifier Setup",2504.21752v1,DBLP:conf/uss/BohlerK20,\cite{DBLP:conf/uss/BohlerK20},Secure Multi-party Computation of Differentially Private Median,,,True,False,"Jonas B{\""{o}}hler and
                  Florian Kerschbaum",2020.0,,https://www.usenix.org/conference/usenixsecurity20/presentation/boehler,,,Secure Multi-party Computation of Differentially Private Median,Secure Multi-party Computation of Differentially Private Median,https://www.usenix.org/conference/usenixsecurity20/presentation/boehler,"Existing solutions to compute the differentially private median provide good accuracy only for large amounts of users (local model), by using a trusted third party (central model), or for a very small data universe (secure multi-party computation). We present a multi-party computation to efficiently compute the exponential mechanism for the"
"VDDP: Verifiable Distributed Differential Privacy under the
  Client-Server-Verifier Setup",2504.21752v1,DBLP:conf/ccs/BohlerK21,\cite{DBLP:conf/ccs/BohlerK21},Secure Multi-party Computation of Differentially Private Heavy Hitters,,,True,False,"Jonas B{\""{o}}hler and
                  Florian Kerschbaum",2021.0,,https://doi.org/10.1145/3460120.3484557,10.1145/3460120.3484557,,Secure Multi-party Computation of Differentially Private Heavy Hitters,Secure Multi-party Computation of Differentially Private Heavy Hitters,https://www.researchgate.net/publication/356203263_Secure_Multi-party_Computation_of_Differentially_Private_Heavy_Hitters,Secure multi-party computation (MPC) [17] allows a group of parties to synchronously compute a function and obtain accurate representations of the final value while protecting their private data
"VDDP: Verifiable Distributed Differential Privacy under the
  Client-Server-Verifier Setup",2504.21752v1,DBLP:journals/corr/abs-2109-10074,\cite{DBLP:journals/corr/abs-2109-10074},"{STAR:} Distributed Secret Sharing for Private Threshold Aggregation
                  Reporting",,,True,False,"Alex Davidson and
                  Peter Snyder and
                  E. B. Quirk and
                  Joseph Genereux and
                  Benjamin Livshits",2021.0,,https://arxiv.org/abs/2109.10074,,CoRR,"{STAR:} Distributed Secret Sharing for Private Threshold Aggregation
                  Reporting",STAR: Distributed Secret Sharing for Private Threshold Aggregation ...,https://www.ietf.org/archive/id/draft-dss-star-02.html,STAR: Distributed Secret Sharing for Private Threshold Aggregation Reporting Abstract. Servers often need to collect data from clients that can be privacy-sensitive if the server is able to associate the collected data with a particular user.
"VDDP: Verifiable Distributed Differential Privacy under the
  Client-Server-Verifier Setup",2504.21752v1,DBLP:conf/ccs/WeiYFCW23,\cite{DBLP:conf/ccs/WeiYFCW23},"Securely Sampling Discrete Gaussian Noise for Multi-Party Differential
                  Privacy",,,True,False,"Chengkun Wei and
                  Ruijing Yu and
                  Yuan Fan and
                  Wenzhi Chen and
                  Tianhao Wang",2023.0,,https://doi.org/10.1145/3576915.3616641,10.1145/3576915.3616641,,"Securely Sampling Discrete Gaussian Noise for Multi-Party Differential
                  Privacy",Securely Sampling Discrete Gaussian Noise for Multi-Party Differential ...,https://dl.acm.org/doi/10.1145/3576915.3616641,"Our work presents the first MPC solution for sampling discrete Gaussian, a common type of noise used for constructing DP mechanisms, which plays nicely with malicious secure MPC protocols. Our solution is both generic, supporting various MPC protocols and any number of parties, and efficient, relying primarily on bit operations and avoiding"
"Birdie: Natural Language-Driven Table Discovery Using Differentiable
  Search Index",2504.21282v1,TabelDiscovery,\cite{TabelDiscovery},Table Discovery in Data Lakes: State-of-the-art and Future Directions,,,True,False,"Grace Fan and
                  Jin Wang and
                  Yuliang Li and
                  Ren{\'{e}}e J. Miller",2023.0,,,,,Table Discovery in Data Lakes: State-of-the-art and Future Directions,Table Discovery in Data Lakes: State-of-the-art and Future Directions ...,https://dl.acm.org/doi/10.1145/3555041.3589409,"Data discovery refers to a set of tasks that enable users and downstream applications to explore and gain insights from massive collections of data sources such as data lakes. In this tutorial, we will provide a comprehensive overview of the most recent table discovery techniques developed by the data management community."
"Birdie: Natural Language-Driven Table Discovery Using Differentiable
  Search Index",2504.21282v1,AdelfioS13,\cite{AdelfioS13},Schema Extraction for Tabular Data on the Web,,,True,False,"Marco D. Adelfio and
                  Hanan Samet",2013.0,,,,Proc. {VLDB} Endow.,Schema Extraction for Tabular Data on the Web,Schema extraction for tabular data on the web,https://dl.acm.org/doi/abs/10.14778/2536336.2536343,"Web data such as web tables, lists, and data records from a wide variety of domains can be combined for different purposes such as querying for information and creating example data sets. Tabular web data location, extraction, and schema discovery and"
"Birdie: Natural Language-Driven Table Discovery Using Differentiable
  Search Index",2504.21282v1,GoogleSearch,\cite{GoogleSearch},"Google Dataset Search: Building a search engine for datasets in an
                  open Web ecosystem",,,True,False,"Dan Brickley and
                  Matthew Burgess and
                  Natasha F. Noy",2019.0,,,,,"Google Dataset Search: Building a search engine for datasets in an
                  open Web ecosystem",Google Dataset Search: Building a search engine for datasets in an open ...,https://dl.acm.org/doi/10.1145/3308558.3313685,"In this paper, we discuss Google Dataset Search, a dataset-discovery tool that provides search capabilities over potentially all datasets published on the Web. The approach relies on an open ecosystem, where dataset owners and providers publish semantically enhanced metadata on their own sites. We then aggregate, normalize, and reconcile this"
"Birdie: Natural Language-Driven Table Discovery Using Differentiable
  Search Index",2504.21282v1,JOSIE,\cite{JOSIE},{JOSIE:} Overlap Set Similarity Search for Finding Joinable Tables in Data Lakes,,,True,False,"Erkang Zhu and
                  Dong Deng and
                  Fatemeh Nargesian and
                  Ren{\'{e}}e J. Miller",2019.0,,,,,{JOSIE:} Overlap Set Similarity Search for Finding Joinable Tables in Data Lakes,JOSIE: Overlap Set Similarity Search for Finding Joinable Tables in ...,https://dl.acm.org/doi/10.1145/3299869.3300065,"We present a new solution for finding joinable tables in massive data lakes: given a table and one join column, find tables that can be joined with the given table on the largest number of distinct values. ... JOSIE: Overlap Set Similarity Search for Finding Joinable Tables in Data Lakes. Information systems. Data management systems"
"Birdie: Natural Language-Driven Table Discovery Using Differentiable
  Search Index",2504.21282v1,TUS,\cite{TUS},Table Union Search on Open Data,,,True,False,"Fatemeh Nargesian and
                  Erkang Zhu and
                  Ken Q. Pu and
                  Ren{\'{e}}e J. Miller",2018.0,,,,Proc. {VLDB} Endow.,Table Union Search on Open Data,Metadata-driven Table Union Search: Leveraging Semantics for Restricted ...,https://arxiv.org/html/2502.20945v1,"Table union search involves identifying a set of candidate tables C within a data lake that can be unioned with a given query table Q [Nargesian et al., 2018]. The concept was first formalized by [Nargesian et al., 2018], who introduced a labeled benchmark for Table Union Search (TUS) on open data. They considered a candidate table unionable"
"Birdie: Natural Language-Driven Table Discovery Using Differentiable
  Search Index",2504.21282v1,OpenWiki,\cite{OpenWiki},"Open-WikiTable : Dataset for Open Domain Question Answering with Complex
                  Reasoning over Table",,,True,False,"Sunjun Kweon and
                  Yeonsu Kwon and
                  Seonhee Cho and
                  Yohan Jo and
                  Edward Choi",2023.0,,,,,"Open-WikiTable : Dataset for Open Domain Question Answering with Complex
                  Reasoning over Table",Open-WikiTable : Dataset for Open Domain Question Answering with ...,https://aclanthology.org/2023.findings-acl.526/,"Despite recent interest in open domain question answering (ODQA) over tables, many studies still rely on datasets that are not truly optimal for the task with respect to utilizing structural nature of table. ... we release Open-WikiTable, the first ODQA dataset that requires complex reasoning over tables. Open-WikiTable is built upon WikiSQL"
"Birdie: Natural Language-Driven Table Discovery Using Differentiable
  Search Index",2504.21282v1,DSI++,\cite{DSI++},{DSI++:} Updating Transformer Memory with New Documents,,,True,False,"Sanket Vaibhav Mehta and
                  Jai Gupta and
                  Yi Tay and
                  Mostafa Dehghani and
                  Vinh Q. Tran and
                  Jinfeng Rao and
                  Marc Najork and
                  Emma Strubell and
                  Donald Metzler",2023.0,,,,,{DSI++:} Updating Transformer Memory with New Documents,DSI++: Updating Transformer Memory with New Documents,https://aclanthology.org/2023.emnlp-main.510/,"DSI++: Updating Transformer Memory with New Documents. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8198-8213, Singapore. Association for Computational Linguistics. Cite (Informal): DSI++: Updating Transformer Memory with New Documents (Mehta et al., EMNLP 2023) Copy Citation:"
"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated
  Active Learning",2504.17448v1,ErroDetection,\cite{ErroDetection},Exploiting Active Learning in Novel Refractive Error Detection with Smartphones,,,True,False,"Fu, Eugene Yujun and Yang, Zhongqi and Leong, Hong Va and Ngai, Grace and Do, Chi-wai and Chan, Lily",2020.0,,,,,Exploiting Active Learning in Novel Refractive Error Detection with Smartphones,Exploiting Active Learning in Novel Refractive Error Detection with ...,https://dl.acm.org/doi/10.1145/3394171.3413748,"Evidential query-by-committee active learning for pedestrian detection in high-density crowds. International Journal of Approximate Reasoning, Vol. 104 (2019), 166--184. Crossref"
"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated
  Active Learning",2504.17448v1,ImageCaption,\cite{ImageCaption},Structural Semantic Adversarial Active Learning for Image Captioning,,,True,False,"Zhang, Beichen and Li, Liang and Su, Li and Wang, Shuhui and Deng, Jincan and Zha, Zheng-Jun and Huang, Qingming",2020.0,,,,,Structural Semantic Adversarial Active Learning for Image Captioning,Structural Semantic Adversarial Active Learning for Image Captioning ...,https://dl.acm.org/doi/10.1145/3394171.3413885,"Most image captioning models achieve superior performances with the help of large-scale surprised training data, but it is prohibitively costly to label the image captions. To solve this problem, we propose a structural semantic adversarial active learning (SSAAL) model that leverages both visual and textual information for deriving the most"
"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated
  Active Learning",2504.17448v1,PersonIdentification,\cite{PersonIdentification},Cluster and Scatter: A Multi-Grained Active Semi-Supervised Learning Framework for Scalable Person Re-Identification,,,True,False,"Hu, Bingyu and Zha, Zheng-Jun and Liu, Jiawei and Zhu, Xierong and Xie, Hongtao",2021.0,,,,,Cluster and Scatter: A Multi-Grained Active Semi-Supervised Learning Framework for Scalable Person Re-Identification,[PDF] arXiv:2204.10008v1 [cs.CV] 21 Apr 2022,https://arxiv.org/pdf/2204.10008,"Cluster and scatter: A multi-grained active semi-supervised learning framework for scalable person re-identification. In ACMMM, pages. 2605"
"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated
  Active Learning",2504.17448v1,lewis1994heterogeneous,\cite{lewis1994heterogeneous},Heterogeneous uncertainty sampling for supervised learning,,,True,False,"Lewis, David D and Catlett, Jason",1994.0,,,,,Heterogeneous uncertainty sampling for supervised learning,Heterogeneous Uncertainty Sampling for Supervised Learning,https://www.sciencedirect.com/science/article/pii/B978155860335650026X,"The intuition behind the model is that Heterogeneous Uncertainty Sampling for Supervised Learning 151 1. all words occurring in at least 0.2% of the instances, (2) is a plausible approximation (exact if certain independence assumptions and class priors hold) to the likelihood ratio 2. all words occurring in two or more positive instances, and 3."
"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated
  Active Learning",2504.17448v1,luo2013latent,\cite{luo2013latent},Latent structured active learning,,,True,False,"Luo, Wenjie and Schwing, Alex and Urtasun, Raquel",2013.0,,,,NeurIPS,Latent structured active learning,Latent Structured Active Learning - papers.nips.cc,https://papers.nips.cc/paper/4953-latent-structured-active-learning,"Latent Structured Active Learning. Part of Advances in Neural Information Processing Systems 26 (NIPS 2013 ... In this paper we present active learning algorithms in the context of structured prediction problems. To reduce the amount of labeling necessary to learn good models, our algorithms only label subsets of the output."
"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated
  Active Learning",2504.17448v1,settles2012active,\cite{settles2012active},Active learning: Synthesis lectures on artificial intelligence and machine learning,,,True,False,"Settles, Burr",2012.0,,,,Morgan {\&} Claypool Publishers,Active learning: Synthesis lectures on artificial intelligence and machine learning,Active Learning - SpringerLink,https://link.springer.com/book/10.1007/978-3-031-01560-1,Part of the book series: Synthesis Lectures on Artificial Intelligence and Machine Learning (SLAIML) 9340 Accesses. Buy print copy. Softcover Book USD 37.99 . Price excludes VAT (USA) ... The key idea behind active learning is that a machine learning algorithm can perform better with less training if it is allowed to choose the data from which
"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated
  Active Learning",2504.17448v1,huang2021semi,\cite{huang2021semi},Semi-Supervised Active Learning with Temporal Output Discrepancy,,,True,False,"Huang, Siyu and Wang, Tianyang and Xiong, Haoyi and Huan, Jun and Dou, Dejing",2021.0,,,,,Semi-Supervised Active Learning with Temporal Output Discrepancy,Temporal Output Discrepancy for Active Learning - GitHub,https://github.com/siyuhuang/TOD,"PyTorch implementation of Semi-Supervised Active Learning with Temporal Output Discrepancy, ICCV 2021. Introduction We present a loss measurement Temporal Output Discrepancy (TOD) that estimates the loss of unlabeled samples by evaluating the distance of model outputs at different SGD steps."
"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated
  Active Learning",2504.17448v1,guo2010active,\cite{guo2010active},Active instance sampling via matrix partition.,,,True,False,"Guo, Yuhong",2010.0,,,,,Active instance sampling via matrix partition.,Active instance sampling via matrix partition | Proceedings of the 24th ...,https://dl.acm.org/doi/10.5555/2997189.2997279,"By employing a Gaussian process framework, this mutual information based instance selection problem can be formulated as a matrix partition problem. Although matrix partition is an NP-hard combinatorial optimization problem, we show that a good local solution can be obtained by exploiting an effective local optimization technique on a relaxed"
"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated
  Active Learning",2504.17448v1,yang2015multi,\cite{yang2015multi},Multi-class active learning by uncertainty sampling with diversity maximization,,,True,False,"Yang, Yi and Ma, Zhigang and Nie, Feiping and Chang, Xiaojun and Hauptmann, Alexander G",2015.0,,,,Int. J. Comput. Vis.,Multi-class active learning by uncertainty sampling with diversity maximization,Multi-Class Active Learning by Uncertainty Sampling with Diversity ...,https://link.springer.com/article/10.1007/s11263-014-0781-x,"Motivated by the state of the art of active learning, particularly the semi-supervised active learning algorithm (Hoi et al. 2008, 2009), we propose a new multi-class active learning algorithm, namely Uncertainty Sampling with Diversity Maximization (USDM), which carefully addresses the small seed set problem by leveraging all the data in the"
"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated
  Active Learning",2504.17448v1,nguyen2004active,\cite{nguyen2004active},Active learning using pre-clustering,,,True,False,"Nguyen, Hieu T and Smeulders, Arnold",2004.0,,,,,Active learning using pre-clustering,Pre-clustering active learning method for automatic classification of ...,https://www.sciencedirect.com/science/article/pii/S0952197623005663,"To address these known challenges, this paper proposes a pre-clustering active learning (AL)-based classification method. This study built on our previous work (Zhou and Chang, 2021) on building structure classification in three primary ways.First, this paper proposes using AL to substantially reduce the labeling effort required by supervised ML while still achieving promising levels of"
"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated
  Active Learning",2504.17448v1,fu2021agreement,\cite{fu2021agreement},Agreement-Discrepancy-Selection: Active learning with progressive distribution alignment,,,True,False,"Fu, Mengying and Yuan, Tianning and Wan, Fang and Xu, Songcen and Ye, Qixiang",2021.0,,,,,Agreement-Discrepancy-Selection: Active learning with progressive distribution alignment,Welcome to Mengying's Pages,https://fumengying19.github.io/,"My research interests include computer vision and machine learning, specifically for active learning, classification and detection. Publications Mengying Fu, Tianning Yuan, Fang Wan, Songcen Xu and Qixiang Ye, ""Agreement-Discrepancy-Selection: Active Learning with Progressive Distribution Alignment"" , the Association for the Advance of"
"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated
  Active Learning",2504.17448v1,ahmed2020active,\cite{ahmed2020active},Active learning based federated learning for waste and natural disaster image classification,,,True,False,"Ahmed, Lulwa and Ahmad, Kashif and Said, Naina and Qolomany, Basheer and Qadir, Junaid and Al-Fuqaha, Ala",2020.0,,,,IEEE Access,Active learning based federated learning for waste and natural disaster image classification,PDF,https://www.researchgate.net/profile/Kashif-Ahmad-2/publication/346647904_Active_Learning_Based_Federated_Learning_for_Waste_and_Natural_Disaster_Image_Classification/links/5fcb6520299bf188d4f606d0/Active-Learning-Based-Federated-Learning-for-Waste-and-Natural-Disaster-Image-Classification.pdf,"L. Ahmed et al.: Active Learning Based Federated Learning for Waste and Natural Disaster Image Classification side, it allows ML algorithms to choose the data from which it learns, and eases the"
"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated
  Active Learning",2504.17448v1,mohammad21flare,\cite{mohammad21flare},"{FLARE:} Federated active learning assisted by naming for responding
               to emergencies",,,True,False,"Mittal, Viyom and Jahanian, Mohammad and Ramakrishnan, K. K.",2021.0,,,,,"{FLARE:} Federated active learning assisted by naming for responding
               to emergencies",FLARE: Federated Active Learning Assisted by Naming for Responding to ...,https://par.nsf.gov/servlets/purl/10297352,"In this paper, we propose FLARE (Federated active Learning As-sisted by naming for Responding to Emergencies), a framework to coordinate disaster response among the many different participants using social media, and a name-based information dissemination architecture. A key notion in FLARE is a Social Media Engine (SME),"
"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated
  Active Learning",2504.17448v1,10184650,\cite{10184650},Distribution-Regularized Federated Learning on Non-IID Data,,,True,False,"Wang, Yansheng and Tong, Yongxin and Zhou, Zimu and Zhang, Ruisheng and Pan, Sinno Jialin and Fan, Lixin and Yang, Qiang",2023.0,,,,,Distribution-Regularized Federated Learning on Non-IID Data,Distribution-Regularized Federated Learning on Non-IID Data,https://ieeexplore.ieee.org/document/10184650,"Federated learning (FL) has emerged as a popular machine learning paradigm recently. Compared with traditional distributed learning, its unique challenges mainly lie in communication efficiency and non-IID (heterogeneous data) problem. While the widely adopted framework FedAvg can reduce communication overhead significantly, its effectiveness on non-IID data still lacks exploration. In this"
"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated
  Active Learning",2504.17448v1,li2021sample,\cite{li2021sample},Sample-level data selection for federated learning,,,True,False,"Li, Anran and Zhang, Lan and Tan, Juntao and Qin, Yaxuan and Wang, Junhao and Li, Xiang-Yang",2021.0,,,,,Sample-level data selection for federated learning,Sample-level Data Selection for Federated Learning,https://ieeexplore.ieee.org/document/9488723,"Federated learning (FL) enables participants to collaboratively construct a global machine learning model without sharing their local training data to the remote server. In FL systems, the selection of training samples has a significant impact on model performances, e.g., selecting participants whose datasets have erroneous samples, skewed categorical distributions, and low content diversity"
"CHASe: Client Heterogeneity-Aware Data Selection for Effective Federated
  Active Learning",2504.17448v1,shin2022sample,\cite{shin2022sample},Sample selection with deadline control for efficient federated learning on heterogeneous clients,,,True,False,"Shin, Jaemin and Li, Yuanchun and Liu, Yunxin and Lee, Sung-Ju",2022.0,,,,,Sample selection with deadline control for efficient federated learning on heterogeneous clients,Sample Selection with Deadline Control for Efficient Federated Learning ...,https://arxiv.org/abs/2201.01601v1,"Federated Learning (FL) trains a machine learning model on distributed clients without exposing individual data. Unlike centralized training that is usually based on carefully-organized data, FL deals with on-device data that are often unfiltered and imbalanced. As a result, conventional FL training protocol that treats all data equally leads to a waste of local computational resources and"
"Stitching Inner Product and Euclidean Metrics for Topology-aware Maximum
  Inner Product Search",2504.14861v1,asai2023retrieval,\cite{asai2023retrieval},Retrieval-based language models and applications,,,True,False,"Asai, Akari and Min, Sewon and Zhong, Zexuan and Chen, Danqi",2023.0,,,,,Retrieval-based language models and applications,T6: Retrieval-based Language Models and Applications,https://virtual2023.aclweb.org/tutorial_t6.html,"Learn about the recent advances in retrieval-based language models (LMs) for diverse NLP tasks. This tutorial covers the foundations, architectures, learning approaches, and applications of retrieval-based LMs, with an exercise and chat option."
"Stitching Inner Product and Euclidean Metrics for Topology-aware Maximum
  Inner Product Search",2504.14861v1,wei2024det,\cite{wei2024det},Det-lsh: a locality-sensitive hashing scheme with dynamic encoding tree for approximate nearest neighbor search,,,True,False,"Wei, Jiuqi and Peng, Botao and Lee, Xiaodong and Palpanas, Themis",2024.0,,,,arXiv preprint arXiv:2406.10938,Det-lsh: a locality-sensitive hashing scheme with dynamic encoding tree for approximate nearest neighbor search,DET-LSH: A Locality-Sensitive Hashing Scheme with Dynamic Encoding Tree ...,https://arxiv.org/abs/2406.10938,"Locality-sensitive hashing (LSH) is a well-known solution for approximate nearest neighbor (ANN) search in high-dimensional spaces due to its robust theoretical guarantee on query accuracy. Traditional LSH-based methods mainly focus on improving the efficiency and accuracy of the query phase by designing different query strategies, but pay little attention to improving the efficiency of the"
"Stitching Inner Product and Euclidean Metrics for Topology-aware Maximum
  Inner Product Search",2504.14861v1,bachrach2014speeding,\cite{bachrach2014speeding},Speeding up the xbox recommender system using a euclidean transformation for inner-product spaces,,,True,False,"Bachrach, Yoram and Finkelstein, Yehuda and Gilad-Bachrach, Ran and Katzir, Liran and Koenigstein, Noam and Nice, Nir and Paquet, Ulrich",2014.0,,,,,Speeding up the xbox recommender system using a euclidean transformation for inner-product spaces,Speeding up the Xbox recommender system using a euclidean ...,https://dl.acm.org/doi/abs/10.1145/2645710.2645741,"The end result is a system which allows approximate matches (items with relatively high inner product, but not necessarily the highest one). We evaluate our techniques on two large-scale recommendation datasets, Xbox Movies and Yahoo~Music, and show that this technique allows trading off a slight degradation in the recommendation quality for a"
"Stitching Inner Product and Euclidean Metrics for Topology-aware Maximum
  Inner Product Search",2504.14861v1,zhao2023fargo,\cite{zhao2023fargo},FARGO: Fast maximum inner product search via global multi-probing,,,True,False,"Zhao, Xi and Zheng, Bolong and Yi, Xiaomeng and Luan, Xiaofan and Xie, Charles and Zhou, Xiaofang and Jensen, Christian S",2023.0,,,,PVLDB,FARGO: Fast maximum inner product search via global multi-probing,FARGO: Fast Maximum Inner Product Search via Global Multi-Probing ...,https://dl.acm.org/doi/10.14778/3579075.3579084,"Maximum inner product search (MIPS) in high-dimensional spaces has wide applications but is computationally expensive due to the curse of dimensionality. ... FARGO: Fast Maximum Inner Product Search via Global Multi-Probing. Authors: Xi Zhao, Bolong Zheng, ... LSH is applied without taking into account the properties of the inner product. In"
"Stitching Inner Product and Euclidean Metrics for Topology-aware Maximum
  Inner Product Search",2504.14861v1,song2021promips,\cite{song2021promips},ProMIPS: Efficient high-dimensional C-approximate maximum inner product search with a lightweight index,,,True,False,"Song, Yang and Gu, Yu and Zhang, Rui and Yu, Ge",2021.0,,,,,ProMIPS: Efficient high-dimensional C-approximate maximum inner product search with a lightweight index,ProMIPS: Efficient High-Dimensional c-Approximate Maximum Inner Product ...,https://arxiv.org/pdf/2104.04406,"ProMIPS: Efﬁcient High-Dimensional c-Approximate Maximum Inner Product Search with a Lightweight Index Yang Song School of Computer Science and Engineering Northeastern University Shenyang, China ysqyw1994@163.com Yu Gu∗ School of Computer Science and Engineering Northeastern University Shenyang, China guyu@mail.neu.edu.cn Rui Zhang www"
"Stitching Inner Product and Euclidean Metrics for Topology-aware Maximum
  Inner Product Search",2504.14861v1,ma2024reconsidering,\cite{ma2024reconsidering},Reconsidering Tree based Methods for k-Maximum Inner-Product Search: The LRUS-CoverTree,,,True,False,"Ma, Hengzhao and Li, Jianzhong and Zhang, Yong",2024.0,,,,,Reconsidering Tree based Methods for k-Maximum Inner-Product Search: The LRUS-CoverTree,Maximum inner-product search using cone trees,https://dl.acm.org/doi/abs/10.1145/2339530.2339677,Ma H Li J Zhang Y (2024) Reconsidering Tree based Methods for k-Maximum Inner-Product Search: The LRUS-CoverTree 2024 IEEE 40th International Conference on Data Engineering (ICDE) 10.1109/ICDE60146.2024.00355 (4671-4684) Online publication date: 13-May-2024
"Stitching Inner Product and Euclidean Metrics for Topology-aware Maximum
  Inner Product Search",2504.14861v1,morozov2018non,\cite{morozov2018non},Non-metric similarity graphs for maximum inner product search,,,True,False,"Morozov, Stanislav and Babenko, Artem",2018.0,,,,,Non-metric similarity graphs for maximum inner product search,Non-metric Similarity Graphs for Maximum Inner Product Search,https://www.aiqianji.com/blog/article/4242,"Non-metric Similarity Graphs for Maximum Inner Product Search 基于非度量相似图的极大内积搜索. Stanislav Morozov Yandex, Lomonosov Moscow State University stanis-morozov@yandex.ru. Stanislav Morozov Yandex, 莫斯科国立大学 stanis-morozov@yandex.ru. Abstract"
"Stitching Inner Product and Euclidean Metrics for Topology-aware Maximum
  Inner Product Search",2504.14861v1,zhou2019mobius,\cite{zhou2019mobius},"M{\""o}bius transformation for fast inner product search on graph",,,True,False,"Zhou, Zhixin and Tan, Shulong and Xu, Zhaozhuo and Li, Ping",2019.0,,,,,"M{\""o}bius transformation for fast inner product search on graph",Möbius Transformation for Fast Inner Product Search on Graph,https://proceedings.neurips.cc/paper/2019/hash/0fd7e4f42a8b4b4ef33394d35212b13e-Abstract.html,"Our proposed method is based on the property that Möbius transformation introduces an isomorphism between a subgraph of l^2-Delaunay graph and Delaunay graph for inner product. Under this observation, we propose a simple but novel graph indexing and searching algorithm to find the optimal solution with the largest inner product with the query."
"Stitching Inner Product and Euclidean Metrics for Topology-aware Maximum
  Inner Product Search",2504.14861v1,tan2021norm,\cite{tan2021norm},Norm adjusted proximity graph for fast inner product retrieval,,,True,False,"Tan, Shulong and Xu, Zhaozhuo and Zhao, Weijie and Fei, Hongliang and Zhou, Zhixin and Li, Ping",2021.0,,,,,Norm adjusted proximity graph for fast inner product retrieval,Norm Adjusted Proximity Graph for Fast Inner Product Retrieval,https://dl.acm.org/doi/10.1145/3447548.3467412,"In this paper, we propose a new index graph construction method named norm adjusted proximity graph (NAPG), for efficient MIPS. ... Hui Li, Tsz Nam Chan, Man Lung Yiu, and Nikos Mamoulis. Fexipro: fast and exact inner product retrieval in recommender systems. In Proceedings of the 2017 ACM International Conference on Management of Data (SIGMOD"
AWDIT: An Optimal Weak Database Isolation Tester,2504.06975v1,Terry1994a,\cite{Terry1994a},Session Guarantees for Weakly Consistent Replicated Data,,,True,False,"Terry, D.B. and Demers, A.J. and Petersen, K. and Spreitzer, M.J. and Theimer, M.M. and Welch, B.B.",1994.0,,https://ieeexplore.ieee.org/document/331722,10.1109/PDIS.1994.331722,,Session Guarantees for Weakly Consistent Replicated Data,Session guarantees for weakly consistent replicated data,https://ieeexplore.ieee.org/document/331722,"Four per-session guarantees are proposed to aid users and applications of weakly consistent replicated data: ""read your writes"", ""monotonic reads"", ""writes foll"
AWDIT: An Optimal Weak Database Isolation Tester,2504.06975v1,Adya2000,\cite{Adya2000},Generalized Isolation Level Definitions,,,True,False,"Adya, A. and Liskov, B. and O'Neil, P.",2000.0,,,10.1109/ICDE.2000.839388,,Generalized Isolation Level Definitions,Generalized isolation level definitions - IEEE Xplore,https://ieeexplore.ieee.org/document/839388,"Generalized isolation level definitions Abstract: Commercial databases support different isolation levels to allow programmers to trade off consistency for a potential gain in performance. The isolation levels are defined in the current ANSI standard, but the definitions are ambiguous and revised definitions proposed to correct the problem are"
AWDIT: An Optimal Weak Database Isolation Tester,2504.06975v1,Crooks2017,\cite{Crooks2017},Seeing Is {{Believing}}: {{A Client-Centric Specification}} of {{Database Isolation}},,,True,False,"Crooks, Natacha and Pu, Youer and Alvisi, Lorenzo and Clement, Allen",2017.0,,,10.1145/3087801.3087802,,Seeing Is {{Believing}}: {{A Client-Centric Specification}} of {{Database Isolation}},PDF,https://www.cs.cornell.edu/lorenzo/papers/Crooks17Seeing.pdf,"guarantee. We show how a client-centric implementation of parallel snapshot isolation can be more resilient to slowdown cascades, a common phenomenon in large-scale datacenters. ACM Reference format: Natacha Crooks, Youer Pu, Lorenzo Alvisi, and Allen Clement. 2017. See-ing is Believing: A Client-Centric Spefi of Database Isolation. In"
AWDIT: An Optimal Weak Database Isolation Tester,2504.06975v1,Burckhardt2014,\cite{Burckhardt2014},"Replicated data types: specification, verification, optimality",,,True,False,"Burckhardt, Sebastian and Gotsman, Alexey and Yang, Hongseok and Zawirski, Marek",2014.0,,https://doi.org/10.1145/2535838.2535848,10.1145/2535838.2535848,,"Replicated data types: specification, verification, optimality",PDF,https://inria.hal.science/hal-00934311/file/Replicated_Data_Types-_Specification_Verification_Optimality_Marek_Alexey_Burckhardt_popl14.pdf,"Replicated Data Types: Specification, Verification, Optimality Sebastian Burckhardt, Alexey Gotsman, Hongseok Yang, Marek Zawirski ... Alexey Gotsman, Hongseok Yang, Marek Zawirski. Replicated Data Types: Specification, Verification, Optimality. POPL 2014: 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, Jan 2014, San"
AWDIT: An Optimal Weak Database Isolation Tester,2504.06975v1,Cerone2015,\cite{Cerone2015},A {{Framework}} for {{Transactional Consistency Models}} with {{Atomic Visibility}},,,True,False,"Cerone, Andrea and Bernardi, Giovanni and Gotsman, Alexey",2015.0,,,10.4230/LIPIcs.CONCUR.2015.58,,A {{Framework}} for {{Transactional Consistency Models}} with {{Atomic Visibility}},"Reading ""A Framework for Transactional Consistency Models with Atomic ...",https://github.com/lorin/alloy-consistency-models,"In the paper A Framework for Transactional Consistency Models with Atomic Visibility by Cerone, Bernardi and Gostman, the authors propose a formal approach for reasoning about consistency models of transactional databases.. This paper seemed like a good opportunity to practice working with Alloy, and so I used Alloy to encode some of the specifications in the paper."
AWDIT: An Optimal Weak Database Isolation Tester,2504.06975v1,Liu2024a,\cite{Liu2024a},"Plume: Efficient and Complete Black-Box Checking of Weak Isolation
                  Levels",,,True,False,"Si Liu and
                  Long Gu and
                  Hengfeng Wei and
                  David A. Basin",2024.0,,https://doi.org/10.1145/3689742,10.1145/3689742,Proc. {ACM} Program. Lang.,"Plume: Efficient and Complete Black-Box Checking of Weak Isolation
                  Levels",Plume: Efficient and Complete Black-Box Checking of Weak Isolation ...,https://dl.acm.org/doi/abs/10.1145/3689742,"In this paper we present Plume, the first efficient, complete, black-box checker for weak isolation levels. Plume builds on modular, fine-grained, transactional anomalous patterns, with which we establish sound and complete characterizations of representative weak isolation levels, including read committed, read atomicity, and transactional"
AWDIT: An Optimal Weak Database Isolation Tester,2504.06975v1,Tan2020,\cite{Tan2020},Cobra: Making Transactional Key-Value Stores Verifiably Serializable,,,True,False,"Cheng Tan and
                  Changgeng Zhao and
                  Shuai Mu and
                  Michael Walfish",2020.0,,https://www.usenix.org/conference/osdi20/presentation/tan,,,Cobra: Making Transactional Key-Value Stores Verifiably Serializable,Cobra: Making Transactional Key-Value Stores Verifiably Serializable,https://www.usenix.org/conference/osdi20/presentation/tan,"Thus, clients might like to know whether the databases are meeting their contract. To that end, we introduce cobra; cobra applies to transactional key-value stores. It is the first system that combines (a) black-box checking, of (b) serializability, while (c) scaling to real-world online transactional processing workloads."
AWDIT: An Optimal Weak Database Isolation Tester,2504.06975v1,Zhang2023a,\cite{Zhang2023a},Viper: {{A Fast Snapshot Isolation Checker}},,,True,False,"Zhang, Jian and Ji, Ye and Mu, Shuai and Tan, Cheng",2023.0,,,10.1145/3552326.3567492,,Viper: {{A Fast Snapshot Isolation Checker}},'Viper: A Fast Snapshot Isolation Checker' - Northeastern Global News,https://news.northeastern.edu/research/archives/viper-a-fast-snapshot-isolation-checker/,"""Snapshot isolation (SI) is supported by most commercial databases and is widely used by applications. However, checking SI today — given a set of transactions, checking if they obey SI — is either slow or gives up soundness. We present viper, an SI checker that is sound, complete and fast. Viper checks black-box databases and hence is transparent to both users and databases. To be fast"
AWDIT: An Optimal Weak Database Isolation Tester,2504.06975v1,Papadimitriou1979a,\cite{Papadimitriou1979a},The Serializability of Concurrent Database Updates,,,True,False,"Papadimitriou, Christos H.",1979.0,,https://dl.acm.org/doi/10.1145/322154.322158,10.1145/322154.322158,Journal of the ACM,The Serializability of Concurrent Database Updates,The serializability of concurrent database updates,https://dl.acm.org/doi/10.1145/322154.322158,"BERNSTEIN, P A, PAPADIMITRIOU, C H., AND ROTHNIE, J B Resolving certam concurrent update problems wnhout locking an abstract Proc IEEE Workshop on OS and DBMS, Chtcago, I11, 1977. ... The serializability of concurrent database updates. Computing methodologies. Concurrent computing methodologies. Information systems. Data management systems."
AWDIT: An Optimal Weak Database Isolation Tester,2504.06975v1,Furbach2015,\cite{Furbach2015},Memory-Model-Aware Testing: A Unified Complexity Analysis,,,True,False,"Furbach, Florian and Meyer, Roland and Schneider, Klaus and Senftleben, Maximilian",2015.0,,https://doi.org/10.1145/2753761,10.1145/2753761,ACM Trans. Embed. Comput. Syst.,Memory-Model-Aware Testing: A Unified Complexity Analysis,Memory-Model-Aware Testing: A Unified Complexity Analysis,https://dl.acm.org/doi/10.1145/2753761,"F. Furbach, R. Meyer, K. Schneider, and M. Senftleben. 2014. Memory model-aware testing—a unified complexity analysis. In Proceedings of the 2014 14th International Conference on Application of Concurrency to System Design (ACSD'14). IEEE, Los Alamitos, CA, 92--101."
AWDIT: An Optimal Weak Database Isolation Tester,2504.06975v1,Gibbons1997,\cite{Gibbons1997},Testing {{Shared Memories}},,,True,False,"Gibbons, Phillip B. and Korach, Ephraim",1997.0,,http://epubs.siam.org/doi/10.1137/S0097539794279614,10.1137/S0097539794279614,SIAM Journal on Computing,Testing {{Shared Memories}},Testing Shared Memories | SIAM Journal on Computing,https://epubs.siam.org/doi/10.1137/S0097539794279614,"Sequential consistency is the most widely used correctness condition for multiprocessor memory systems. This paper studies the problem of testing shared-memory multiprocessors to determine if they are indeed providing a sequentially consistent memory. It presents the first formal study of this problem, which has applications to testing new memory system designs and realizations, providing run"
AWDIT: An Optimal Weak Database Isolation Tester,2504.06975v1,Gibbons1994,\cite{Gibbons1994},On testing cache-coherent shared memories,,,True,False,"Gibbons, Phillip B and Korach, Ephraim",1994.0,,,,,On testing cache-coherent shared memories,On testing cache-coherent shared memories | Proceedings of the sixth ...,https://dl.acm.org/doi/10.1145/181014.181328,"We present a series of results for testing an execution of a shared memory under scenarios that exploit the cache-coherence protocol. In addition to reads and writes to the shared memory, we consider the more powerful read-modify-write, load-reserved, and store-conditional operations available in many cache-coherent multiprocessors."
AWDIT: An Optimal Weak Database Isolation Tester,2504.06975v1,Abdulla2019b,\cite{Abdulla2019b},{Optimal stateless model checking for reads-from equivalence under sequential consistency},,,True,False,Parosh Aziz Abdulla and Mohamed Faouzi Atig and Bengt Jonsson and Magnus L{\aa}ng and Tuan Phong Ngo and Konstantinos Sagonas,,,https://dblp.org/rec/journals/pacmpl/AbdullaAJLNS19.bib,10.1145/3360576,Proc. {ACM} Program. Lang.,{Optimal stateless model checking for reads-from equivalence under sequential consistency},Optimal stateless model checking for reads-from equivalence under ...,https://dl.acm.org/doi/10.1145/3360576,"We present a new approach for stateless model checking (SMC) of multithreaded programs under Sequential Consistency (SC) semantics. To combat state-space explosion, SMC is often equipped with a partial-order reduction technique, which defines an equivalence on executions, and only needs to explore one execution in each equivalence class."
AWDIT: An Optimal Weak Database Isolation Tester,2504.06975v1,Baty2011,\cite{Baty2011},Mathematizing C++ concurrency,,,True,False,"Batty, Mark and Owens, Scott and Sarkar, Susmit and Sewell, Peter and Weber, Tjark",2011.0,,https://doi.org/10.1145/1926385.1926394,10.1145/1926385.1926394,,Mathematizing C++ concurrency,Mathematizing C++ concurrency | Proceedings of the 38th annual ACM ...,https://dl.acm.org/doi/10.1145/1926385.1926394,"Mathematizing C++ concurrency. POPL '11 . Shared-memory concurrency in C and C++ is pervasive in systems programming, but has long been poorly defined. This motivated an ongoing shared effort by the standards committees to specify concurrent behaviour in the next versions of both languages."
AWDIT: An Optimal Weak Database Isolation Tester,2504.06975v1,Lahav2015,\cite{Lahav2015},Owicki-{{Gries Reasoning}} for {{Weak Memory Models}},,,True,False,"Lahav, Ori and Vafeiadis, Viktor",2015.0,,https://link.springer.com/10.1007/978-3-662-47666-6_25,10.1007/978-3-662-47666-6_25,,Owicki-{{Gries Reasoning}} for {{Weak Memory Models}},Owicki-Gries Reasoning for Weak Memory Models,https://link.springer.com/chapter/10.1007/978-3-662-47666-6_25,"We show that even in the absence of auxiliary variables, the well-known Owicki-Gries method for verifying concurrent programs is unsound for weak memory models. By strengthening its non-interference check, however, we obtain OGRA, a program logic that is sound for reasoning about programs in the release-acquire fragment of the C11 memory model."
SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model,2506.01833v1,zhou2015predicting,\cite{zhou2015predicting},Predicting effects of noncoding variants with deep learning--based sequence model,,,True,False,"Zhou, Jian and Troyanskaya, Olga G",2015.0,,,,Nature methods,Predicting effects of noncoding variants with deep learning--based sequence model,A semi-supervised deep learning approach for predicting the functional ...,https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-03999-8,"Understanding the functional effects of non-coding variants is important as they are often associated with gene-expression alteration and disease development. Over the past few years, many computational tools have been developed to predict their functional impact. ... Predicting effects of noncoding variants with deep learning-based sequence"
SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model,2506.01833v1,kelley2018sequential,\cite{kelley2018sequential},Sequential regulatory activity prediction across chromosomes with convolutional neural networks,,,True,False,"Kelley, David R and Reshef, Yakir A and Bileschi, Maxwell and Belanger, David and McLean, Cory Y and Snoek, Jasper",2018.0,,,,Genome research,Sequential regulatory activity prediction across chromosomes with convolutional neural networks,Sequential regulatory activity prediction across chromosomes with ...,https://pubmed.ncbi.nlm.nih.gov/29588361/,"Sequential regulatory activity prediction across chromosomes with convolutional neural networks Genome Res. 2018 May;28(5):739-750. doi: 10.1101/gr.227819.117. Epub 2018 Mar 27. Authors David R Kelley 1 ... By use of convolutional neural networks, this system identifies promoters and distal regulatory elements and synthesizes their content to"
SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model,2506.01833v1,zhou2018deep,\cite{zhou2018deep},Deep learning sequence-based ab initio prediction of variant effects on expression and disease risk,,,True,False,"Zhou, Jian and Theesfeld, Chandra L and Yao, Kevin and Chen, Kathleen M and Wong, Aaron K and Troyanskaya, Olga G",2018.0,,,,Nature genetics,Deep learning sequence-based ab initio prediction of variant effects on expression and disease risk,HumanBase ExPecto - Variant Effects - Simons Foundation,https://hb.flatironinstitute.org/expecto/about,"ExPecto is a framework for ab initio sequence-based prediction of mutation gene expression effects and disease risks. With this web interface, we provide an explorer of tissue-specific expression effect predictions. ... Aaron K. Wong, and Olga G. Troyanskaya, Deep learning sequence-based ab initio prediction of variant effects on expression and"
SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model,2506.01833v1,chen2022sequence,\cite{chen2022sequence},A sequence-based global map of regulatory activity for deciphering human genetics,,,True,False,"Chen, Kathleen M and Wong, Aaron K and Troyanskaya, Olga G and Zhou, Jian",2022.0,,,,Nature genetics,A sequence-based global map of regulatory activity for deciphering human genetics,A sequence-based global map of regulatory activity for deciphering ...,https://www.nature.com/articles/s41588-022-01102-2,Sei is a new framework for integrating human genetics data with a sequence-based mapping of predicted regulatory activities to elucidate mechanisms contributing to complex traits and diseases.
SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model,2506.01833v1,enformer,\cite{enformer},Effective gene expression prediction from sequence by integrating long-range interactions,,,True,False,"Avsec, {\v{Z}}iga and Agarwal, Vikram and Visentin, Daniel and Ledsam, Joseph R and Grabska-Barwinska, Agnieszka and Taylor, Kyle R and Assael, Yannis and Jumper, John and Kohli, Pushmeet and Kelley, David R",2021.0,,,,Nature methods,Effective gene expression prediction from sequence by integrating long-range interactions,Enhancing gene expression prediction using long-range genome interactions,https://pmc.ncbi.nlm.nih.gov/articles/PMC8550815/,"A sequence-to-expression machine learning model achieves higher accuracy by incorporating information about potential long-range interactions. ... Effective gene expression prediction from sequence by integrating long-range interactions. Nature Methods, in press, 2021."
SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model,2506.01833v1,NT,\cite{NT},Nucleotide Transformer: building and evaluating robust foundation models for human genomics,,,True,False,"Dalla-Torre, Hugo and Gonzalez, Liam and Mendoza-Revilla, Javier and Lopez Carranza, Nicolas and Grzywaczewski, Adam Henryk and Oteri, Francesco and Dallago, Christian and Trop, Evan and de Almeida, Bernardo P and Sirelkhatim, Hassan and others",2024.0,,,,Nature Methods,Nucleotide Transformer: building and evaluating robust foundation models for human genomics,Nucleotide Transformer: building and evaluating robust foundation ...,https://pubmed.ncbi.nlm.nih.gov/39609566/,"Nucleotide Transformer: building and evaluating robust foundation models for human genomics Nat Methods. 2025 Feb;22(2) :287-297. ... Here, we present an extensive study of foundation models pre-trained on DNA sequences, named Nucleotide Transformer, ranging from 50 million up to 2.5 billion parameters and integrating information from 3,202"
SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model,2506.01833v1,DNABert,\cite{DNABert},DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome,,,True,False,"Ji, Yanrong and Zhou, Zhihan and Liu, Han and Davuluri, Ramana V",2021.0,,,,Bioinformatics,DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome,DNABERT: pre-trained Bidirectional Encoder Representations from ...,https://pubmed.ncbi.nlm.nih.gov/33538820/,"## Save citation to file # DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome # DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks. The source code, pretrained and finetuned model for DNABERT are available at GitHub ( PubMed Disclaimer DNABERT significantly outperforms other models in identifying promoter regions. DNABERT significantly outperforms other models in finding splice sites.(  a  ) (Left to…"
SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model,2506.01833v1,celikkanatrevisiting,\cite{celikkanatrevisiting},Revisiting K-mer Profile for Effective and Scalable Genome Representation Learning,,,True,False,"Celikkanat, Abdulkadir and Masegosa, Andres R and Nielsen, Thomas Dyhre",2024.0,,,,,Revisiting K-mer Profile for Effective and Scalable Genome Representation Learning,Revisiting k-mer profile for effective and scalable genome ...,https://dl.acm.org/doi/10.5555/3737916.3741694,"In this paper, we revisit k-mer-based representations of genomes and provide a theoretical analysis of their use in representation learning. Based on the analysis, we propose a lightweight and scalable model for performing metagenomic binning at the genome read level, relying only on the k -mer compositions of the DNA fragments."
SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model,2506.01833v1,DNABert2,\cite{DNABert2},DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genomes,,,True,False,"Zhou, Zhihan and Ji, Yanrong and Li, Weijian and Dutta, Pratik and Davuluri, Ramana V and Liu, Han",2024.0,,,,,DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genomes,DNABERT-2: Efficient Foundation Model and Benchmark for Multi-Species ...,https://github.com/MAGICS-LAB/DNABERT_2,"DNABERT-2 is a foundation model trained on large-scale multi-species genome that achieves the state-of-the-art performance on $28$ tasks of the GUE benchmark. It replaces k-mer tokenization with BPE, positional embedding with Attention with Linear Bias (ALiBi), and incorporate other techniques to improve the efficiency and effectiveness of DNABERT."
SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model,2506.01833v1,sanabria2024dna,\cite{sanabria2024dna},DNA language model GROVER learns sequence context in the human genome,,,True,False,"Sanabria, Melissa and Hirsch, Jonas and Joubert, Pierre M and Poetsch, Anna R",2024.0,,,,Nature Machine Intelligence,DNA language model GROVER learns sequence context in the human genome,PoetschLab/GROVER - Hugging Face,https://huggingface.co/PoetschLab/GROVER,"This is the official pre-trained model introduced in DNA language model GROVER learns sequence context in the human genome. ... {DNA language model GROVER learns sequence context in the human genome}, author={Sanabria, Melissa and Hirsch, Jonas and Joubert, Pierre M and Poetsch, Anna R}, journal={Nature Machine Intelligence}, pages={1--13"
SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model,2506.01833v1,nguyen2024sequence,\cite{nguyen2024sequence},Sequence modeling and design from molecular to genome scale with Evo,,,True,False,"Nguyen, Eric and Poli, Michael and Durrant, Matthew G and Kang, Brian and Katrekar, Dhruva and Li, David B and Bartie, Liam J and Thomas, Armin W and King, Samuel H and Brixi, Garyk and others",2024.0,,,,Science,Sequence modeling and design from molecular to genome scale with Evo,Sequence modeling and design from molecular to genome scale with Evo - AAAS,https://www.science.org/doi/10.1126/science.ado9336,"Evo learns both of these representations from the whole-genome sequences of millions of organisms to enable prediction and design tasks from the molecular to genome scale. Further development of large-scale biological sequence models like Evo, combined with advances in DNA synthesis and genome engineering, will accelerate our ability to"
SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model,2506.01833v1,MoE0,\cite{MoE0},Adaptive Mixtures of Local Experts,,,True,False,"Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.",1991.0,,,10.1162/neco.1991.3.1.79,Neural Computation,Adaptive Mixtures of Local Experts,Adaptive Mixtures of Local Experts - IEEE Xplore,https://ieeexplore.ieee.org/abstract/document/6797059,"Adaptive Mixtures of Local Experts Abstract: We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of"
SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model,2506.01833v1,deepseek,\cite{deepseek},"Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model",,,True,False,"Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Dengr, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and others",2024.0,,,,arXiv preprint arXiv:2405.04434,"Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model","DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts ...",https://arxiv.org/abs/2405.04434,"(57 additional authors not shown); ""Show Entire Author List"") View a PDF of the paper titled DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model, by DeepSeek-AI and 156 other authors View PDF HTML (experimental) Abstract:We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times."
"Spectral Insights into Data-Oblivious Critical Layers in Large Language
  Models",2506.00382v1,DBLP:conf/iclr/NguyenRK21,\cite{DBLP:conf/iclr/NguyenRK21},"Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural
                  Network Representations Vary with Width and Depth",,,True,False,"Thao Nguyen and
                  Maithra Raghu and
                  Simon Kornblith",2021.0,,https://openreview.net/forum?id=KJNcAkY8tY4,,,"Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural
                  Network Representations Vary with Width and Depth",谷歌团队新发现：宽域网络和深层网络所学习的东西相同吗？ - 知乎,https://zhuanlan.zhihu.com/p/389976540,在 Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth 一文中， 谷歌 团队通过对同一架构家族的宽度和 深度网络 的隐含表征和最终输出的 lens，系统地研究了两者间的相似性。
"Spectral Insights into Data-Oblivious Critical Layers in Large Language
  Models",2506.00382v1,DBLP:conf/emnlp/ZhaoLLZ024,\cite{DBLP:conf/emnlp/ZhaoLLZ024},"Defending Large Language Models Against Jailbreak Attacks via Layer-specific
                  Editing",,,True,False,"Wei Zhao and
                  Zhe Li and
                  Yige Li and
                  Ye Zhang and
                  Jun Sun",2024.0,,https://aclanthology.org/2024.findings-emnlp.293,,,"Defending Large Language Models Against Jailbreak Attacks via Layer-specific
                  Editing",PDF,https://aclanthology.org/2024.findings-emnlp.293.pdf,"Defending Large Language Models Against Jailbreak Attacks via Layer-specic Editing Wei Zhao* 1, Zhe Li* 1, Yige Li1, Ye Zhang2, Jun Sun 1, 1Singapore Management University 2 National University of Singapore {wzhao,zheli,yigeli,junsun}@smu.edu.sg Abstract Large language models (LLMs) are increas-ingly being adopted in a wide range of real-world"
"Unlocking the Power of Rehearsal in Continual Learning: A Theoretical
  Perspective",2506.00205v1,cbrs2020,\cite{cbrs2020},Online continual learning from imbalanced data,,,True,False,"Chrysakis, Aristotelis and Moens, Marie-Francine",2020.0,,,,,Online continual learning from imbalanced data,Online Continual Learning via Logit Adjusted Softmax,https://arxiv.org/abs/2311.06460,"Online continual learning is a challenging problem where models must learn from a non-stationary data stream while avoiding catastrophic forgetting. Inter-class imbalance during training has been identified as a major cause of forgetting, leading to model prediction bias towards recently learned classes. In this paper, we theoretically analyze that inter-class imbalance is entirely attributed"
"Unlocking the Power of Rehearsal in Continual Learning: A Theoretical
  Perspective",2506.00205v1,chaudhry2018efficient,\cite{chaudhry2018efficient},Efficient Lifelong Learning with A-GEM,,,True,False,"Chaudhry, Arslan and Ranzato, Marc’Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed",2018.0,,,,,Efficient Lifelong Learning with A-GEM,[1812.00420] Efficient Lifelong Learning with A-GEM - arXiv.org,https://arxiv.org/abs/1812.00420,"A-GEM is a variant of GEM that improves the efficiency of lifelong learning by reducing the sample, computational and memory cost. It also learns faster with task descriptors and outperforms other methods on standard benchmarks."
"Unlocking the Power of Rehearsal in Continual Learning: A Theoretical
  Perspective",2506.00205v1,chaudhry2019continual,\cite{chaudhry2019continual},Continual learning with tiny episodic memories,,,True,False,"Dokania, P and Torr, P and Ranzato, M",2019.0,,,,,Continual learning with tiny episodic memories,[1902.10486] On Tiny Episodic Memories in Continual Learning - arXiv.org,https://arxiv.org/abs/1902.10486,"View a PDF of the paper titled On Tiny Episodic Memories in Continual Learning, by Arslan Chaudhry and 6 other authors. View PDF Abstract: In continual learning (CL), an agent learns from a stream of tasks leveraging prior experience to transfer knowledge to future tasks. It is an ideal framework to decrease the amount of supervision in the"
"Unlocking the Power of Rehearsal in Continual Learning: A Theoretical
  Perspective",2506.00205v1,gargtic2024,\cite{gargtic2024},TiC-CLIP: Continual Training of CLIP Models,,,True,False,"Garg, Saurabh and Farajtabar, Mehrdad and Pouransari, Hadi and Vemulapalli, Raviteja and Mehta, Sachin and Tuzel, Oncel and Shankar, Vaishaal and Faghri, Fartash",2024.0,,,,,TiC-CLIP: Continual Training of CLIP Models,TiC-CLIP: Continual Training of CLIP Models - Apple Machine Learning ...,https://machinelearning.apple.com/research/tic-clip,"A paper on continual large-scale training of CLIP models with data distribution shifts by time. The paper proposes benchmarks, evaluations, and methods to reduce the performance gap in forward transfer."
"Timing is important: Risk-aware Fund Allocation based on Time-Series
  Forecasting",2505.24835v1,Pyraformer,\cite{Pyraformer},"Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time
                  Series Modeling and Forecasting",,,True,False,"Shizhan Liu and
                  Hang Yu and
                  Cong Liao and
                  Jianguo Li and
                  Weiyao Lin and
                  Alex X. Liu and
                  Schahram Dustdar",2022.0,,https://openreview.net/forum?id=0EXmFzUn5I,,,"Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time
                  Series Modeling and Forecasting",Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time ...,https://openreview.net/forum?id=0EXmFzUn5I,"Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting | OpenReview Blind Submission by Conference • Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting One-sentence Summary: We propose a multiresolution pyramidal attention mechanism for long-range dependence modeling and time series forecasting, successfully reducing the maximum length of the signal traversing path to O(1) while achieving linear time and space complexity Comment: The authors propose a multi-resolution pyramidal attention mechanism to capture long-range dependencies in time series forecasting, achieving linear time and space complexity. This paper presents a new hierarchical transformer architecture with constant connection path length and linear time and space complexity for long-range time series modeling. This paper proposes Pyraformer, a low-complexity pyramidal attention model for long-range time-series modelling and forecasting."
"Timing is important: Risk-aware Fund Allocation based on Time-Series
  Forecasting",2505.24835v1,Crossformer,\cite{Crossformer},"Crossformer: Transformer Utilizing Cross-Dimension Dependency for
                  Multivariate Time Series Forecasting",,,True,False,"Yunhao Zhang and
                  Junchi Yan",2023.0,,https://openreview.net/forum?id=vSVLM2j9eie,,,"Crossformer: Transformer Utilizing Cross-Dimension Dependency for
                  Multivariate Time Series Forecasting",Crossformer: Transformer Utilizing Cross-Dimension Dependency for ...,https://bohrium.dp.tech/paper/arxiv/fb45d31cc89207aec392dbac8908cc24db2df871,"Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting. ... a Transformer-based model utilizing cross-dimension dependency for MTS forecasting. In Crossformer, the input MTS is embedded into a 2D vector array through the Dimension-Segment-Wise (DSW) embedding to preserve time and dimension"
"Timing is important: Risk-aware Fund Allocation based on Time-Series
  Forecasting",2505.24835v1,MICN,\cite{MICN},"{MICN:} Multi-scale Local and Global Context Modeling for Long-term
                  Series Forecasting",,,True,False,"Huiqiang Wang and
                  Jian Peng and
                  Feihu Huang and
                  Jince Wang and
                  Junhui Chen and
                  Yifei Xiao",2023.0,,https://openreview.net/forum?id=zt53IDUR1U,,,"{MICN:} Multi-scale Local and Global Context Modeling for Long-term
                  Series Forecasting",,,
"Timing is important: Risk-aware Fund Allocation based on Time-Series
  Forecasting",2505.24835v1,D3VAE,\cite{D3VAE},"Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement",,,True,False,"Yan Li and
                  Xinjiang Lu and
                  Yaqing Wang and
                  Dejing Dou",2022.0,,http://papers.nips.cc/paper\_files/paper/2022/hash/91a85f3fb8f570e6be52b333b5ab017a-Abstract-Conference.html,,,"Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement","Generative Time Series Forecasting with Diffusion, Denoise, and ... - NIPS",https://papers.nips.cc/paper_files/paper/2022/hash/91a85f3fb8f570e6be52b333b5ab017a-Abstract-Conference.html,"D3VAE is a bidirectional variational auto-encoder that uses diffusion, denoise, and disentanglement to forecast time series data. It augments the data with a coupled diffusion probabilistic model and integrates multiscale denoising score matching to improve the prediction accuracy and interpretability."
"Timing is important: Risk-aware Fund Allocation based on Time-Series
  Forecasting",2505.24835v1,CF-RNN,\cite{CF-RNN},Conformal Time-series Forecasting,,,True,False,"Kamile Stankeviciute and
                  Ahmed M. Alaa and
                  Mihaela van der Schaar",2021.0,,https://proceedings.neurips.cc/paper/2021/hash/312f1ba2a72318edaaa995a67835fad5-Abstract.html,,,Conformal Time-series Forecasting,Neural Conformal Control for Time Series Forecasting,https://arxiv.org/abs/2412.18144,"We introduce a neural network conformal prediction method for time series that enhances adaptivity in non-stationary environments. Our approach acts as a neural controller designed to achieve desired target coverage, leveraging auxiliary multi-view data with neural network encoders in an end-to-end manner to further enhance adaptivity. Additionally, our model is designed to enhance the"
"Timing is important: Risk-aware Fund Allocation based on Time-Series
  Forecasting",2505.24835v1,EnbPI,\cite{EnbPI},Conformal prediction interval for dynamic time-series,,,True,False,"Chen Xu and
                  Yao Xie",2021.0,,http://proceedings.mlr.press/v139/xu21h.html,,,Conformal prediction interval for dynamic time-series,Conformal prediction interval for dynamic time-series - PMLR,https://proceedings.mlr.press/v139/xu21h.html,"A paper that introduces a method to construct sequential prediction intervals for dynamic time-series using bootstrap ensemble estimators and conformal prediction framework. The method is theoretically valid, computationally efficient, and scalable to various regression functions and time-series models."
"Timing is important: Risk-aware Fund Allocation based on Time-Series
  Forecasting",2505.24835v1,PTOCA,\cite{PTOCA},A Predict-Then-Optimize Couriers Allocation Framework for Emergency Last-mile Logistics,,,True,False,"Kaiwen Xia and
                  Li Lin and
                  Shuai Wang and
                  Haotian Wang and
                  Desheng Zhang and
                  Tian He",2023.0,,,10.1145/3580305.3599766,,A Predict-Then-Optimize Couriers Allocation Framework for Emergency Last-mile Logistics,Selected Publications - Haotian Wang,https://sbuhaotian.github.io/SBUhaotian/publications/,"[KDD'23] A Predict-Then-Optimize Couriers Allocation Framework for Emergency Last-mile Logistics Kaiwen Xia, Li Lin, Shuai Wang, Haotian Wang, Desheng Zhang, Tian He [IJCAI'23] A Prediction-and-Scheduling Framework for Efficient Order Transfer in Logistics Wenjun Lyu, Haotian Wang, Yiwei Song, Yunhuai Liu, Tian He, Desheng Zhang"
"Timing is important: Risk-aware Fund Allocation based on Time-Series
  Forecasting",2505.24835v1,PtOorPnO,\cite{PtOorPnO},"Predict-then-optimize or predict-and-optimize? An empirical evaluation
                  of cost-sensitive learning strategies",,,True,False,"Toon Vanderschueren and
                  Tim Verdonck and
                  Bart Baesens and
                  Wouter Verbeke",2022.0,,,10.1016/J.INS.2022.02.021,Inf. Sci.,"Predict-then-optimize or predict-and-optimize? An empirical evaluation
                  of cost-sensitive learning strategies","GitHub - toonvds/CostSensitiveLearning: Code for the paper ""Predict ...",https://github.com/toonvds/CostSensitiveLearning,"""Predict-then-optimize or predict-and-optimize? An empirical evaluation of cost-sensitive learning strategies"" Information Sciences. ""A new perspective on classification: Optimally allocating limited resources to uncertain tasks"" Decision Support Systems."
Aligning Protein Conformation Ensemble Generation with Physical Feedback,2505.24203v1,jumper2021highly,\cite{jumper2021highly},Highly accurate protein structure prediction with AlphaFold,,,True,False,"Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others",2021.0,,,,Nature,Highly accurate protein structure prediction with AlphaFold,Highly accurate protein structure prediction with AlphaFold,https://pubmed.ncbi.nlm.nih.gov/34265844/,"# Highly accurate protein structure prediction with AlphaFold # Highly accurate protein structure prediction with AlphaFold Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. PubMed Disclaimer Pritzel, T.G., M.F., O.R., R.B., A.B., S.A.A.K., D.R. and A.W.S. have filed non-provisional patent applications 16/701,070 and PCT/EP2020/084238, and provisional patent applications 63/107,362, 63/118,917, 63/118,918, 63/118,921 and 63/118,919, each in the name of DeepMind Technologies Limited, each pending, relating to machine learning for predicting protein structures."
Aligning Protein Conformation Ensemble Generation with Physical Feedback,2505.24203v1,arts2023two,\cite{arts2023two},Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics,,,True,False,"Arts, Marloes and Satorras, Victor Garcia and Huang, Chin-Wei and Zuegner, Daniel and Federici, Marco and Clementi, Cecilia and No{\'e}, Frank and Pinsler, Robert and Berg, Rianne van den",2023.0,,,,arXiv preprint arXiv:2302.00600,Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics,GitHub - microsoft/two-for-one-diffusion: This Denoising Force Field ...,https://github.com/microsoft/two-for-one-diffusion,This Denoising Force Field (DFF) codebase provides a Pytorch framework for the method presented in Two for one: Diffusion models and force fields for coarse-grained molecular dynamics. This paper features a denoising diffusion model that is trained on coarse-grained protein dynamics data and can be
Aligning Protein Conformation Ensemble Generation with Physical Feedback,2505.24203v1,ouyang2022training,\cite{ouyang2022training},Training language models to follow instructions with human feedback,,,True,False,Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke E. Miller and Maddie Simens and Amanda Askell and P. Welinder and P. Christiano and J. Leike and Ryan J. Lowe,2022.0,,,,Neural Information Processing Systems,Training language models to follow instructions with human feedback,Training language models to follow instructions with human feedback,https://arxiv.org/abs/2203.02155,The authors propose a method to align language models with user intent by fine-tuning GPT-3 with human feedback. They show that their InstructGPT models outperform GPT-3 on prompts and reduce toxic output generation.
Aligning Protein Conformation Ensemble Generation with Physical Feedback,2505.24203v1,fan2024reinforcement,\cite{fan2024reinforcement},Reinforcement learning for fine-tuning text-to-image diffusion models,,,True,False,"Fan, Ying and Watkins, Olivia and Du, Yuqing and Liu, Hao and Ryu, Moonkyung and Boutilier, Craig and Abbeel, Pieter and Ghavamzadeh, Mohammad and Lee, Kangwook and Lee, Kimin",2024.0,,,,Advances in Neural Information Processing Systems,Reinforcement learning for fine-tuning text-to-image diffusion models,DPOK - Google Sites,https://sites.google.com/view/dpok-t2i-diffusion/home,"DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models In this work, we propose using online reinforcement learning (RL) to fine-tune text-to-image models. We focus on diffusion models, defining the fine-tuning task as an RL problem, and updating the pre-trained text-to-image diffusion models using policy gradient to maximize the feedback- trained reward. Moreover, we present DPOK: Diffusion Policy Optimization with KL regularization, which utilizes KL regularization w.r.t. the pre-trained text-to-image model as an implicit reward to stabilize RL fine-tuning 2.   We study incorporating KL regularization into supervised fine-tuning of diffusion models, which can mitigate some failure modes (e.g., generating over-saturated images. title={DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models},"
Aligning Protein Conformation Ensemble Generation with Physical Feedback,2505.24203v1,alford2017rosetta,\cite{alford2017rosetta},The Rosetta all-atom energy function for macromolecular modeling and design,,,True,False,"Alford, Rebecca F and Leaver-Fay, Andrew and Jeliazkov, Jeliazko R and O’Meara, Matthew J and DiMaio, Frank P and Park, Hahnbeom and Shapovalov, Maxim V and Renfrew, P Douglas and Mulligan, Vikram K and Kappel, Kalli and others",2017.0,,,,Journal of chemical theory and computation,The Rosetta all-atom energy function for macromolecular modeling and design,The Rosetta All-Atom Energy Function for Macromolecular Modeling and ...,https://pubmed.ncbi.nlm.nih.gov/28430426/,The Rosetta All-Atom Energy Function for Macromolecular Modeling and Design J Chem Theory Comput. 2017 Jun 13 ... Central to Rosetta's success is the energy function: a model parametrized from small-molecule and X-ray crystal structure data used to approximate the energy associated with each biomolecule conformation. This paper describes the
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,GPT4,\cite{GPT4},{GPT-4} Technical Report,,,True,False,OpenAI,2023.0,,https://doi.org/10.48550/arXiv.2303.08774,10.48550/ARXIV.2303.08774,CoRR,{GPT-4} Technical Report,GPT-4 Technical Report - Papers With Code,https://paperswithcode.com/paper/gpt-4-technical-report-1,"Add a new dataset here Save GPT-4 Technical Report Preprint 2023·OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, Barret Zoph· Edit social preview We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4."
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,anthropic_claude,\cite{anthropic_claude},Claude: A Family of AI Models,,,True,False,Anthropic,2024.0,,https://www.anthropic.com/product,,,Claude: A Family of AI Models,Models overview - Anthropic,https://docs.anthropic.com/en/docs/about-claude/models/overview,"Claude is a family of state-of-the-art large language models developed by Anthropic. Introducing Claude 4, our latest generation of models: Claude Opus 4 - Our most capable and intelligent model yet. Claude Sonnet 4 - Our high-performance model with exceptional reasoning and efficiency Claude Opus 4 ------------- Our most powerful and capable model  Text and image input  Text output  200k context window  Superior reasoning capabilitiesClaude Sonnet 4 --------------- High-performance model with exceptional reasoning capabilities  Text and image input  Text output  200k context window While aliases are useful for experimentation, we recommend using specific model versions (e.g., `claude-sonnet-4-20250514`) in production applications to ensure consistent behavior. Claude 4 models excel in: Output quality: When migrating from previous model generations to Claude 4, you may notice larger improvements in overall performance."
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,AIIndex2024,\cite{AIIndex2024},AI Index Report,,,True,False,stanford,,,,,,AI Index Report,The 2025 AI Index Report - Stanford HAI,https://hai.stanford.edu/ai-index/2025-ai-index-report,"Recognized as a trusted resource by global media, governments, and leading companies, the AI Index equips policymakers, business leaders, and the public with rigorous, objective insights into AI’s technical progress, economic influence, and societal impact. The AI Index is an independent initiative at the Stanford Institute for Human-Centered Artificial Intelligence (HAI), led by the AI Index Steering Committee, an interdisciplinary group of experts from across academia and industry. The AI Index is an independent initiative at the Stanford Institute for Human-Centered Artificial Intelligence (HAI), led by the AI Index Steering Committee, an interdisciplinary group of experts from across academia and industry. The 2025 AI Index Report | Stanford HAI"
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,apple_ai,\cite{apple_ai},Apple Intelligence,,,True,False,Apple,2024.0,,,,,Apple Intelligence,Apple Intelligence - Wikipedia,https://en.wikipedia.org/wiki/Apple_Intelligence,"Apple Intelligence **Apple Intelligence** is an artificial intelligence system developed by Apple Inc. Relying on a combination of on-device and server processing, it was announced on June 10, 2024, at WWDC 2024, as a built-in feature of Apple's iOS 18, iPadOS 18, and macOS Sequoia, which were announced alongside Apple Intelligence. iPhones and iPads with the A17 Pro chip or later are also supported with iOS 18.1 and later Apple claims the less capable Neural Engine of older chips, which is used for AI tasks, are not powerful enough for Apple Intelligence features. **^**""How to get Apple Intelligence"". ""Apple Intelligence: here's a full list of the iPhones, iPads and Macs that'll get Apple's new AI powers"". Apple Intelligence"
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,app6_laptops,\cite{app6_laptops},Creating Large Language Models on Your Laptop,,,True,False,Xinyu Ye and Zhe Wang and Haihao Shen and Yu Luo and Hanwen Chang,2023.0,,,,,Creating Large Language Models on Your Laptop,Creating Large Language Models on Your Laptop - Medium,https://medium.com/intel-analytics-software/creating-your-own-llms-on-your-laptop-a08cc4f7c91b,QLoRA is an approach to reduce the memory usage of large language models (LLM) fine-tuning. ... Creating Large Language Models on Your Laptop. Making Fine-Tuning Possible on Your Personal Computer.
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,hubara2018quantized,\cite{hubara2018quantized},Quantized neural networks: Training neural networks with low precision weights and activations,,,True,False,"Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua",2018.0,,,,Journal of Machine Learning Research,Quantized neural networks: Training neural networks with low precision weights and activations,Quantized Neural Networks: Training Neural Networks with Low Precision ...,https://ar5iv.labs.arxiv.org/html/1609.07061,"We introduce a method to train Quantized-Neural-Networks (QNNs), neural networks with low precision weights and activations, at run-time, and when computing the parameter gradients at train-time. In the extreme case QNNs use only 1-bit per weight and activation(i.e., Binarized NN; see Section 2). •"
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,GPT3.int8,\cite{GPT3.int8},LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale,,,True,False,"Tim Dettmers and
                  Mike Lewis and
                  Younes Belkada and
                  Luke Zettlemoyer",2022.0,,http://papers.nips.cc/paper\_files/paper/2022/hash/c3ba4962c05c49636d4c6206a97e9c8a-Abstract-Conference.html,,,LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale,LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale,https://arxiv.org/abs/2208.07339,"A paper published at NeurIPS 2022 that proposes a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which reduces the GPU memory for inference by half. The paper shows how to cope with emergent features in transformer language models that dominate performance, and provides an open-source software."
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,cnn_pruning,\cite{cnn_pruning},Pruning convolutional neural networks for resource efficient inference,,,True,False,"Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan",2016.0,,,,arXiv preprint arXiv:1611.06440,Pruning convolutional neural networks for resource efficient inference,Pruning Convolutional Neural Networks for Resource Efficient Inference,https://arxiv.org/abs/1611.06440,We propose a new formulation for pruning convolutional kernels in neural networks to enable efficient inference. We interleave greedy criteria-based pruning with fine-tuning by backpropagation - a computationally efficient procedure that maintains good generalization in the pruned network. We propose a new criterion based on Taylor expansion that approximates the change in the cost function
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,Pruner-Zero,\cite{Pruner-Zero},"Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large
                  Language Models",,,True,False,"Peijie Dong and
                  Lujun Li and
                  Zhenheng Tang and
                  Xiang Liu and
                  Xinglin Pan and
                  Qiang Wang and
                  Xiaowen Chu",2024.0,,https://openreview.net/forum?id=1tRLxQzdep,,,"Pruner-Zero: Evolving Symbolic Pruning Metric From Scratch for Large
                  Language Models",GitHub - pprp/Pruner-Zero: [ICML24] Pruner-Zero: Evolving Symbolic ...,https://github.com/pprp/Pruner-Zero,"Official PyTorch implementation of Pruner-Zero, accepted by ICML2024. Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models Peijie Dong*, Lujun Li* (indicates equal contribution), Zhenheng Tang, Xiang Liu, Xinglin Pan, Qiang Wang, Xiaowen Chu HKUST(GZ), HKUST, HKBU, HIT(SZ)"
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,pytorch-1,\cite{pytorch-1},PyTorch: An Open Source Machine Learning Framework,,,True,False,{PyTorch Contributors},2024.0,,https://pytorch.org/,,,PyTorch: An Open Source Machine Learning Framework,PyTorch - Wikipedia,https://en.wikipedia.org/wiki/PyTorch,"PyTorch is a machine learninglibrary based on the Torch library,[4][5][6] used for applications such as computer vision and natural language processing,[7] originally developed by Meta AI and now part of the Linux Foundation umbrella.[8][9][10][11] It is one of the most popular deep learning frameworks, alongside others such as TensorFlow,[12] offering free and open-source software released under the modified BSD license. Main article: Tensor (machine learning) Main article: Neural network (machine learning) PyTorch defines a module called nn (torch.nn) to describe neural networks and to support training. ""Why AI and machine learning researchers are beginning to embrace PyTorch"". ^""PyTorch 2.0 brings new fire to open-source machine learning"". ^""An Introduction to PyTorch – A Simple yet Powerful Deep Learning Library""."
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,tensorflow,\cite{tensorflow},TensorFlow: An Open Source Machine Learning Framework for Everyone,,,True,False,{TensorFlow Contributors},2024.0,,https://www.tensorflow.org/,,,TensorFlow: An Open Source Machine Learning Framework for Everyone,GitHub - tensorflow/tensorflow: An Open Source Machine Learning ...,https://github.com/tensorflow/tensorflow,"TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries, and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications. TensorFlow was originally developed by researchers and engineers working within the Machine Intelligence team at Google"
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,deepspeed,\cite{deepspeed},DeepSpeed: Advancing the Science of AI Through Efficient Training of Large Models,,,True,False,{Microsoft DeepSpeed Team},2024.0,,https://www.deepspeed.ai/,,,DeepSpeed: Advancing the Science of AI Through Efficient Training of Large Models,GitHub - deepspeedai/DeepSpeed: DeepSpeed is a deep learning ...,https://github.com/deepspeedai/DeepSpeed,"The DeepSpeed library (this repository) implements and packages the innovations and technologies in DeepSpeed Training, Inference and Compression Pillars into a single easy-to-use, open-sourced repository. Many DeepSpeed features are supported on Windows for both training and inference. (2022) DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale arXiv:2201.05596 and ICML 2022. (2022) Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model arXiv:2201.11990. (2023) ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation arXiv:2303.08302 and ENLSP2023 Workshop at NeurIPS2023 (/deepspeedai/DeepSpeed/blob/master/docs/assets/files/zeroquant_series.pdf) (2023) DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales arXiv:2308.01320. (2023) ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats arXiv:2307.09782 and ENLSP2023 Workshop at NeurIPS2023 (/deepspeedai/DeepSpeed/blob/master/docs/assets/files/zeroquant_series.pdf)"
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,huggingface_transformers,\cite{huggingface_transformers},Transformers Documentation,,,True,False,{HuggingFace Team},2024.0,,https://huggingface.co/docs/transformers/index,,,Transformers Documentation,Transformers — transformers 3.0.2 documentation - Hugging Face,https://huggingface.co/transformers/v3.0.2/index.html,"Transformers provides general-purpose architectures and pretrained models for Natural Language Understanding and Generation. Learn how to use the library, explore the features, and find the documentation of each model."
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,Orca,\cite{Orca},"Orca: {A} Distributed Serving System for Transformer-Based Generative
                  Models",,,True,False,"Gyeong{-}In Yu and
                  Joo Seong Jeong and
                  Geon{-}Woo Kim and
                  Soojeong Kim and
                  Byung{-}Gon Chun",2022.0,,https://www.usenix.org/conference/osdi22/presentation/yu,,,"Orca: {A} Distributed Serving System for Transformer-Based Generative
                  Models",Orca: A Distributed Serving System for Transformer-Based Generative Models,https://www.usenix.org/conference/osdi22/presentation/yu,"Orca: A Distributed Serving System for Transformer-Based Generative Models | USENIX - Poster Session - Poster Session # Orca: A Distributed Serving System for Transformer-Based Generative Models In this paper, we propose iteration-level scheduling, a new scheduling mechanism that schedules execution at the granularity of iteration (instead of request) where the scheduler invokes the execution engine to run only a single iteration of the model on the batch. Based on these two techniques, we have implemented a distributed serving system called ORCA, with additional designs for scalability to models with hundreds of billions of parameters. USENIX is committed to Open Access to the research presented at our events. Support USENIX and our commitment to Open Access. title = {Orca: A Distributed Serving System for {Transformer-Based} Generative Models},   url = {https://www.usenix.org/conference/osdi22/presentation/yu},   - Poster Session"
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,zhao4,\cite{zhao4},A Reconfigurable 0.69-1.02 nJ/Classification Biomedical AI Processor for Intelligent Health Monitoring Devices,,,True,False,"Zhao, Yuanzhe and Wang, Yuheng and Wang, Zijian and Zhu, Yan and Martins, RP and Chan, Chi-Hang and Zhang, Minglei",2025.0,,,,,A Reconfigurable 0.69-1.02 nJ/Classification Biomedical AI Processor for Intelligent Health Monitoring Devices,A Reconfigurable 0.69-1.02nJ/Classification Biomedical AI Processor for ...,https://ieeexplore.ieee.org/document/10983210,"Ultra-low-power biomedical AI processors are crucial for effective intelligent health monitoring systems (HMSs) in wearable devices. Traditional HMSs rely on feature-based methods [1], [2] that necessitate human intervention for pre-extract features before classification. Unfortunately, this approach often leads to poor accuracy <90%) and can be time-consuming. Recently, end-to-end neural"
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,zhao5,\cite{zhao5},A 28nm Value-Wise Hybrid-Domain Compute-in-Memory Macro with Heterogeneous Memory Fabric and Asynchronous Sparsity Manager,,,True,False,"Zhao, Yuanzhe and Wang, Yang and Wang, Yuheng and Xie, Heng and Zhu, Yan and Martins, RP and Chan, Chi-Hang and Yin, Shouyi and Zhang, Minglei",2025.0,,,,,A 28nm Value-Wise Hybrid-Domain Compute-in-Memory Macro with Heterogeneous Memory Fabric and Asynchronous Sparsity Manager,dblp: Shouyi Yin,https://dblp.org/pid/98/3428,A 28nm Value-Wise Hybrid-Domain Compute-in-Memory Macro with Heterogeneous Memory Fabric and Asynchronous Sparsity Manager. CICC 2025: 1-3 [c170] view. electronic edition via DOI ... A 28nm 4170-TFLOPS/W/b and 195-TFLOPS/mm 2 /b Multiply-Free Fully-Digital Floating-Point Compute-In-Memory Macro with Mitchell's Approximation. VLSI Technology and
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,zhao6,\cite{zhao6},A One-Shot Floating-Point Compute-in-Memory Macro Featuring PVT Robustness and Mismatch Tolerance for Edge LLMs,,,True,False,"Zhao, Yuanzhe and Xie, Heng and Wang, Zijian and Tian, Chunlin and Li, Li and Zhu, Yan and Martins, RP and Chan, Chi-Hang and Zhang, Minglei",2025.0,,,,,A One-Shot Floating-Point Compute-in-Memory Macro Featuring PVT Robustness and Mismatch Tolerance for Edge LLMs,Chunlin Tian,https://clin0212.github.io/,"[ICLR SCOPE 2025] ""AsymLoRA: Unlocking the Power of Multimodal LLMs via Asymmetric LoRA"" Xuyang Wei, Chunlin Tian, Li Li SCOPE workshop of 13th International Conference on Learning Representations [CICC 2025] ""A One-Shot Floating-Point Compute-in-Memory Macro Featuring PVT Robustness and Mismatch Tolerance for Edge LLMs."""
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,zhao7,\cite{zhao7},A 28-nm 3.32-nJ/Frame Compute-in-Memory CNN Processor With Layer Fusion for Always-on Applications,,,True,False,"Zhao, Yuanzhe and He, Pengyu and Zhu, Yan and Martins, Rui P and Chan, Chi-Hang and Zhang, Minglei",2025.0,,,,IEEE Transactions on Circuits and Systems I: Regular Papers,A 28-nm 3.32-nJ/Frame Compute-in-Memory CNN Processor With Layer Fusion for Always-on Applications,A 28-nm 3.32-nJ/Frame Compute-in-Memory CNN Processor With Layer Fusion ...,https://ieeexplore.ieee.org/document/10902457,"The prototype processor is fabricated in a 28-nm CMOS process, demonstrating a fully on-chip MNIST inference with 97.9% accuracy. It operates at 3,508 frames per second while consuming 11.6 μW at a 0.5-V supply; the achieved efficiency of 3.32 nJ/frame is over 50-fold than the state-of-the-art end-to-end MNIST accelerators."
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,zhao8,\cite{zhao8},A Reconfigurable Floating-Point Compute-In-Memory With Analog Exponent Pre-Processes,,,True,False,"He, Pengyu and Zhao, Yuanzhe and Xie, Heng and Wang, Yang and Yin, Shouyi and Li, Li and Zhu, Yan and Martins, Rui P and Chan, Chi-Hang and Zhang, Minglei",2024.0,,,,IEEE Solid-State Circuits Letters,A Reconfigurable Floating-Point Compute-In-Memory With Analog Exponent Pre-Processes,A Reconfigurable Floating-Point Compute-in-Memory With Analog Exponent ...,https://ieeexplore.ieee.org/document/10683795,"This letter presents a reconfigurable floating-point compute-in-memory (FP-CIM) macro that preprocesses the exponent in the analog domain, enhancing the energy efficiency of edge devices for the floating-point (FP) inference. The presented FP-CIM macro supports FP8 inference, while can be configured to BP16 precision in a segmented computation manner. Furthermore, a time-domain analog-to"
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,zhao2023approxcaliper,\cite{zhao2023approxcaliper},Approxcaliper: A programmable framework for application-aware neural network optimization,,,True,False,"Zhao, Yifan and Sharif, Hashim and Pao-Huang, Peter and Shah, Vatsin and Sivakumar, Arun Narenthiran and Valverde Gasparino, Mateus and Mahmoud, Abdulrahman and Zhao, Nathan and Adve, Sarita and Chowdhary, Girish and others",2023.0,,,,Proceedings of Machine Learning and Systems,Approxcaliper: A programmable framework for application-aware neural network optimization,About Me - Abdulrahman Mahmoud,https://ma3mool.github.io/,"[Feb 2023] Our paper titled ""ApproxCaliper: A Programmable Framework for Application-aware Neural Network Optimization"" has been accepted for publication at MLSys 2023! [Jan 2023] Excited to join the YArch 2023 program committee! Submit your best work!"
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,liberis2023differentiable,\cite{liberis2023differentiable},Differentiable neural network pruning to enable smart applications on microcontrollers,,,True,False,"Liberis, Edgar and Lane, Nicholas D",2023.0,,,,"Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",Differentiable neural network pruning to enable smart applications on microcontrollers,Differentiable Neural Network Pruning to Enable Smart Applications on ...,https://www.mendeley.com/catalogue/473eef3f-36e9-332f-8b72-3e1734ac270c/,"Differentiable Neural Network Pruning to Enable Smart Applications on Microcontrollers. Liberis E; Lane N; Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (2023) 6(4) DOI: 10.1145/3569468. 18 Citations. Citations of this article. 37 Readers."
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,zhao2024felix,\cite{zhao2024felix},Felix: Optimizing Tensor Programs with Gradient Descent,,,True,False,"Zhao, Yifan and Sharif, Hashim and Adve, Vikram and Misailovic, Sasa",2024.0,,,,,Felix: Optimizing Tensor Programs with Gradient Descent,Felix: Optimizing Tensor Programs with Gradient Descent,https://dl.acm.org/doi/10.1145/3620666.3651348,"We present Felix, a novel gradient-based compiler optimization framework for tensor-based programs.Felix creates a differentiable space of tensor programs that is amenable to search by gradient descent. Felix applies continuous relaxation on the space of programs and creates differentiable estimator of program latency, allowing efficient search of program candidates using gradient descent, in"
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,tam2024fedhybrid,\cite{tam2024fedhybrid},FedHybrid: Breaking the Memory Wall of Federated Learning via Hybrid Tensor Management,,,True,False,"Tam, Kahou and Tian, Chunlin and Li, Li and Zhao, Haikai and Xu, ChengZhong",2024.0,,,,,FedHybrid: Breaking the Memory Wall of Federated Learning via Hybrid Tensor Management,Chunlin Tian,https://clin0212.github.io/,"FedHybrid: Breaking the Memory Wall of Federated Learning via Hybrid Tensor Management. Kahou Tam, Chunlin Tian, Li Li, Haikai Zhao, Chengzhong Xu 22nd ACM Conference on Embedded Networked Sensor Systems (SenSys), 2024 [Mobile System Conf., CCF-B, AR: 58/313 = 18.5%]"
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,dvfsasplos,\cite{dvfsasplos},"Expanding Datacenter Capacity with {DVFS} Boosting: {A} safe and scalable
                  deployment experience",,,True,False,"Leonardo Piga and
                  Iyswarya Narayanan and
                  Aditya Sundarrajan and
                  Matt Skach and
                  Qingyuan Deng and
                  Biswadip Maity and
                  Manoj Chakkaravarthy and
                  Alison Huang and
                  Abhishek Dhanotia and
                  Parth Malani",2024.0,,https://doi.org/10.1145/3617232.3624853,10.1145/3617232.3624853,,"Expanding Datacenter Capacity with {DVFS} Boosting: {A} safe and scalable
                  deployment experience",Expanding Datacenter Capacity with DVFS Boosting: A safe and scalable ...,https://dl.acm.org/doi/10.1145/3617232.3624853,"This paper describes our experience in deploying DVFS boosting to expand capacity. There are several challenges in deploying DVFS boosting at scale. First, frequency scaling incurs additional power demand, which can exacerbate power over-subscription and incur unexpected capacity loss for the services due to power capping."
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,dfvs-4,\cite{dfvs-4},Improving {DVFS} in NoCs with Coherence Prediction,,,True,False,"Robert Hesse and
                  Natalie D. Enright Jerger",2015.0,,https://doi.org/10.1145/2786572.2786595,10.1145/2786572.2786595,,Improving {DVFS} in NoCs with Coherence Prediction,Improving DVFS in NoCs with Coherence Prediction,https://dl.acm.org/doi/abs/10.1145/2786572.2786595,"In this work, we propose to utilize highly predictable properties of cache-coherence communication to derive more specific and reliable NoC traffic predictions. A DVFS mechanism based on our traffic predictions, reduces power by 41% compared to a baseline without DVFS and by 21% on average when compared to a state-of-the-art DVFS implementation"
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,dvfs-2,\cite{dvfs-2},Variation-aware dynamic voltage/frequency scaling,,,True,False,"Sebastian Herbert and
                  Diana Marculescu",2009.0,,https://doi.org/10.1109/HPCA.2009.4798265,10.1109/HPCA.2009.4798265,,Variation-aware dynamic voltage/frequency scaling,Variation-aware dynamic voltage/frequency scaling - IEEE Xplore,https://ieeexplore.ieee.org/document/4798265,Variation-aware dynamic voltage/frequency scaling Abstract: Fine-grained dynamic voltage/frequency scaling (DVFS) is an important tool in managing the balance between power and performance in chip-multiprocessors. Although manufacturing process variations are giving rise to significant core-to-core variations in power and performance
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,dvfs-3,\cite{dvfs-3},"System level analysis of fast, per-core {DVFS} using on-chip switching
                  regulators",,,True,False,"Wonyoung Kim and
                  Meeta Sharma Gupta and
                  Gu{-}Yeon Wei and
                  David M. Brooks",2008.0,,https://doi.org/10.1109/HPCA.2008.4658633,10.1109/HPCA.2008.4658633,,"System level analysis of fast, per-core {DVFS} using on-chip switching
                  regulators","System level analysis of fast, per-core DVFS using on-chip switching ...",https://www.researchgate.net/publication/224343605_System_level_analysis_of_fast_per-core_DVFS_using_on-chip_switching_regulators,Voltage regulators that are integrated onto the same chip as the microprocessor core provide the benefit of both nanosecond-scale voltage switching and per-core voltage control.
"CLONE: Customizing LLMs for Efficient Latency-Aware Inference at the
  Edge",2506.02847v1,bateni2020neuos,\cite{bateni2020neuos},$\{$NeuOS$\}$: A $\{$Latency-Predictable$\}$$\{$Multi-Dimensional$\}$ Optimization Framework for $\{$DNN-driven$\}$ Autonomous Systems,,,True,False,"Bateni, Soroush and Liu, Cong",2020.0,,,,,$\{$NeuOS$\}$: A $\{$Latency-Predictable$\}$$\{$Multi-Dimensional$\}$ Optimization Framework for $\{$DNN-driven$\}$ Autonomous Systems,USENIX ATC '20 - NeuOS: A Latency-Predictable Multi-Dimensional ...,https://www.youtube.com/watch?v=3KPzvjePQPM,"NeuOS: A Latency-Predictable Multi-Dimensional Optimization Framework for DNN-driven Autonomous SystemsSoroush Bateni and Cong Liu, University of Texas at Da"
Refining Datapath for Microscaling ViTs,2505.22194v1,zhang2018lqnets,\cite{zhang2018lqnets},LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks,,,True,False,Dongqing Zhang and Jiaolong Yang and Dongqiangzi Ye and Gang Hua,2018.0,,,,,LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks,GitHub - microsoft/LQ-Nets: LQ-Nets: Learned Quantization for Highly ...,https://github.com/Microsoft/LQ-Nets,"This repository contains the training code of LQ-Nets introduced in our ECCV 2018 paper: D. Zhang*, J. Yang*, D. Ye* and G. Hua. LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks."
Refining Datapath for Microscaling ViTs,2505.22194v1,krishnamoorthi2018quantizing,\cite{krishnamoorthi2018quantizing},Quantizing deep convolutional networks for efficient inference: A whitepaper,,,True,False,"Krishnamoorthi, Raghuraman",2018.0,,,,arXiv preprint arXiv:1806.08342,Quantizing deep convolutional networks for efficient inference: A whitepaper,Quantizing deep convolutional networks for efficient inference: A ...,https://arxiv.org/abs/1806.08342,"A whitepaper by Raghuraman Krishnamoorthi that reviews techniques for quantizing CNNs for inference with integer weights and activations. It covers post-training quantization, quantization-aware training, tools, best practices and hardware acceleration for quantized networks."
Refining Datapath for Microscaling ViTs,2505.22194v1,dai2021vs,\cite{dai2021vs},Vs-quant: Per-vector scaled quantization for accurate low-precision neural network inference,,,True,False,"Dai, Steve and Venkatesan, Rangha and Ren, Mark and Zimmer, Brian and Dally, William and Khailany, Brucek",2021.0,,,,Proceedings of Machine Learning and Systems,Vs-quant: Per-vector scaled quantization for accurate low-precision neural network inference,VS-Quant: Per-vector Scaled Quantization for Accurate Low-Precision ...,https://arxiv.org/abs/2102.04503,Abstract page for arXiv paper 2102.04503: VS-Quant: Per-vector Scaled Quantization for Accurate Low-Precision Neural Network Inference Quantization enables efficient acceleration of deep neural networks by reducing model memory footprint and exploiting low-cost integer math hardware units.
Refining Datapath for Microscaling ViTs,2505.22194v1,harma2022accuracy,\cite{harma2022accuracy},Accuracy Boosters: Epoch-Driven Mixed-Mantissa Block Floating-Point for DNN Training,,,True,False,"Harma, Simla Burcu and S{\""o}nmez, Canberk and Falsafi, Babak and Jaggi, Martin and Oh, Yunho",2022.0,,,,arXiv preprint arXiv:2211.10737,Accuracy Boosters: Epoch-Driven Mixed-Mantissa Block Floating-Point for DNN Training,Accuracy Boosters: Epoch-Driven Mixed-Mantissa Block Floating Point for ...,https://openreview.net/pdf?id=nfmfqzQ4Mwl,"the design of Accuracy Boosters, a DNN training mechanism performing a large fraction of epochs in low precision, i.e. HBFP4. Our method improves epoch-wise mixed-precision training by introducing high precision, i.e. HBFP6, to the training process only at the last epoch. Accuracy Boosters enable increasing arithmetic density by up to 21.3×"
Refining Datapath for Microscaling ViTs,2505.22194v1,darvish2020pushing,\cite{darvish2020pushing},Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point,,,True,False,"Darvish Rouhani, Bita and Lo, Daniel and Zhao, Ritchie and Liu, Ming and Fowers, Jeremy and Ovtcharov, Kalin and Vinogradsky, Anna and Massengill, Sarah and Yang, Lita and Bittner, Ray and others",2020.0,,,,Advances in neural information processing systems,Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point,A Microsoft custom data type for efficient inference,https://www.microsoft.com/en-us/research/blog/a-microsoft-custom-data-type-for-efficient-inference/,"Our latest work is detailed in a paper accepted at the 34th Conference on Neural Information Processing Systems (NeurIPS 2020), titled ""Pushing the Limits of Narrow Precision Inferencing at Cloud Scale with Microsoft Floating Point."""
Refining Datapath for Microscaling ViTs,2505.22194v1,song2020drq,\cite{song2020drq},Drq: dynamic region-based quantization for deep neural network acceleration,,,True,False,"Song, Zhuoran and Fu, Bangqi and Wu, Feiyang and Jiang, Zhaoming and Jiang, Li and Jing, Naifeng and Liang, Xiaoyao",2020.0,,,,,Drq: dynamic region-based quantization for deep neural network acceleration,DRQ: Dynamic Region-based Quantization for Deep Neural Network Acceleration,https://www.computer.org/csdl/proceedings-article/isca/2020/09138970/1lsaxGOT32g,"Quantization is an effective technique for Deep Neural Network (DNN) inference acceleration. ... In this paper, we propose a dynamic region-based quantization, namely DRQ, which can change the precision of a DNN model dynamically based on the sensitive regions in the feature map to achieve greater acceleration while reserving better NN accuracy"
Refining Datapath for Microscaling ViTs,2505.22194v1,zhao2021cambricon,\cite{zhao2021cambricon},Cambricon-Q: A hybrid architecture for efficient training,,,True,False,"Zhao, Yongwei and Liu, Chang and Du, Zidong and Guo, Qi and Hu, Xing and Zhuang, Yimin and Zhang, Zhenxing and Song, Xinkai and Li, Wei and Zhang, Xishan and others",2021.0,,,,,Cambricon-Q: A hybrid architecture for efficient training,Cambricon-Q: A Hybrid Architecture for Efficient Training,https://ieeexplore.ieee.org/document/9499944,"To address this problem, we propose the first customized architecture for efficient quantized training with negligible accuracy loss, which is named as Cambricon-Q. Cambricon-Q features a hybrid architecture consisting of an ASIC acceleration core and a near-data-processing (NDP) engine. The acceleration core mainly targets at improving the"
Refining Datapath for Microscaling ViTs,2505.22194v1,dettmers2022llm,\cite{dettmers2022llm},Llm. int8 (): 8-bit matrix multiplication for transformers at scale,,,True,False,"Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",2022.0,,,,arXiv preprint arXiv:2208.07339,Llm. int8 (): 8-bit matrix multiplication for transformers at scale,LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale,https://arxiv.org/abs/2208.07339,"We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9% of values are multiplied in 8-bit. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs."
Refining Datapath for Microscaling ViTs,2505.22194v1,liu2023psq,\cite{liu2023psq},PSQ: An Automatic Search Framework for Data-Free Quantization on PIM-based Architecture,,,True,False,"Liu, Fangxin and Yang, Ning and Jiang, Li",2023.0,,,,,PSQ: An Automatic Search Framework for Data-Free Quantization on PIM-based Architecture,SJTU,https://www.cs.sjtu.edu.cn/~jiangli/Publication.html,"Fangxin Liu, Ning Yang and Li Jiang*, ""PSQ: An Automatic Search Framework for Data-Free Quantization on PIM-based Architecture,"" to appear in IEEE International Conference on Computer Design (ICCD), 2023 (CCF-B) [PDF] 63."
Refining Datapath for Microscaling ViTs,2505.22194v1,liu2024spark,\cite{liu2024spark},SPARK: Scalable and Precision-Aware Acceleration of Neural Networks via Efficient Encoding,,,True,False,"Liu, Fangxin and Yang, Ning and Li, Haomin and Wang, Zongwu and Song, Zhuoran and Pei, Songwen and Jiang, Li",2024.0,,,,,SPARK: Scalable and Precision-Aware Acceleration of Neural Networks via Efficient Encoding,Publications - Fangxin Liu's Homepage,https://mxhx7199.github.io/publications/,"CMC: Video Transformer Acceleration via CODEC Assisted Matrix Condensing (Acceptance Rate: 24%) HPCA 2024 (Top Conf. in Computer Architecture) Fangxin Liu=, Ning Yang=, Haomin Li, Zongwu Wang, Zhuoran Song, Songwen Pei, Li Jiang SPARK: Scalable and Precision-Aware Acceleration of Neural Networks via Efficient Encoding (Acceptance Rate: 18%"
Refining Datapath for Microscaling ViTs,2505.22194v1,wu2023msd,\cite{wu2023msd},MSD: Mixing Signed Digit Representations for Hardware-efficient DNN Acceleration on FPGA with Heterogeneous Resources,,,True,False,"Wu, Jiajun and Zhou, Jiajun and Gao, Yizhao and Ding, Yuhao and Wong, Ngai and So, Hayden Kwok-Hay",2023.0,,,,,MSD: Mixing Signed Digit Representations for Hardware-efficient DNN Acceleration on FPGA with Heterogeneous Resources,MSD: Mixing Signed Digit Representations for Hardware-efficient DNN ...,https://ieeexplore.ieee.org/document/10171562,"MSD: Mixing Signed Digit Representations for Hardware-efficient DNN Acceleration on FPGA with Heterogeneous Resources Abstract: By quantizing weights with different precision for different parts of a network, mixed-precision quantization promises to reduce the hardware cost and improve the speed of deep neural network (DNN) accelerators that"
Refining Datapath for Microscaling ViTs,2505.22194v1,ham2021elsa,\cite{ham2021elsa},"ELSA: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks",,,True,False,"Ham, Tae Jun and Lee, Yejin and Seo, Seong Hoon and Kim, Soosung and Choi, Hyunji and Jung, Sung Jun and Lee, Jae W",2021.0,,,,,"ELSA: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks",ELSA | Proceedings of the 48th Annual International Symposium on ...,https://dl.acm.org/doi/10.1109/ISCA52012.2021.00060,"ELSA: hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks. Authors: Tae Jun Ham, Yejin Lee, Seong Hoon Seo, ... Thus, this paper presents ELSA (Efficient, Lightweight Self-Attention), a hardware-software co-designed solution to substantially reduce the runtime as well as energy spent on the self"
Refining Datapath for Microscaling ViTs,2505.22194v1,lu2021sanger,\cite{lu2021sanger},Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture,,,True,False,"Lu, Liqiang and Jin, Yicheng and Bi, Hangrui and Luo, Zizhang and Li, Peng and Wang, Tao and Liang, Yun",2021.0,,,,,Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture,PDF,https://liqianglu-zju.github.io/files/conference/2021/MICRO_2021_Sanger.pdf,"Transformer, attention, sparse, reconfigurable architecture, systolic array, hardware-software co-design ACM Reference Format: Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang Luo, Peng Li, Tao Wang, and Yun Liang. 2021. Sanger: A Co-Design Framework for Enabling Sparse Attention using Reconfigurable Architecture. InMICRO-54: 54th Annual"
Refining Datapath for Microscaling ViTs,2505.22194v1,qin2023fact,\cite{qin2023fact},FACT: FFN-attention Co-optimized transformer architecture with eager correlation prediction,,,True,False,"Qin, Yubin and Wang, Yang and Deng, Dazheng and Zhao, Zhiren and Yang, Xiaolong and Liu, Leibo and Wei, Shaojun and Hu, Yang and Yin, Shouyi",2023.0,,,,,FACT: FFN-attention Co-optimized transformer architecture with eager correlation prediction,FACT: FFN-Attention Co-optimized Transformer Architecture with Eager ...,https://www.mendeley.com/catalogue/9bf4ad30-134d-3300-8ef1-58a8ba662a05/,"Further, we propose FACT accelerator to efficiently support eager prediction with three designs. It avoids the large overhead of prediction by using log-based add-only operations for prediction. It eliminates the latency of prediction through an out-of-order scheduler that makes the eager prediction and computation work in full pipeline."
Refining Datapath for Microscaling ViTs,2505.22194v1,li2022auto,\cite{li2022auto},Auto-vit-acc: An fpga-aware automatic acceleration framework for vision transformer with mixed-scheme quantization,,,True,False,"Li, Zhengang and Sun, Mengshu and Lu, Alec and Ma, Haoyu and Yuan, Geng and Xie, Yanyue and Tang, Hao and Li, Yanyu and Leeser, Miriam and Wang, Zhangyang and others",2022.0,,,,,Auto-vit-acc: An fpga-aware automatic acceleration framework for vision transformer with mixed-scheme quantization,Late Breaking Results: FPGA-Aware Automatic Acceleration,https://par.nsf.gov/servlets/purl/10323475,"Figure 1: Overview of Auto-ViT-Acc. This work develops Auto-ViT-Acc, an FPGA-aware automatic acceleration framework for ViTs with mixed-scheme quantization, where fixed-point (Fixed) and power-of-two (PoT) quantization schemes are combined and assigned down to the row (weight filter) level for each layer. The Fixed scheme is used for preserving the"
Refining Datapath for Microscaling ViTs,2505.22194v1,huang2023integer,\cite{huang2023integer},An Integer-Only and Group-Vector Systolic Accelerator for Efficiently Mapping Vision Transformer on Edge,,,True,False,"Huang, Mingqiang and Luo, Junyi and Ding, Chenchen and Wei, Zikun and Huang, Sixiao and Yu, Hao",2023.0,,,,IEEE Transactions on Circuits and Systems I: Regular Papers,An Integer-Only and Group-Vector Systolic Accelerator for Efficiently Mapping Vision Transformer on Edge,Awesome Vit Quantization and Acceleration - GitHub,https://github.com/DD-DuDa/awesome-vit-quantization-acceleration,"""An Integer-Only and Group-Vector Systolic Accelerator for Efficiently Mapping Vision Transformer on Edge"" ... , title={Model Quantization and Hardware Acceleration for Vision Transformers: A Comprehensive Survey}, author={Dayou Du and Gu Gong and Xiaowen Chu}, year={2024}, eprint={2405.00314}, archivePrefix={arXiv}, primaryClass={cs.LG} }"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,DBLP:conf/rtss/BrandenburgG16,\cite{DBLP:conf/rtss/BrandenburgG16},"Global Scheduling Not Required: Simple, Near-Optimal Multiprocessor
                  Real-Time Scheduling with Semi-Partitioned Reservations",,,True,False,"Bj{\""{o}}rn B. Brandenburg and
                  Mahircan Gul",2016.0,,,,,"Global Scheduling Not Required: Simple, Near-Optimal Multiprocessor
                  Real-Time Scheduling with Semi-Partitioned Reservations",PDF,https://people.mpi-sws.org/~bbb/papers/pdf/rtss16b.pdf,"Global Scheduling Not Required: Simple, Near-Optimal Multiprocessor Real-Time Scheduling with Semi-Partitioned Reservations Björn B. Brandenburg Mahircan Gül Max Planck Institute for Software Systems (MPI-SWS) Abstract—Prior work has identiﬁed several optimal algorithms for scheduling independent, implicit-deadline sporadic (or peri-"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,ekberg2021partitioned,\cite{ekberg2021partitioned},Partitioned Scheduling of Recurrent Real-Time Tasks,,,True,False,"Ekberg, Pontus and Baruah, Sanjoy",2021.0,,,,,Partitioned Scheduling of Recurrent Real-Time Tasks,Partitioned Scheduling and Parallelism Assignment for Real-Time DNN ...,https://dl.acm.org/doi/10.1145/3649329.3655979,"Partitioned Scheduling of Recurrent Real-Time Tasks. In RTSS. 356--367. Google Scholar [8] ... Existing real-time multicore schedulers use either global or partitioned scheduling technique to schedule real-time tasks. Partitioned scheduling is a static approach in which, a task is mapped to a per-processor ready queue prior to scheduling it and"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Burchard:1995,\cite{Burchard:1995},New strategies for assigning real-time tasks to multiprocessor systems,,,True,False,"Burchard, Almut and Liebeherr, Jörg and Oh, Yingfeng and Son, Sang H.",1995.0,,,,IEEE Transactions on Computers,New strategies for assigning real-time tasks to multiprocessor systems,New Strategies for Assigning Real-Time Tasks to Multiprocessor Systems,https://www.computer.org/csdl/journal/tc/1995/12/t1429/13rRUwd9CF8,"Abstract—Optimal scheduling of real-time tasks on multiprocessor systems is known to be computationally intractable for large task sets. Any practical scheduling algorithm for assigning real-time tasks to a multiprocessor system presents a trade-off between its computational complexity and its performance. In this study, new schedulability conditions are presented for homogeneous"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Dhall:1978,\cite{Dhall:1978},On a real-time scheduling problem,,,True,False,"Dhall, Sudarshan K and Liu, Chung Laung",1978.0,,,,Operations research,On a real-time scheduling problem,Real-Time Scheduling - SpringerLink,https://link.springer.com/chapter/10.1007/978-3-031-11992-7_10,"Finally, the scheduling problem in distributed systems is touched and some ideas about alternative scheduling strategies such as feedback scheduling are given. 10.1 The Scheduling Problem A hard real-time system must execute a set of concurrent real-time tasks in such a way that all time-critical tasks meet their specified deadlines."
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Baruah:2005,\cite{Baruah:2005},IEEE International Real-Time Systems Symposium (RTSS),,,True,False,"Baruah, Sanjoy and Fisher, Nathan",2005.0,,,,,IEEE International Real-Time Systems Symposium (RTSS),Rtss 2024 - Rtss 2024,https://2024.rtss.org/,"45th IEEE Real-Time Systems Symposium December 10-13, 2024 / York, United Kingdom The IEEE Real-Time Systems Symposium (RTSS) is the premier conference in the field of real-time systems and is a venue for researchers and practitioners to showcase innovations covering all aspects of real-time systems, including theory, design, analysis"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Lopez:2000,\cite{Lopez:2000},Euromicro Conference on Real-Time Systems (ECRTS),,,True,False,"José María López and Garcia, Manuel and Díaz, Jose and Garcia, Frk Daniel",2000.0,,,,,Euromicro Conference on Real-Time Systems (ECRTS),ECRTS 2025 - 37th Euromicro Conference on Real-Time Systems,https://www.ecrts.org/,"Welcome to the 37th Euromicro Conference on Real-Time Systems to be held in Brussels, Belgium. ECRTS will be held as a physical conference but will also offer streaming possibilities for those that cannot join us in Brussels in person. ... industry challenge, and real-time pitches. ECRTS 2025 will have a workshop day with a number of exciting"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Fisher:2006,\cite{Fisher:2006},The partitioned multiprocessor scheduling of non-preemptive sporadic task systems,,,True,False,"Fisher, Nathan and Baruah, Sanjoy",2006.0,,,,,The partitioned multiprocessor scheduling of non-preemptive sporadic task systems,The partitioned multiprocessor scheduling of sporadic task systems ...,https://ieeexplore.ieee.org/document/1563119,"A polynomial-time algorithm is presented for partitioning a collection of sporadic tasks among the processors of an identical multiprocessor platform. Since the partitioning problem is NP-hard in the strong sense, this algorithm is unlikely to be optimal. A quantitative characterization of its worst-case performance is provided in terms of resource augmentation; it is shown that any set of"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Senoussaoui:2020,\cite{Senoussaoui:2020},Allocation of Real-Time Tasks onto Identical Core Platforms under Deferred fixed Preemption-Point Model,,,True,False,"Senoussaoui, Ikram and Zahaf, Houssam-Eddine and Benhaoua, Mohammed Kamel and Lipari, Giuseppe and Olejnik, Richard",2020.0,,,10.1145/3394810.3394821,,Allocation of Real-Time Tasks onto Identical Core Platforms under Deferred fixed Preemption-Point Model,Allocation of Real-Time Tasks onto Identical Core Platforms under ...,https://hal.science/hal-02886816/document,"Olejnik. Allocation of Real-Time Tasks onto Identical Core Platforms under Deferred fixed Preemption-Point Model. RTNS 2020: 28th International Conference on Real-Time Networks and Systems, Jun 2020, Paris France, France. pp.34-43, �10.1145/3394810.3394821�. �hal-02886816�"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,fonseca2016response,\cite{fonseca2016response},Response time analysis of sporadic DAG tasks under partitioned scheduling,,,True,False,"Fonseca, Jos{\'e} and Nelissen, Geoffrey and Nelis, Vincent and Pinho, Lu{\'\i}s Miguel",2016.0,,,,,Response time analysis of sporadic DAG tasks under partitioned scheduling,Response time analysis of sporadic DAG tasks under partitioned ...,https://ieeexplore.ieee.org/document/7509443,"This paper considers instead the schedulability of partitioned parallel tasks. More precisely, we present a response time analysis for sporadic DAG tasks atop multiprocessors under partitioned fixed-priority scheduling. We assume the partitioning to be given. We show that a partitioned DAG task can be modeled as a set of self-suspending tasks."
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,casini2018partitioned,\cite{casini2018partitioned},Partitioned fixed-priority scheduling of parallel tasks without preemptions,,,True,False,"Casini, Daniel and Biondi, Alessandro and Nelissen, Geoffrey and Buttazzo, Giorgio",2018.0,,,,,Partitioned fixed-priority scheduling of parallel tasks without preemptions,Partitioned Fixed-Priority Scheduling of Parallel Tasks Without Preemptions,https://ieeexplore.ieee.org/document/8603232,"The study of parallel task models executed with predictable scheduling approaches is a fundamental problem for real-time multiprocessor systems. Nevertheless, to date, limited efforts have been spent in analyzing the combination of partitioned scheduling and non-preemptive execution, which is arguably one of the most predictable schemes that can be envisaged to handle parallel tasks. This"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Zahaf:2020,\cite{Zahaf:2020},2020 IEEE 26th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA),,,True,False,"Zahaf, Houssam-Eddine and Lipari, Giuseppe and Niar, Smail and Hassan Benyamina, Abou El",2020.0,,,10.1109/RTCSA50079.2020.9203643,,2020 IEEE 26th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA),RTCSA 2020. The 26th IEEE International Conference on Embedded and Real ...,https://rtcsa2020.github.io/index/,"The safety and well-being of all conference participants is our top priority. After studying and evaluating the announcements, guidance, and news released by relevant national departments, we have made the decision that RTCSA 2020 will continue as scheduled on 19-21 August, by being converted to an exciting, fully virtual conference, and will not take place physically in Korea."
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Ueter:2021,\cite{Ueter:2021},{Hard Real-Time Stationary GANG-Scheduling},,,True,False,"Ueter, Niklas and G\""{u}nzel, Mario and von der Br\""{u}ggen, Georg and Chen, Jian-Jia",2021.0,,,10.4230/LIPIcs.ECRTS.2021.10,,{Hard Real-Time Stationary GANG-Scheduling},Hard Real-Time Stationary GANG-Scheduling - Dagstuhl,https://drops.dagstuhl.de/entities/document/10.4230/LIPIcs.ECRTS.2021.10,"The scheduling of parallel real-time tasks enables the efficient utilization of modern multiprocessor platforms for systems with real-time constrains. In this situation, the gang task model, in which each parallel sub-job has to be executed simultaneously, has shown significant performance benefits due to reduced context switches and more efficient intra-task synchronization."
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,sun2024partitioned,\cite{sun2024partitioned},Partitioned scheduling and parallelism assignment for real-time DNN inference tasks on multi-TPU,,,True,False,"Sun, Binqi and Kloda, Tomasz and Wu, Chu-ge and Caccamo, Marco",2024.0,,,,,Partitioned scheduling and parallelism assignment for real-time DNN inference tasks on multi-TPU,Binqi Sun,https://binqi-sun.github.io/,"Our paper: Minimizing cache usage with fixed-priority and earliest deadline first scheduling is accepted for publication in Real-Time Systems. Feb 27, 2024: I will present our paper: Partitioned Scheduling and Parallelism Assignment for Real-Time DNN Inference Tasks on Multi-TPU in DAC 2024, San Francisco, USA. Jan 21, 2024"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,abeni2022partitioning,\cite{abeni2022partitioning},Partitioning real-time workloads on multi-core virtual machines,,,True,False,"Abeni, Luca and Biondi, Alessandro and Bini, Enrico",2022.0,,,,Journal of Systems Architecture,Partitioning real-time workloads on multi-core virtual machines,Partitioning real-time workloads on multi-core virtual machines ...,https://www.sciencedirect.com/science/article/pii/S1383762122002181,"Real-time scheduling theory already provides techniques for analyzing the schedulability of real-time applications executed in virtual machines, but most of the previous work focused on global scheduling while, excluding a few exceptions, the problem of partitioning real-time workloads on multi-core VMs has not been properly investigated yet."
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Mo:2023,\cite{Mo:2023},Energy Optimized Task Mapping for Reliable and Real-Time Networked Systems,,,True,False,"Mo, Lei and Zhou, Qi and Kritikakou, Angeliki and Cao, Xianghui",2023.0,,,10.1145/3584985,ACM Trans. Sen. Netw.,Energy Optimized Task Mapping for Reliable and Real-Time Networked Systems,Energy Optimized Task Mapping for Reliable and Real-Time Networked Systems,https://dl.acm.org/doi/10.1145/3584985,"Energy efficiency, real-time response, and data transmission reliability are important objectives during networked systems design. This paper aims to develop an efficient task mapping scheme to balance these important but conflicting objectives. To"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,MDBCCP:13,\cite{MDBCCP:13},{IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)},,,True,False,"Mancuso, Renato and Dudko, Roman and Betti, Emiliano and Cesati, Marco and Caccamo, Marco and Pellizzoni, Rodolfo",2013.0,,,,,{IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)},Rtas 2025 - Rtas 2025,https://2025.rtas.org/,"RTAS 2025. 31st IEEE Real-Time and Embedded Technology and Applications Symposium. RTAS is a top-tier conference with a focus on systems with timing requirements. RTAS'25 welcomes papers describing applications, case studies, methodologies, tools, algorithms or operating systems, middleware or hardware innovations that contribute to the state"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Kim16:EMSOFT,\cite{Kim16:EMSOFT},International Conference on Embedded Software (EMSOFT),,,True,False,"Kim, Hyoseung and Rajkumar, Ragunathan",2016.0,,,,,International Conference on Embedded Software (EMSOFT),EMSOFT - Embedded Systems Week,https://esweek.org/emsoft/,"EMSOFT: International Conference on Embedded Software. The ACM SIGBED International Conference on Embedded Software (EMSOFT) brings together researchers and developers from academia, industry, and government to advance the science, engineering, and technology of embedded software development. Since 2001, EMSOFT has been the premier venue for"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,KWCFAS:17,\cite{KWCFAS:17},Attacking the One-Out-Of-m Multicore Problem by Combining Hardware Management with Mixed-Criticality Provisioning,,,True,False,"Kim, Namhoon and Ward, Bryan C. and Chisholm, Micaiah and Fu, Cheng-Yang and Anderson, James H. and Smith, F. Donelson",2017.0,,,,Real-Time Systems,Attacking the One-Out-Of-m Multicore Problem by Combining Hardware Management with Mixed-Criticality Provisioning,PDF,https://www.cs.unc.edu/~anderson/papers/rtj17.pdf,"Attacking the One-Out-Of-m Multicore Problem by Combining Hardware Management with Mixed-Criticality Provisioning Namhoon Kim Bryan C. Ward Micaiah Chisholm James H. Anderson F. Donelson Smith Abstract The multicore revolution is having limited impact in safety-critical applica-tion domains. A key reason is the ""one-out-of-m"" problem: when"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,KSMCV:19,\cite{KSMCV:19},IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),,,True,False,Tomasz {Kloda} and Marco  {Solieri} and Renato {Mancuso} and Nicola {Capodieci} and Paolo {Valente} and Marko {Bertogna},2019.0,,,,,IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),RTAS 2024 - 30th IEEE Real-Time and Embedded Technology and ...,https://2024.rtas.org/,"30th IEEE Real-Time and Embedded Technology and Applications Symposium RTAS is a top-tier conference with a focus on time-sensitive systems . RTAS'24 invites papers describing case studies, applications, methodologies, and algorithms that contribute to the state of practice in design, implementation, verification, validation, and evolution of"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,xilinx-xen-cache-color,\cite{xilinx-xen-cache-color},{Xilinx Xen Support with Cache-Coloring},,,True,False,Xilinx,,,,,,{Xilinx Xen Support with Cache-Coloring},Cache Coloring: Interference-free Real-time Virtualization - Xen Project,https://xenproject.org/blog/cache-coloring-interference-free-real-time-virtualization/,"At this year's Xen Developer and Design Summit, Stefano Stabellini from Xilinx gave a talk on Cache Coloring, a new feature for Xen that helps better support real-time workloads. Many embedded deployments require deterministic IRQ latency, which can be difficult to pull off as even small spikes lead to failure."
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,minerva-jailhouse,\cite{minerva-jailhouse},{Memory-aware Jailhouse hypervisor},,,True,False,{Minerva Systems},,,,,,{Memory-aware Jailhouse hypervisor},虚拟化 之一 详解 jailhouse 架构及原理、软硬件要求、源码文件、基本组件 - 技术栈,https://jishuzhan.net/article/1801433120067555329,为此，Jailhouse 通过虚拟 ivshmem（Inter-VM shared memory） PCI 设备在 Cell 之间提供共享内存和信号机制。一个通道将两个分区 1：1 对应地连接起来。 环境要求. Jailhouse 是一个依托于 Linux Kernel 的开放性从而直接使用硬件平台提供的虚拟化技术来实现的虚拟化解决方案。
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Survey-Way-Part,\cite{Survey-Way-Part},IEEE Silchar Subsection Conference (SILCON),,,True,False,"Das, Purnendu and Barbhuiya, Nurulla Mansur and Ranjan Roy, Bishwa",2023.0,,,,,IEEE Silchar Subsection Conference (SILCON),Ieee Silcon 2025,http://ieeesilcon.org.in/,"IEEE SILCON is the Flagship Annual International Conference of IEEE Silchar Subsection. IEEE SILCON conference series targets to bring forth the researchers, academicians, professionals, industry delegates and students in the region, covering all the technical areas related to IEEE. The SILCON targets to offer a platform for research collaboration, networking, and presentation of recent"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,arm-dynamiciq,\cite{arm-dynamiciq},{Arm DynamIQ Shared Unit Technical Reference Manual},,,True,False,Arm,,,,,,{Arm DynamIQ Shared Unit Technical Reference Manual},Arm® DynamIQ™ Shared Unit Technical Reference Manual,https://developer.arm.com/documentation/100453/0401,Arm DynamIQ Shared Unit Technical Reference Manual. Introduction. The DynamIQ Shared Unit. Technical overview. Clocks and resets. Power management. L3 cache. ... Arm® DynamIQ™ Shared Unit Technical Reference Manual. Revision: r4p1. Release Information. Issue Date Confidentiality Change; 0000-00: 30 September 2016: Confidential: First release
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,yun2013memguard,\cite{yun2013memguard},{MemGuard}: Memory bandwidth reservation system for efficient performance isolation in multi-core platforms,,,True,False,"Yun, Heechul and Yao, Gang and Pellizzoni, Rodolfo and Caccamo, Marco and Sha, Lui",2013.0,,,,,{MemGuard}: Memory bandwidth reservation system for efficient performance isolation in multi-core platforms,Memory Bandwidth Reservation System for Efficient Performance Isolation ...,https://github.com/heechul/memguard,Memory Bandwidth Reservation System for Efficient Performance Isolation in Multi-core Processors - heechul/memguard ... MemGuard is a memory bandwidth reservation system for multi-core platforms. ChangeLog. Dec 2022 5.15+ kernel support; May 2022 read/write separate reservation (from RTAS'19)
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,MemPol,\cite{MemPol},{MemPol}: Policing Core Memory Bandwidth from Outside of the Cores,,,True,False,"Alexander Zuepke and
                  Andrea Bastoni and
                  Weifan Chen and
                  Marco Caccamo and
                  Renato Mancuso",2023.0,,,,,{MemPol}: Policing Core Memory Bandwidth from Outside of the Cores,MemPol: Policing Core Memory Bandwidth from Outside of the Cores,https://ieeexplore.ieee.org/abstract/document/10155717,"MemPol: Policing Core Memory Bandwidth from Outside of the Cores Abstract: In today's multiprocessor systems-on-a-chip (MP- SoC), the shared memory subsystem is a known source of temporal interference. The problem causes logically independent cores to affect each other's performance, leading to pessimistic worstcase execution time (WCET"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,hassan2019reduced,\cite{hassan2019reduced},{Reduced latency DRAM for multi-core safety-critical real-time systems},,,True,False,"Hassan, Mohamed",2019.0,,,,Real-Time Systems,{Reduced latency DRAM for multi-core safety-critical real-time systems},Reduced latency DRAM for multi-core safety-critical real-time systems,https://link.springer.com/article/10.1007/s11241-019-09338-8,"Predictable execution time upon accessing shared memories in multi-core real-time systems is a stringent requirement. A plethora of existing works focus on the analysis of Double Data Rate Dynamic Random Access Memories (DDR DRAMs), or redesigning its memory to provide predictable memory behavior. In this paper, we show that DDR DRAMs by construction suffer inherent limitations associated with"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,BRU:20,\cite{BRU:20},IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),,,True,False,"Farshchi, Farzad and Huang, Qijing and Yun, Heechul",2020.0,,,,,IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),RTAS 2024 - 30th IEEE Real-Time and Embedded Technology and ...,https://2024.rtas.org/,"30th IEEE Real-Time and Embedded Technology and Applications Symposium RTAS is a top-tier conference with a focus on time-sensitive systems . RTAS'24 invites papers describing case studies, applications, methodologies, and algorithms that contribute to the state of practice in design, implementation, verification, validation, and evolution of"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,intel-rdt,\cite{intel-rdt},{Resource Director Technology},,,True,False,Intel,,,,,,{Resource Director Technology},Intel® Resource Director Technology (Intel® RDT),https://www.intel.com/content/www/us/en/architecture-and-technology/resource-director-technology.html,"As software-defined infrastructure and advanced resource-aware orchestration technologies increasingly transform the industry, Intel® RDT is a key feature set to optimize application performance and enhance the capabilities of orchestration and virtualization management server systems using Intel® Xeon® processors. With Intel® Resource Director Technology (Intel® RDT), software-guided hardware capabilities intelligently monitor and control the allocation of key shared system resources to help ensure quality of service (QoS) and deliver monitoring insight and control where you need it. If you visit an Intel Site or service using a device with a United States IP address, you have not enabled a privacy-preference signal, and your Ad Targeting and Sharing Choices are set to Active, Intel may collect device information for the purpose of Ad Targeting uses."
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,XPCLLLL:19,\cite{XPCLLLL:19},IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),,,True,False,"Xu, Meng and Phan, Linh Thi Xuan and Choi, Hyon-Young and Lin, Yuhan and Li, Haoran and Lu, Chenyang and Lee, Insup",2019.0,,,,,IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),RTAS 2024 - 30th IEEE Real-Time and Embedded Technology and ...,https://2024.rtas.org/,"30th IEEE Real-Time and Embedded Technology and Applications Symposium RTAS is a top-tier conference with a focus on time-sensitive systems . RTAS'24 invites papers describing case studies, applications, methodologies, and algorithms that contribute to the state of practice in design, implementation, verification, validation, and evolution of"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,SBMYK:22,\cite{SBMYK:22},Proceedings of the 30th International Conference on Real-Time Networks and Systems (RTNS),,,True,False,"Sohal, Parul and Bechtel, Michael and Mancuso, Renato and Yun, Heechul and Krieger, Orran",2022.0,,,,,Proceedings of the 30th International Conference on Real-Time Networks and Systems (RTNS),Proceedings of the 30th International Conference on Real-Time Networks ...,https://dl.acm.org/doi/proceedings/10.1145/3534879,"RTNS '22: Proceedings of the 30th International Conference on Real-Time Networks and Systems. June 2022. Read More. 2022 Proceeding. Editors: ... RTNS 2022: The 30th International Conference on Real-Time Networks and Systems Paris France June 7 - 8, 2022. ISBN: 978-1-4503-9650-9"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,arm-mpam,\cite{arm-mpam},{Arm Memory System Resource Partitioning and Monitoring (MPAM) System Component Specification},,,True,False,Arm,,,,,,{Arm Memory System Resource Partitioning and Monitoring (MPAM) System Component Specification},Documentation - Arm Developer,https://developer.arm.com/documentation/107768/0100/Overview,Arm Architecture Reference Manual for A-profile architecture: Covers the processor MPAM features and behaviors. Arm Memory System Resource Partitioning and Monitoring (MPAM) System Component Specification: Covers the system impacts of MPAM and describes the interface used by memory system components (MSC).
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Altmeyer:2014,\cite{Altmeyer:2014},2014 26th Euromicro Conference on Real-Time Systems,,,True,False,"Altmeyer, Sebastian and Douma, Roeland and Lunniss, Will and Davis, Robert I.",2014.0,,,10.1109/ECRTS.2014.11,,2014 26th Euromicro Conference on Real-Time Systems,2014 26th Euromicro Conference on Real-Time Systems (ECRTS) - Computer,https://www.computer.org/csdl/proceedings/ecrts/2014/12OmNx5GU2x,"Conference Proceedings; Institutional Subscriptions ... Cart; Advanced Search. 2014 26th Euromicro Conference on Real-Time Systems (ECRTS) July 8 2014 to July 11 2014. Madrid, Spain. Table of Contents. Dynamic Command Scheduling for Real-Time Memory Controllers pp. 3-14. OUTSTANDING PAPER: Evaluation of Cache Partitioning for Hard Real-Time"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Altmeyer:2016,\cite{Altmeyer:2016},On the effectiveness of cache partitioning in hard real-time systems,,,True,False,Sebastian A. Altmeyer and Roeland Douma and Will Lunniss and Robert I. Davis,2016.0,,,,Real Time Systems,On the effectiveness of cache partitioning in hard real-time systems,On the effectiveness of cache partitioning in hard real-time systems ...,https://link.springer.com/article/10.1007/s11241-015-9246-8,"In hard real-time systems, cache partitioning is often suggested as a means of increasing the predictability of caches in pre-emptively scheduled systems: when a task is assigned its own cache partition, inter-task cache eviction is avoided, and timing verification is reduced to the standard worst-case execution time analysis used in non-pre-emptive systems. The downside of cache partitioning"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Bui:2008,\cite{Bui:2008},IEEE International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA),,,True,False,"Bui, Bach D. and Caccamo, Marco and Sha, Lui and Martinez, Joseph",2008.0,,,,,IEEE International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA),(RTCSA 2025) 2025 IEEE 31st International Conference on Embedded and ...,https://www.iconf.org/conference/rtcsa2025,"IEEE RTCSA 2025, the 31st IEEE International Conference on Embedded and Real-Time Computing Systems and Application, will be held in Singapore on August 20-22, 2025. It will be co-located with NVMSA 2025. Accepted papers will be submitted for inclusion into IEEE Xplore, subject to meeting IEEE Xplore's scope and quality requirements."
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Meroni:2023,\cite{Meroni:2023},2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA),,,True,False,"Meroni, Carlo and Craciunas, Silviu S. and Finzi, Anaïs and Pop, Paul",2023.0,,,10.1109/ETFA54631.2023.10275547,,2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA),ETFA: Emerging Technologies and Factory Automation,http://www.wikicfp.com/cfp/program?id=937,"ETFA 2023 is the 28th Annual Conference of the IEEE Industrial Electronics Society (IES) focusing on the latest developments and new technologies in the field of industrial and factory automation. The conference aims to disseminate novel ideas and emerging trends, research results and practical achievements."
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,sun2023minimizing,\cite{sun2023minimizing},Minimizing cache usage for real-time systems,,,True,False,"Sun, Binqi and Kloda, Tomasz and Arribas Garcia, Sergio and Gracioli, Giovani and Caccamo, Marco",2023.0,,,,,Minimizing cache usage for real-time systems,Minimizing cache usage with fixed-priority and earliest ... - Springer,https://link.springer.com/article/10.1007/s11241-024-09423-7,"We review different cache partitioning techniques and discuss how these techniques can be adapted to minimize cache usage in real-time systems. In single-core systems, the allocation of cache segments to different real-time tasks can reduce the preemption cost related to the cache (Kirk 1989; Kirk et al. 1991; Kim et al. 2013)."
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,sun2024minimizing,\cite{sun2024minimizing},Minimizing cache usage with fixed-priority and earliest deadline first scheduling,,,True,False,"Sun, Binqi and Kloda, Tomasz and Garcia, Sergio Arribas and Gracioli, Giovani and Caccamo, Marco",2024.0,,,,Real-Time Systems,Minimizing cache usage with fixed-priority and earliest deadline first scheduling,Minimizing Cache Usage for Real-time Systems,https://dl.acm.org/doi/10.1145/3575757.3593651,Sun B Kloda T Garcia S Gracioli G Caccamo M (2024) Minimizing cache usage with fixed-priority and earliest deadline first scheduling Real-Time Systems 10.1007/s11241-024-09423-7 60:4 (625-664) Online publication date: 28-Jun-2024
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,aghilinasab2020dynamic,\cite{aghilinasab2020dynamic},Dynamic memory bandwidth allocation for real-time GPU-based SoC platforms,,,True,False,"Aghilinasab, Homa and Ali, Waqar and Yun, Heechul and Pellizzoni, Rodolfo",2020.0,,,,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,Dynamic memory bandwidth allocation for real-time GPU-based SoC platforms,Dynamic Memory Bandwidth Allocation for Real-Time GPU-Based SoC Platforms,https://par.nsf.gov/servlets/purl/10267567,"effort applications. In this paper, we propose a novel memory bandwidth allocation scheme where we dynamically monitor the progress of a real-time application and increase the bandwidth share of best-effort ones whenever it is safe to do so. Speciﬁcally, we demonstrate our approach by protecting a real-time GPU ker-nel from best-effort CPU tasks."
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,park2019copart,\cite{park2019copart},Copart: Coordinated partitioning of last-level cache and memory bandwidth for fairness-aware workload consolidation on commodity servers,,,True,False,"Park, Jinsu and Park, Seongbeom and Baek, Woongki",2019.0,,,,,Copart: Coordinated partitioning of last-level cache and memory bandwidth for fairness-aware workload consolidation on commodity servers,CoPart: Coordinated Partitioning of Last-Level Cache and Memory ...,https://scispace.com/papers/copart-coordinated-partitioning-of-last-level-cache-and-1bfgibvm1b,(DOI: 10.1145/3302424.3303963) Workload consolidation is a widely-used technique to maximize server resource utilization in cloud and datacenter computing Recent commodity CPUs support last-level cache (LLC) and memory bandwidth partitioning functionalities that can be used to ensure the fairness of the consolidated workloads While prior work has proposed a variety of resource partitioning
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,CWKA:15,\cite{CWKA:15},IEEE Real-Time Systems Symposium (RTSS),,,True,False,"Chisholm, Micaiah and Ward, Bryan C. and Kim, Namhoon and Anderson, James H.",2015.0,,,,,IEEE Real-Time Systems Symposium (RTSS),Rtss 2024 - Rtss 2024,https://2024.rtss.org/,"The IEEE Real-Time Systems Symposium (RTSS) is the premier conference in the field of real-time systems and is a venue for researchers and practitioners to showcase innovations covering all aspects of real-time systems, including theory, design, analysis, implementation, evaluation, and experience. RTSS ’24, celebrating the 45th anniversary of the event, continues the trend of making RTSS an expansive and inclusive event, striving to embrace new and emerging areas of real-time systems research. Best Paper Award IEEE Guidelines for Artificial Intelligence (AI)-Generated Text: The use of artificial intelligence (AI)–generated text in an article shall be disclosed in the acknowledgements section of any paper submitted to an IEEE Conference or Periodical. Paper Submission Deadline | Thursday, May 23, 2024 Submission of camera-ready papers: | Friday, October 4, 2024 Author Registration Deadline | Sunday, October 27, 2024"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Berna:2012,\cite{Berna:2012},{PDPA}: Period Driven Task and Cache Partitioning Algorithm for Multi-Core Systems,,,True,False,"Berna, Brice and Puaut, Isabelle",2012.0,,,,,{PDPA}: Period Driven Task and Cache Partitioning Algorithm for Multi-Core Systems,PDF,https://www.irisa.fr/alf/downloads/puaut/papers/RTNS2012pdpa.pdf,"In this work we present PDPA, for Period Driven Parti-tioning Algorithm, a joint task and cache partitioning algo-rithm for multi-core systems scheduled using non-preemptive Earliest Deadline First (NP-EDF). On the one hand, PDPA takes advantage of relationships between tasks periods for partitioning tasks among cores. On the other hand, tasks"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Paolieri:2011,\cite{Paolieri:2011},IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),,,True,False,"Paolieri, Marco and Quiñones, Eduardo and Cazorla, Francisco J. and Davis, Robert I. and Valero, Mateo",2011.0,,,,,IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS),RTAS 2024 - 30th IEEE Real-Time and Embedded Technology and ...,https://2024.rtas.org/,"30th IEEE Real-Time and Embedded Technology and Applications Symposium RTAS is a top-tier conference with a focus on time-sensitive systems . RTAS'24 invites papers describing case studies, applications, methodologies, and algorithms that contribute to the state of practice in design, implementation, verification, validation, and evolution of"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Meng:2019,\cite{Meng:2019},Holistic multi-resource allocation for multicore real-time virtualization,,,True,False,"Xu, Meng and Gifford, Robert and Phan, Linh Thi Xuan",2019.0,,,,,Holistic multi-resource allocation for multicore real-time virtualization,Holistic multi-resource allocation for multicore real-time virtualization,https://ieeexplore.ieee.org/document/8807010,"Abstract: This paper presents vC 2 M, a holistic multi-resource allocation framework for real-time multicore virtualization. vC 2 M integrates shared cache allocation with memory bandwidth regulation to mitigate interferences among concurrent tasks, thus providing better timing isolation among tasks and VMs. It reduces the abstraction overhead through task and VCPU release synchronization and"
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,Nie:2022,\cite{Nie:2022},Holistic Resource Allocation Under Federated Scheduling for Parallel Real-time Tasks,,,True,False,"Nie, Lanshun and Fan, Chenghao and Lin, Shuang and Zhang, Li and Li, Yajuan and Li, Jing",2022.0,,,,ACM Trans. Embed. Comput. Syst.,Holistic Resource Allocation Under Federated Scheduling for Parallel Real-time Tasks,Holistic Resource Allocation Under Federated Scheduling for Parallel ...,https://dl.acm.org/doi/10.1145/3489467,"Such resource interferences due to concurrent accesses can be even more severe for embedded platforms or edge servers, where the computing power and cache/memory space are limited. To tackle this issue, in this work, we present a holistic resource allocation framework for parallel real-time tasks under federated scheduling."
"Multi-Objective Memory Bandwidth Regulation and Cache Partitioning for
  Multicore Real-Time Systems",2505.11554v1,gifford2021dna,\cite{gifford2021dna},{DNA}: Dynamic resource allocation for soft real-time multicore systems,,,True,False,"Gifford, Robert and Gandhi, Neeraj and Phan, Linh Thi Xuan and Haeberlen, Andreas",2021.0,,,,,{DNA}: Dynamic resource allocation for soft real-time multicore systems,Unlocking Multi-Core Potential: Robert Gifford's Breakthroughs in Real ...,https://ai.seas.upenn.edu/news/unlocking-multi-core-potential-robert-giffords-breakthroughs-in-real-time-system-safety/,"Gifford's first major contribution is Dynamic Resource Allocation (DNA), a groundbreaking technique for distributing resources in soft real-time multicore systems. Before DNA, these systems typically used a static approach — shared resources like CPU, cache and memory bandwidth were allocated to computational tasks only once, using their"
"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome
  Assembly",2505.08071v1,zhou2021ultra,\cite{zhou2021ultra},Ultra efficient acceleration for de novo genome assembly via near-memory computing,,,True,False,"Zhou, Minxuan and Wu, Lingxi and Li, Muzhou and Moshiri, Niema and Skadron, Kevin and Rosing, Tajana",2021.0,,,,,Ultra efficient acceleration for de novo genome assembly via near-memory computing,Ultra Efficient Acceleration for De Novo Genome Assembly via Near ...,https://par.nsf.gov/servlets/purl/10345546,"Ultra Efﬁcient Acceleration for De Novo Genome Assembly via Near-Memory Computing Minxuan Zhou§ University of California, San Diego miz087@eng.ucsd.edu Lingxi Wu§ University of Virginia lw2ef@virginia.edu Muzhou Li University of California, San Diego mul023@eng.ucsd.edu Niema Moshiri University of California, San Diego niema@ucsd.edu Kevin"
"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome
  Assembly",2505.08071v1,angizi2020pim,\cite{angizi2020pim},Pim-assembler: A processing-in-memory platform for genome assembly,,,True,False,"Angizi, Shaahin and Fahmi, Naima Ahmed and Zhang, Wei and Fan, Deliang",2020.0,,,,,Pim-assembler: A processing-in-memory platform for genome assembly,PIM-Assembler: A Processing-in-Memory Platform for Genome Assembly ...,https://ieeexplore.ieee.org/document/9218653,"Abstract: In this paper, for the first time, we propose a high-throughput and energy-efficient Processing-in-DRAM-accelerated genome assembler called PIM-Assembler based on an optimized and hardware-friendly genome assembly algorithm. PIM-Assembler can assemble large-scale DNA sequence dataset from all-pair overlaps. We first develop PIM-Assembler platform that harnesses DRAM as computational"
"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome
  Assembly",2505.08071v1,wu2024abakus,\cite{wu2024abakus},Abakus: Accelerating k-mer Counting with Storage Technology,,,True,False,"Wu, Lingxi and Zhou, Minxuan and Xu, Weihong and Venkat, Ashish and Rosing, Tajana and Skadron, Kevin",2024.0,,,,ACM Transactions on Architecture and Code Optimization,Abakus: Accelerating k-mer Counting with Storage Technology,Abakus: Accelerating k -mer Counting with Storage Technology,https://dl.acm.org/doi/10.1145/3632952,"This work seeks to leverage Processing-with-storage-technology (PWST) to accelerate a key bioinformatics kernel called k-mer counting, which involves processing large files of sequence data on the disk to build a histogram of fixed-size genome sequence substrings and thereby entails prohibitively high I/O overhead.In particular, this work proposes a set of accelerator designs called Abakus"
"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome
  Assembly",2505.08071v1,awan2021accelerating,\cite{awan2021accelerating},Accelerating large scale de novo metagenome assembly using GPUs,,,True,False,"Awan, Muaaz Gul and Hofmeyr, Steven and Egan, Rob and Ding, Nan and Buluc, Aydin and Deslippe, Jack and Oliker, Leonid and Yelick, Katherine",2021.0,,,,,Accelerating large scale de novo metagenome assembly using GPUs,Accelerating large scale de novo metagenome assembly using GPUs ...,https://dl.acm.org/doi/abs/10.1145/3458817.3476212,"In this paper we present the first of its kind GPU-accelerated implementation of the local assembly approach that is an integral part of a widely used large-scale metagenome assembler, MetaHipMer. Local assembly uses algorithms that induce random memory accesses and non-deterministic workloads, which make GPU offloading a challenging task."
"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome
  Assembly",2505.08071v1,goswami2018gpu,\cite{goswami2018gpu},Gpu-accelerated large-scale genome assembly,,,True,False,"Goswami, Sayan and Lee, Kisung and Shams, Shayan and Park, Seung-Jong",2018.0,,,,,Gpu-accelerated large-scale genome assembly,GPU-Accelerated Large-Scale Genome Assembly - IEEE Xplore,https://ieeexplore.ieee.org/document/8425235,"In this paper, we present a new GPU-accelerated genome assembler called LaSAGNA, which can assemble large-scale sequence datasets using a single GPU by building string graphs from approximate all-pair overlaps. LaSAGNA can also run on multiple GPUs across multiple compute nodes connected by a high-speed network to expedite the assembly process."
"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome
  Assembly",2505.08071v1,mahmood2011gpu,\cite{mahmood2011gpu},Gpu-euler: Sequence assembly using gpgpu,,,True,False,"Mahmood, Syed Faraz and Rangwala, Huzefa",2011.0,,,,,Gpu-euler: Sequence assembly using gpgpu,GPU-Euler: Sequence Assembly Using GPGPU - IEEE Xplore,https://ieeexplore.ieee.org/document/6062988,"GPU-Euler: Sequence Assembly Using GPGPU Abstract: Advances in sequencing technologies have revolutionized the field of genomics by providing cost effective and high throughput solutions. In this paper, we develop a parallel sequence assembler implemented on general purpose graphic processor units (GPUs). Our work was largely motivated by a"
"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome
  Assembly",2505.08071v1,jain2013gagm,\cite{jain2013gagm},GAGM: Genome assembly on GPU using mate pairs,,,True,False,"Jain, Ashutosh and Garg, Anshuj and Paul, Kolin",2013.0,,,,,GAGM: Genome assembly on GPU using mate pairs,GAGM: Genome assembly on GPU using mate pairs,https://www.computer.org/csdl/proceedings-article/hipc/2013/06799107/12OmNzZWbxW,In this paper we present the design and development of a GPU based assembler (GAGM) for sequence assembly using Nvidia's GPUs with the CUDA programming model. Our assembler utilizes the mate pair reads produced by the current NGS technologies to build paired de Bruijn graph. Every paired read is broken into paired k-mers and l-mers.
"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome
  Assembly",2505.08071v1,swiercz2018grasshopper,\cite{swiercz2018grasshopper},GRASShopPER—An algorithm for de novo assembly based on GPU alignments,,,True,False,"Swiercz, Aleksandra and Frohmberg, Wojciech and Kierzynka, Michal and Wojciechowski, Pawel and Zurkowski, Piotr and Badura, Jan and Laskowski, Artur and Kasprzak, Marta and Blazewicz, Jacek",2018.0,,,,PloS one,GRASShopPER—An algorithm for de novo assembly based on GPU alignments,GRASShopPER - de novo Assembly based on GPU Alignments,https://mybiosoftware.com/grasshopper-de-novo-assembly-based-gpu-alignments.html,"GRASShopPER:: DESCRIPTION. GRASShopPER (GPU overlap GRaph ASSembler using Paired End Reads) is the novel assembly method that follows the approach of overlap-layout-consensus (OLC). In the method, a very efficient GPU implementation of the exact reads alignment algorithm has been used for calculating the scores and shifts on the arcs of the graph."
"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome
  Assembly",2505.08071v1,bwa-mem2,\cite{bwa-mem2},2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS),,,True,False,"Vasimuddin, Md. and Misra, Sanchit and Li, Heng and Aluru, Srinivas",2019.0,,,10.1109/IPDPS.2019.00041,,2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS),PDF,https://www.proceedings.com/content/050/050155webtoc.pdf,IPDPS 2019 Technical Program xxiv IPDPS 2019 Organization xxvii IPDPS 2019 Reviewers xxxv Keynote 1 Coding the Continuum 1 Ian Foster (Argonne National Laboratory; University of Chicago) Session 1: Graph Algorithms LACC: A Linear-Algebraic Algorithm for Finding Connected Components in Distributed Memory 2 Ariful Azad (Indiana University) and
"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome
  Assembly",2505.08071v1,mm2-fast,\cite{mm2-fast},Accelerating minimap2 for long-read sequencing applications on modern CPUs,,,True,False,"Kalikar, Saurabh and Jain, Chirag and Vasimuddin, Md and Misra, Sanchit",2022.0,,,,Nature Computational Science,Accelerating minimap2 for long-read sequencing applications on modern CPUs,Accelerating minimap2 for long-read sequencing applications on modern CPUs,https://www.nature.com/articles/s43588-022-00201-8,"mm2-fast is an accelerated version of minimap2, a popular software for long-read data analysis. mm2-fast introduces high-performance parallel computing techniques to reduce the overall runtime of"
"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome
  Assembly",2505.08071v1,mm2-ax,\cite{mm2-ax},Accelerating Minimap2 for accurate long read alignment on GPUs,,,True,False,"Sadasivan, Harisankar and Maric, Milos and Dawson, Eric and Iyer, Vishanth and Israeli, Johnny and Narayanasamy, Satish",2023.0,,,,Journal of biotechnology and biomedicine,Accelerating Minimap2 for accurate long read alignment on GPUs,Accelerating Minimap2 for Accurate Long Read Alignment on GPUs,https://pubmed.ncbi.nlm.nih.gov/36937168/,"On the other hand, most Point-of-Care computational workflows in long read sequencing use Graphics Processing Units (GPUs). We present minimap2-accelerated (mm2-ax), a heterogeneous design for sequence mapping and alignment where minimap2's compute intensive chaining step is sped up on the GPU and demonstrate its time and cost benefits."
"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome
  Assembly",2505.08071v1,mm2-gb,\cite{mm2-gb},mm2-gb: GPU Accelerated Minimap2 for Long Read DNA Mapping,,,True,False,"Dong, Juechu and Liu, Xueshen and Sadasivan, Harisankar and Sitaraman, Sriranjani and Narayanasamy, Satish",2024.0,,,,bioRxiv,mm2-gb: GPU Accelerated Minimap2 for Long Read DNA Mapping,mm2-gb: GPU Accelerated Minimap2 for Long Read DNA Mapping,https://dl.acm.org/doi/10.1145/3698587.3701366,"Long-read DNA sequencing is becoming increasingly popular for genetic diagnostics. Minimap2 is the state-of-the-art long-read aligner. However, Minimap2's chaining step is slow on the CPU and takes 40-68% of the time especially for long DNA reads."
"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome
  Assembly",2505.08071v1,guo2019hardware,\cite{guo2019hardware},Hardware acceleration of long read pairwise overlapping in genome sequencing: A race between fpga and gpu,,,True,False,"Guo, Licheng and Lau, Jason and Ruan, Zhenyuan and Wei, Peng and Cong, Jason",2019.0,,,,,Hardware acceleration of long read pairwise overlapping in genome sequencing: A race between fpga and gpu,Hardware Acceleration of Long Read Pairwise Overlapping in Genome ...,https://ieeexplore.ieee.org/document/8735515,"In genome sequencing, it is a crucial but time-consuming task to detect potential overlaps between any pair of the input reads, especially those that are ultra-long. The state-of-the-art overlapping tool Minimap2 outperforms other popular tools in speed and accuracy. It has a single computing hot-spot, chaining, that takes 70% of the time and needs to be accelerated. There are several crucial"
"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome
  Assembly",2505.08071v1,zhang2024harp,\cite{zhang2024harp},Harp: Leveraging Quasi-Sequential Characteristics to Accelerate Sequence-to-Graph Mapping of Long Reads,,,True,False,"Zhang, Yichi and Chen, Dibei and Zeng, Gang and Zhu, Jianfeng and Li, Zhaoshi and Chen, Longlong and Wei, Shaojun and Liu, Leibo",2024.0,,,,,Harp: Leveraging Quasi-Sequential Characteristics to Accelerate Sequence-to-Graph Mapping of Long Reads,papers [Seminar in Computer Architecture - Spring 2025],https://safari.ethz.ch/architecture_seminar/spring2025/doku.php?id=papers,"﻿Harp: Leveraging Quasi-Sequential Characteristics to Accelerate Sequence-to-Graph Mapping of Long Reads, ASPLOS 2024 . Understanding RowHammer Under Reduced Refresh Latency: Experimental Analysis of Real DRAM Chips and Implications on Future Solutions, HPCA 2025 ... 2025/03/09 19:31 by kkoliogeorgi. Page Tools. Show pagesource;"
"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome
  Assembly",2505.08071v1,gu2023gendp,\cite{gu2023gendp},GenDP: A Framework of Dynamic Programming Acceleration for Genome Sequencing Analysis,,,True,False,"Gu, Yufeng and Subramaniyan, Arun and Dunn, Tim and Khadem, Alireza and Chen, Kuan-Yu and Paul, Somnath and Vasimuddin, Md and Misra, Sanchit and Blaauw, David and Narayanasamy, Satish and others",2023.0,,,,,GenDP: A Framework of Dynamic Programming Acceleration for Genome Sequencing Analysis,GitHub - Yufeng98/GenDP: GenDP: A Dynamic Programming Framework for ...,https://github.com/Yufeng98/GenDP,GenDP: A Framework of Dynamic Programming Acceleration for Genome Sequencing Analysis. In Proceedings of the 50th Annual International Symposium on Computer Architecture (pp. 1-15).
"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome
  Assembly",2505.08071v1,pavon2024quetzal,\cite{pavon2024quetzal},QUETZAL: Vector Acceleration Framework for Modern Genome Sequence Analysis Algorithms,,,True,False,"Pavon, Julian and Valdivieso, Ivan Vargas and Rojas, Carlos and Hernandez, Cesar and Aslan, Mehmet and Figueras, Roger and Yuan, Yichao and Lindegger, Jo{\""e}l and Alser, Mohammed and Moll, Francesc and others",2024.0,,,,,QUETZAL: Vector Acceleration Framework for Modern Genome Sequence Analysis Algorithms,PDF,https://people.inf.ethz.ch/omutlu/pub/QUETZAL_isca24.pdf,• QUETZAL: a programmable vector framework to accelerate a wide range of genome sequence analysis algorithms that offers an average speedup of 5.7× compared to a baseline CPU architecture with a small 1.4% area overhead. II. BACKGROUND AND MOTIVATION This section provides an overview of classical and modern genome sequence analysis algorithms.
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,ark,\cite{ark},2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO),,,True,False,"Kim, Jongmin and Lee, Gwangho and Kim, Sangpyo and Sohn, Gina and Rhu, Minsoo and Kim, John and Ahn, Jung Ho",2022.0,,,10.1109/MICRO56248.2022.00086,,2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO),2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO ...,https://www.computer.org/csdl/proceedings/micro/2022/1HMSv9tKEus,"2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO) Oct. 1 2022 to Oct. 5 2022. Chicago, IL, USA. ISBN: 978-1-6654-6272-3. Table of Contents. Hermes: Accelerating Long-Latency Load Requests via Perceptron-Based Off-Chip Load Prediction pp. 1-18."
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,sharp,\cite{sharp},SHARP: A Short-Word Hierarchical Accelerator for Robust and Practical Fully Homomorphic Encryption,,,True,False,"Kim, Jongmin and Kim, Sangpyo and Choi, Jaewan and Park, Jaiyoung and Kim, Donghwan and Ahn, Jung Ho",2023.0,,https://doi.org/10.1145/3579371.3589053,10.1145/3579371.3589053,,SHARP: A Short-Word Hierarchical Accelerator for Robust and Practical Fully Homomorphic Encryption,SHARP: A Short-Word Hierarchical Accelerator for Robust and Practical ...,https://dl.acm.org/doi/abs/10.1145/3579371.3589053,"Fully homomorphic encryption (FHE) is an emerging cryptographic technology that guarantees the privacy of sensitive user data by enabling direct computations on encrypted data. Despite the security benefits of this approach, FHE is associated with prohibitively high levels of computational and memory overhead, preventing its widespread use in"
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,clake,\cite{clake},CraterLake: A Hardware Accelerator for Efficient Unbounded Computation on Encrypted Data,,,True,False,"Samardzic, Nikola and Feldmann, Axel and Krastev, Aleksandar and Manohar, Nathan and Genise, Nicholas and Devadas, Srinivas and Eldefrawy, Karim and Peikert, Chris and Sanchez, Daniel",2022.0,,https://doi.org/10.1145/3470496.3527393,10.1145/3470496.3527393,,CraterLake: A Hardware Accelerator for Efficient Unbounded Computation on Encrypted Data,CraterLake: a hardware accelerator for efficient unbounded computation ...,https://dl.acm.org/doi/10.1145/3470496.3527393,"To tackle this challenge, CraterLake introduces a new hardware architecture that efficiently scales to very large cipher-texts, novel functional units to accelerate key kernels, and new algorithms and compiler techniques to reduce data movement."
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,rpu,\cite{rpu},2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),,,True,False,"Soni, Deepraj and Neda, Negar and Zhang, Naifeng and Reynwar, Benedict and Gamil, Homer and Heyman, Benjamin and Nabeel, Mohammed and Badawi, Ahmad Al and Polyakov, Yuriy and Canida, Kellie and Pedram, Massoud and Maniatakos, Michail and Cousins, David Bruce and Franchetti, Franz and French, Matthew and Schmidt, Andrew and Reagen, Brandon",2023.0,,,10.1109/ISPASS57527.2023.00034,,2023 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),ISPASS-2023 Home,https://ispass.org/ispass2023/,"This year the International Symposium on Performance Analysis of Systems and Software will be held in Raleigh, North Carolina April 23-25, 2023. ... ISPASS-2023 2023 IEEE International Symposium on Performance Analysis of Systems and Software. April 23-25, 2023. Raleigh, North Carolina. Home. Committee."
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,ciflow,\cite{ciflow},2024 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),,,True,False,"Neda, Negar and Ebel, Austin and Reynwar, Benedict and Reagen, Brandon",2024.0,,,10.1109/ISPASS61541.2024.00016,,2024 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),ISPASS-2024 Home,https://ispass.org/ispass2024/,"This year the International Symposium on Performance Analysis of Systems and Software will be held in Indianapolis, Indiana May 5-7, 2024. ... ISPASS-2024 2024 IEEE International Symposium on Performance Analysis of Systems and Software. May 5-7, 2024. Indianapolis, Indiana. Home. Committee."
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,priorMSM,\cite{priorMSM},PriorMSM: An Efficient Acceleration Architecture for Multi-Scalar Multiplication,,,True,False,"Liu, Changxu and Zhou, Hao and Dai, Patrick and Shang, Li and Yang, Fan",2024.0,,,,ACM Transactions on Design Automation of Electronic Systems,PriorMSM: An Efficient Acceleration Architecture for Multi-Scalar Multiplication,PriorMSM: An Efficient Acceleration Architecture for Multi-Scalar ...,https://dl.acm.org/doi/abs/10.1145/3678006,"Multi-Scalar Multiplication (MSM) is a computationally intensive task that operates on elliptic curves based on GF(P). ... In this article, we present PriorMSM, an efficient acceleration architecture for MSM. We propose a Priority-Based Scheduling Mechanism (PBSM) based on a multi-FIFO and multi-bank architecture to accelerate the"
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,distMSM,\cite{distMSM},Accelerating Multi-Scalar Multiplication for Efficient Zero Knowledge Proofs with Multi-GPU Systems,,,True,False,"Ji, Zhuoran and Zhang, Zhiyuan and Xu, Jiming and Ju, Lei",2024.0,,https://doi.org/10.1145/3620666.3651364,10.1145/3620666.3651364,,Accelerating Multi-Scalar Multiplication for Efficient Zero Knowledge Proofs with Multi-GPU Systems,Accelerating Multi-Scalar Multiplication for Efficient Zero Knowledge ...,https://dl.acm.org/doi/abs/10.1145/3620666.3651364,"Ji Z Zhao J Gao P Yin X Ju L Eeckhout L Smaragdakis G Liang K Sampson A Kim M Rossbach C (2025) Accelerating Number Theoretic Transform with Multi-GPU Systems for Efficient Zero Knowledge Proof Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1 10.1145/3669940."
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,cuZK,\cite{cuZK},cuZK: Accelerating Zero-Knowledge Proof with A Faster Parallel Multi-Scalar Multiplication Algorithm on GPUs,,,True,False,Tao Lu and Chengkun Wei and Ruijing Yu and Chaochao Chen and Wenjing Fang and Lei Wang and Zeke Wang and Wenzhi Chen,2022.0,,https://eprint.iacr.org/2022/1321,,,cuZK: Accelerating Zero-Knowledge Proof with A Faster Parallel Multi-Scalar Multiplication Algorithm on GPUs,cuZK: Accelerating Zero-Knowledge Proof with A Faster Parallel Multi ...,https://eprint.iacr.org/2022/1321,"Zero-knowledge proof is a critical cryptographic primitive. Its most practical type, called zero-knowledge Succinct Non-interactive ARgument of Knowledge (zkSNARK), has been deployed in various privacy-preserving applications such as cryptocurrencies and verifiable machine learning. Unfortunately, zkSNARK like Groth16 has a high overhead on its proof generation step, which consists of several"
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,gypso,\cite{gypso},Gypsophila: A Scalable and Bandwidth-Optimized Multi-Scalar Multiplication Architecture,,,True,False,"Liu, Changxu and Zhou, Hao and Yang, Lan and Xu, Jiamin and Dai, Patrick and Yang, Fan",2024.0,,https://doi.org/10.1145/3649329.3658259,10.1145/3649329.3658259,,Gypsophila: A Scalable and Bandwidth-Optimized Multi-Scalar Multiplication Architecture,PriorMSM: An Efficient Acceleration Architecture for Multi-Scalar ...,https://dl.acm.org/doi/abs/10.1145/3678006,"Multi-Scalar Multiplication (MSM) is a computationally intensive task that operates on elliptic curves based on GF(P). ... Jiamin Xu, Patrick Dai, and Fan Yang. 2024. Gypsophila: A scalable and bandwidth-optimized multi-scalar multiplication architecture. In 2024 61st ACM/IEEE Design Automation Conference (DAC'24 ... An Efficient Acceleration"
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,reZK,\cite{reZK},ReZK: A Highly Reconfigurable Accelerator for Zero-Knowledge Proof,,,True,False,"Zhou, Hao and Liu, Changxu and Yang, Lan and Shang, Li and Yang, Fan",2024.0,,,,IEEE Transactions on Circuits and Systems I: Regular Papers,ReZK: A Highly Reconfigurable Accelerator for Zero-Knowledge Proof,SZKP: A Scalable Accelerator Architecture for Zero-Knowledge Proofs ...,https://dl.acm.org/doi/10.1145/3656019.3676898,A zero-knowledge proof allows a prover to convince a verifier of an assertion without revealing any further information beyond the fact that the assertion ... Zhou H Liu C Yang L Shang L Yang F (2025) ReZK: A Highly Reconfigurable Accelerator for Zero-Knowledge Proof IEEE Transactions on Circuits and Systems I: Regular Papers 10.1109/TCSI.2024.
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,myotosis,\cite{myotosis},Myosotis: An Efficiently Pipelined and Parameterized Multi-Scalar Multiplication Architecture via Data Sharing,,,True,False,"Liu, Changxu and Zhou, Hao and Yang, Lan and Wu, Zheng and Dai, Patrick and Li, Yinlong and Wu, Shiyong and Yang, Fan",2024.0,,,10.1109/TCAD.2024.3524364,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,Myosotis: An Efficiently Pipelined and Parameterized Multi-Scalar Multiplication Architecture via Data Sharing,PDF,https://austinliu01.github.io/files/Myosotis.pdf,"propose Myosotis, an efficiently pipelined and parameterized Multi-Scalar Multiplication architecture. By sharing input data and allocating cache effectively, it mitigates average transmission bandwidth in runtime. Myosotis also supports the use of multiple Point Addition (PADD) units to achieve performance gains,"
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,MSMAC,\cite{MSMAC},MSMAC: Accelerating Multi-Scalar Multiplication for Zero-Knowledge Proof,,,True,False,"Qiu, Pengcheng and Wu, Guiming and Chu, Tingqiang and Wei, Changzheng and Luo, Runzhou and Yan, Ying and Wang, Wei and Zhang, Hui",2024.0,,https://doi.org/10.1145/3649329.3655672,10.1145/3649329.3655672,,MSMAC: Accelerating Multi-Scalar Multiplication for Zero-Knowledge Proof,MSMAC: Accelerating Multi-Scalar Multiplication for Zero-Knowledge Proof,https://dl.acm.org/doi/10.1145/3649329.3655672,"Multi-scalar multiplication (MSM) is the most computation-intensive part in proof generation of Zero-knowledge proof (ZKP). In this paper, we propose MSMAC, an FPGA accelerator for large-scale MSM. MSMAC adopts a specially designed Instruction Set Architecture (ISA) for MSM and optimizes pipelined Point Addition Unit (PAU) with hybrid Karatsuba"
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,intel_zkp,\cite{intel_zkp},if-ZKP: Intel FPGA-Based Acceleration of Zero Knowledge Proofs,,,True,False,Shahzad Ahmad Butt and Benjamin Reynolds and Veeraraghavan Ramamurthy and Xiao Xiao and Pohrong Chu and Setareh Sharifian and Sergey Gribok and Bogdan Pasca,2024.0,,https://arxiv.org/abs/2412.12481,,,if-ZKP: Intel FPGA-Based Acceleration of Zero Knowledge Proofs,if-ZKP: Intel FPGA-Based Acceleration of Zero Knowledge Proofs,https://arxiv.org/abs/2412.12481,"Zero-Knowledge Proofs (ZKPs) have emerged as an important cryptographic technique allowing one party (prover) to prove the correctness of a statement to some other party (verifier) and nothing else. ZKPs give rise to user's privacy in many applications such as blockchains, digital voting, and machine learning. Traditionally, ZKPs suffered from poor scalability but recently, a sub-class of ZKPs"
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,elastic_msm,\cite{elastic_msm},"Elastic {MSM}: A Fast, Elastic and Modular Preprocessing Technique for Multi-Scalar Multiplication Algorithm on {GPUs}",,,True,False,Xudong Zhu and Haoqi He and Zhengbang Yang and Yi Deng and Lutan Zhao and Rui Hou,2024.0,,https://eprint.iacr.org/2024/057,,,"Elastic {MSM}: A Fast, Elastic and Modular Preprocessing Technique for Multi-Scalar Multiplication Algorithm on {GPUs}","Elastic MSM: A Fast, Elastic and Modular Preprocessing Technique for ...",https://eprint.iacr.org/2024/057,"From another perspective, elastic MSM could also be regarded as a preprocessing technique over the well-known Pippenger algorithm, which is modular and could be used to accelerate almost all the most advanced parallel Pippenger algorithms on GPUs. Meanwhile, elastic MSM provides an adaptive trade-off between the running time and the extra"
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,tches_ntt_msm,\cite{tches_ntt_msm},A High-performance NTT/MSM Accelerator for Zero-knowledge Proof Using Load-balanced Fully-pipelined Montgomery Multiplier,,,True,False,"Chen, Xiangren and Yang, Bohan and Zhu, Wenping and Wang, Hanning and Tao, Qichao and Yin, Shuying and Zhu, Min and Wei, Shaojun and Liu, Leibo",2024.0,Dec.,https://tches.iacr.org/index.php/TCHES/article/view/11930,10.46586/tches.v2025.i1.275-313,IACR Transactions on Cryptographic Hardware and Embedded Systems,A High-performance NTT/MSM Accelerator for Zero-knowledge Proof Using Load-balanced Fully-pipelined Montgomery Multiplier,CHES 2025 Accepted Papers,https://ches.iacr.org/2025/acceptedpapers.php,"A High-performance NTT/MSM Accelerator for Zero-knowledge Proof Using Load-balanced Fully-pipelined Montgomery Multiplier. Xiangren Chen, Bohan Yang, Wenping Zhu, Hanning Wang, Qichao Tao, Shuying Yin, Min Zhu, Shaojun Wei, Leibo Liu Tsinghua University; Wuxi Micro Innovation Integrated Circuit Design Co. Ltd. TCHES PDF"
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,sam,\cite{sam},2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD),,,True,False,"Wang, Cheng and Gao, Mingyu",2023.0,,,10.1109/ICCAD57390.2023.10323744,,2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD),2023 IEEE/ACM International Conference On Computer Aided Design,https://ieee-cas.org/event/conference/2023-ieeeacm-international-conference-computer-aided-design,"ICCAD is the premier forum to explore new challenges and emerging technologies in the CAD research areas. It covers device, circuit, system and post-CMOS design topics and will take place in San Francisco, California, USA in October-November 2023."
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,legozk,\cite{legozk},LegoZK: a Dynamically Reconfigurable Accelerator for Zero Knowledge Proof,,,True,False,"Yang, Zhengbang and Zhao, Lutan and Li, Peinan and Liu, Han and Li, Kai and Zhao, Boyan and Meng, Dan and Hou, Rui",,,,,,LegoZK: a Dynamically Reconfigurable Accelerator for Zero Knowledge Proof,LegoZK: A Dynamically Reconfigurable Accelerator for Zero-Knowledge Proof,https://www.computer.org/csdl/proceedings-article/hpca/2025/064700a113/25Ko8zGEQ0M,"Zero-knowledge proof (ZKP) allows a prover to convince a verifier of the truth of a statement without revealing any secret information. This property is utilized in numerous privacy-preserving applications. However, the huge overhead of proof generation impedes the widespread adoption of ZKP. As a result, many ZKP accelerators have been developed to speed up proof generation."
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,unizk,\cite{unizk},UniZK: Accelerating Zero-Knowledge Proof with Unified Hardware and Flexible Kernel Mapping,,,True,False,"Wang, Cheng and Gao, Mingyu",2025.0,,https://doi.org/10.1145/3669940.3707228,10.1145/3669940.3707228,,UniZK: Accelerating Zero-Knowledge Proof with Unified Hardware and Flexible Kernel Mapping,Mingyu Gao | Publications - Tsinghua University,https://people.iiis.tsinghua.edu.cn/~gaomy/publications.html,"Cheng Wang, and Mingyu Gao, "" UniZK: Accelerating Zero-Knowledge Proof with Unified Hardware and Flexible Kernel Mapping,"" in Proceedings of the 30th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), Volume 1, pp. 1101-1117, Rotterdam, Netherlands, Mar 2025."
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,graz,\cite{graz},Chiplet-Based Techniques for Scalable and Memory-Aware Multi-Scalar Multiplication,,,True,False,Florian Hirner and Florian Krieger and Sujoy Sinha Roy,2025.0,,https://eprint.iacr.org/2025/252,,,Chiplet-Based Techniques for Scalable and Memory-Aware Multi-Scalar Multiplication,[Resource Topic] 2025/252: Chiplet-Based Techniques for Scalable and ...,https://askcryp.to/t/resource-topic-2025-252-chiplet-based-techniques-for-scalable-and-memory-aware-multi-scalar-multiplication/23547,"Welcome to the resource topic for 2025/252 Title: Chiplet-Based Techniques for Scalable and Memory-Aware Multi-Scalar Multiplication. Authors: Florian Hirner, Florian Krieger, Sujoy Sinha Roy Abstract: This paper presents a high-performance architecture for accelerating Multi-Scalar Multiplication (MSM) on ASIC platforms, targeting cryptographic applications with high throughput demands."
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,batchzk,\cite{batchzk},{BatchZK}: A Fully Pipelined {GPU}-Accelerated System for Batch Generation of Zero-Knowledge Proofs,,,True,False,Tao Lu and Yuxun Chen and Zonghui Wang and Xiaohang Wang and Wenzhi Chen and Jiaheng Zhang,2024.0,,https://eprint.iacr.org/2024/1862,,,{BatchZK}: A Fully Pipelined {GPU}-Accelerated System for Batch Generation of Zero-Knowledge Proofs,BatchZK: A Fully Pipelined GPU-Accelerated System for Batch Generation ...,https://dl.acm.org/doi/10.1145/3669940.3707270,"We propose a fully pipelined GPU-accelerated system for batch generation of zero-knowledge proofs. Our system has three features to improve throughput. First, we design a pipelined approach that enables each GPU thread to continuously execute its designated proof generation task without being idle."
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,nocap,\cite{nocap},Accelerating Zero-Knowledge Proofs Through Hardware-Algorithm Co-Design,,,True,False,Nikola Samardzic and Simon Langowski and Srinivas Devadas and Daniel Sanchez,2024.0,,,,,Accelerating Zero-Knowledge Proofs Through Hardware-Algorithm Co-Design,Accelerating Zero-Knowledge Proofs Through Hardware-Algorithm Co-Design ...,https://ieeexplore.ieee.org/document/10764644,"Accelerating Zero-Knowledge Proofs Through Hardware-Algorithm Co-Design Abstract: Zero-Knowledge Proofs (ZKPs) are a cryptographic tool that enables one party (a prover) to prove to another (a verifier) that a statement is true, without requiring the prover to disclose any data to the verifier. ZKPs have many use cases, such as letting clients"
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,gzkp,\cite{gzkp},GZKP: A GPU Accelerated Zero-Knowledge Proof System,,,True,False,"Ma, Weiliang and Xiong, Qian and Shi, Xuanhua and Ma, Xiaosong and Jin, Hai and Kuang, Haozhao and Gao, Mingyu and Zhang, Ye and Shen, Haichen and Hu, Weifang",2023.0,,https://doi.org/10.1145/3575693.3575711,10.1145/3575693.3575711,,GZKP: A GPU Accelerated Zero-Knowledge Proof System,GZKP: A GPU Accelerated Zero-Knowledge Proof System,https://dl.acm.org/doi/10.1145/3575693.3575711,"A significant obstacle in using ZKP for online applications is the performance overhead of its proof generation. We develop GZKP, a GPU accelerated zero-knowledge proof system that supports different levels of security requirements and brings significant speedup toward making ZKP truly usable."
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,pipezk,\cite{pipezk},PipeZK: Accelerating Zero-Knowledge Proof with a Pipelined Architecture,,,True,False,Ye Zhang and Shuo Wang and Xian Zhang and Jiangbin Dong and Xingzhong Mao and Fan Long and Cong Wang and Dong Zhou and Mingyu Gao and and Guangyu Sun,2021.0,,,,,PipeZK: Accelerating Zero-Knowledge Proof with a Pipelined Architecture,Accelerating Zero-Knowledge Proof with a Pipelined Architecture,https://docslib.org/doc/10408292/accelerating-zero-knowledge-proof-with-a-pipelined-architecture,"To appear on ISCA 2021 This is a draft version. Camera-ready version is coming soon. PipeZK: Accelerating Zero-Knowledge Proof with a Pipelined Architecture Abstract—Zero-knowledge proof (ZKP) is a promising cryp- newly invented ones, zk-SNARK, which stands for Zero- tographic protocol for both computation integrity and privacy."
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,ceremony,\cite{ceremony},What is the ZCash Ceremony? The Complete Beginners Guide,,,True,False,,2023.0,,,,,What is the ZCash Ceremony? The Complete Beginners Guide,What is the ZCash Ceremony? The Complete Beginners Guide,https://coinbureau.com/education/zcash-ceremony/,The Zcash Ceremony was the process that the private cryptocurrency went through in order to ensure that it remained completely secure. We take an in-depth look at one of the most interesting and elaborate procedures ever completed to secure a blockchain.
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,trusted_set_up,\cite{trusted_set_up},"Zcash Nixes Trusted Setup, Enters New Era With Major Network Update",,,True,False,"Nelson, Jason",2022.0,Jun,https://decrypt.co/101762/zcash-nixes-trusted-setup-enters-new-era-with-major-network-update,,Decrypt,"Zcash Nixes Trusted Setup, Enters New Era With Major Network Update","Zcash Nixes Trusted Setup, Enters New Era With Major Network Update ...",https://www.youtube.com/watch?v=olxCzpzHZZ4,⚡ Curated Crypto Currency News ⚡Crypto Speaks To Me does not claim that curated content will be read with 100% accuracy.You can find the original post at: ht
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,garuda,\cite{garuda},Garuda and Pari: Faster and Smaller {SNARKs} via Equifficient Polynomial Commitments,,,True,False,Michel Dellepere and Pratyush Mishra and Alireza Shirzad,2024.0,,https://eprint.iacr.org/2024/1245,,,Garuda and Pari: Faster and Smaller {SNARKs} via Equifficient Polynomial Commitments,Penn's Security and Privacy Lab: - splab.cis.upenn.edu,https://splab.cis.upenn.edu/seminars.html,Garuda and Pari: Faster and Smaller SNARKs via Equifficient Polynomial Commitments SNARK sare powerful cryptographic primitives that allow a prover to produce a succinct proof of a computation. Two key goals of SNARK research are to minimize the size of the proof and to minimize the time required to generate the proof.
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,verizexe,\cite{verizexe},VeriZexe: Decentralized Private Computation with Universal Setup,,,True,False,"Alex Luoyuan Xiong and
                  Binyi Chen and
                  Zhenfei Zhang and
                  Benedikt B{\""{u}}nz and
                  Ben Fisch and
                  Fernando Krell and
                  Philippe Camacho",2023.0,,https://www.usenix.org/conference/usenixsecurity23/presentation/xiong,,,VeriZexe: Decentralized Private Computation with Universal Setup,VeriZexe: Decentralized Private Computation with Universal Setup,https://www.usenix.org/conference/usenixsecurity23/presentation/xiong,We propose a new DPC instantiation VeriZexe that is highly efficient and requires only a single universal setup to support an arbitrary number of applications. Our benchmark improves the state-of-the-art by 9x in transaction generation time and by 3.4x in memory usage. ... {VeriZexe}: Decentralized Private Computation with Universal Setup
Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs,2504.06211v1,poseidon,\cite{poseidon},Poseidon: A New Hash Function for Zero-Knowledge Proof Systems,,,True,False,Lorenzo Grassi and Dmitry Khovratovich and Christian Rechberger and Arnab Roy and Markus Schofnegger,2019.0,,https://eprint.iacr.org/2019/458,,,Poseidon: A New Hash Function for Zero-Knowledge Proof Systems,Poseidon: A New Hash Function for Zero-Knowledge Proof Systems,https://www.usenix.org/conference/usenixsecurity21/presentation/grassi,"A notable example is a zero-knowledge proof of coin ownership in the Zcash cryptocurrency, where the inadequacy of the SHA-256 hash function for such a circuit caused a huge computational penalty. In this paper, we present a modular framework and concrete instances of cryptographic hash functions which work natively with GF(p) objects."
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,li2022help,\cite{li2022help},Help rather than recycle: Alleviating cold startup in serverless computing through $\{$Inter-Function$\}$ container sharing,,,True,False,"Li, Zijun and Guo, Linsong and Chen, Quan and Cheng, Jiagan and Xu, Chuhao and Zeng, Deze and Song, Zhuo and Ma, Tao and Yang, Yong and Li, Chao and others",2022.0,,,,,Help rather than recycle: Alleviating cold startup in serverless computing through $\{$Inter-Function$\}$ container sharing,论文阅读 — Help Rather Than Recycle - 知乎 - 知乎专栏,https://zhuanlan.zhihu.com/p/568799268,前言. 本文是由 上海交通大学 EPCC实验室和阿里云的研究人员，发表在ATC'22上的文章，标题为《Help Rather Than Recycle: Alleviating Cold Startup in Serverless Computing Through Inter-Function Container Sharing》.. 原文链接： 本文的核心贡献在于提出一套缓解Serverless冷启动的机制。其核心是将空闲的待回收(Recycle)的容器转化
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,saxena2022memory,\cite{saxena2022memory},Memory deduplication for serverless computing with medes,,,True,False,"Saxena, Divyanshu and Ji, Tao and Singhvi, Arjun and Khalid, Junaid and Akella, Aditya",2022.0,,,,,Memory deduplication for serverless computing with medes,PDF,https://divyanshusaxena.github.io/assets/pdf/medes.pdf,"and scope of the memory-performance trade-off in serverless computing. We present Medes (Memory Deduplication for Serverless), a novel serverless framework that incorporates the dedup sandbox state. Medes makes use of a novel deduplication mechanism that can identify potentially similar chunks in the memory states of sandboxes across the cluster."
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,chen2023s,\cite{chen2023s},S-cache: Function caching for serverless edge computing,,,True,False,"Chen, Chen and Nagel, Lars and Cui, Lin and Tso, Fung Po",2023.0,,,,,S-cache: Function caching for serverless edge computing,S-Cache: Function Caching for Serverless Edge Computing,https://dl.acm.org/doi/10.1145/3578354.3592865,"In this paper, we study the request distribution and caching problem for serverless edge computing. We devise an online request distribution algorithm with performance guarantee and present an adaptive caching policy which incorporates container frequency, container size and cold-start time."
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,basu2024codecrunch,\cite{basu2024codecrunch},CodeCrunch: Improving Serverless Performance via Function Compression and Cost-Aware Warmup Location Optimization,,,True,False,"Basu Roy, Rohan and Patel, Tirthak and Garg, Rohan and Tiwari, Devesh",2024.0,,,,,CodeCrunch: Improving Serverless Performance via Function Compression and Cost-Aware Warmup Location Optimization,Rohan Basu Roy,https://rohanbasuroy.github.io/,"SIGMETRICS 2024 StarShip: Mitigating I/O Bottlenecks in Serverless Computing for Scientific Workflows; Rohan Basu Roy, and Devesh Tiwari. International Conference on Measurement and Modeling of Computer Systems. ASPLOS 2024 CodeCrunch: Improving Serverless Performance via Function Compression and Cost-Aware Warmup Location Optimization"
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,pan2023sustainable,\cite{pan2023sustainable},Sustainable serverless computing with cold-start optimization and automatic workflow resource scheduling,,,True,False,"Pan, Shanxing and Zhao, Hongyu and Cai, Zinuo and Li, Dongmei and Ma, Ruhui and Guan, Haibing",2023.0,,,,IEEE Transactions on Sustainable Computing,Sustainable serverless computing with cold-start optimization and automatic workflow resource scheduling,Sustainable Serverless Computing With Cold-Start Optimization and ...,https://ieeexplore.ieee.org/abstract/document/10237322,"Sustainable Serverless Computing With Cold-Start Optimization and Automatic Workflow Resource Scheduling Abstract: In recent years, serverless computing has garnered significant attention owing to its high scalability, pay-as-you-go billing model, and efficient resource management provided by cloud service providers. Optimal resource scheduling"
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,bhasi2021kraken,\cite{bhasi2021kraken},Kraken: Adaptive container provisioning for deploying dynamic dags in serverless platforms,,,True,False,"Bhasi, Vivek M and Gunasekaran, Jashwant Raj and Thinakaran, Prashanth and Mishra, Cyan Subhra and Kandemir, Mahmut Taylan and Das, Chita",2021.0,,,,,Kraken: Adaptive container provisioning for deploying dynamic dags in serverless platforms,Kraken | Proceedings of the ACM Symposium on Cloud Computing,https://dl.acm.org/doi/10.1145/3472883.3486992,"Kraken: Adaptive Container Provisioning for Deploying Dynamic DAGs in Serverless Platforms Authors : Vivek M. Bhasi , Jashwant Raj Gunasekaran , Prashanth Thinakaran , Cyan Subhra Mishra , Mahmut Taylan Kandemir , Chita Das Authors Info & Claims"
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,roy2022icebreaker,\cite{roy2022icebreaker},Icebreaker: Warming serverless functions better with heterogeneity,,,True,False,"Roy, Rohan Basu and Patel, Tirthak and Tiwari, Devesh",2022.0,,,,,Icebreaker: Warming serverless functions better with heterogeneity,IceBreaker: warming serverless functions better with heterogeneity ...,https://dl.acm.org/doi/10.1145/3503222.3507750,"By employing heterogeneity, IceBreaker allows for more number of nodes under the same cost budget and hence, keeps more number of functions warm and reduces the wait time during high load. Our real-system evaluation confirms that IceBreaker reduces the overall keep-alive cost by 45% and execution time by 27% using representative serverless"
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,sui2024pre,\cite{sui2024pre},Pre-Warming is Not Enough: Accelerating Serverless Inference With Opportunistic Pre-Loading,,,True,False,"Sui, Yifan and Yu, Hanfei and Hu, Yitao and Li, Jianxun and Wang, Hao",2024.0,,,,,Pre-Warming is Not Enough: Accelerating Serverless Inference With Opportunistic Pre-Loading,Pre-Warming is Not Enough: Accelerating Serverless Inference With ...,https://dl.acm.org/doi/10.1145/3698038.3698509,"Consequently, pre-warming alone is not enough to mitigate the ML inference function's cold-starts. This paper introduces InstaInfer, an opportunistic preloading technique to achieve instant inference by eliminating the latency associated with loading ML artifacts, thereby achieving minimal time cost in function execution."
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,ao2022faasnap,\cite{ao2022faasnap},Faasnap: Faas made fast using snapshot-based vms,,,True,False,"Ao, Lixiang and Porter, George and Voelker, Geoffrey M",2022.0,,,,,Faasnap: Faas made fast using snapshot-based vms,ucsdsysnet/faasnap - GitHub,https://github.com/ucsdsysnet/faasnap,"FaaSnap: FaaS Made Fast Using Snapshot-based VMs. In Seventeenth European Conference on Computer Systems (EuroSys '22), April 5-8, 2022, RENNES, France. ACM, New York, NY, USA, 17 pages. ... In ""faasnap"" base_path is where snapshot files location. Choose a directory in a local SSD. kernels are the locations of vanilla and sanpage kernels."
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,du2020catalyzer,\cite{du2020catalyzer},Catalyzer: Sub-millisecond startup for serverless computing with initialization-less booting,,,True,False,"Du, Dong and Yu, Tianyi and Xia, Yubin and Zang, Binyu and Yan, Guanglu and Qin, Chenggang and Wu, Qixuan and Chen, Haibo",2020.0,,,,,Catalyzer: Sub-millisecond startup for serverless computing with initialization-less booting,Catalyzer: 加速沙盒启动 - HASLAB.ORG,https://haslab.org/2021/07/09/Catalyzer.html,Catalyzer: Sub-millisecond Startup for Serverless Computing with Initialization-less Booting. 论文PDF下载链接. 1. 背景介绍. 无服务器运算（Serverless computing）是云计算的一种模型，终端客户不需要部署、配置或管理服务器服务，只需要向平台提交需要运行的代码 (handler function)，代码运行所需要的服务器服务皆由云端平台
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,silva2020prebaking,\cite{silva2020prebaking},Prebaking functions to warm the serverless cold start,,,True,False,"Silva, Paulo and Fireman, Daniel and Pereira, Thiago Emmanuel",2020.0,,,,,Prebaking functions to warm the serverless cold start,Prebaking runtime environments to improve the FaaS cold start latency ...,https://www.sciencedirect.com/science/article/pii/S0167739X24000190,"Our research has demonstrated that implementing Prebaking to launch serverless functions can significantly reduce the time required for the runtime environment to start-up. ... Prebaking functions to warm the serverless cold start. Proceedings of the 21st International Middleware Conference, Middleware '20, Association for Computing Machinery"
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,provisionedConcurrencyAWS,\cite{provisionedConcurrencyAWS},Provisioned concurrency for lambda functions,,,True,False,,2022.0,,,,,Provisioned concurrency for lambda functions,New - Provisioned Concurrency for Lambda Functions,https://aws.amazon.com/blogs/aws/new-provisioned-concurrency-for-lambda-functions/,"When you enable Provisioned Concurrency for a function, the Lambda service will initialize the requested number of execution environments so they can be ready to respond to invocations. Configuring Provisioned Concurrency I create two Lambda functions that use the same Java code and can be triggered by Amazon API Gateway. To simulate a"
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,improveColdstartByIncreasingMemory,\cite{improveColdstartByIncreasingMemory},Architecting a Serverless web application in AWS,,,True,False,,2016.0,,,,,Architecting a Serverless web application in AWS,Building a Serverless Web Application with AWS: Guide,https://medium.com/codex/building-a-serverless-web-application-with-aws-guide-4b83fee68b50,"Building a serverless web application on AWS is a journey that involves careful planning, design, and integration of various AWS services. The result is a scalable, efficient, and cost-effective"
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,optimisingServerlessForBBC,\cite{optimisingServerlessForBBC},Optimising serverless for BBC Online,,,True,False,,2021.0,,,,,Optimising serverless for BBC Online,Optimising serverless for BBC Online,https://www.bbc.com/articles/ce38vkdx9vvo,"The BBC makes use of many different technology providers to deliver its online services. We make use of AWS's Lambda, external service to deliver the serverless functionality for Web Core. There"
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,fuerst2021faascache,\cite{fuerst2021faascache},FaasCache: keeping serverless computing alive with greedy-dual caching,,,True,False,"Fuerst, Alexander and Sharma, Prateek",2021.0,,,,,FaasCache: keeping serverless computing alive with greedy-dual caching,FaasCache: keeping serverless computing alive with greedy-dual caching ...,https://dl.acm.org/doi/abs/10.1145/3445814.3446757,"Our caching-inspired Greedy-Dual keep-alive policy can be effective in reducing the cold-start overhead by more than 3× compared to current approaches. Caching concepts such as reuse distances and hit-ratio curves can also be used for auto-scaled server resource provisioning, which can reduce the resource requirement of FaaS providers by 30%"
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,pan2022retention,\cite{pan2022retention},Retention-aware container caching for serverless edge computing,,,True,False,"Pan, Li and Wang, Lin and Chen, Shutong and Liu, Fangming",2022.0,,,,,Retention-aware container caching for serverless edge computing,Retention-Aware Container Caching for Serverless Edge Computing,https://ieeexplore.ieee.org/document/9796705,"Serverless edge computing adopts an event-based model where Internet-of-Things (IoT) services are executed in lightweight containers only when requested, leading to significantly improved edge resource utilization. Unfortunately, the startup latency of containers degrades the responsiveness of IoT services dramatically. Container caching, while masking this latency, requires retaining"
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,yu2024rainbowcake,\cite{yu2024rainbowcake},RainbowCake: Mitigating Cold-starts in Serverless with Layer-wise Container Caching and Sharing,,,True,False,"Yu, Hanfei and Basu Roy, Rohan and Fontenot, Christian and Tiwari, Devesh and Li, Jian and Zhang, Hong and Wang, Hao and Park, Seung-Jong",2024.0,,,,,RainbowCake: Mitigating Cold-starts in Serverless with Layer-wise Container Caching and Sharing,IntelliSys-Lab/RainbowCake-ASPLOS24 - GitHub,https://github.com/IntelliSys-Lab/RainbowCake-ASPLOS24,"This repo contains a demo implementation of our ASPLOS 2024 paper, RainbowCake: Mitigating Cold-starts in Serverless with Layer-wise Container Caching and Sharing. Serverless computing has grown rapidly as a new cloud computing paradigm that promises ease-of-management, cost-efficiency, and auto-scaling by shipping functions via self-contained virtualized containers."
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,lee2021mitigating,\cite{lee2021mitigating},Mitigating cold start problem in serverless computing with function fusion,,,True,False,"Lee, Seungjun and Yoon, Daegun and Yeo, Sangho and Oh, Sangyoon",2021.0,,,,Sensors,Mitigating cold start problem in serverless computing with function fusion,PDF,https://faculty.washington.edu/wlloyd/courses/tcss591/papers/Mitigating_Cold_Start_Problem_in_Serverless_Computing_A_Reinforcement_Learning_Approach.pdf,"Mitigating Cold Start Problem in Serverless Computing: A Reinforcement Learning Approach Parichehr Vahidinia1, Bahar Farahani2, Fereidoon Shams Aliee1 Abstract—Serverless computing has revolutionized the world of cloud-based and event-driven applications with the introduction of Function-as-a-Service (FaaS) as the latest cloud computing model."
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,abgaz2023decomposition,\cite{abgaz2023decomposition},Decomposition of monolith applications into microservices architectures: A systematic review,,,True,False,"Abgaz, Yalemisew and McCarren, Andrew and Elger, Peter and Solan, David and Lapuz, Neil and Bivol, Marin and Jackson, Glenn and Yilmaz, Murat and Buckley, Jim and Clarke, Paul",2023.0,,,,IEEE Transactions on Software Engineering,Decomposition of monolith applications into microservices architectures: A systematic review,Decomposition of Monolith Applications Into Microservices Architectures ...,https://ieeexplore.ieee.org/document/10160171,"Microservices architecture has gained significant traction, in part owing to its potential to deliver scalable, robust, agile, and failure-resilient software products. Consequently, many companies that use large and complex software systems are actively looking for automated solutions to decompose their monolith applications into microservices. This paper rigorously examines 35 research papers"
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,frostig2018compiling,\cite{frostig2018compiling},Compiling machine learning programs via high-level tracing,,,True,False,"Frostig, Roy and Johnson, Matthew James and Leary, Chris",2018.0,,,,Systems for Machine Learning,Compiling machine learning programs via high-level tracing,GitHub - n2cholas/awesome-jax: JAX - A curated list of resources https ...,https://github.com/n2cholas/awesome-jax,"Compiling machine learning programs via high-level tracing. Roy Frostig, Matthew James Johnson, Chris Leary. MLSys 2018. - White paper describing an early version of JAX, detailing how computation is traced and compiled. JAX, M.D.: A Framework for Differentiable Physics. Samuel S. Schoenholz, Ekin D. Cubuk. NeurIPS 2020."
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,graalvm,\cite{graalvm},The GraalVM native image,,,True,False,GraalVM,2003.0,,,,,The GraalVM native image,GraalVM Native Image Quick Start - Oracle Help Center,https://docs.oracle.com/en/learn/graalvm-native-image-quick-start/index.html,"GraalVM Native Image technology compiles Java code ahead-of-time into a self-contained executable file. Only the code that is required at run time by the application gets added into the executable file. An executable file produced by Native Image has several important advantages, in that it:"
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,proguard,\cite{proguard},The industry-leading Java optimizer for Android apps,,,True,False,ProGuard,2002.0,,,,,The industry-leading Java optimizer for Android apps,Java Obfuscator and Android App Optimizer | ProGuard - Guardsquare,https://www.guardsquare.com/proguard,"Java Obfuscator and Android App Optimizer | ProGuard *   ProGuardUse ProGuard, Guardsquare’s open-source shrinker for Java bytecode, to enhance & optimize your code Use ProGuard to shrink any Java or Kotlin app, whether on mobile, embedded or on desktop. Using ProGuard Playground, you can experiment with and share your keep rules — that is, the rules that preserve your app’s libraries, resources, resource files and assets after ProGuard shrinks your mobile app — without needing to rebuild your app. Although ProGuard is a great start to mobile app optimization, its primary purpose is to shrink Java/Kotlin apps. Discover how ProGuard optimizes and shrinks Android and Java/Kotlin apps."
"Efficient Serverless Cold Start: Reducing Library Loading Overhead by
  Profile-guided Optimization",2504.19283v1,r8_android,\cite{r8_android},R8 Compiler for Android applications,,,True,False,R8,2019.0,,,,,R8 Compiler for Android applications,What is R8 and how we enabled it - Medium,https://stefma.medium.com/what-is-r8-and-how-we-enabled-it-4f5764a7ff9c,"But in the early days of Android, the R8 compiler didn't exist. (Much like Kotlin. It didn't exist back then either 😀). Before Google decided to build its own shrinking and obfuscating tool and named it R8, Android used ProGuard. ProGuard is, like the R8 compiler, a shrinking and obfuscating tool."
"Kubernetes in the Cloud vs. Bare Metal: A Comparative Study of Network
  Costs",2504.11007v1,marino2023dynamic,\cite{marino2023dynamic},Dynamic Optimization of Provider-Based Scheduling for HPC Workloads,,,True,False,"Marino, Jacopo and Risso, Fulvio and Bighi, Mauro",2023.0,,,,,Dynamic Optimization of Provider-Based Scheduling for HPC Workloads,Dynamic Optimization of Provider-Based Scheduling for HPC Workloads ...,https://ieeexplore.ieee.org/document/10271608,"The vast array of cloud providers present in today's market proffer a suite of High-Performance Computing (HPC) services. However, these offerings are characterized by significant variations in execution times and cost structures. Consequently, selecting the optimal cloud provider and configuring the features of the chosen computing instance (e.g. virtual machines) proves to be a challenging"
"Kubernetes in the Cloud vs. Bare Metal: A Comparative Study of Network
  Costs",2504.11007v1,verreydt2019leveraging,\cite{verreydt2019leveraging},Leveraging Kubernetes for adaptive and cost-efficient resource management,,,True,False,"Verreydt, Stef and Beni, Emad Heydari and Truyen, Eddy and Lagaisse, Bert and Joosen, Wouter",2019.0,,,,,Leveraging Kubernetes for adaptive and cost-efficient resource management,Leveraging Kubernetes for adaptive and cost-efficient resource ...,https://dl.acm.org/doi/abs/10.1145/3366615.3368357,"Leveraging Kubernetes for adaptive and cost-efficient resource management. Authors: Stef Verreydt, ... the resource management concepts of Kubernetes allow to simulate vertical scaling without a negative effect on performance. The effectiveness of the default horizontal autoscaler, however, depends on the type of application and the user"
"Kubernetes in the Cloud vs. Bare Metal: A Comparative Study of Network
  Costs",2504.11007v1,gao2020hierarchical,\cite{gao2020hierarchical},Hierarchical multi-agent optimization for resource allocation in cloud computing,,,True,False,"Gao, Xiangqiang and Liu, Rongke and Kaushik, Aryan",2020.0,,,,IEEE Transactions on Parallel and Distributed Systems,Hierarchical multi-agent optimization for resource allocation in cloud computing,Hierarchical Multi-Agent Optimization for Resource Allocation in Cloud ...,https://ieeexplore.ieee.org/document/9224163,"In cloud computing, an important concern is to allocate the available resources of service nodes to the requested tasks on demand and to make the objective function optimum, i.e., maximizing resource utilization, payoffs, and available bandwidth. This article proposes a hierarchical multi-agent optimization (HMAO) algorithm in order to maximize the resource utilization and make the bandwidth"
"Kubernetes in the Cloud vs. Bare Metal: A Comparative Study of Network
  Costs",2504.11007v1,dong2023agent,\cite{dong2023agent},Agent-based cloud simulation model for resource management,,,True,False,"Dong, Dapeng",2023.0,,,,Journal of Cloud Computing,Agent-based cloud simulation model for resource management,Agent-based cloud simulation model for resource management,https://dl.acm.org/doi/10.1186/s13677-023-00540-5,"This paper presents an agent-based cloud simulation model for resource management. The focus is on how service placement strategies, service migration, and server consolidation affect the overall performance of homogeneous and heterogeneous clouds, in terms of energy consumption, resource utilization, and violation of service-level agreements."
"Kubernetes in the Cloud vs. Bare Metal: A Comparative Study of Network
  Costs",2504.11007v1,cho2020cost,\cite{cho2020cost},A cost estimation model for cloud services and applying to PC laboratory platforms,,,True,False,"Cho, KyungWoon and Bahn, Hyokyung",2020.0,,,,Processes,A cost estimation model for cloud services and applying to PC laboratory platforms,A Cost Estimation Model for Cloud Services and Applying to PC ...,https://www.mdpi.com/2227-9717/8/1/76,"To quantify the accuracy of our cost estimation model, we utilized CLABO in two classes that require cloud PC laboratory platforms during the 2018 fall semester. The number of students was 50, and we generated 50 instances on AWS and compared the total cost charged by AWS and the cost estimated by our instant model."
"Kubernetes in the Cloud vs. Bare Metal: A Comparative Study of Network
  Costs",2504.11007v1,xu2018cost,\cite{xu2018cost},Cost-effective cloud server provisioning for predictable performance of big data analytics,,,True,False,"Xu, Fei and Zheng, Haoyue and Jiang, Huan and Shao, Wujie and Liu, Haikun and Zhou, Zhi",2018.0,,,,IEEE Transactions on Parallel and Distributed Systems,Cost-effective cloud server provisioning for predictable performance of big data analytics,Cost-Effective Cloud Server Provisioning - Java Projects | S-Logix,https://slogix.in/big-data/cost-effective-cloud-server-provisioning-for-predictable-performance-of-big-data-analytics/,"Cost-Effective Cloud Server Provisioning for Predictable Performance of Big Data Analytics - 2018. Research Area: ... iSpot is able to guarantee the performance of big data analytics running on cloud transient servers while reducing the job budget by up to 83.8 percent in comparison to the state-of-the-art server provisioning strategies, yet"
"Kubernetes in the Cloud vs. Bare Metal: A Comparative Study of Network
  Costs",2504.11007v1,de2023cost,\cite{de2023cost},Cost-Profiling Microservice Applications Using an APM Stack,,,True,False,"de Vries, Sjouke and Blaauw, Frank and Andrikopoulos, Vasilios",2023.0,,,,Future Internet,Cost-Profiling Microservice Applications Using an APM Stack,Cost-Profiling Microservice Applications Using an APM Stack,https://doaj.org/article/0ebb2db4fd444caf9a27b2504062d11b,"Sjouke de Vries, Frank Blaauw, Vasilios Andrikopoulos; Affiliations Sjouke de Vries Researchable BV, 9747 AN Groningen, The Netherlands"
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,NVIDIA_blackwell,\cite{NVIDIA_blackwell},NVIDIA blackwell architecture,,,True,False,NVIDIA,,,,,,NVIDIA blackwell architecture,Blackwell (microarchitecture) - Wikipedia,https://en.wikipedia.org/wiki/Blackwell_(microarchitecture),"Donate Create account Log in [x] Personal tools Donate Create account Log in Pages for logged out editors learn more Contributions Talk [x] Toggle the table of contents Contents move to sidebar hide (Top) 1 History 2 ArchitectureToggle Architecture subsection 2.1 Process node 2.2 Streaming multiprocessor 2.2.1 CUDA cores 2.2.2 Tensor Cores 3 Blackwell dies 4 See also 5 References Blackwell (microarchitecture) [x] 4 languages Català Deutsch Français 中文 Edit links Article Talk [x] English Read Edit View history [x] Tools Tools move to sidebar hide Actions Read Edit View history General What links here Related changes Upload file Permanent link Page information Cite this page Get shortened URL Download QR code Expand all Edit interlanguage links Print/export Download as PDF Printable version In other projects Wikidata item From Wikipedia, the free encyclopedia GPU microarchitecture designed by Nvidia Blackwell| Launched | Q4 2024 | | --- | | Designed by | Nvidia | | Manufactured by | * TSMC | | Fabrication process | TSMC 4NP (Datacenter[1]) TSMC 4N (Consumer[2]) | | Codename(s) | GB100 GB20x | | Product Series | | Desktop | * GeForce RTX 50 series | | Professional/workstation | * RTX PRO Blackwell series | | Specifications | | Memory support | GDDR7 (Consumer) HBM3e (Datacenter) | | PCIe support | PCIe 5.0 (Consumer) PCIe 6.0 (Datacenter) | | Supported Graphics APIs | | DirectX | DirectX 12 Ultimate (Feature Level 12_2) | | Direct3D | Direct3D 12 | | Shader Model | Shader Model 6.8 | | OpenCL | OpenCL 3.0 (64-bit only, 32-bit support removed)[3] | | OpenGL | OpenGL 4.6 | | Vulkan | Vulkan 1.4 | | Supported Compute APIs | | CUDA | Compute Capability 10.x (64-bit only, 32-bit support removed)[3] Compute Capability 12.x (64-bit only)[3] | | DirectCompute | Yes | | Media Engine | | Encoder(s) supported | NVENC | | History | | Predecessor | Ada Lovelace(consumer) Hopper(datacenter) | | Successor | Rubin | Blackwell is a graphics processing unit (GPU) microarchitecture developed by Nvidia as the successor to the Hopper and Ada Lovelace microarchitectures. Named after statistician and mathematician David Blackwell, the name of the Blackwell architecture was leaked in 2022 with the B40 and B100 accelerators being confirmed in October 2023 with an official Nvidia roadmap shown during an investors presentation. [4] It was officially announced at Nvidia's GTC 2024 keynote on March 18, 2024. [7] Nvidia reportedly sold 500,000 Hopper-based H100 accelerators in Q3 2023 alone. Process node [edit] Blackwell is fabricated on the custom 4NP process node for datacentre products, and on the custom 4N process node for consumer products, from TSMC."
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,NVProf,\cite{NVProf},NVProf,,,True,False,NVIDIA,,,,,,NVProf,NVIDIA Visual Profiler,https://developer.nvidia.com/nvidia-visual-profiler,"NVIDIA Visual Profiler is a tool for optimizing CUDA C/C++ applications on Linux, Windows, and ARM. It provides tables, graphs, timelines, and analysis of CUDA API calls, memory transfers, GPU hardware counters, and more."
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,CUPTI,\cite{CUPTI},CUPTI,,,True,False,NVIDIA,,,,,,CUPTI,NVIDIA CUDA Profiling Tools Interface (CUPTI) - CUDA Toolkit,https://developer.nvidia.com/cupti,"CUPTI is a library that enables the creation of profiling and tracing tools for CUDA applications. Learn about its features, updates, requirements, and how to install it with CUDA Toolkit 12.9."
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,Nsight,\cite{Nsight},Nsight,,,True,False,NVIDIA,,,,,,Nsight,Nsight,https://www.nsight.com/,"Nsight is committed to growing our business in a thoughtful, forward-thinking and, above all, caring way. Guided by the belief that we must give back to the communities we are privileged to serve, Nsight is actively involved in and supports organizations that embody our company's values and extend our mission."
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,pytorch-kineto,\cite{pytorch-kineto},PyTorch Kineto,,,True,False,Kineto,,,,,,PyTorch Kineto,Automated trace collection and analysis - PyTorch,https://pytorch.org/blog/automated-trace-collection/,Kineto is the subsystem within Profiler that interfaces with CUPTI. The PyTorch Profiler leverages the Kineto library to collect GPU traces. To enable automated profiling of training workloads at scale without any user side code instrumentation we made a few fundamental changes to PyTorch. These changes enable trace collection without any user
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,radford2019language,\cite{radford2019language},Language models are unsupervised multitask learners,,,True,False,"Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",2019.0,,,,OpenAI blog,Language models are unsupervised multitask learners,PDF,https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf,"Language Models are Unsupervised Multitask Learners Alec Radford  1 Jeffrey Wu  1 Rewon Child 1 David Luan 1 Dario Amodei  1 Ilya Sutskever  1 Abstract Natural language processing tasks, such as ques-tion answering, machine translation, reading com-prehension, and summarization, are typically approached with supervised learning on task-speciﬁc datasets. When only minimal or no supervised data is available, another line of work has demonstrated the promise of language models to perform speciﬁc tasks, such as commonsense reasoning (Schwartz et al., 2017) and sentiment analysis (Radford et al., 2017). Language Models are Unsupervised Multitask Learners English reference GPT-2 French translation This re-release, titled The Next Day Extra, was presented in the form of three disks: the original album, unpublished studio sessions and remixes, plus a DVD containing the four clips that have already been unveiled."
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,narayanan2021efficient,\cite{narayanan2021efficient},Efficient large-scale language model training on gpu clusters using megatron-lm,,,True,False,"Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others",2021.0,,,,,Efficient large-scale language model training on gpu clusters using megatron-lm,Efficient Large-Scale Language Model Training on GPU Clusters Using ...,https://arxiv.org/abs/2104.04473,"The paper presents a method to train large language models with trillions of parameters on thousands of GPUs using tensor, pipeline, and data parallelism. It compares different parallelism techniques and proposes a novel interleaved pipeline schedule to improve throughput and memory efficiency."
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,shoeybi2019megatron,\cite{shoeybi2019megatron},Megatron-lm: Training multi-billion parameter language models using model parallelism,,,True,False,"Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan",2019.0,,,,arXiv preprint arXiv:1909.08053,Megatron-lm: Training multi-billion parameter language models using model parallelism,PDF,https://deepsense.ai/wp-content/uploads/2023/04/1909.08053.pdf,Megatron-LM is a technique for training transformer models with billions of parameters using intra-layer model parallelism in PyTorch. It achieves high scaling efficiency and state of the art results on several natural language processing tasks.
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,sabne2020xla,\cite{sabne2020xla},Xla: Compiling machine learning for peak performance,,,True,False,"Sabne, Amit",2020.0,,,,Google Res,Xla: Compiling machine learning for peak performance,"GitHub - openxla/xla: A machine learning compiler for GPUs, CPUs, and ...",https://github.com/openxla/xla,"XLA (Accelerated Linear Algebra) is an open-source machine learning (ML) compiler for GPUs, CPUs, and ML accelerators. The XLA compiler takes models from popular ML frameworks such as PyTorch, TensorFlow, and JAX, and optimizes them for high-performance execution across different hardware platforms including GPUs, CPUs, and ML accelerators."
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,zheng2022alpa,\cite{zheng2022alpa},Alpa: Automating inter-and $\{$Intra-Operator$\}$ parallelism for distributed deep learning,,,True,False,"Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Xing, Eric P and others",2022.0,,,,,Alpa: Automating inter-and $\{$Intra-Operator$\}$ parallelism for distributed deep learning,Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed ...,https://arxiv.org/abs/2201.12023,"Alpa automates model-parallel training of large deep learning (DL) models by generating execution plans that unify data, operator, and pipeline parallelism. Existing model-parallel training systems either require users to manually create a parallelization plan or automatically generate one from a limited space of model parallelism configurations. They do not suffice to scale out complex DL"
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,moolchandani2023amped,\cite{moolchandani2023amped},Amped: An analytical model for performance in distributed training of transformers,,,True,False,"Moolchandani, Diksha and Kundu, Joyjit and Ruelens, Frederik and Vrancx, Peter and Evenblij, Timon and Perumkunnil, Manu",2023.0,,,,,Amped: An analytical model for performance in distributed training of transformers,AMPeD: An Analytical Model for Performance in Distributed Training of ...,https://ieeexplore.ieee.org/document/10158196,"In this work, we propose AMPeD, an analytical model for performance in distributed training of transformers. It exposes all the transformer model parameters, potential parallelism choices (along with their mapping onto the system), the accelerator as well as system architecture specffications as tunable knobs, thereby enabling hardware-software"
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,isaev2023calculon,\cite{isaev2023calculon},Calculon: a methodology and tool for high-level co-design of systems and large language models,,,True,False,"Isaev, Mikhail and McDonald, Nic and Dennison, Larry and Vuduc, Richard",2023.0,,,,,Calculon: a methodology and tool for high-level co-design of systems and large language models,Calculon: a methodology and tool for high-level co-design of systems ...,https://dl.acm.org/doi/abs/10.1145/3581784.3607102,"Calculon: a methodology and tool for high-level co-design of systems and large language models Authors : Mikhail Isaev , Nic Mcdonald , Larry Dennison , Richard Vuduc Authors Info & Claims SC '23: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis"
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,zhao2022apollo,\cite{zhao2022apollo},Apollo: Automatic partition-based operator fusion through layer by layer optimization,,,True,False,"Zhao, Jie and Gao, Xiong and Xia, Ruijie and Zhang, Zhaochuang and Chen, Deshi and Chen, Lei and Zhang, Renwei and Geng, Zhen and Cheng, Bin and Jin, Xuefeng",2022.0,,,,Proceedings of Machine Learning and Systems,Apollo: Automatic partition-based operator fusion through layer by layer optimization,图算融合论文分享：Apollo - 知乎 - 知乎专栏,https://zhuanlan.zhihu.com/p/622494022,图算融合论文分享：Apollo本文给大家分享一篇有关多层规约融合的论文 Apollo，Apollo是华为MindSpore图算融合团队和赵捷老师、陈雷老师合作出品的；同时这篇论文也是MindSpore团队与赵捷老师合作的第三篇论文。 ... Apollo: Automatic Partition-based Operator Fusion through Layer by
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,jia2019taso,\cite{jia2019taso},TASO: optimizing deep learning computation with automatic generation of graph substitutions,,,True,False,"Jia, Zhihao and Padon, Oded and Thomas, James and Warszawski, Todd and Zaharia, Matei and Aiken, Alex",2019.0,,,,,TASO: optimizing deep learning computation with automatic generation of graph substitutions,TASO: The Tensor Algebra SuperOptimizer for Deep Learning,https://github.com/jiazhihao/TASO,"TASO: Optimizing Deep Learning Computation with Automated Generation of Graph Substitutions. In Proceedings of the Symposium on Operating Systems Principles (SOSP), Ontario, Canada, October 2019. Zhihao Jia, James Thomas, Todd Warszawski, Mingyu Gao, Matei Zaharia, and Alex Aiken. Optimizing DNN Computation with Relaxed Graph Substitutions. In"
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,rashidi2020astra,\cite{rashidi2020astra},Astra-sim: Enabling sw/hw co-design exploration for distributed dl training platforms,,,True,False,"Rashidi, Saeed and Sridharan, Srinivas and Srinivasan, Sudarshan and Krishna, Tushar",2020.0,,,,,Astra-sim: Enabling sw/hw co-design exploration for distributed dl training platforms,PDF,https://www.bnl.gov/modsim/events/2021/files/talks/tusharkrishna.pdf,"DL Training Co-Design Stack S. Rashidi et al.,""ASTRA-SIM: Enabling SW/HW Co-Design Exploration for Distributed DL Training Platforms"", ISPASS 2020 Figure Courtesy: Srinivas Sridharan (Facebook) We can view the design-space as three layers: Workload layer (aka training loop): •Parallelism approach •Communication size & type •Dependency"
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,won2023astra,\cite{won2023astra},Astra-sim2. 0: Modeling hierarchical networks and disaggregated systems for large-model training at scale,,,True,False,"Won, William and Heo, Taekyung and Rashidi, Saeed and Sridharan, Srinivas and Srinivasan, Sudarshan and Krishna, Tushar",2023.0,,,,,Astra-sim2. 0: Modeling hierarchical networks and disaggregated systems for large-model training at scale,ASTRA-sim2.0: Modeling Hierarchical Networks and Disaggregated Systems ...,https://arxiv.org/abs/2303.14006,"Abstract page for arXiv paper 2303.14006: ASTRA-sim2.0: Modeling Hierarchical Networks and Disaggregated Systems for Large-model Training at Scale As deep learning models and input data are scaling at an unprecedented rate, it is inevitable to move towards distributed training platforms to fit the model and increase training throughput."
"Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM
  Training",2504.09307v1,hu2022dpro,\cite{hu2022dpro},dpro: A generic performance diagnosis and optimization toolkit for expediting distributed dnn training,,,True,False,"Hu, Hanpeng and Jiang, Chenyu and Zhong, Yuchen and Peng, Yanghua and Wu, Chuan and Zhu, Yibo and Lin, Haibin and Guo, Chuanxiong",2022.0,,,,Proceedings of Machine Learning and Systems,dpro: A generic performance diagnosis and optimization toolkit for expediting distributed dnn training,dPRO: A Generic Performance Diagnosis and Optimization Toolkit for ...,https://arxiv.org/pdf/2205.02473,"there exists no software tool which diagnoses performance issues and helps expedite distributed DNN training, while the training can be run using different deep learning frameworks. This paper proposes dPRO, a toolkit that includes: (1) an efﬁcient proﬁler that collects runtime traces of distributed DNN training across multiple"
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,covington2016deep,\cite{covington2016deep},Deep neural networks for youtube recommendations,,,True,False,"Covington, Paul and Adams, Jay and Sargin, Emre",2016.0,,,,,Deep neural networks for youtube recommendations,Deep Neural Networks for YouTube Recommendations,https://paperswithcode.com/paper/deep-neural-networks-for-youtube,"Learn how YouTube uses deep learning to improve its recommendation system. The paper describes the system architecture, the candidate generation model, the ranking model, and the practical insights."
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,pinsage,\cite{pinsage},Graph convolutional neural networks for web-scale recommender systems,,,True,False,"Ying, Rex and He, Ruining and Chen, Kaifeng and Eksombatchai, Pong and Hamilton, William L and Leskovec, Jure",2018.0,,,,,Graph convolutional neural networks for web-scale recommender systems,Graph Convolutional Neural Networks for Web-Scale Recommender Systems,https://arxiv.org/abs/1806.01973,"The authors present PinSage, a large-scale deep recommendation engine based on graph convolutional networks (GCNs) for web-scale recommender systems. They apply GCNs to a graph of 3 billion nodes and 18 billion edges representing pins and boards, and show that PinSage outperforms other methods."
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,lin2024effective,\cite{lin2024effective},Effective Job-market Mobility Prediction with Attentive Heterogeneous Knowledge Learning and Synergy,,,True,False,"Lin, Sida and Zhang, Zhouyi and Chen, Yankai and Ma, Chenhao and Fang, Yixiang and Dai, Shan and Lu, Guangli",2024.0,,,,,Effective Job-market Mobility Prediction with Attentive Heterogeneous Knowledge Learning and Synergy,PDF,https://chenhao-ma.github.io/papers/CIKM24jobpred.pdf,"Dai, and Guangli Lu. 2024. Effective Job-market Mobility Prediction with Attentive Heterogeneous Knowledge Learning and Synergy. In Proceedings ∗Equal Contribution. †Corresponding Authors. Permission to make digital or hard copies of all or part of this work for personal or"
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,zhang2022knowledge,\cite{zhang2022knowledge},Knowledge-aware neural networks with personalized feature referencing for cold-start recommendation,,,True,False,"Zhang, Xinni and Chen, Yankai and Gao, Cuiyun and Liao, Qing and Zhao, Shenglin and King, Irwin",2022.0,,,,arXiv preprint arXiv:2209.13973,Knowledge-aware neural networks with personalized feature referencing for cold-start recommendation,Knowledge-aware Neural Networks with Personalized Feature Referencing ...,https://arxiv.org/abs/2209.13973,"Incorporating knowledge graphs (KGs) as side information in recommendation has recently attracted considerable attention. Despite the success in general recommendation scenarios, prior methods may fall short of performance satisfaction for the cold-start problem in which users are associated with very limited interactive information. Since the conventional methods rely on exploring the"
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,luo2025rank,\cite{luo2025rank},Rank Gap Sensitive Deep AUC maximization for CTR prediction,,,True,False,"Luo, Fangyuan and Chen, Yankai and Wu, Jun and Li, Yidong",2025.0,,,,Pattern Recognition,Rank Gap Sensitive Deep AUC maximization for CTR prediction,Rank Gap Sensitive Deep AUC maximization for CTR prediction,https://library.plu.edu/discovery/fulldisplay/cdi_crossref_primary_10_1016_j_patcog_2025_111496/01PACLUTH_INST:PACLUTH,"Deep Neural Network (DNN) stands out as one widely adopted and effective technique for Click-Through Rate (CTR) prediction in live recommender systems. However, the prevalent DNN-based CTR methods exhibit two main drawbacks. On one hand, they fail to align their optimization objectives with the benchmark metric, such as the Area Under the ROC Curve (AUC), designed for ranking tasks. On the"
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,chen2017attentive,\cite{chen2017attentive},Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention,,,True,False,"Chen, Jingyuan and Zhang, Hanwang and He, Xiangnan and Nie, Liqiang and Liu, Wei and Chua, Tat-Seng",2017.0,,,,,Attentive collaborative filtering: Multimedia recommendation with item-and component-level attention,Attentive Collaborative Filtering: Multimedia Recommendation with Item ...,https://www.researchgate.net/publication/318764160_Attentive_Collaborative_Filtering_Multimedia_Recommendation_with_Item-_and_Component-Level_Attention,Request PDF | Attentive Collaborative Filtering: Multimedia Recommendation with Item- and Component-Level Attention | Multimedia content is dominating today's Web information. The nature of
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,wu2023survey,\cite{wu2023survey},A survey on graph embedding techniques for biomedical data: Methods and applications,,,True,False,"Wu, Yaozu and Chen, Yankai and Yin, Zhishuai and Ding, Weiping and King, Irwin",2023.0,,,,Information Fusion,A survey on graph embedding techniques for biomedical data: Methods and applications,A survey on graph embedding techniques for biomedical data: Methods and ...,https://www.sciencedirect.com/science/article/pii/S1566253523002257,"A survey on graph embedding techniques for biomedical data: Methods and applications ... As a result of the expeditious advancement of biomedical technologies, a plethora of relational data linking biomedical entities such as genes, proteins, and drugs have been collected for modern biomedical research."
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,zhang2024geometric,\cite{zhang2024geometric},Geometric view of soft decorrelation in self-supervised learning,,,True,False,"Zhang, Yifei and Zhu, Hao and Song, Zixing and Chen, Yankai and Fu, Xinyu and Meng, Ziqiao and Koniusz, Piotr and King, Irwin",2024.0,,,,,Geometric view of soft decorrelation in self-supervised learning,Geometric View of Soft Decorrelation in Self-Supervised Learning,https://dl.acm.org/doi/10.1145/3637528.3671914,"We provide a new perspective on the geometric distance between positive definite matrices to investigate why the soft decorrelation cannot efficiently solve the dimensional collapse. Furthermore, we construct a family of loss functions utilizing the Bregman Matrix Divergence (BMD), with the soft decorrelation representing a specific instance"
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,zhang2023contrastive,\cite{zhang2023contrastive},Contrastive cross-scale graph knowledge synergy,,,True,False,"Zhang, Yifei and Chen, Yankai and Song, Zixing and King, Irwin",2023.0,,,,,Contrastive cross-scale graph knowledge synergy,Contrastive Cross-scale Graph Knowledge Synergy,https://dl.acm.org/doi/10.1145/3580305.3599286,"Graph representation learning via Contrastive Learning (GCL) has drawn considerable attention recently. Efforts are mainly focused on gathering more global information via contrasting on a single high-level graph view, which, however, underestimates the inherent complex and hierarchical properties in many real-world networks, leading to sub-optimal embeddings."
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,wang2022aep,\cite{wang2022aep},AEP: Aligning knowledge graphs via embedding propagation,,,True,False,"Wang, Chenxu and Wan, Yue and Huang, Zhenhao and Meng, Panpan and Wang, Pinghui",2022.0,,,,Neurocomputing,AEP: Aligning knowledge graphs via embedding propagation,AEP: Aligning knowledge graphs via embedding propagation,https://www.sciencedirect.com/science/article/pii/S0925231222009936,"This paper presents AEP, a novel knowledge graph alignment framework via embedding propagation. AEP handles the interference of heterogeneity and overfitting problems for entity representation learning. First, AEP employs Fasttext [26] to obtain the initialized entity embeddings which encode entity names' semantic information. Next, we"
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,lsh,\cite{lsh},Similarity search in high dimensions via hashing,,,True,False,"Gionis, Aristides and Indyk, Piotr and Motwani, Rajeev and others",1999.0,,,,,Similarity search in high dimensions via hashing,Similarity Search in High Dimensions via Hashing,https://dl.acm.org/doi/10.5555/645925.671516,"Binary hashing has been widely used for efficient similarity search. Learning efficient codes has become a research focus and it is still a challenge. In many cases, the real-world data often lies on a low-dimensional manifold, which should be taken"
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,li2014two,\cite{li2014two},Two-Stage Hashing for Fast Document Retrieval.,,,True,False,"Li, Hao and Liu, Wei and Ji, Heng",2014.0,,,,,Two-Stage Hashing for Fast Document Retrieval.,Two-Stage Hashing for Fast Document Retrieval - KIPDF.COM,https://kipdf.com/two-stage-hashing-for-fast-document-retrieval_5ae857227f8b9adb5b8b4626.html,"Two-Stage Hashing for Fast Document Retrieval Hao Li∗ Wei Liu† Heng Ji∗ ∗ Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA"
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,zhang2016discrete,\cite{zhang2016discrete},Discrete collaborative filtering,,,True,False,"Zhang, Hanwang and Shen, Fumin and Liu, Wei and He, Xiangnan and Luan, Huanbo and Chua, Tat-Seng",2016.0,,,,,Discrete collaborative filtering,Discrete limited attentional collaborative filtering for fast social ...,https://www.sciencedirect.com/science/article/pii/S0952197623006218,"Discrete Collaborative Filtering (DCF) is the first proposed unified framework that directly optimizes discrete values and imposes balance and de-correlation constraints to learn compact yet informative hash codes (Zhang et al., 2016)."
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,zhang2017discrete,\cite{zhang2017discrete},Discrete personalized ranking for fast collaborative filtering from implicit feedback,,,True,False,"Zhang, Yan and Lian, Defu and Yang, Guowu",2017.0,,,,,Discrete personalized ranking for fast collaborative filtering from implicit feedback,Discrete personalized ranking for fast collaborative filtering from ...,https://dl.acm.org/doi/10.5555/3298239.3298481,"Discrete personalized ranking for fast collaborative filtering from implicit feedback. ... we propose a learning-based hashing framework called Discrete Personalized Ranking (DPR), to map users and items to a Hamming space, where user-item affinity can be efficiently calculated via Hamming distance. ... Gantner, Z.; and Schmidt-Thieme, L. 2009"
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,li2019learning,\cite{li2019learning},Learning binary codes with neural collaborative filtering for efficient recommendation systems,,,True,False,"Li, Yang and Wang, Suhang and Pan, Quan and Peng, Haiyun and Yang, Tao and Cambria, Erik",2019.0,,,,KBS,Learning binary codes with neural collaborative filtering for efficient recommendation systems,Learning binary codes with neural collaborative filtering for efficient ...,https://www.sciencedirect.com/science/article/pii/S0950705119300735,"Hash collaborative filtering, which learns the binary representation of users and items, has become a popular efficient recommendation technique [7], [8], [9], [10].By learning binary codes, the storage requirement can be significantly reduced as storing each binary code only require 4 bytes if the code length is 32."
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,hashgnn,\cite{hashgnn},Learning to hash with GNNs for recommender systems,,,True,False,"Tan, Qiaoyu and Liu, Ninghao and Zhao, Xing and Yang, Hongxia and Zhou, Jingren and Hu, Xia",2020.0,,,,,Learning to hash with GNNs for recommender systems,Learning to Hash with Graph Neural Networks for Recommender Systems,https://ar5iv.labs.arxiv.org/html/2003.01917,"The developed model is generally applicable to arbitrary GNNs. The learned hash codes could improve the efficiency of item retrieval, while the associated continuous representations could improve the capacity of ranking model in modern recommender system. ... Deep learning based recommender system: A survey and new perspectives. ACM Computing"
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,chen2021towards,\cite{chen2021towards},Towards low-loss 1-bit quantization of user-item representations for top-k recommendation,,,True,False,"Chen, Yankai and Zhang, Yifei and Zhang, Yingxue and Guo, Huifeng and Li, Jingjie and Tang, Ruiming and He, Xiuqiang and King, Irwin",2021.0,,,,arXiv preprint arXiv:2112.01944,Towards low-loss 1-bit quantization of user-item representations for top-k recommendation,Towards Low-loss 1-bit Quantization of User-item Representations for ...,https://arxiv.org/abs/2112.01944,"Abstract page for arXiv paper 2112.01944: Towards Low-loss 1-bit Quantization of User-item Representations for Top-K Recommendation Due to the promising advantages in space compression and inference acceleration, quantized representation learning for recommender systems has become an emerging research direction recently."
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,bigcn,\cite{bigcn},Bi-gcn: Binary graph convolutional network,,,True,False,"Wang, Junfu and Wang, Yunhong and Yang, Zhen and Yang, Liang and Guo, Yuanfang",2021.0,,,,,Bi-gcn: Binary graph convolutional network,Bi-GCN: Binary Graph Convolutional Network - IEEE Xplore,https://ieeexplore.ieee.org/document/9578420,"In this paper, we pioneer to propose a Binary Graph Convolutional Network (Bi-GCN), which binarizes both the network parameters and input node features. Besides, the original matrix multiplications are revised to binary operations for accelerations. According to the theoretical analysis, our Bi-GCN can reduce the memory consumption by an"
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,wang2024graph,\cite{wang2024graph},Graph contrastive learning with high-order feature interactions and adversarial Wasserstein-distance-based alignment,,,True,False,"Wang, Chenxu and Wan, Zhizhong and Meng, Panpan and Wang, Shihao and Wang, Zhanggong",2024.0,,,,IJMLC,Graph contrastive learning with high-order feature interactions and adversarial Wasserstein-distance-based alignment,Unsupervised Graph Alignment with Wasserstein Distance Discriminator ...,https://dl.acm.org/doi/10.1145/3447548.3467332,"Global Alignment of Multiple Protein Interaction Networks with Application to Functional Orthology detection. Proceedings of the National Academy of Sciences, Vol. 105, 35 (2008), 12763--12768. ... Graph contrastive learning with high-order feature interactions and adversarial Wasserstein-distance-based alignment International Journal of"
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,wu2022knowledge,\cite{wu2022knowledge},Knowledge distillation improves graph structure augmentation for graph neural networks,,,True,False,"Wu, Lirong and Lin, Haitao and Huang, Yufei and Li, Stan Z",2022.0,,,,NeurIPS,Knowledge distillation improves graph structure augmentation for graph neural networks,Knowledge distillation improves graph structure augmentation for graph ...,https://dl.acm.org/doi/10.5555/3600270.3601128,"To tackle this problem, we propose a novel Knowledge Distillation for Graph Augmentation (KDGA) framework, which helps to reduce the potential negative effects of distribution shifts, i.e., negative augmentation problem. Specifically, KDGA extracts the knowledge of any GNN teacher model trained on the augmented graphs and injects it into a"
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,liu2024fine,\cite{liu2024fine},Fine-grained learning behavior-oriented knowledge distillation for graph neural networks,,,True,False,"Liu, Kang and Huang, Zhenhua and Wang, Chang-Dong and Gao, Beibei and Chen, Yunwen",2024.0,,,,TNNLS,Fine-grained learning behavior-oriented knowledge distillation for graph neural networks,Fine-Grained Learning Behavior-Oriented Knowledge Distillation for ...,https://ieeexplore.ieee.org/document/10599879,"Knowledge distillation (KD), as an effective compression technology, is used to reduce the resource consumption of graph neural networks (GNNs) and facilitate their deployment on resource-constrained devices. Numerous studies exist on GNN distillation, and however, the impacts of knowledge complexity and differences in learning behavior between teachers and students on distillation efficiency"
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,su2021semi,\cite{su2021semi},Semi-supervised knowledge distillation for cross-modal hashing,,,True,False,"Su, Mingyue and Gu, Guanghua and Ren, Xianlong and Fu, Hao and Zhao, Yao",2021.0,,,,IEEE Transactions on Multimedia,Semi-supervised knowledge distillation for cross-modal hashing,Semi-Supervised Knowledge Distillation for Cross-Modal Hashing,https://ieeexplore.ieee.org/document/9623504,"Moreover, most cross-modal hashing methods only handle two modalities of image and text, without taking the scene of multiple modalities into consideration. In this paper, we propose a novel semi-supervised approach called semi-supervised knowledge distillation for cross-modal hashing (SKDCH) to overcome the above-mentioned challenges, which"
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,tan2022teacher,\cite{tan2022teacher},Teacher-student learning: Efficient hierarchical message aggregation hashing for cross-modal retrieval,,,True,False,"Tan, Wentao and Zhu, Lei and Li, Jingjing and Zhang, Huaxiang and Han, Junwei",2022.0,,,,IEEE Transactions on Multimedia,Teacher-student learning: Efficient hierarchical message aggregation hashing for cross-modal retrieval,Teacher-Student Learning: Efficient Hierarchical Message Aggregation ...,https://ieeexplore.ieee.org/abstract/document/9782694,"On the student end, we train a couple of student modules that learn hash functions to support cross-modal retrieval. We design a cross-modal correlation knowledge distillation strategy which seamlessly transfers the modelled fine-grained multi-modal semantic correlations from the teacher to the lightweight student modules. With the fine-grained"
"Learning Binarized Representations with Pseudo-positive Sample
  Enhancement for Efficient Graph Collaborative Filtering",2506.02750v1,yu2024unsupervised,\cite{yu2024unsupervised},Unsupervised Multimodal Graph Contrastive Semantic Anchor Space Dynamic Knowledge Distillation Network for Cross-Media Hash Retrieval,,,True,False,"Yu, Yang and Liang, Meiyu and Yin, Mengran and Lu, Kangkang and Du, Junping and Xue, Zhe",2024.0,,,,,Unsupervised Multimodal Graph Contrastive Semantic Anchor Space Dynamic Knowledge Distillation Network for Cross-Media Hash Retrieval,Unsupervised Multimodal Graph Contrastive Semantic Anchor Space Dynamic ...,https://ieeexplore.ieee.org/document/10598086,"To address these challenges, we propose a novel unsupervised multimodal graph contrastive semantic anchor space dynamic knowledge distillation network for cross-media hash retrieval (GASKN). Firstly, to obtain a multimodal semantic anchor space, we construct a large multimodal fusion teacher model using the BEiT-3 model as the backbone."
"What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile
  App Reviews",2505.23452v1,Riccosan2023,\cite{Riccosan2023},Multilabel multiclass sentiment and emotion dataset from Indonesian mobile application review,,,True,False,Riccosan and K. E. Saputra,2023.0,,,10.1016/j.dib.2023.109576,Data in Brief,Multilabel multiclass sentiment and emotion dataset from Indonesian mobile application review,Multilabel multiclass sentiment and emotion dataset from indonesian ...,https://www.sciencedirect.com/science/article/pii/S2352340923006662,"The research's dataset was assembled from indonesian public reviews of mobile applications in Indonesia. This dataset consists of sentences with multi-label properties because they contain a sentiment label and an emotion label, and it is multi-class since it has 3 sentiment classes (positive, negative, neutral) and 6 emotion classes (anger, sad, fear, happy, love, neutral)."
"What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile
  App Reviews",2505.23452v1,Keertipati2016,\cite{Keertipati2016},Approaches for prioritizing feature improvements extracted from app reviews,,,True,False,"Keertipati, Swetha and Savarimuthu, Bastin Tony Roy and Licorish, Sherlock A.",2016.0,,,10.1145/2915970.2916003,,Approaches for prioritizing feature improvements extracted from app reviews,Approaches for prioritizing feature improvements extracted from app reviews,https://dl.acm.org/doi/10.1145/2915970.2916003,"App reviews contain valuable feedback about what features should be fixed and improved. This feedback could be 'mined' to facilitate app maintenance and evolution. While requirements are routinely extracted from post-release users' feedback in traditional projects, app reviews are often generated by a much larger client-base with competing"
"What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile
  App Reviews",2505.23452v1,Singh2022101929,\cite{Singh2022101929},An empirical analysis of mobile learning app usage experience,,,True,False,Yashdeep Singh and Pradeep Kumar Suri,2022.0,,,10.1016/j.techsoc.2022.101929,Technology in Society,An empirical analysis of mobile learning app usage experience,An empirical analysis of mobile learning app usage experience - IDEAS/RePEc,https://ideas.repec.org/a/eee/teinso/v68y2022ics0160791x22000707.html,"The article concludes with a discussion on the learners' experience using mobile learning apps, implications for practice, limitations, and future research directions. ... 2022. ""An empirical analysis of mobile learning app usage experience,"" Technology in Society, Elsevier, vol. 68(C). Handle: RePEc:eee:teinso:v:68:y:2022:i:c:s0160791x22000707"
"What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile
  App Reviews",2505.23452v1,Savarimuthu2023,\cite{Savarimuthu2023},Improving Information Systems Sustainability by Applying Machine Learning to Detect and Reduce Data Waste,,,True,False,B. Savarimuthu and J. Corbett and M. Yasir and V. Lakshmi,2023.0,,,10.17705/1CAIS.05308,Communications of the Association for Information Systems,Improving Information Systems Sustainability by Applying Machine Learning to Detect and Reduce Data Waste,Improving Information Systems Sustainability by Applying Machine ...,https://www.semanticscholar.org/paper/Improving-Information-Systems-Sustainability-by-to-Savarimuthu-Corbett/e6086542728a859d8eda008b0c6954d482874870,"The first component comprises 13 machine learning models for detecting data waste. Applying these to 35,576 online reviews in two domains reveals data waste of 1.9% for restaurant reviews compared to 35.8% for app reviews. ... This work contributes to design knowledge relating to sustainable information systems by highlighting the new class of"
"What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile
  App Reviews",2505.23452v1,Cabellos2022,\cite{Cabellos2022},"Do pro-social video games promote moral activity?: an analysis of user reviews of Papers, Please",,,True,False,B. Cabellos and J. I. Pozo and K. Mar{\'i}n-Rubio and others,2022.0,,,10.1007/s10639-022-11072-x,Education and Information Technologies,"Do pro-social video games promote moral activity?: an analysis of user reviews of Papers, Please",Do pro-social video games promote... preview & related info - Mendeley,https://www.mendeley.com/catalogue/42c00dd7-4dea-340a-a685-8de48f957e67/,"Do pro-social video games promote moral activity?: an analysis of user reviews of Papers, Please. Cabellos B; Pozo J; Marín-Rubio K; et al. See more; Education and Information Technologies (2022) 27(8) 11411-11442. DOI: 10.1007/s10639-022-11072-x. 9 Citations. Citations of this article."
"What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile
  App Reviews",2505.23452v1,Hou2024,\cite{Hou2024},Large Language Models for Software Engineering: A Systematic Literature Review,,,True,False,"Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu",2024.0,,,10.1145/3695988,ACM Trans. Softw. Eng. Methodol.,Large Language Models for Software Engineering: A Systematic Literature Review,Large Language Models for Software Engineering: A Systematic Literature ...,https://arxiv.org/abs/2308.10620,"arXiv:2308.10620 (cs) View a PDF of the paper titled Large Language Models for Software Engineering: A Systematic Literature Review, by Xinyi Hou and 9 other authors Abstract:Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Subjects:Software Engineering (cs.SE); Artificial Intelligence (cs.AI)Cite as:arXiv:2308.10620 [cs.SE] (or arXiv:2308.10620v6 [cs.SE] for this version) https://doi.org/10.48550/arXiv.2308.10620Focus to learn morearXiv-issued DOI via DataCite View a PDF of the paper titled Large Language Models for Software Engineering: A Systematic Literature Review, by Xinyi Hou and 9 other authors Bibliographic Explorer Toggle Connected Papers Toggle Litmaps Toggle scite.ai Toggle alphaXiv Toggle Links to Code Toggle DagsHub Toggle GotitPub Toggle Huggingface Toggle Links to Code Toggle ScienceCast Toggle Replicate Toggle Spaces Toggle Spaces Toggle Core recommender toggle"
"What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile
  App Reviews",2505.23452v1,Heseltine2024,\cite{Heseltine2024},Large language models as a substitute for human experts in annotating political text,,,True,False,"Heseltine, Thomas and Hohenberg, John",2024.0,,,,Research \& Politics,Large language models as a substitute for human experts in annotating political text,Large language models as a substitute for human experts in annotating ...,https://journals.sagepub.com/doi/10.1177/20531680241236239,"Recent works have, however, shown the potential for large language models (LLMs) such as the GPT family to perform a range of tasks in the social sciences, including ideological scaling (Wu et al., 2023), the classification of legislation (), and the detection of hate speech (Huang et al., 2023).LLM classification may therefore be a viable means of reducing manual annotation labour and cutting"
"What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile
  App Reviews",2505.23452v1,Sayeed2024,\cite{Sayeed2024},{Annotating Materials Science Text: A Semi-automated Approach for Crafting Outputs with Gemini Pro},,,True,False,"Sayeed, Hasan M.
and Mohanty, Trupti
and Sparks, Taylor D.",2024.0,Jun,,10.1007/s40192-024-00356-4,Integrating Materials and Manufacturing Innovation,{Annotating Materials Science Text: A Semi-automated Approach for Crafting Outputs with Gemini Pro},Semi-automatic annotation of materials science text accelerates data ...,https://ceramics.org/ceramic-tech-today/semi-automatic-annotation-of-materials-science-text-accelerates-data-extraction-from-literature/,"The paper, published in Integrating Materials and Manufacturing Innovation, is ""Annotating materials science text: A semi-automated approach for crafting outputs with Gemini Pro"" (DOI: 10.1007/s40192-024-00356-4)."
"What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile
  App Reviews",2505.23452v1,Shan2024,\cite{Shan2024},Using Large Language Models to Automate Annotation and Part-of-Math Tagging of Math Equations,,,True,False,"Shan, Ruocheng and Youssef, Abdou",2024.0,,,10.1007/978-3-031-66997-2_1,,Using Large Language Models to Automate Annotation and Part-of-Math Tagging of Math Equations,Using Large Language Models to Automate Annotation and Part-of-Math ...,https://link.springer.com/chapter/10.1007/978-3-031-66997-2_1,"This paper explores the potential of leveraging Large Language Models (LLMs) for the tasks of automated annotation and Part-of-Math (POM) tagging of equations. Traditional methods for math term annotation and POM tagging rely heavily on manually crafted rules and limited datasets, which often result in scalability issues and insufficient"
"What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile
  App Reviews",2505.23452v1,Aguda2024,\cite{Aguda2024},Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency,,,True,False,"Aguda, Toyin D.  and
      Siddagangappa, Suchetha  and
      Kochkina, Elena  and
      Kaur, Simerjot  and
      Wang, Dongsheng  and
      Smiley, Charese",2024.0,,https://aclanthology.org/2024.lrec-main.885/,,,Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency,Large Language Models as Financial Data Annotators: A Study on ...,https://aclanthology.org/2024.lrec-main.885/,"Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024) , pages 10124-10145, Torino, Italia."
"What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile
  App Reviews",2505.23452v1,kim-etal-2024-meganno,\cite{kim-etal-2024-meganno},{MEGA}nno+: A Human-{LLM} Collaborative Annotation System,,,True,False,"Kim, Hannah  and
      Mitra, Kushan  and
      Li Chen, Rafael  and
      Rahman, Sajjadur  and
      Zhang, Dan",2024.0,,,,,{MEGA}nno+: A Human-{LLM} Collaborative Annotation System,MEGAnno+: A Human-LLM Collaborative Annotation System,https://aclanthology.org/2024.eacl-demo.18/,"Therefore, we advocate a collaborative approach where humans and LLMs work together to produce reliable and high-quality labels. We present MEGAnno+, a human-LLM collaborative annotation system that offers effective LLM agent and annotation management, convenient and robust LLM annotation, and exploratory verification of LLM labels by humans."
"What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile
  App Reviews",2505.23452v1,Wang2024,\cite{Wang2024},Human-LLM Collaborative Annotation Through Effective Verification of LLM Labels,,,True,False,"Wang, Xinru and Kim, Hannah and Rahman, Sajjadur and Mitra, Kushan and Miao, Zhengjie",2024.0,,,10.1145/3613904.3641960,,Human-LLM Collaborative Annotation Through Effective Verification of LLM Labels,Human-LLM Collaborative Annotation Through Effective Verification of ...,https://dl.acm.org/doi/full/10.1145/3613904.3641960,"This paper presents a multi-step human-LLM collaborative approach where (1) LLMs generate labels and provide explanations, (2) a verifier assesses the quality of LLM-generated labels, and (3) human annotators re-annotate a subset of labels with lower verification scores."
"What About Emotions? Guiding Fine-Grained Emotion Extraction from Mobile
  App Reviews",2505.23452v1,Rouzegar2024,\cite{Rouzegar2024},Enhancing Text Classification through LLM-Driven Active Learning and Human Annotation,,,True,False,Hamidreza Rouzegar and Masoud Makrehchi,2024.0,March,https://aclanthology.org/2024.law-1.10,10.18653/v1/2024.law-1.10,,Enhancing Text Classification through LLM-Driven Active Learning and Human Annotation,Enhancing Text Classification through LLM-Driven Active Learning and ...,https://arxiv.org/html/2406.12114v1,"The paper demonstrates that combining Large Language Models (LLMs), such as GPT-3.5, with human annotators in an Active Learning framework significantly enhances text classification tasks. This hybrid approach, which selectively employs either GPT-3.5 or human annotations based on confidence thresholds, efficiently balances cost and accuracy."
Revisiting Self-attention for Cross-domain Sequential Recommendation,2505.21811v1,wu2017recurrent,\cite{wu2017recurrent},Recurrent recommender networks,,,True,False,"Wu, Chao-Yuan and Ahmed, Amr and Beutel, Alex and Smola, Alexander J and Jing, How",2017.0,,,,,Recurrent recommender networks,Recurrent Recommender Networks | Proceedings of the Tenth ACM ...,https://dl.acm.org/doi/10.1145/3018661.3018689,"We propose Recurrent Recommender Networks (RRN) that are able to predict future behavioral trajectories. This is achieved by endowing both users and movies with a Long Short-Term Memory (LSTM) autoregressive model that captures dynamics, in addition to a more traditional low-rank factorization. On multiple real-world datasets, our model offers"
Revisiting Self-attention for Cross-domain Sequential Recommendation,2505.21811v1,liu2018stamp,\cite{liu2018stamp},STAMP: short-term attention/memory priority model for session-based recommendation,,,True,False,"Liu, Qiao and Zeng, Yifu and Mokhosi, Refuoe and Zhang, Haibin",2018.0,,,,,STAMP: short-term attention/memory priority model for session-based recommendation,STAMP | Proceedings of the 24th ACM SIGKDD International Conference on ...,https://dl.acm.org/doi/10.1145/3219819.3219950,"STAMP: Short-Term Attention/Memory Priority Model for Session-based Recommendation. Authors: Qiao Liu, ... A novel short-term attention/memory priority model is proposed as a remedy, which is capable of capturing users' general interests from the long-term memory of a session context, whilst taking into account users' current interests from the"
Revisiting Self-attention for Cross-domain Sequential Recommendation,2505.21811v1,fan2021lighter,\cite{fan2021lighter},Lighter and better: low-rank decomposed self-attention networks for next-item recommendation,,,True,False,"Fan, Xinyan and Liu, Zheng and Lian, Jianxun and Zhao, Wayne Xin and Xie, Xing and Wen, Ji-Rong",2021.0,,,,,Lighter and better: low-rank decomposed self-attention networks for next-item recommendation,Lighter and Better: Low-Rank Decomposed Self-Attention Networks for ...,https://dl.acm.org/doi/10.1145/3404835.3462978,"This is the video about our work: Lighter and Better: Low-Rank Decomposed Self-Attention Networks for Next-Item Recommendation. In this video, we firstly analyze two major shortcomings of self-attention-based recommenders. And we introduce our approach-the low-rank decomposed self-attention networks (LightSANs)."
Revisiting Self-attention for Cross-domain Sequential Recommendation,2505.21811v1,hou2024large,\cite{hou2024large},Large language models are zero-shot rankers for recommender systems,,,True,False,"Hou, Yupeng and Zhang, Junjie and Lin, Zihan and Lu, Hongyu and Xie, Ruobing and McAuley, Julian and Zhao, Wayne Xin",2024.0,,,,,Large language models are zero-shot rankers for recommender systems,PDF,https://link.springer.com/content/pdf/10.1007/978-3-031-56060-6_24.pdf?pdf=inline+link,This paper investigates the capacity of large language models (LLMs) to rank candidate items for recommender systems without fine-tuning. It shows that LLMs have promising zero-shot ranking abilities but face challenges such as order perception and position bias.
Revisiting Self-attention for Cross-domain Sequential Recommendation,2505.21811v1,cui2022m6,\cite{cui2022m6},M6-rec: Generative pretrained language models are open-ended recommender systems,,,True,False,"Cui, Zeyu and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia",2022.0,,,,arXiv preprint arXiv:2205.08084,M6-rec: Generative pretrained language models are open-ended recommender systems,【论文笔记】M6-Rec - 知乎 - 知乎专栏,https://zhuanlan.zhihu.com/p/619468162,Title: M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems Journal: arxiv:2205.08084 From: 达摩院 Source code: 无 Abstract: 推荐系统正在变得越来越复杂，因为推荐涉及众多领域，面临众多任务。目前的主流方法都是针对特定领域特定任务设计特定的算法。
Revisiting Self-attention for Cross-domain Sequential Recommendation,2505.21811v1,gao2013cross,\cite{gao2013cross},Cross-domain recommendation via cluster-level latent factor model,,,True,False,"Gao, Sheng and Luo, Hao and Chen, Da and Li, Shantao and Gallinari, Patrick and Guo, Jun",2013.0,,,,,Cross-domain recommendation via cluster-level latent factor model,Cross-domain recommendation via cluster-level latent factor model ...,https://dl.acm.org/doi/10.5555/3120137.3120151,"In this paper, we propose a novel cluster-level based latent factor model to enhance the cross-domain recommendation, which can not only learn the common rating pattern shared across domains with the flexibility in controlling the optimal level of sharing, but also learn the domain-specific rating patterns of users in each domain that involve"
Revisiting Self-attention for Cross-domain Sequential Recommendation,2505.21811v1,singh2008relational,\cite{singh2008relational},Relational learning via collective matrix factorization,,,True,False,"Singh, Ajit P and Gordon, Geoffrey J",2008.0,,,,,Relational learning via collective matrix factorization,Collective Matrix Factorization - GitHub,https://github.com/david-cortes/cmfrec,"Implementation of collective matrix factorization, based on ""Relational learning via collective matrix factorization"", with some enhancements and alternative models for cold-start recommendations as described in ""Cold-start recommendations in Collective Matrix Factorization"", and adding implicit-feedback variants as described in ""Collaborative filtering for implicit feedback datasets""."
Revisiting Self-attention for Cross-domain Sequential Recommendation,2505.21811v1,liu2020cross,\cite{liu2020cross},Cross domain recommendation via bi-directional transfer graph collaborative filtering networks,,,True,False,"Liu, Meng and Li, Jianjun and Li, Guohui and Pan, Peng",2020.0,,,,,Cross domain recommendation via bi-directional transfer graph collaborative filtering networks,Cross Domain Recommendation via Bi-directional Transfer Graph ...,https://dl.acm.org/doi/10.1145/3340531.3412012,"By leveraging the knowledge from relevant domains, the cross-domain recommendation technique can be an effective way of alleviating the data sparsity problem. In this paper, we propose a novel Bi-directional Transfer learning method for cross-domain recommendation by using Graph Collaborative Filtering network as the base model (BiTGCF)."
Revisiting Self-attention for Cross-domain Sequential Recommendation,2505.21811v1,zhu2019dtcdr,\cite{zhu2019dtcdr},Dtcdr: A framework for dual-target cross-domain recommendation,,,True,False,"Zhu, Feng and Chen, Chaochao and Wang, Yan and Liu, Guanfeng and Zheng, Xiaolin",2019.0,,,,,Dtcdr: A framework for dual-target cross-domain recommendation,DTCDR: A Framework for Dual-Target Cross-Domain Recommendation,https://www.researchgate.net/publication/337018321_DTCDR_A_Framework_for_Dual-Target_Cross-Domain_Recommendation,"To this end, in this paper, we propose a new framework, DTCDR, for Dual-Target Cross-Domain Recommendation. In DTCDR, we first extensively utilize rating and multi-source content information to"
Revisiting Self-attention for Cross-domain Sequential Recommendation,2505.21811v1,li2023one,\cite{li2023one},"One for all, all for one: Learning and transferring user embeddings for cross-domain recommendation",,,True,False,"Li, Chenglin and Xie, Yuanzhen and Yu, Chenyun and Hu, Bo and Li, Zang and Shu, Guoqiang and Qie, Xiaohu and Niu, Di",2023.0,,,,,"One for all, all for one: Learning and transferring user embeddings for cross-domain recommendation","One for All, All for One: Learning and Transferring User Embeddings for ...",https://arxiv.org/abs/2211.11964,"Abstract page for arXiv paper 2211.11964: One for All, All for One: Learning and Transferring User Embeddings for Cross-Domain Recommendation. Cross-domain recommendation is an important method to improve recommender system performance, especially when observations in target domains are sparse. However, most existing techniques focus on"
Revisiting Self-attention for Cross-domain Sequential Recommendation,2505.21811v1,hwang2024multi,\cite{hwang2024multi},Multi-Domain Sequential Recommendation via Domain Space Learning,,,True,False,"Hwang, Junyoung and Ju, Hyunjun and Kang, SeongKu and Jang, Sanghwan and Yu, Hwanjo",2024.0,,,,,Multi-Domain Sequential Recommendation via Domain Space Learning,Multi-Domain Sequential Recommendation via Domain Space Learning ...,https://dl.acm.org/doi/10.1145/3626772.3657685,"In such cases, the history of users in the target domain is limited or not recent, leading the sequential recommender system to capture inaccurate domain-specific sequential preferences. To address this limitation, this paper introduces Multi-Domain Sequential Recommendation via Domain Space Learning (MDSR-DSL)."
Revisiting Self-attention for Cross-domain Sequential Recommendation,2505.21811v1,zhang2024mdmtrec,\cite{zhang2024mdmtrec},MDMTRec: An Adaptive Multi-Task Multi-Domain Recommendation Framework,,,True,False,"Zhang, Zijian and Liu, Shuchang and Yu, Jiaao and Cai, Qingpeng and Zhao, Xiangyu and Zhang, Chunxu and Liu, Ziru and Liu, Qidong and Zhao, Hongwei and Hu, Lantao and others",2024.0,,,,,MDMTRec: An Adaptive Multi-Task Multi-Domain Recommendation Framework,Publications - GitHub Pages,https://zhaoxyai.github.io/publications.html,MDMTRec is an adaptive multi-task multi-domain recommendation framework that learns from multiple tasks and domains simultaneously. It aims to improve the recommendation performance and generalization across different scenarios and domains.
Revisiting Self-attention for Cross-domain Sequential Recommendation,2505.21811v1,ma2019pi,\cite{ma2019pi},$\pi$-net: A parallel information-sharing network for shared-account cross-domain sequential recommendations,,,True,False,"Ma, Muyang and Ren, Pengjie and Lin, Yujie and Chen, Zhumin and Ma, Jun and Rijke, Maarten de",2019.0,,,,,$\pi$-net: A parallel information-sharing network for shared-account cross-domain sequential recommendations,π-Net: A Parallel Information-sharing Network for Shared-account Cross ...,https://dl.acm.org/doi/10.1145/3331184.3331200,We formulate the Shared-account Cross-domain Sequential Recommendation (SCSR) task as a parallel sequential recommendation problem. We propose a Parallel Information-sharing Network (π-Net) to simultaneously generate recommendations for two domains where user behaviors on two domains are synchronously shared at each timestamp. π-Net contains two core units: a shared account filter unit (SFU
"Measure Domain's Gap: A Similar Domain Selection Principle for
  Multi-Domain Recommendation",2505.20227v1,gu2021self,\cite{gu2021self},Self-supervised learning on users' spontaneous behaviors for multi-scenario ranking in e-commerce,,,True,False,"Gu, Yulong and Bao, Wentian and Ou, Dan and Li, Xiang and Cui, Baoliang and Ma, Biyu and Huang, Haikuan and Liu, Qingwen and Zeng, Xiaoyi",2021.0,,,,,Self-supervised learning on users' spontaneous behaviors for multi-scenario ranking in e-commerce,"""Self-Supervised Learning on Users' Spontaneous Behaviors for ..."" - dblp",https://dblp.org/rec/conf/cikm/GuBOLCMHLZ21,Bibliographic details on Self-Supervised Learning on Users' Spontaneous Behaviors for Multi-Scenario Ranking in E-commerce. Stop the war! Остановите войну! solidarity - ... Self-Supervised Learning on Users' Spontaneous Behaviors for Multi-Scenario Ranking in E-commerce. CIKM 2021: 3828-3837. a service of .
"Measure Domain's Gap: A Similar Domain Selection Principle for
  Multi-Domain Recommendation",2505.20227v1,lqd2,\cite{lqd2},Llm-esr: Large language models enhancement for long-tailed sequential recommendation,,,True,False,"Liu, Qidong and Wu, Xian and Wang, Yejing and Zhang, Zijian and Tian, Feng and Zheng, Yefeng and Zhao, Xiangyu",2024.0,,,,Advances in Neural Information Processing Systems,Llm-esr: Large language models enhancement for long-tailed sequential recommendation,Large Language Models Enhanced Sequential Recommendation for Long-tail ...,https://openreview.net/forum?id=fcc2Ena74Z,"In this study, we introduce the Large Language Models Enhancement framework for Sequential Recommendation (LLM-ESR), which leverages semantic embeddings from LLMs to enhance SRS performance without increasing computational overhead. ... To address the long-tail user challenge, we introduce a retrieval augmented self-distillation technique to"
"Measure Domain's Gap: A Similar Domain Selection Principle for
  Multi-Domain Recommendation",2505.20227v1,hyp2,\cite{hyp2},Generating Long Semantic IDs in Parallel for Recommendation,,,True,False,Yupeng Hou and Jiacheng Li and Ashley Shin and Jinsung Jeon and Abhishek Santhanam and Wei Shao and Kaveh Hassani and Ning Yao and Julian McAuley,2025.0,,,,,Generating Long Semantic IDs in Parallel for Recommendation,GitHub - facebookresearch/RPG_KDD2025: This repository provides the ...,https://github.com/facebookresearch/RPG_KDD2025,"This repository provides the code for implementing RPG described in our KDD'25 paper ""Generating Long Semantic IDs in Parallel for Recommendation"". Semantic ID-based recommendation models tokenize each item into multiple discrete tokens, improving performance, scalability, and memory efficiency. While recent generative models adopt this"
"Measure Domain's Gap: A Similar Domain Selection Principle for
  Multi-Domain Recommendation",2505.20227v1,luo2023mamdr-multi-domain-rec,\cite{luo2023mamdr-multi-domain-rec},MAMDR: A model agnostic learning framework for multi-domain recommendation,,,True,False,"Luo, Linhao and Li, Yumeng and Gao, Buyu and Tang, Shuai and Wang, Sinan and Li, Jiancheng and Zhu, Tanchao and Liu, Jiancai and Li, Zhao and Pan, Shirui",2023.0,,,,,MAMDR: A model agnostic learning framework for multi-domain recommendation,MAMDR: A Model Agnostic Learning Framework for Multi-Domain Recommendation,https://www.computer.org/csdl/proceedings-article/icde/2023/222700d079/1PBylOZcdi0,"To address the problems of MDR methods, we propose a novel model agnostic learning framework, namely MAMDR, for the multi-domain recommendation. Specifically, we first propose a Domain Negotiation (DN) strategy to alleviate the conflict between domains. Then, we develop a Domain Regularization (DR) to improve the generalizability of specific"
"Measure Domain's Gap: A Similar Domain Selection Principle for
  Multi-Domain Recommendation",2505.20227v1,fu2023unified-llm-multi-domain-rec,\cite{fu2023unified-llm-multi-domain-rec},A unified framework for multi-domain ctr prediction via large language models,,,True,False,"Fu, Zichuan and Li, Xiangyang and Wu, Chuhan and Wang, Yichao and Dong, Kuicai and Zhao, Xiangyu and Zhao, Mengchen and Guo, Huifeng and Tang, Ruiming",2023.0,,,,ACM Transactions on Information Systems,A unified framework for multi-domain ctr prediction via large language models,A Unified Framework for Multi-Domain CTR Prediction via Large Language ...,https://arxiv.org/abs/2312.10743,"View a PDF of the paper titled A Unified Framework for Multi-Domain CTR Prediction via Large Language Models, by Zichuan Fu and 8 other authors View PDF HTML (experimental) Abstract: Click-Through Rate (CTR) prediction is a crucial task in online recommendation platforms as it involves estimating the probability of user engagement with"
"Measure Domain's Gap: A Similar Domain Selection Principle for
  Multi-Domain Recommendation",2505.20227v1,li2022gromov-cross-domain-rec,\cite{li2022gromov-cross-domain-rec},Gromov-wasserstein guided representation learning for cross-domain recommendation,,,True,False,"Li, Xinhang and Qiu, Zhaopeng and Zhao, Xiangyu and Wang, Zihao and Zhang, Yong and Xing, Chunxiao and Wu, Xian",2022.0,,,,,Gromov-wasserstein guided representation learning for cross-domain recommendation,Gromov-Wasserstein Guided Representation Learning for Cross-Domain ...,https://dl.acm.org/doi/10.1145/3511808.3557338,"To this end, we propose a novel framework that improves the effect of representation learning on the target domain by aligning the representation distributions between the source and target domains. In addition, GWCDR can be easily integrated with existing single-domain collaborative filtering methods to achieve cross-domain recommendation."
"Measure Domain's Gap: A Similar Domain Selection Principle for
  Multi-Domain Recommendation",2505.20227v1,fan2023adversarial-cross-domain-rec,\cite{fan2023adversarial-cross-domain-rec},Adversarial attacks for black-box recommender systems via copying transferable cross-domain user profiles,,,True,False,"Fan, Wenqi and Zhao, Xiangyu and Li, Qing and Derr, Tyler and Ma, Yao and Liu, Hui and Wang, Jianping and Tang, Jiliang",2023.0,,,,IEEE Transactions on Knowledge and Data Engineering,Adversarial attacks for black-box recommender systems via copying transferable cross-domain user profiles,Adversarial Attacks for Black-Box Recommender Systems via Copying ...,https://ieeexplore.ieee.org/abstract/document/10114977,"Adversarial Attacks for Black-Box Recommender Systems via Copying Transferable Cross-Domain User Profiles Abstract: As widely used in data-driven decision-making, recommender systems have been recognized for their capabilities to provide users with personalized services in many user-oriented online services, such as E-commerce (e.g., Amazon"
"Measure Domain's Gap: A Similar Domain Selection Principle for
  Multi-Domain Recommendation",2505.20227v1,gao2023autotransfer-cross-domain-rec,\cite{gao2023autotransfer-cross-domain-rec},AutoTransfer: Instance transfer for cross-domain recommendations,,,True,False,"Gao, Jingtong and Zhao, Xiangyu and Chen, Bo and Yan, Fan and Guo, Huifeng and Tang, Ruiming",2023.0,,,,,AutoTransfer: Instance transfer for cross-domain recommendations,AutoTransfer: Instance Transfer for Cross-Domain Recommendations ...,https://dl.acm.org/doi/10.1145/3539618.3591701,"AutoTransfer: Instance Transfer for Cross-Domain Recommendations. Authors: Jingtong Gao, Xiangyu Zhao, Bo Chen, ... Cross-domain recommendations can assist users in selecting suitable items in the target domain by aggregating or transferring the abundant available data from the auxiliary domain, which has gradually become a promising research"
"Measure Domain's Gap: A Similar Domain Selection Principle for
  Multi-Domain Recommendation",2505.20227v1,zhu2024m,\cite{zhu2024m},M-scan: A Multi-Scenario Causal-driven Adaptive Network for Recommendation,,,True,False,"Zhu, Jiachen and Wang, Yichao and Lin, Jianghao and Qin, Jiarui and Tang, Ruiming and Zhang, Weinan and Yu, Yong",2024.0,,,,,M-scan: A Multi-Scenario Causal-driven Adaptive Network for Recommendation,M-scan: A Multi-Scenario Causal-driven Adaptive Network for Recommendation,https://arxiv.org/abs/2404.07581,"We primarily focus on the field of multi-scenario recommendation, which poses a significant challenge in effectively leveraging data from different scenarios to enhance predictions in scenarios with limited data. Current mainstream efforts mainly center around innovative model network architectures, with the aim of enabling the network to implicitly acquire knowledge from diverse scenarios"
"Measure Domain's Gap: A Similar Domain Selection Principle for
  Multi-Domain Recommendation",2505.20227v1,tang2020ple-multi-task-rec,\cite{tang2020ple-multi-task-rec},Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations,,,True,False,"Tang, Hongyan and Liu, Junning and Zhao, Ming and Gong, Xudong",2020.0,,,,,Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations,【论文笔记】Progressive Layered Extraction (PLE): A Novel Multi-Task Learning ...,https://blog.csdn.net/m0_61899108/article/details/125914210,论文题目：Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations. 收录于：RecSys2020最佳长论文. 论文地址：Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations | Fourteenth ACM Conference on Recommender Systems
"Measure Domain's Gap: A Similar Domain Selection Principle for
  Multi-Domain Recommendation",2505.20227v1,wang2023plate-multi-domain-rec,\cite{wang2023plate-multi-domain-rec},PLATE: A prompt-enhanced paradigm for multi-scenario recommendations,,,True,False,"Wang, Yuhao and Zhao, Xiangyu and Chen, Bo and Liu, Qidong and Guo, Huifeng and Liu, Huanshuo and Wang, Yichao and Zhang, Rui and Tang, Ruiming",2023.0,,,,,PLATE: A prompt-enhanced paradigm for multi-scenario recommendations,PLATE: A Prompt-Enhanced Paradigm for Multi-Scenario Recommendations ...,https://dl.acm.org/doi/10.1145/3539618.3591750,"In this work, we propose a novel prompt-enhanced paradigm for multi-scenario recommendation. Specifically, a unified DRS backbone model is first pre-trained using data from all the domains in order to capture the commonality across domains."
"Measure Domain's Gap: A Similar Domain Selection Principle for
  Multi-Domain Recommendation",2505.20227v1,wang2024diff-cold-multi-domain-rec,\cite{wang2024diff-cold-multi-domain-rec},Diff-MSR: A diffusion model enhanced paradigm for cold-start multi-scenario recommendation,,,True,False,"Wang, Yuhao and Liu, Ziru and Wang, Yichao and Zhao, Xiangyu and Chen, Bo and Guo, Huifeng and Tang, Ruiming",2024.0,,,,,Diff-MSR: A diffusion model enhanced paradigm for cold-start multi-scenario recommendation,Diff-MSR: A Diffusion Model Enhanced Paradigm for Cold-Start Multi ...,https://dl.acm.org/doi/10.1145/3616855.3635807,"Diff-MSR: A Diffusion Model Enhanced Paradigm for Cold-Start Multi-Scenario Recommendation. Authors: Yuhao Wang, ... there is an increasing number of studies on multi-scenario recommendation (MSR) which trains the recommender system with the data from multiple scenarios, aiming to improve the recommendation performance on all these scenarios"
"Measure Domain's Gap: A Similar Domain Selection Principle for
  Multi-Domain Recommendation",2505.20227v1,zhu2022user,\cite{zhu2022user},User-tag profile modeling in recommendation system via contrast weighted tag masking,,,True,False,"Zhu, Chenxu and Du, Peng and Zhu, Xianghui and Zhang, Weinan and Yu, Yong and Cao, Yang",2022.0,,,,,User-tag profile modeling in recommendation system via contrast weighted tag masking,User-tag Profile Modeling in Recommendation System via Contrast ...,https://dl.acm.org/doi/abs/10.1145/3534678.3539102,"User-tag Profile Modeling in Recommendation System via Contrast Weighted Tag Masking. Authors ... User-tag profile modeling has become one of the novel and significant trends for the future development of industrial recommendation systems, which can be divided into two fundamental tasks: User Preferred Tag (UPT) and Tag Preferred User (TPU) in"
"Measure Domain's Gap: A Similar Domain Selection Principle for
  Multi-Domain Recommendation",2505.20227v1,he2024efficient-multi-modal,\cite{he2024efficient-multi-modal},Efficient Modality Selection in Multimodal Learning,,,True,False,"He, Yifei and Cheng, Runxiang and Balasubramaniam, Gargi and Tsai, Yao-Hung Hubert and Zhao, Han",2024.0,,,,Journal of Machine Learning Research,Efficient Modality Selection in Multimodal Learning,PDF,https://jmlr.org/papers/volume25/23-0439/23-0439.pdf,"Keywords: Multimodal Learning, Modality Selection, Submodular Optimization, Fea-tureImportance 1. Introduction ... Efficient Modality Selection in Multimodal Learning utility has an optimality guarantee via a greedy submodular function maximization algo-rithm. Speciﬁcally,"
"Optimized Text Embedding Models and Benchmarks for Amharic Passage
  Retrieval",2505.19356v1,Robertson2009ThePR,\cite{Robertson2009ThePR},The Probabilistic Relevance Framework: {BM25} and Beyond,,,True,False,Stephen E. Robertson and Hugo Zaragoza,2009.0,,,,,The Probabilistic Relevance Framework: {BM25} and Beyond,The Probabilistic Relevance Framework: BM25 and Beyond,https://dl.acm.org/doi/10.1561/1500000019,"The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970—1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in"
"Optimized Text Embedding Models and Benchmarks for Amharic Passage
  Retrieval",2505.19356v1,formal2021splade,\cite{formal2021splade},SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking,,,True,False,"Formal, Thibault and Piwowarski, Benjamin and Clinchant, St{\'e}phane",2021.0,,,,,SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking,SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking ...,https://dl.acm.org/doi/10.1145/3404835.3463098,"A recent line of work in first-stage Neural Information Retrieval has focused on learning sparse lexical representations instead of dense embeddings. One such work is SPLADE, which has been shown to lead to state-of-the-art results in both the in-domain"
"Optimized Text Embedding Models and Benchmarks for Amharic Passage
  Retrieval",2505.19356v1,dai2020context,\cite{dai2020context},Context-aware term weighting for first stage passage retrieval,,,True,False,"Dai, Zhuyun and Callan, Jamie",2020.0,,,,,Context-aware term weighting for first stage passage retrieval,Context-Aware Term Weighting For First Stage Passage Retrieval,https://dl.acm.org/doi/abs/10.1145/3397271.3401204,"But term frequency ignores how a term interacts with its text context, which is key to estimating document-specific term weights. This paper proposes a Deep Contextualized Term Weighting framework (DeepCT) that maps the contextualized term representations from BERT to into context-aware term weights for passage retrieval."
"Optimized Text Embedding Models and Benchmarks for Amharic Passage
  Retrieval",2505.19356v1,Xiong2020ApproximateNN,\cite{Xiong2020ApproximateNN},ICLR,,,True,False,Lee Xiong and Chenyan Xiong and Ye Li and Kwok-Fung Tang and Jialin Liu and Paul Bennett and Junaid Ahmed and Arnold Overwijk,2021.0,,,,,ICLR,ICLR 2025 - Amazon Science,https://www.amazon.science/conferences-and-events/iclr-2025,"ICLR 2025 is the premier gathering of professionals dedicated to the advancement of representation learning, also known as deep learning. The conference will take place in Singapore in April 2025 and feature accepted publications, workshops, and sponsorship details."
"Optimized Text Embedding Models and Benchmarks for Amharic Passage
  Retrieval",2505.19356v1,AfriBERTa,\cite{AfriBERTa},Small Data? {No} Problem! {Exploring} the Viability of Pretrained Multilingual Language Models for Low-resourced Languages,,,True,False,"Ogueji, Kelechi  and
      Zhu, Yuxin  and
      Lin, Jimmy",2021.0,,https://aclanthology.org/2021.mrl-1.11/,10.18653/v1/2021.mrl-1.11,,Small Data? {No} Problem! {Exploring} the Viability of Pretrained Multilingual Language Models for Low-resourced Languages,Small Data? No Problem! Exploring the Viability of Pretrained ...,https://aclanthology.org/2021.mrl-1.11/,"Submit Abstract Pretrained multilingual language models have been shown to work well on many languages for a variety of downstream NLP tasks. This consequently leaves out a huge percentage of the world’s languages as they are under-resourced. In this work, we challenge this assumption and present the first attempt at training a multilingual language model on only low-resource languages. We show that it is possible to train competitive multilingual language models on less than 1 GB of text. Evaluations on named entity recognition and text classification spanning 10 languages show that our model outperforms mBERT and XLM-Rin several languages and is very competitive overall."
"Optimized Text Embedding Models and Benchmarks for Amharic Passage
  Retrieval",2505.19356v1,Izacard2021UnsupervisedDI,\cite{Izacard2021UnsupervisedDI},TMLR,,,True,False,Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave,2022.0,,,,,TMLR,Transactions on Machine Learning Research,https://jmlr.org/tmlr/,"TMLR is a venue for disseminating machine learning research that complements JMLR and supports the unmet needs of a growing ML community. TMLR publishes shorter format manuscripts, provides fast turnarounds and double blind reviewing, and hosts the review process on OpenReview."
"Optimized Text Embedding Models and Benchmarks for Amharic Passage
  Retrieval",2505.19356v1,2AIRTC,\cite{2AIRTC},{2AIRTC}: The {Amharic} Adhoc Information Retrieval Test Collection,,,True,False,"Yeshambel, Tilahun and Mothe, Josiane and Assabie, Yaregal",2020.0,,https://doi.org/10.1007/978-3-030-58219-7_5,10.1007/978-3-030-58219-7_5,,{2AIRTC}: The {Amharic} Adhoc Information Retrieval Test Collection,2AIRTC: The Amharic Adhoc Information Retrieval Test Collection - Springer,https://link.springer.com/chapter/10.1007/978-3-030-58219-7_5,"This collection, named 2AIRTC, can serve as a reliable resource for the evaluation and comparison of various Amharic IR systems. ... 2AIRTC: The Amharic Adhoc Information Retrieval Test Collection. In: Arampatzis, A., et al. Experimental IR Meets Multilinguality, Multimodality, and Interaction. CLEF 2020. Lecture Notes in Computer Science"
"Aligning Web Query Generation with Ranking Objectives via Direct
  Preference Optimization",2505.19307v1,lu-etal-2021-less,\cite{lu-etal-2021-less},{Less is More: Pretrain a Strong {S}iamese Encoder for Dense Text Retrieval Using a Weak Decode},,,True,False,"Lu, Shuqi  and
      He, Di  and
      Xiong, Chenyan  and
      Ke, Guolin  and
      Malik, Waleed  and
      Dou, Zhicheng  and
      Bennett, Paul  and
      Liu, Tie-Yan  and
      Overwijk, Arnold",2021.0,,,,,{Less is More: Pretrain a Strong {S}iamese Encoder for Dense Text Retrieval Using a Weak Decode},Less is More: Pretrain a Strong Siamese Encoder for Dense Text ...,https://paperswithcode.com/paper/less-is-more-pretrain-a-strong-siamese,Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder ... Dense retrieval requires high-quality text sequence embeddings to support effective search in the representation space. Autoencoder-based language models are appealing in dense retrieval as they train the encoder to output high-quality embedding
"Aligning Web Query Generation with Ranking Objectives via Direct
  Preference Optimization",2505.19307v1,DBLP:conf/sigir/MaGZFC22,\cite{DBLP:conf/sigir/MaGZFC22},"{Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive
                  Span Prediction}",,,True,False,"Xinyu Ma and
                  Jiafeng Guo and
                  Ruqing Zhang and
                  Yixing Fan and
                  Xueqi Cheng",2022.0,,,,,"{Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive
                  Span Prediction}",Pre-train a Discriminative Text Encoder for Dense Retrieval via ...,https://dl.acm.org/doi/10.1145/3477495.3531772,"Therefore, in this work, we introduce a novel contrastive span prediction task to pre-train the encoder alone, but still retain the bottleneck ability of the autoencoder. In this way, we can 1) learn discriminative text representations efficiently with the group-wise contrastive learning over spans and, 2) avoid the bypass effect of the decoder"
"Aligning Web Query Generation with Ranking Objectives via Direct
  Preference Optimization",2505.19307v1,nogueira2019doc2query,\cite{nogueira2019doc2query},From doc2query to docTTTTTquery,,,True,False,Rodrigo Nogueira and Jimmy Lin,2019.0,,,,,From doc2query to docTTTTTquery,(PDF) From doc2query to docTTTTTquery - ResearchGate,https://www.researchgate.net/publication/360890853_From_doc2query_to_docTTTTTquery,"Interestingly, doc2query and docTTTTTquery produce similar proportions of copied (67%) and new words (33%) with respect to the original document. This analysis was performed for both models using"
"Aligning Web Query Generation with Ranking Objectives via Direct
  Preference Optimization",2505.19307v1,DBLP:conf/ecir/GospodinovMM23,\cite{DBLP:conf/ecir/GospodinovMM23},Doc2Query-: When Less is More,,,True,False,"Mitko Gospodinov and
                  Sean MacAvaney and
                  Craig Macdonald",2023.0,,,,,Doc2Query-: When Less is More,[2301.03266] Doc2Query--: When Less is More - arXiv.org,https://arxiv.org/abs/2301.03266,"View a PDF of the paper titled Doc2Query--: When Less is More, by Mitko Gospodinov and 2 other authors. View PDF Abstract: Doc2Query -- the process of expanding the content of a document before indexing using a sequence-to-sequence model -- has emerged as a prominent technique for improving the first-stage retrieval effectiveness of search"
"Aligning Web Query Generation with Ranking Objectives via Direct
  Preference Optimization",2505.19307v1,DBLP:journals/corr/abs-2301-01820,\cite{DBLP:journals/corr/abs-2301-01820},"{InPars-v2: Large Language Models as Efficient Dataset Generators for
               Information Retrieval}",,,True,False,"Vitor Jeronymo and
               Luiz Henrique Bonifacio and
               Hugo Abonizio and
               Marzieh Fadaee and
               Roberto de Alencar Lotufo and
               Jakub Zavrel and
               Rodrigo Frassetto Nogueira",2023.0,,,,ArXiv,"{InPars-v2: Large Language Models as Efficient Dataset Generators for
               Information Retrieval}",(PDF) InPars-v2: Large Language Models as Efficient Dataset ...,https://www.researchgate.net/publication/366902520_InPars-v2_Large_Language_Models_as_Efficient_Dataset_Generators_for_Information_Retrieval,"Abstract. Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot"
"Benchmarking Recommendation, Classification, and Tracing Based on
  Hugging Face Knowledge Graph",2505.17507v1,DEKR,\cite{DEKR},"{DEKR:} Description Enhanced Knowledge Graph for Machine Learning
                  Method Recommendation",,,True,False,"Xianshuai Cao and
                  Yuliang Shi and
                  Han Yu and
                  Jihu Wang and
                  Xinjun Wang and
                  Zhongmin Yan and
                  Zhiyong Chen",2021.0,,https://doi.org/10.1145/3404835.3462900,10.1145/3404835.3462900,,"{DEKR:} Description Enhanced Knowledge Graph for Machine Learning
                  Method Recommendation",DEKR | Proceedings of the 44th International ACM SIGIR Conference on ...,https://dl.acm.org/doi/abs/10.1145/3404835.3462900,"Presentation video for the paper ""DEKR: Description Enhanced Knowledge Graph for Machine Learning Method Recommendation"". The video is presented in four aspects: background, method, experiments, and conclusion. ... (2025) A survey on knowledge graph-based click-through rate prediction Expert Systems with Applications 10.1016/j.eswa.2025.127501"
"Benchmarking Recommendation, Classification, and Tracing Based on
  Hugging Face Knowledge Graph",2505.17507v1,tse23,\cite{tse23},"Task-Oriented {ML/DL} Library Recommendation Based on a Knowledge
                  Graph",,,True,False,"Mingwei Liu and
                  Chengyuan Zhao and
                  Xin Peng and
                  Simin Yu and
                  Haofen Wang and
                  Chaofeng Sha",2023.0,,https://doi.org/10.1109/TSE.2023.3285280,10.1109/TSE.2023.3285280,{IEEE} Trans. Software Eng.,"Task-Oriented {ML/DL} Library Recommendation Based on a Knowledge
                  Graph",Task-Oriented ML/DL Library Recommendation based on a Knowledge Graph,https://www.researchgate.net/publication/371549606_Task-Oriented_MLDL_Library_Recommendation_based_on_a_Knowledge_Graph,"Based on the findings of the study, we propose a task-oriented ML/DL library recommendation approach, called MLTaskKG. It constructs a knowledge graph that captures AI tasks, ML/DL models, model"
"Benchmarking Recommendation, Classification, and Tracing Based on
  Hugging Face Knowledge Graph",2505.17507v1,OAGBench,\cite{OAGBench},OAG-Bench: {A} Human-Curated Benchmark for Academic Graph Mining,,,True,False,"Fanjin Zhang and
                  Shijie Shi and
                  Yifan Zhu and
                  Bo Chen and
                  Yukuo Cen and
                  Jifan Yu and
                  Yelin Chen and
                  Lulu Wang and
                  Qingfei Zhao and
                  Yuqing Cheng and
                  Tianyi Han and
                  Yuwei An and
                  Dan Zhang and
                  Weng Lam Tam and
                  Kun Cao and
                  Yunhe Pang and
                  Xinyu Guan and
                  Huihui Yuan and
                  Jian Song and
                  Xiaoyan Li and
                  Yuxiao Dong and
                  Jie Tang",2024.0,,https://doi.org/10.1145/3637528.3672354,10.1145/3637528.3672354,,OAG-Bench: {A} Human-Curated Benchmark for Academic Graph Mining,OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining,https://dl.acm.org/doi/10.1145/3637528.3672354,"In this paper, we present OAG-Bench, a comprehensive, multi-aspect, and fine-grained human-curated benchmark based on the Open Academic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines, and 120+ experimental results to date."
"Benchmarking Recommendation, Classification, and Tracing Based on
  Hugging Face Knowledge Graph",2505.17507v1,RepoRecommendation,\cite{RepoRecommendation},"Personalized Repository Recommendation Service for Developers with
                  Multi-modal Features Learning",,,True,False,"Yueshen Xu and
                  Yuhong Jiang and
                  Xinkui Zhao and
                  Ying Li and
                  Rui Li",2023.0,,https://doi.org/10.1109/ICWS60048.2023.00064,10.1109/ICWS60048.2023.00064,,"Personalized Repository Recommendation Service for Developers with
                  Multi-modal Features Learning",Personalized Repository Recommendation Service for Developers with ...,https://ieeexplore.ieee.org/document/10248331,"In this paper, we develop a new personalized repository recommendation service with multi-modal features learning. We propose to mine two modes of features and jointly utilize the mined multimodal features. One of the features is the developers' sequential behavior features and the other is text features of repositories."
"Benchmarking Recommendation, Classification, and Tracing Based on
  Hugging Face Knowledge Graph",2505.17507v1,GRETA,\cite{GRETA},{GRETA:} Graph-Based Tag Assignment for GitHub Repositories,,,True,False,"Xuyang Cai and
                  Jiangang Zhu and
                  Beijun Shen and
                  Yuting Chen",2016.0,,https://doi.org/10.1109/COMPSAC.2016.124,10.1109/COMPSAC.2016.124,,{GRETA:} Graph-Based Tag Assignment for GitHub Repositories,GRETA: Graph-Based Tag Assignment for GitHub Repositories,https://base.sjtu.edu.cn/home/lib/exe/fetch.php?media=pub:greta_graph-based_tag_assignment_for_github_repositories.pdf,"tags are assigned to the GitHub repositories. This paper makes the following contributions: 1) Approach. GRETA is a novel, graph-based approach to tag assignment for repositories on GitHub, which allows tags to be assigned by some graph algorithms. GRETA is also a cross-community approach, which utilizes the domain knowledge from StackOverﬂow for"
"Benchmarking Recommendation, Classification, and Tracing Based on
  Hugging Face Knowledge Graph",2505.17507v1,issue-PR-link-prediction,\cite{issue-PR-link-prediction},"Improving Issue-PR Link Prediction via Knowledge-Aware Heterogeneous
                  Graph Learning",,,True,False,"Shuotong Bai and
                  Huaxiao Liu and
                  Enyan Dai and
                  Lei Liu",2024.0,,https://doi.org/10.1109/TSE.2024.3408448,10.1109/TSE.2024.3408448,{IEEE} Trans. Software Eng.,"Improving Issue-PR Link Prediction via Knowledge-Aware Heterogeneous
                  Graph Learning",Improving Issue-PR Link Prediction via Knowledge-Aware Heterogeneous ...,https://ieeexplore.ieee.org/document/10546471,"Improving Issue-PR Link Prediction via Knowledge-Aware Heterogeneous Graph Learning Abstract: Links between issues and pull requests (PRs) assist GitHub developers in tackling technical challenges, gaining development inspiration, and improving repository maintenance. In realistic repositories, these links are still insufficiently established"
"Unlearning for Federated Online Learning to Rank: A Reproducibility
  Study",2505.12791v1,kharitonov2019federated,\cite{kharitonov2019federated},Federated online learning to rank with evolution strategies,,,True,False,"Kharitonov, Eugene",2019.0,,,,,Federated online learning to rank with evolution strategies,Federated Online Learning to Rank with Evolution Strategies,https://dl.acm.org/doi/10.1145/3289600.3290968,"Online Learning to Rank is a powerful paradigm that allows to train ranking models using only online feedback from its users.In this work, we consider Federated Online Learning to Rank setup (FOLtR) where on-mobile ranking models are trained in a way that respects the users' privacy."
"Unlearning for Federated Online Learning to Rank: A Reproducibility
  Study",2505.12791v1,wang2021federated,\cite{wang2021federated},Federated online learning to rank with evolution strategies: a reproducibility study,,,True,False,"Wang, Shuyi and Zhuang, Shengyao and Zuccon, Guido",2021.0,,,,,Federated online learning to rank with evolution strategies: a reproducibility study,Federated Online Learning to Rank with Evolution Strategies,https://dl.acm.org/doi/10.1145/3289600.3290968,"Federated Online Learning to Rank with Evolution Strategies. Author: Eugene Kharitonov Authors Info & Claims. ... Federated Online Learning to Rank with Evolution Strategies: A Reproducibility Study. ... Federated online learning to rank (FOLTR) aims to preserve user privacy by not sharing their searchable data and search interactions, while"
"Unlearning for Federated Online Learning to Rank: A Reproducibility
  Study",2505.12791v1,wang2021effective,\cite{wang2021effective},Effective and privacy-preserving federated online learning to rank,,,True,False,"Wang, Shuyi and Liu, Bing and Zhuang, Shengyao and Zuccon, Guido",2021.0,,,,,Effective and privacy-preserving federated online learning to rank,Effective and Privacy-preserving Federated Online Learning to Rank ...,https://dl.acm.org/doi/10.1145/3471158.3472236,"Federated online learning to rank (FOLTR) aims to preserve user privacy by not sharing their searchable data and search interactions, while guaranteeing high search effectiveness, especially in contexts where individual users have scarce training data"
"Unlearning for Federated Online Learning to Rank: A Reproducibility
  Study",2505.12791v1,wang2022non,\cite{wang2022non},Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval,,,True,False,"Wang, Shuyi and Zuccon, Guido",2022.0,,,,,Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval,"SIGIR 2025, Padua, 13-18 July | Home",https://sigir2025.dei.unipd.it/,"The 48th International ACM SIGIR Conference on Research and Development in Information Retrieval | July 13-18, 2025 in Padua, Italy. SIGIR is the premier international forum for the presentation of new research results and for the demonstration of new systems and techniques in information retrieval."
"Unlearning for Federated Online Learning to Rank: A Reproducibility
  Study",2505.12791v1,liu2021federaser,\cite{liu2021federaser},Federaser: Enabling efficient client-level data removal from federated learning models,,,True,False,"Liu, Gaoyang and Ma, Xiaoqiang and Yang, Yang and Wang, Chen and Liu, Jiangchuan",2021.0,,,,,Federaser: Enabling efficient client-level data removal from federated learning models,FedEraser: Enabling Efficient Client-Level Data Removal from Federated ...,https://ieeexplore.ieee.org/document/9521274,"Therefore, how to enable efficient data removal from FL models remains largely under-explored. In this paper, we take the first step to fill this gap by presenting FedEraser, the first federated unlearning method-ology that can eliminate the influence of a federated client's data on the global FL model while significantly reducing the time"
"Unlearning for Federated Online Learning to Rank: A Reproducibility
  Study",2505.12791v1,shejwalkar2021manipulating,\cite{shejwalkar2021manipulating},"28th Annual Network and Distributed System Security Symposium (NDSS), February 21-25, 2021",,,True,False,"Shejwalkar, Virat and Houmansadr, Amir",2021.0,,,,,"28th Annual Network and Distributed System Security Symposium (NDSS), February 21-25, 2021",Network and Distributed System Security Symposium (NDSS) 2021,https://www.ndss-symposium.org/ndss2021/,"NDSS Symposium 2021 The Network and Distributed System Security Symposium (NDSS) 2021 conference was held virtually from 21-25 February 2021. The Network and Distributed System Security Symposium (NDSS) is a top venue that fosters information exchange among researchers and practitioners of computer, network and distributed system security."
"Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval
  Knowledge Acquisition",2505.07166v1,karpukhin2020dense,\cite{karpukhin2020dense},Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),,,True,False,"Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau",2020.0,,,,,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),Proceedings of the 2020 Conference on Empirical Methods in Natural ...,https://aclanthology.org/2020.emnlp-main.0/,"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (Webber et al., EMNLP 2020) ACL. Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu. 2020. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online."
"Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval
  Knowledge Acquisition",2505.07166v1,izacard2021contriever,\cite{izacard2021contriever},Proceedings of the 2021 Conference on Neural Information Processing Systems (NeurIPS),,,True,False,"Izacard, Gautier and Grave, Edouard",2021.0,,,,,Proceedings of the 2021 Conference on Neural Information Processing Systems (NeurIPS),dblp: NeurIPS 2021,https://dblp.org/db/conf/nips/neurips2021,"Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual. 2021. view. electronic edition @ neurips.cc (open access) details & citations . export record."
"Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval
  Knowledge Acquisition",2505.07166v1,reimers2019sentence,\cite{reimers2019sentence},Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing,,,True,False,"Reimers, Nils and Gurevych, Iryna",2019.0,,,,,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing,Proceedings of the 2019 Conference on Empirical Methods in Natural ...,https://findscholars.unh.edu/display/conference-proceedings-of-the-2019-conference-on-empirical-methods-in-natural-language-processing-and-the-9th-international-joint-conference-on-natural-language-processing-emnlp-ijcnlp,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) Conference. Publications Publications. Related Documents . Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation
"Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval
  Knowledge Acquisition",2505.07166v1,gao2021simcse,\cite{gao2021simcse},Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,,,True,False,"Gao, Tianyu and Yao, Xing and Chen, Dan",2021.0,,,,,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,PDF,https://www.proceedings.com/content/073/073501webtoc.pdf,"The proceedings of the 58th Annual Meeting of the Association for Computational Linguistics Conference on Empirical Methods in Natural Language Processing (EMNLP 2021) held in Punta Cana, Dominican Republic and online. The volume contains papers on various topics in natural language processing, such as machine translation, dialogue, summarization, keyphrase extraction, and relation extraction."
"Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval
  Knowledge Acquisition",2505.07166v1,replama2021,\cite{replama2021},Proceedings of the 2021 Conference on Information Retrieval (SIGIR),,,True,False,"Smith, John and Doe, Jane",2021.0,,,,,Proceedings of the 2021 Conference on Information Retrieval (SIGIR),SIGIR '21 : proceedings of the 44th International ACM SIGIR Conference ...,https://catalog.library.vanderbilt.edu/discovery/fulldisplay/alma991043859788603276/01VAN_INST:vanui,"Welcome to the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021). SIGIR is the premier scientific conference in the broad area of information retrieval. SIGIR 2021 was originally planned to be held in Montréal, Quebec, but, due to the global pandemic, had to be shifted to a virtual conference. In addition to the traditional"
"Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval
  Knowledge Acquisition",2505.07166v1,promptreps2021,\cite{promptreps2021},Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP),,,True,False,"Lee, Alex and Kumar, Rahul",2021.0,,,,,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP),Proceedings of the 2021 Conference on Empirical Methods in Natural ...,https://aclanthology.org/2021.emnlp-main.0/,"%0 Conference Proceedings %T Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing %E Moens, Marie-Francine %E Huang, Xuanjing %E Specia, Lucia %E Yih, Scott Wen-tau %D 2021 %8 November %I Association for Computational Linguistics %C Online and Punta Cana, Dominican Republic %F emnlp-2021 %U https://aclanthology"
"Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval
  Knowledge Acquisition",2505.07166v1,msmarco,\cite{msmarco},Proceedings of the 2016 Conference on Machine Learning and Information Retrieval,,,True,False,"Nguyen, Thang and others",2016.0,,,,,Proceedings of the 2016 Conference on Machine Learning and Information Retrieval,Proceedings of Machine Learning Research | Proceedings of The 33rd ...,https://proceedings.mlr.press/v48/,"Proceedings of The 33rd International Conference on Machine Learning Held in New York, New York, USA on 20-22 June 2016 Published as Volume 48 by the Proceedings of Machine Learning Research on 11 June 2016. Volume Edited by: Maria Florina Balcan Kilian Q. Weinberger Series Editors: Neil D. Lawrence Mark Reid"
"Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval
  Knowledge Acquisition",2505.07166v1,naturalquestions,\cite{naturalquestions},Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP),,,True,False,"Kwiatkowski, Tom and Palomaki, Jenna and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, David and Filatov, Yury and Khashabi, Daniel and Sabharwal, Ashish and others",2019.0,,,,,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP),dblp: EMNLP,https://dblp.org/db/conf/emnlp/index,"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019 - Tutorial Abstracts. Association for Computational Linguistics 2019"
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,Frequency23,\cite{Frequency23},Proc. of SIGIR,,,True,False,"Du, Xinyu and Yuan, Huanhuan and Zhao, Pengpeng and Qu, Jianfeng and Zhuang, Fuzhen and Liu, Guanfeng and Liu, Yanchi and Sheng, Victor S",2023.0,,,,,Proc. of SIGIR,Proceedings of the 47th International ACM SIGIR Conference ... - Researchr,https://researchr.org/publication/sigir-2024,"Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, Yi Zhang 0001, editors, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024. ACM, 2024. Conference: sigir2024"
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,DL4,\cite{DL4},Deep learning based recommender system: A survey and new perspectives,,,True,False,"Zhang, Shuai and Yao, Lina and Sun, Aixin and Tay, Yi",2019.0,,,,CSUR,Deep learning based recommender system: A survey and new perspectives,Deep Learning based Recommender System: A Survey and New Perspectives,https://arxiv.org/abs/1707.07435,This paper reviews recent research on deep learning based recommender systems and provides a taxonomy of models. It also discusses current trends and new perspectives in the field of deep learning for information retrieval.
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,Xavier,\cite{Xavier},AISTATS,,,True,False,"Glorot, Xavier and Bengio, Yoshua",2010.0,,,,,AISTATS,AISTATS 2025 - 2025 Conference,https://virtual.aistats.org/,"AISTATS is an interdisciplinary conference on artificial intelligence and statistics, held in Phuket, Thailand in May 2025. Find out the dates, calls, organization, registration, accepted papers, and more on the official website."
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,sse-pt,\cite{sse-pt},RecSys,,,True,False,"Wu, Liwei and Li, Shuqing and Hsieh, Cho-Jui and Sharpnack, James",2020.0,,,,,RecSys,Home | RecSys,https://recsys.com/,"RecSys offers online dating, image libraries, music, TV & movies, e-commerce, publishing and other industries with the world's best recommendation engines. Using big data and machine learning, behavioural analytics, semantic and content analysis, RecSys helps you optimize your customers' journey and relevance."
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,FMLP,\cite{FMLP},Proceedings of the ACM web conference 2022,,,True,False,"Zhou, Kun and Yu, Hui and Zhao, Wayne Xin and Wen, Ji-Rong",2022.0,,,,,Proceedings of the ACM web conference 2022,Main Proceedings - TheWebConf 2022 - International World Wide Web ...,https://archives.iw3c2.org/www2022/main-proceedings/,"WWW '22: Proceedings of the ACM Web Conference 2022 Full Citation in the ACM Digital Library. Keynote Talks Search Engines: From the Lab to the Engine Room, and Back: Keynote Talk. Prabhakar Raghavan; Prabhakar Raghavan has given a Keynote Talk at The ACM Web Conference 2022 on Wednesday 27th April 2022. This paper provides a summary of the"
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,strec,\cite{strec},Proceedings of the 17th ACM Conference on Recommender Systems,,,True,False,"Li, Chengxi and Wang, Yejing and Liu, Qidong and Zhao, Xiangyu and Wang, Wanyu and Wang, Yiqi and Zou, Lixin and Fan, Wenqi and Li, Qing",2023.0,,,,,Proceedings of the 17th ACM Conference on Recommender Systems,Proceedings of the 17th ACM Conference on Recommender Systems | ACM ...,https://dl.acm.org/doi/proceedings/10.1145/3604915,"RecSys '23: Proceedings of the 17th ACM Conference on Recommender Systems. September 2023. Read More. 2023 Proceeding. Editors: Jie Zhang, Li Chen, Shlomo Berkovsky, Min Zhang, Tommaso di Noia, ... Proceedings of the 17th ACM Conference on Recommender Systems. Applied computing. Computers in other domains. Personal computers and PC applications."
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,MLM4Rec,\cite{MLM4Rec},Learning Global and Multi-granularity Local Representation with MLP for Sequential Recommendation,,,True,False,"Long, Chao and Yuan, Huanhuan and Fang, Junhua and Xian, Xuefeng and Liu, Guanfeng and Sheng, Victor S and Zhao, Pengpeng",2024.0,,,,ACM Transactions on Knowledge Discovery from Data,Learning Global and Multi-granularity Local Representation with MLP for Sequential Recommendation,Learning Global and Multi-granularity Local Representation with MLP for ...,https://dl.acm.org/doi/10.1145/3638562,"To this end, we proposed a parallel architecture for capturing global representation and Multi-granularity Local dependencies with MLP for sequential Recommendation (MLM4Rec). For global representation, we utilize modified MLP-Mixer to capture global information of user sequences due to its simplicity and efficiency."
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,PEPNet,\cite{PEPNet},Proc. of KDD,,,True,False,"Chang, Jianxin and Zhang, Chenbin and Hui, Yiqun and Leng, Dewei and Niu, Yanan and Song, Yang and Gai, Kun",2023.0,,,,,Proc. of KDD,KDD process: What you need to know | Outsource Accelerator,https://www.outsourceaccelerator.com/articles/kdd-process/,"KDD is a method of finding, transforming, and refining meaningful data and patterns from a raw database. These enhanced data sets are to be used in different domains or applications. It comprises an organized procedure of extracting valuable, previously unknown information from large and complex sets of data."
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,mb-str,\cite{mb-str},Proc. of SIGIR,,,True,False,"Yuan, Enming and Guo, Wei and He, Zhicheng and Guo, Huifeng and Liu, Chengkai and Tang, Ruiming",2022.0,,,,,Proc. of SIGIR,Proceedings of the 47th International ACM SIGIR Conference ... - Researchr,https://researchr.org/publication/sigir-2024,"Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, Yi Zhang 0001, editors, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024. ACM, 2024. Conference: sigir2024"
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,lightsan,\cite{lightsan},Proc. of SIGIR,,,True,False,"Fan, Xinyan and Liu, Zheng and Lian, Jianxun and Zhao, Wayne Xin and Xie, Xing and Wen, Ji-Rong",2021.0,,,,,Proc. of SIGIR,Proceedings of the 47th International ACM SIGIR Conference ... - Researchr,https://researchr.org/publication/sigir-2024,"Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, Yi Zhang 0001, editors, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024. ACM, 2024. Conference: sigir2024"
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,autoseqrec,\cite{autoseqrec},Proceedings of the 32nd ACM CIKM,,,True,False,"Liu, Sijia and Liu, Jiahao and Gu, Hansu and Li, Dongsheng and Lu, Tun and Zhang, Peng and Gu, Ning",2023.0,,,,,Proceedings of the 32nd ACM CIKM,dblp: 32nd CIKM 2023,https://dblp.org/db/conf/cikm/cikm2023,"32nd CIKM 2023: Birmingham, UK. export records of this page. first 1000 hits only: XML; JSON; JSONP; ... Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 2023, Birmingham, United Kingdom, October 21-25, 2023. ACM 2023. Keynote Talks. view. electronic edition via DOI; unpaywalled version; details"
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,HRNN,\cite{HRNN},proceedings of the Eleventh ACM Conference on Recommender Systems,,,True,False,"Quadrana, Massimo and Karatzoglou, Alexandros and Hidasi, Bal{\'a}zs and Cremonesi, Paolo",2017.0,,,,,proceedings of the Eleventh ACM Conference on Recommender Systems,Deep Learning for Recommender Systems | Proceedings of the Eleventh ACM ...,https://dl.acm.org/doi/10.1145/3109859.3109933,"In Proceedings of the 10th ACM Conference on Recommender Systems. ACM, 241--248. Digital Library. Google Scholar [7] ... RecSys '17: Proceedings of the Eleventh ACM Conference on Recommender Systems. August 2017. 466 pages. ISBN: 9781450346528. DOI: 10.1145/3109859. General Chairs:"
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,zhao2023user,\cite{zhao2023user},Proceedings of the ACM Web Conference 2023,,,True,False,"Zhao, Kesen and Zou, Lixin and Zhao, Xiangyu and Wang, Maolin and Yin, Dawei",2023.0,,,,,Proceedings of the ACM Web Conference 2023,The Web Conference (WWW) - dblp,https://dblp.org/db/conf/www/index,"Companion Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023. ACM 2023, ISBN 978-1-4503-9419-2. 31st WWW 2022: Virtual Event / Lyon, France. view. table of contents in dblp; electronic edition via DOI; unpaywalled version; details & citations; authority control: export record."
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,DMAN,\cite{DMAN},Proc. of AAAI,,,True,False,"Tan, Qiaoyu and Zhang, Jianwei and Liu, Ninghao and Huang, Xiao and Yang, Hongxia and Zhou, Jingren and Hu, Xia",2021.0,,,,,Proc. of AAAI,Proceedings of the AAAI Conference on Artificial Intelligence,https://ojs.aaai.org/index.php/AAAI/index,"The AAAI Conference on Artificial Intelligence — AAAI's primary conference — promotes theoretical and applied AI research as well as intellectual interchange among researchers and practitioners. Begun in 1980 (when it was called the National Conference on Artificial Intelligence), the proceedings has been published continuously since that"
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,Kang01,\cite{Kang01},ICDM,,,True,False,"Kang, Wang-Cheng and McAuley, Julian",2018.0,,,,,ICDM,"ICDM 2024, International Conference on Data Mining",https://icdm2024.org/,"ICDM 2024 is the world's premier research conference in Data Mining, to be held on 9-12 December 2024 at the Abu Dhabi National Exhibition Centre. Learn about the dates, program, venue, calls for papers, workshops, tutorials, demos, registration, sponsorship, and awards."
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,bert4rec,\cite{bert4rec},Proc. of CIKM,,,True,False,"Sun, Fei and Liu, Jun and Wu, Jian and Pei, Changhua and Lin, Xiao and Ou, Wenwu and Jiang, Peng",2019.0,,,,,Proc. of CIKM,dblp: CIKM,https://dblp.org/db/conf/cikm/index,"Proceedings of the CIKM 2022 Workshops co-located with 31st ACM International Conference on Information and Knowledge Management (CIKM 2022), Atlanta, USA, October 17-21, 2022. CEUR Workshop Proceedings 3318, CEUR-WS.org 2023. view. table of contents in dblp;"
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,Linrec,\cite{Linrec},Proc. of SIGIR,,,True,False,"Liu, Langming and Cai, Liu and Zhang, Chi and Zhao, Xiangyu and Gao, Jingtong and Wang, Wanyu and Lv, Yifu and Fan, Wenqi and Wang, Yiqi and He, Ming and others",2023.0,,,,,Proc. of SIGIR,Proceedings of the 47th International ACM SIGIR Conference ... - Researchr,https://researchr.org/publication/sigir-2024,"Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, Yi Zhang 0001, editors, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024. ACM, 2024. Conference: sigir2024"
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,GLINTours25,\cite{GLINTours25},GLINT-RU: Gated Lightweight Intelligent Recurrent Units for Sequential Recommender Systems,,,True,False,"Zhang, Sheng and Wang, Maolin and Zhao, Xiangyu",2024.0,,,,arXiv preprint arXiv:2406.10244,GLINT-RU: Gated Lightweight Intelligent Recurrent Units for Sequential Recommender Systems,GLINT-RU: Gated Lightweight Intelligent Recurrent Units for Sequential ...,https://ar5iv.labs.arxiv.org/html/2406.10244,"GLINT-RU: Gated Lightweight Intelligent Recurrent Units for Sequential Recommender Systems. Sheng Zhang City University of HongKong szhang844-c@my.cityu.edu.hk, ... In this paper, we have presented an innovative dense selective GRU framework GLINT-RU for sequential recommendation tasks. Due to the parallel netork design and the implementation"
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,HiPPOs21,\cite{HiPPOs21},There is HOPE to Avoid HiPPOs for Long-memory State Space Models,,,True,False,"Yu, Annan and Mahoney, Michael W and Erichson, N Benjamin",2024.0,,,,arXiv preprint arXiv:2405.13975,There is HOPE to Avoid HiPPOs for Long-memory State Space Models,There is HOPE to Avoid HiPPOs for Long-memory State Space Models,https://arxiv.org/abs/2405.13975v1,"State-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. However, these models typically face several challenges: (i) they require specifically designed initializations of the system matrices to achieve state-of-the-art performance, (ii) they require training of state matrices on a logarithmic scale with very"
"STAR-Rec: Making Peace with Length Variance and Pattern Diversity in
  Sequential Recommendation",2505.03484v1,liu2024bidirectional,\cite{liu2024bidirectional},Bidirectional gated mamba for sequential recommendation,,,True,False,"Liu, Ziwei and Liu, Qidong and Wang, Yejing and Wang, Wanyu and Jia, Pengyue and Wang, Maolin and Liu, Zitao and Chang, Yi and Zhao, Xiangyu",2024.0,,,,arXiv preprint arXiv:2408.11451,Bidirectional gated mamba for sequential recommendation,Bidirectional Gated Mamba for Sequential Recommendation - arXiv.org,https://arxiv.org/html/2408.11451v2,"To address these challenges and better leverage Mamba's strengths, we propose an innovative framework called S elect I ve G ated MA mba for Sequential Recommendation (SIGMA). Our approach introduces the Partially Flipped Mamba (PF-Mamba), a specialized bidirectional structure that captures contextual information (Liu et al., 2024a; Jiang et al., 2024)."
Graph Spectral Filtering with Chebyshev Interpolation for Recommendation,2505.00552v1,perozziDeepwalk2014,\cite{perozziDeepwalk2014},Deep{W}alk: Online learning of social representations,,,True,False,"Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven",2014.0,,,,,Deep{W}alk: Online learning of social representations,[1403.6652] DeepWalk: Online Learning of Social Representations - arXiv.org,https://arxiv.org/abs/1403.6652,"We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning) from sequences of words to graphs. DeepWalk uses"
Graph Spectral Filtering with Chebyshev Interpolation for Recommendation,2505.00552v1,huangGraphRecurrentNetworks2019,\cite{huangGraphRecurrentNetworks2019},Graph recurrent networks with attributed random walks,,,True,False,"Huang, Xiao and Song, Qingquan and Li, Yuening and Hu, Xia",2019.0,,,,,Graph recurrent networks with attributed random walks,Graph Recurrent Networks With Attributed Random Walks,https://dl.acm.org/doi/10.1145/3292500.3330941,"To bridge the gap, we explore to perform joint random walks on attributed networks, and utilize them to boost the deep node representation learning. The proposed framework GraphRNA consists of two major components, i.e., a collaborative walking mechanism - AttriWalk, and a tailored deep embedding architecture for random walks, named graph"
Graph Spectral Filtering with Chebyshev Interpolation for Recommendation,2505.00552v1,nikolentzosRandomwalkgraphneuralnetworks2020,\cite{nikolentzosRandomwalkgraphneuralnetworks2020},Random walk graph neural networks,,,True,False,"Nikolentzos, Giannis and Vazirgiannis, Michalis",2020.0,,,,,Random walk graph neural networks,Random walk graph neural networks | Proceedings of the 34th ...,https://dl.acm.org/doi/10.5555/3495724.3497084,"In this paper, we propose a more intuitive and transparent architecture for graph-structured data, so-called Random Walk Graph Neural Network (RWNN). The first layer of the model consists of a number of trainable ""hidden graphs"" which are compared against the input graphs using a random walk kernel to produce graph representations."
Graph Spectral Filtering with Chebyshev Interpolation for Recommendation,2505.00552v1,jinRawgnn2022,\cite{jinRawgnn2022},Raw-{GNN}: Random walk aggregation based graph neural network,,,True,False,"Jin, Di and Wang, Rui and Ge, Meng and He, Dongxiao and Li, Xiang and Lin, Wei and Zhang, Weixiong",2022.0,,,,arXiv:2206.13953,Raw-{GNN}: Random walk aggregation based graph neural network,PDF,https://www.ijcai.org/proceedings/2022/0293.pdf,"tion mechanism and propose a RAndom Walk aggregation-based Graph Neural Network, short-handed as RAW-GNN. We integrate random walk sampling into graph neural net-works and extend the conventional neighborhoods to k-hop path-based neighborhoods. A k-hop path formed by random walks preserves the original attributes on this knodes and the"
Graph Spectral Filtering with Chebyshev Interpolation for Recommendation,2505.00552v1,defferrardConvolutionalNeuralNetworks2016,\cite{defferrardConvolutionalNeuralNetworks2016},Convolutional neural networks on graphs with fast localized spectral filtering,,,True,False,"Defferrard, Micha{\""e}l and Bresson, Xavier and Vandergheynst, Pierre",2016.0,,,,,Convolutional neural networks on graphs with fast localized spectral filtering,Convolutional neural networks on graphs with fast localized spectral ...,https://dl.acm.org/doi/10.5555/3157382.3157527,"Convolutional neural networks on graphs with fast localized spectral filtering. Authors: Michaël Defferrard, Xavier Bresson, ... In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as"
Graph Spectral Filtering with Chebyshev Interpolation for Recommendation,2505.00552v1,heBernNetLearningArbitrary2021,\cite{heBernNetLearningArbitrary2021},Bern{N}et: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation,,,True,False,"He, Mingguo and Wei, Zhewei and Huang, zengfeng and Xu, Hongteng",2021.0,,,,,Bern{N}et: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation,BernNet: Learning Arbitrary Graph Spectral Filters via Bernstein ...,https://arxiv.org/abs/2106.10994,"Many representative graph neural networks, e.g., GPR-GNN and ChebNet, approximate graph convolutions with graph spectral filters. However, existing work either applies predefined filter weights or learns them without necessary constraints, which may lead to oversimplified or ill-posed filters. To overcome these issues, we propose BernNet, a novel graph neural network with theoretical support"
Graph Spectral Filtering with Chebyshev Interpolation for Recommendation,2505.00552v1,heLightGCNSimplifyingPowering2020,\cite{heLightGCNSimplifyingPowering2020},Light{GCN}: Simplifying and Powering Graph Convolution Network for Recommendation,,,True,False,"He, Xiangnan and Deng, Kuan and Wang, Xiang and Li, Yan and Zhang, YongDong and Wang, Meng",2020.0,,,,,Light{GCN}: Simplifying and Powering Graph Convolution Network for Recommendation,LightGCN: Simplifying and Powering Graph Convolution Network for ...,https://ar5iv.labs.arxiv.org/html/2002.02126,"Graph Convolution Network (GCN) has become new state-of-the-art for collaborative filtering. Nevertheless, the reasons of its effectiveness for recommendation are not well understood. Existing work that adapts GCN to recommendation lacks thorough ablation analyses on GCN, which is originally designed for graph classification tasks and equipped"
Graph Spectral Filtering with Chebyshev Interpolation for Recommendation,2505.00552v1,maoUltraGCNUltraSimplification2021,\cite{maoUltraGCNUltraSimplification2021},Ultra{GCN}: Ultra Simplification of Graph Convolutional Networks for Recommendation,,,True,False,"Mao, Kelong and Zhu, Jieming and Xiao, Xi and Lu, Biao and Wang, Zhaowei and He, Xiuqiang",2021.0,,,,,Ultra{GCN}: Ultra Simplification of Graph Convolutional Networks for Recommendation,GitHub - JoanDING/UltraGCN: [CIKM'21] UltraGCN: Ultra Simplification of ...,https://github.com/JoanDING/UltraGCN,"[CIKM'21] UltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation - JoanDING/UltraGCN ... Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, Xiuqiang He. UltraGCN: Ultra Simplification of Graph Convolutional Networks for Recommendation. Paper in ... we propose an ultra-simplified formulation of GCN, dubbed UltraGCN. UltraGCN"
Graph Spectral Filtering with Chebyshev Interpolation for Recommendation,2505.00552v1,heSGCF2023,\cite{heSGCF2023},Simplifying graph-based collaborative filtering for recommendation,,,True,False,"He, Li and Wang, Xianzhi and Wang, Dingxian and Zou, Haoyuan and Yin, Hongzhi and Xu, Guandong",2023.0,,,,,Simplifying graph-based collaborative filtering for recommendation,heli510/SGCF: WSDM'23 Full Paper Source - GitHub,https://github.com/heli510/SGCF,"A implementation of paper ""Simplifying Graph-based Collaborative Filtering for Recommendation"" Our code is about to be presented, Stay tuned :) Abstract Graph Convolutional Networks (GCNs) are a popular type of machine learning models that use multiple layers of convolutional aggregation operations and non-linear activations to represent data."
Graph Spectral Filtering with Chebyshev Interpolation for Recommendation,2505.00552v1,sunNeighborInteractionAware2020,\cite{sunNeighborInteractionAware2020},Neighbor Interaction Aware Graph Convolution Networks for Recommendation,,,True,False,"Sun, Jianing and Zhang, Yingxue and Guo, Wei and Guo, Huifeng and Tang, Ruiming and He, Xiuqiang and Ma, Chen and Coates, Mark",2020.0,,,,,Neighbor Interaction Aware Graph Convolution Networks for Recommendation,Jianing Sun - GitHub Pages,https://jianing-sun.github.io/publication/,"Neighbor Interaction Aware Graph Convolution Networks for Recommendation. We propose NIA-GCN, which can explicitly model the relational information between neighbor nodes and exploit the heterogeneous nature of the user-item bipartite graph. Furthermore, we generalize our framework to a commercial App store recommendation scenario."
Graph Spectral Filtering with Chebyshev Interpolation for Recommendation,2505.00552v1,jinri2024content,\cite{jinri2024content},Content-based graph reconstruction for cold-start item recommendation,,,True,False,"Kim, Jinri and Kim, Eungi and Yeo, Kwangeun and Jeon, Yujin and Kim, Chanwoo and Lee, Sewon and Lee, Joonseok",2024.0,,,,,Content-based graph reconstruction for cold-start item recommendation,Content-based Graph Reconstruction for Cold-start Item Recommendation ...,https://dl.acm.org/doi/10.1145/3626772.3657801,"In this paper, we introduce Content-based Graph Reconstruction for Cold-start item recommendation (CGRC), employing a masked graph autoencoder structure and multimodal contents to directly incorporate interaction-based high-order connectivity, applicable even in cold-start scenarios."
Graph Spectral Filtering with Chebyshev Interpolation for Recommendation,2505.00552v1,eungi2025reducedgcn,\cite{eungi2025reducedgcn},Reduced{GCN}: Learning to Adapt Graph Convolution for Top-N Recommendation,,,True,False,"Kim, Eungi and Kim, Chanwoo and Yeo, Kwangeun and Kim, Jinri and Jeon, Yujin and Lee, Sewon and Lee, Joonseok",2025.0,,,,,Reduced{GCN}: Learning to Adapt Graph Convolution for Top-N Recommendation,PDF,http://www.joonseok.net/papers/pakdd25_reducedgcn.pdf,"ReducedGCN: Learning to Adapt Graph Convolution for Top-N Recommendation EungiKim,ChanwooKim,KwangeunYeo,JinriKim, YujinJeon,SewonLee,andJoonseokLee[0000 −0002 0786 8086]⋆ GraduateSchoolofDataScience,SeoulNationalUniversity,Seoul,Korea"
Graph Spectral Filtering with Chebyshev Interpolation for Recommendation,2505.00552v1,park2024turbo,\cite{park2024turbo},Turbo-{CF}: Matrix decomposition-free graph filtering for fast recommendation,,,True,False,"Park, Jin-Duk and Shin, Yong-Min and Shin, Won-Yong",2024.0,,,,,Turbo-{CF}: Matrix decomposition-free graph filtering for fast recommendation,GitHub - jindeok/Turbo-CF: (SIGIR'24) Polynomial GF-based ...,https://github.com/jindeok/Turbo-CF,"title={Turbo-cf: Matrix decomposition-free graph filtering for fast recommendation}, author={Park, Jin-Duk and Shin, Yong-Min and Shin, Won-Yong}, booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},"
"Search-Based Interaction For Conversation Recommendation via Generative
  Reward Model Based Simulated User",2504.20458v1,lei2020estimation,\cite{lei2020estimation},Estimation-action-reflection: Towards deep interaction between conversational and recommender systems,,,True,False,"Lei, Wenqiang and He, Xiangnan and Miao, Yisong and Wu, Qingyun and Hong, Richang and Kan, Min-Yen and Chua, Tat-Seng",2020.0,,,,,Estimation-action-reflection: Towards deep interaction between conversational and recommender systems,Estimation-Action-Reflection: Towards Deep Interaction Between ...,https://arxiv.org/pdf/2002.09102,"Estimation-Action-Reflection: Towards Deep Interaction Between Conversational and Recommender Systems Wenqiang Lei1, Xiangnan He2∗, Yisong Miao1, Qingyun Wu3, Richang Hong4, Min-Yen Kan1, Tat-Seng Chua1 1National University of Singapore, 2University of Science and Technology of China 3University of Virginia, 4Hefei University of Technology"
"Search-Based Interaction For Conversation Recommendation via Generative
  Reward Model Based Simulated User",2504.20458v1,wang2022towards,\cite{wang2022towards},Towards unified conversational recommender systems via knowledge-enhanced prompt learning,,,True,False,"Wang, Xiaolei and Zhou, Kun and Wen, Ji-Rong and Zhao, Wayne Xin",2022.0,,,,,Towards unified conversational recommender systems via knowledge-enhanced prompt learning,Towards Unified Conversational Recommender Systems via Knowledge ...,https://dl.acm.org/doi/10.1145/3534678.3539382,"To address this problem, we propose a unified CRS model named UniCRS based on knowledge-enhanced prompt learning. Our approach unifies the recommendation and conversation subtasks into the prompt learning paradigm, and utilizes knowledge-enhanced prompts based on a fixed pre-trained language model (PLM) to fulfill both subtasks in a unified"
"Search-Based Interaction For Conversation Recommendation via Generative
  Reward Model Based Simulated User",2504.20458v1,wang2023improving,\cite{wang2023improving},Improving conversational recommendation systems via counterfactual data simulation,,,True,False,"Wang, Xiaolei and Zhou, Kun and Tang, Xinyu and Zhao, Wayne Xin and Pan, Fan and Cao, Zhao and Wen, Ji-Rong",2023.0,,,,,Improving conversational recommendation systems via counterfactual data simulation,Improving Conversational Recommendation Systems via Counterfactual Data ...,https://ar5iv.labs.arxiv.org/html/2306.02842,"CFCRS is a novel approach that uses counterfactual data augmentation to generate realistic and diverse recommendation dialogues for training conversational recommender systems. It learns user preference and dialogue schema from real data, and simulates alternative scenarios with a conversation flow language model and an adversarial training method."
"Search-Based Interaction For Conversation Recommendation via Generative
  Reward Model Based Simulated User",2504.20458v1,dao2024broadening,\cite{dao2024broadening},Broadening the view: Demonstration-augmented prompt learning for conversational recommendation,,,True,False,"Dao, Huy and Deng, Yang and Le, Dung D and Liao, Lizi",2024.0,,,,,Broadening the view: Demonstration-augmented prompt learning for conversational recommendation,GitHub - huyquangdao/DCRS,https://github.com/huyquangdao/DCRS,"Broadening the View: Demonstration-augmented Prompt Learning for Conversational Recommendation. ... Dung D. and Liao, Lizi}, title = {Broadening the View: Demonstration-augmented Prompt Learning for Conversational Recommendation}, year = {2024}, isbn = {9798400704314}"
"Search-Based Interaction For Conversation Recommendation via Generative
  Reward Model Based Simulated User",2504.20458v1,he2023large,\cite{he2023large},Large language models as zero-shot conversational recommenders,,,True,False,"He, Zhankui and Xie, Zhouhang and Jha, Rahul and Steck, Harald and Liang, Dawen and Feng, Yesu and Majumder, Bodhisattwa Prasad and Kallus, Nathan and McAuley, Julian",2023.0,,,,,Large language models as zero-shot conversational recommenders,Large Language Models as Zero-Shot Conversational Recommenders,https://arxiv.org/abs/2308.10053,"In this paper, we present empirical studies on conversational recommendation tasks using representative large language models in a zero-shot setting with three primary contributions. (1) Data: To gain insights into model behavior in ""in-the-wild"" conversational recommendation scenarios, we construct a new dataset of recommendation-related conversations by scraping a popular discussion website"
"Search-Based Interaction For Conversation Recommendation via Generative
  Reward Model Based Simulated User",2504.20458v1,yang2024unleashing,\cite{yang2024unleashing},Unleashing the Retrieval Potential of Large Language Models in Conversational Recommender Systems,,,True,False,"Yang, Ting and Chen, Li",2024.0,,,,,Unleashing the Retrieval Potential of Large Language Models in Conversational Recommender Systems,Unleashing the Retrieval Potential of Large Language Models in ...,https://dl.acm.org/doi/10.1145/3640457.3688146,"To address these challenges, we propose an end-to-end large-scale CRS model, named as ReFICR, a novel LLM-enhanced conversational recommender that empowers a retrievable large language model to perform conversational recommendation by following retrieval and generation instructions through lightweight tuning."
"Search-Based Interaction For Conversation Recommendation via Generative
  Reward Model Based Simulated User",2504.20458v1,xie2024neighborhood,\cite{xie2024neighborhood},Neighborhood-Based Collaborative Filtering for Conversational Recommendation,,,True,False,"Xie, Zhouhang and Wu, Junda and Jeon, Hyunsik and He, Zhankui and Steck, Harald and Jha, Rahul and Liang, Dawen and Kallus, Nathan and McAuley, Julian",2024.0,,,,,Neighborhood-Based Collaborative Filtering for Conversational Recommendation,Neighborhood-Based Collaborative Filtering for Conversational ...,https://dl.acm.org/doi/abs/10.1145/3640457.3688191,"Following this intuition, we define a class of neighborhood-based CRS that makes recommendations by identifying items commonly associated with similar training dialogue contexts. Experiments on Inspired, Redial, and Reddit-Movie benchmarks show our method outperforms state-of-the-art LLMs with 2 billion parameters, and offers on-par performance"
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,wang2023single,\cite{wang2023single},Single-shot feature selection for multi-task recommendations,,,True,False,"Wang, Yejing and Du, Zhaocheng and Zhao, Xiangyu and Chen, Bo and Guo, Huifeng and Tang, Ruiming and Dong, Zhenhua",2023.0,,,,,Single-shot feature selection for multi-task recommendations,Single-shot Feature Selection for Multi-task Recommendations,https://dl.acm.org/doi/10.1145/3539618.3591767,"Existing feature selection methods may neglect task relations or require significant computation during model training in multi-task setting. To this end, this paper proposes a novel Single-shot Feature Selection framework for MTRSs, referred to as MultiSFS, which is capable of selecting feature fields for each task while considering task"
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,li2022gromov,\cite{li2022gromov},Gromov-wasserstein guided representation learning for cross-domain recommendation,,,True,False,"Li, Xinhang and Qiu, Zhaopeng and Zhao, Xiangyu and Wang, Zihao and Zhang, Yong and Xing, Chunxiao and Wu, Xian",2022.0,,,,,Gromov-wasserstein guided representation learning for cross-domain recommendation,Gromov-Wasserstein Guided Representation Learning for Cross-Domain ...,https://dl.acm.org/doi/10.1145/3511808.3557338,"To this end, we propose a novel framework that improves the effect of representation learning on the target domain by aligning the representation distributions between the source and target domains. In addition, GWCDR can be easily integrated with existing single-domain collaborative filtering methods to achieve cross-domain recommendation."
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,wang2023plate,\cite{wang2023plate},PLATE: A prompt-enhanced paradigm for multi-scenario recommendations,,,True,False,"Wang, Yuhao and Zhao, Xiangyu and Chen, Bo and Liu, Qidong and Guo, Huifeng and Liu, Huanshuo and Wang, Yichao and Zhang, Rui and Tang, Ruiming",2023.0,,,,,PLATE: A prompt-enhanced paradigm for multi-scenario recommendations,PLATE: A Prompt-Enhanced Paradigm for Multi-Scenario Recommendations ...,https://dl.acm.org/doi/10.1145/3539618.3591750,"In this work, we propose a novel prompt-enhanced paradigm for multi-scenario recommendation. Specifically, a unified DRS backbone model is first pre-trained using data from all the domains in order to capture the commonality across domains."
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,gao2023autotransfer,\cite{gao2023autotransfer},AutoTransfer: Instance transfer for cross-domain recommendations,,,True,False,"Gao, Jingtong and Zhao, Xiangyu and Chen, Bo and Yan, Fan and Guo, Huifeng and Tang, Ruiming",2023.0,,,,,AutoTransfer: Instance transfer for cross-domain recommendations,AutoTransfer: Instance Transfer for Cross-Domain Recommendations ...,https://dl.acm.org/doi/10.1145/3539618.3591701,"AutoTransfer: Instance Transfer for Cross-Domain Recommendations. Authors: Jingtong Gao, Xiangyu Zhao, Bo Chen, ... Cross-domain recommendations can assist users in selecting suitable items in the target domain by aggregating or transferring the abundant available data from the auxiliary domain, which has gradually become a promising research"
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,jia2024d3,\cite{jia2024d3},"D3: A Methodological Exploration of Domain Division, Modeling, and Balance in Multi-Domain Recommendations",,,True,False,"Jia, Pengyue and Wang, Yichao and Lin, Shanru and Li, Xiaopeng and Zhao, Xiangyu and Guo, Huifeng and Tang, Ruiming",2024.0,,,,,"D3: A Methodological Exploration of Domain Division, Modeling, and Balance in Multi-Domain Recommendations","D3: A Methodological Exploration of Domain Division, Modeling ...",https://www.researchgate.net/publication/379285737_D3_A_Methodological_Exploration_of_Domain_Division_Modeling_and_Balance_in_Multi-Domain_Recommendations?_tp=eyJjb250ZXh0Ijp7InBhZ2UiOiJzY2llbnRpZmljQ29udHJpYnV0aW9ucyIsInByZXZpb3VzUGFnZSI6bnVsbCwic3ViUGFnZSI6bnVsbH19,"To address these challenges, this paper proposes a universal and flexible framework D3 aimed at optimizing the multi-domain recommendation pipeline from three"
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,liu2024multifs,\cite{liu2024multifs},MultiFS: Automated multi-scenario feature selection in deep recommender systems,,,True,False,"Liu, Dugang and Yang, Chaohua and Tang, Xing and Wang, Yejing and Lyu, Fuyuan and Luo, Weihong and He, Xiuqiang and Ming, Zhong and Zhao, Xiangyu",2024.0,,,,,MultiFS: Automated multi-scenario feature selection in deep recommender systems,MultiFS: Automated Multi-Scenario Feature Selection in Deep Recommender ...,https://dl.acm.org/doi/10.1145/3616855.3635859,"In addition, existing feature selection methods for deep recommender systems may lack the exploration of scenario relations. In this paper, we propose a novel automated multi-scenario feature selection (MultiFS) framework to bridge this gap, which is able to consider scenario relations and utilize a hierarchical gating mechanism to select"
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,li2023strec,\cite{li2023strec},STRec: Sparse transformer for sequential recommendations,,,True,False,"Li, Chengxi and Wang, Yejing and Liu, Qidong and Zhao, Xiangyu and Wang, Wanyu and Wang, Yiqi and Zou, Lixin and Fan, Wenqi and Li, Qing",2023.0,,,,,STRec: Sparse transformer for sequential recommendations,STRec: Sparse Transformer for Sequential Recommendations,https://dl.acm.org/doi/abs/10.1145/3604915.3608779,"STRec: Sparse Transformer for Sequential Recommendations ... In this paper, we identify the sparse attention phenomenon in transformer-based SRS models and propose Sparse Transformer for sequential Recommendation tasks (STRec) to achieve the efficient computation and improved performance. ... and Muyang Li. 2022. MAE4Rec: Storage-saving"
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,liu2023dirac,\cite{liu2023dirac},Disentangling interest and conformity for eliminating popularity bias in session-based recommendation,,,True,False,"Liu, Qidong and Tian, Feng and Zheng, Qinghua and Wang, Qianying",2023.0,,,,Knowledge and Information Systems,Disentangling interest and conformity for eliminating popularity bias in session-based recommendation,Disentangling interest and conformity for eliminating popularity bias ...,https://link.springer.com/article/10.1007/s10115-023-01839-0,"Session-based recommendation (SBR) is to predict the next item, given an anonymous interaction sequence. Recently, many advanced SBR models have shown great recommending performance, but few studies note that they suffer from popularity bias seriously: the model tends to recommend popular items and fails to recommend long-tail items. The only few debias works relieve popularity bias indeed"
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,wu2022graph,\cite{wu2022graph},Graph neural networks in recommender systems: a survey,,,True,False,"Wu, Shiwen and Sun, Fei and Zhang, Wentao and Xie, Xu and Cui, Bin",2022.0,,,,ACM Computing Surveys,Graph neural networks in recommender systems: a survey,Graph Neural Networks in Recommender Systems: A Survey - ADS - NASA/ADS,https://ui.adsabs.harvard.edu/abs/2020arXiv201102260W/abstract,"In recommender systems, the main challenge is to learn the effective user/item representations from their interactions and side information (if any). Recently, graph neural network (GNN) techniques have been widely utilized in recommender systems since most of the information in recommender systems essentially has graph structure and GNN has"
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,xu2023multi,\cite{xu2023multi},A multi-view graph contrastive learning framework for cross-domain sequential recommendation,,,True,False,"Xu, Zitao and Pan, Weike and Ming, Zhong",2023.0,,,,,A multi-view graph contrastive learning framework for cross-domain sequential recommendation,A Multi-view Graph Contrastive Learning Framework for Cross-Domain ...,https://dl.acm.org/doi/10.1145/3604915.3608785,Cross-domain sequential recommendation aims to alleviate this problem by introducing relatively richer source-domain data. ... in this paper we propose a generic framework named multi-view graph contrastive learning (MGCL). ... Multi-level Contrastive Learning Framework for Sequential Recommendation. In Proceedings of the 31st ACM International
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,ma2019pi,\cite{ma2019pi},$\pi$-net: A parallel information-sharing network for shared-account cross-domain sequential recommendations,,,True,False,"Ma, Muyang and Ren, Pengjie and Lin, Yujie and Chen, Zhumin and Ma, Jun and Rijke, Maarten de",2019.0,,,,,$\pi$-net: A parallel information-sharing network for shared-account cross-domain sequential recommendations,π-Net: A Parallel Information-sharing Network for Shared-account Cross ...,https://dl.acm.org/doi/10.1145/3331184.3331200,We formulate the Shared-account Cross-domain Sequential Recommendation (SCSR) task as a parallel sequential recommendation problem. We propose a Parallel Information-sharing Network (π-Net) to simultaneously generate recommendations for two domains where user behaviors on two domains are synchronously shared at each timestamp. π-Net contains two core units: a shared account filter unit (SFU
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,bao2024large,\cite{bao2024large},"Large language models for recommendation: Past, present, and future",,,True,False,"Bao, Keqin and Zhang, Jizhi and Lin, Xinyu and Zhang, Yang and Wang, Wenjie and Feng, Fuli",2024.0,,,,,"Large language models for recommendation: Past, present, and future",PDF,https://generative-rec.github.io/tutorial/files/SIGIR24_Tutorial.pdf,"Large Language Models, Recommender Systems, Generative Rec-ommendation, Generative Models ACM Reference Format: Keqin Bao*, Jizhi Zhang, Xinyu Lin, Yang Zhang, Wenjie Wang, and Fuli Feng. 2024. Large Language Models for Recommendation: Past, Present, and Future. In Proceedings of the 47th International ACM SIGIR Conference *Main contact author."
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,fu2023unified,\cite{fu2023unified},A unified framework for multi-domain ctr prediction via large language models,,,True,False,"Fu, Zichuan and Li, Xiangyang and Wu, Chuhan and Wang, Yichao and Dong, Kuicai and Zhao, Xiangyu and Zhao, Mengchen and Guo, Huifeng and Tang, Ruiming",2023.0,,,,ACM Transactions on Information Systems,A unified framework for multi-domain ctr prediction via large language models,A Unified Framework for Multi-Domain CTR Prediction via Large Language ...,https://arxiv.org/abs/2312.10743,"View a PDF of the paper titled A Unified Framework for Multi-Domain CTR Prediction via Large Language Models, by Zichuan Fu and 8 other authors View PDF HTML (experimental) Abstract: Click-Through Rate (CTR) prediction is a crucial task in online recommendation platforms as it involves estimating the probability of user engagement with"
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,liu2024llmers,\cite{liu2024llmers},"Large language model enhanced recommender systems: Taxonomy, trend, application and future",,,True,False,"Liu, Qidong and Zhao, Xiangyu and Wang, Yuhao and Wang, Yejing and Zhang, Zijian and Sun, Yuqi and Li, Xiang and Wang, Maolin and Jia, Pengyue and Chen, Chong and others",2024.0,,,,arXiv preprint arXiv:2412.13432,"Large language model enhanced recommender systems: Taxonomy, trend, application and future",Awesome LLM-Enhanced Recommender Systems - GitHub,https://github.com/liuqidong07/Awesome-LLM-Enhanced-Recommender-Systems,"A collection of papers on large language model enhanced recommender systems (LLMERS) with taxonomy, trend, application and future. LLMERS augment conventional recommenders with large language models for knowledge, interaction or model enhancement."
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,hu2024enhancing,\cite{hu2024enhancing},Enhancing sequential recommendation via llm-based semantic embedding learning,,,True,False,"Hu, Jun and Xia, Wenwen and Zhang, Xiaolu and Fu, Chilin and Wu, Weichang and Huan, Zhaoxin and Li, Ang and Tang, Zuoli and Zhou, Jun",2024.0,,,,,Enhancing sequential recommendation via llm-based semantic embedding learning,Enhancing Sequential Recommendation via LLM-based Semantic Embedding ...,https://dl.acm.org/doi/10.1145/3589335.3648307,"In this paper, we introduce SAID, a framework that utilizes LLMs to explicitly learn Semantically Aligned item ID embeddings based on texts. For each item, SAID employs a projector module to transform an item ID into an embedding vector, which will be fed into an LLM to elicit the exact descriptive text tokens accompanied by the item."
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,liu2024practice,\cite{liu2024practice},A Practice-Friendly Two-Stage LLM-Enhanced Paradigm in Sequential Recommendation,,,True,False,"Liu, Dugang and Xian, Shenxian and Lin, Xiaolin and Zhang, Xiaolian and Zhu, Hong and Fang, Yuan and Chen, Zhen and Ming, Zhong",2024.0,,,,arXiv preprint arXiv:2406.00333,A Practice-Friendly Two-Stage LLM-Enhanced Paradigm in Sequential Recommendation,A Practice-Friendly Two-Stage LLM-Enhanced Paradigm in Sequential ...,https://arxiv.org/html/2406.00333v1,"To alleviate these problems, this paper proposes a novel practice-friendly two-stage LLM-enhanced paradigm (TSLRec) for SRS. Specifically, in the information reconstruction stage, we design a new user-level SFT task for collaborative information injection with the assistance of a pre-trained SRS model, which is more efficient and compatible"
"Bridge the Domains: Large Language Models Enhanced Cross-domain
  Sequential Recommendation",2504.18383v1,liu2024llm,\cite{liu2024llm},Llm-esr: Large language models enhancement for long-tailed sequential recommendation,,,True,False,"Liu, Qidong and Wu, Xian and Wang, Yejing and Zhang, Zijian and Tian, Feng and Zheng, Yefeng and Zhao, Xiangyu",2024.0,,,,Advances in Neural Information Processing Systems,Llm-esr: Large language models enhancement for long-tailed sequential recommendation,Large Language Models Enhanced Sequential Recommendation for Long-tail ...,https://openreview.net/forum?id=fcc2Ena74Z,"In this study, we introduce the Large Language Models Enhancement framework for Sequential Recommendation (LLM-ESR), which leverages semantic embeddings from LLMs to enhance SRS performance without increasing computational overhead. ... To address the long-tail user challenge, we introduce a retrieval augmented self-distillation technique to"
Replication and Exploration of Generative Retrieval over Dynamic Corpora,2504.17519v1,mehta2022dsi++,\cite{mehta2022dsi++},DSI++: Updating transformer memory with new documents,,,True,False,"Mehta, Sanket Vaibhav and Gupta, Jai and Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Rao, Jinfeng and Najork, Marc and Strubell, Emma and Metzler, Donald",2022.0,,,,arXiv preprint arXiv:2212.09744,DSI++: Updating transformer memory with new documents,DSI++: Updating Transformer Memory with New Documents,https://openreview.net/forum?id=XkwkFYPT6t,"DSI++: Updating Transformer Memory with New Documents. Sanket Vaibhav Mehta, Jai Gupta, Yi Tay, ... In this work, we introduce DSI++, a continual learning challenge for DSI with the goal of continuously indexing new documents while being able to answer queries related to both previously and newly indexed documents. Across different model scales"
Replication and Exploration of Generative Retrieval over Dynamic Corpora,2504.17519v1,guo2024corpusbrain++,\cite{guo2024corpusbrain++},Corpusbrain++: A continual generative pre-training framework for knowledge-intensive language tasks,,,True,False,"Guo, Jiafeng and Zhou, Changjiang and Zhang, Ruqing and Chen, Jiangui and de Rijke, Maarten and Fan, Yixing and Cheng, Xueqi",2024.0,,,,arXiv preprint arXiv:2402.16767,Corpusbrain++: A continual generative pre-training framework for knowledge-intensive language tasks,Selected Publications - Ruqing Zhang,https://daqingchong.github.io/publications/,"CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks. arXiv:2402.16767; Yu-An Liu, Ruqing Zhang, Mingkun Zhang, Wei Chen, Maarten de Rijke, Jiafeng Guo and Xueqi Cheng. Perturbation-Invariant Adversarial Training for Neural Ranking Models: Improving the Effectiveness-Robustness Trade-Off."
"NLCTables: A Dataset for Marrying Natural Language Conditions with Table
  Discovery",2504.15849v1,nargesian_table_2018,\cite{nargesian_table_2018},Table union search on open data,,,True,False,"Nargesian, Fatemeh and Zhu, Erkang and Pu, Ken Q. and Miller, Renée J.",2018.0,,,10.14778/3192965.3192973,Proc. VLDB Endow.,Table union search on open data,PDF,https://www.vldb.org/pvldb/vol11/p813-nargesian.pdf,open-sourced a benchmark of Open Data tables. We show that our table union search outperforms in speed and accuracy existing al-gorithms for ﬁnding related tables and scales to provide efﬁcient search over Open Data repositories containing more than one mil-lion attributes. PVLDB Reference Format:
"NLCTables: A Dataset for Marrying Natural Language Conditions with Table
  Discovery",2504.15849v1,hu_automatic_2023,\cite{hu_automatic_2023},Automatic {Table} {Union} {Search} with {Tabular} {Representation} {Learning},,,True,False,"Hu, Xuming and Wang, Shen and Qin, Xiao and Lei, Chuan and Shen, Zhengyuan and Faloutsos, Christos and Katsifodimos, Asterios and Karypis, George and Wen, Lijie and Yu, Philip S.",2023.0,,,10.18653/v1/2023.findings-acl.233,,Automatic {Table} {Union} {Search} with {Tabular} {Representation} {Learning},Automatic Table Union Search with Tabular Representation Learning,https://aclanthology.org/2023.findings-acl.233/,"Automatic Table Union Search with Tabular Representation Learning (Hu et al., Findings 2023) ACL. Xuming Hu, Shen Wang, Xiao Qin, Chuan Lei, Zhengyuan Shen, Christos Faloutsos, Asterios Katsifodimos, George Karypis, Lijie Wen, and Philip S. Yu. 2023. Automatic Table Union Search with Tabular Representation Learning."
"NLCTables: A Dataset for Marrying Natural Language Conditions with Table
  Discovery",2504.15849v1,yakout_infogather_2012,\cite{yakout_infogather_2012},{InfoGather}: entity augmentation and attribute discovery by holistic matching with web tables,,,True,False,"Yakout, Mohamed and Ganjam, Kris and Chakrabarti, Kaushik and Chaudhuri, Surajit",2012.0,,,10.1145/2213836.2213848,,{InfoGather}: entity augmentation and attribute discovery by holistic matching with web tables,RLGen/LakeBench - GitHub,https://github.com/RLGen/LAKEBENCH,"Here is an example to run InfoGather. ... InfoGather-Entity Augmentation and Attribute Discovery By Holistic Matching with Web Tables. Folder Structure • GettingStart • QuickStart • Result • 🌊 The Web contains a vast corpus of HTML tables, specifically entityattribute tables. We present three core operations, namely entity"
"NLCTables: A Dataset for Marrying Natural Language Conditions with Table
  Discovery",2504.15849v1,zhu_josie_2019,\cite{zhu_josie_2019},{JOSIE}: {Overlap} {Set} {Similarity} {Search} for {Finding} {Joinable} {Tables} in {Data} {Lakes},,,True,False,"Zhu, Erkang and Deng, Dong and Nargesian, Fatemeh and Miller, Renée J.",2019.0,,,10.1145/3299869.3300065,,{JOSIE}: {Overlap} {Set} {Similarity} {Search} for {Finding} {Joinable} {Tables} in {Data} {Lakes},JOSIE: Overlap Set Similarity Search for Finding Joinable Tables in ...,https://dl.acm.org/doi/10.1145/3299869.3300065,- JOSIE: Overlap Set Similarity Search for Finding Joinable Tables in Data Lakes # JOSIE: Overlap Set Similarity Search for Finding Joinable Tables in Data Lakes JOSIE: Overlap Set Similarity Search for Finding Joinable Tables in Data Lakes We show that JOSIE completely out performs the state-of-the-art overlap set similarity search techniques on data lakes. 1. JOSIE: Overlap Set Similarity Search for Finding Joinable Tables in Data Lakes Similarity search for data streams has attracted much attention recently in the area of information recommendation. - Mann WAugsten NJensen CPawlik M(2024)SWOOP: top-k similarity joins over set streamsThe VLDB Journal — The International Journal on Very Large Data Bases10.1007/s00778-024-00880-x**34**:1Online publication date: 23-Dec-2024
"NLCTables: A Dataset for Marrying Natural Language Conditions with Table
  Discovery",2504.15849v1,liu_feature_2022,\cite{liu_feature_2022},Feature {Augmentation} with {Reinforcement} {Learning},,,True,False,"Liu, Jiabin and Chai, Chengliang and Luo, Yuyu and Lou, Yin and Feng, Jianhua and Tang, Nan",2022.0,,,10.1109/ICDE53745.2022.00317,,Feature {Augmentation} with {Reinforcement} {Learning},Feature Augmentation with Reinforcement Learning - IEEE Xplore,https://ieeexplore.ieee.org/document/9835530,"Sufficient good features are indispensable to train well-performed machine learning models. However, it is com-mon that good features are not always enough, where feature augmentation is necessary to enrich high-quality features by joining with other tables. There are two main challenges for the problem. Given a set of tables where we can augment features from, the first challenge is that"
"NLCTables: A Dataset for Marrying Natural Language Conditions with Table
  Discovery",2504.15849v1,deng2024lakebench,\cite{deng2024lakebench},LakeBench: A Benchmark for Discovering Joinable and Unionable Tables in Data Lakes,,,True,False,"Deng, Yuhao and Chai, Chengliang and Cao, Lei and Yuan, Qin and Chen, Siyuan and Yu, Yanrui and Sun, Zhaoze and Wang, Junyi and Li, Jiajun and Cao, Ziqi and others",2024.0,,,,Proc. VLDB Endow.,LakeBench: A Benchmark for Discovering Joinable and Unionable Tables in Data Lakes,RLGen/LakeBench - GitHub,https://github.com/RLGen/LAKEBENCH,"LakeBench: A Benchmark for Discovering Joinable and Unionable Tables in Data Lakes. Community • Folder Structure • GettingStart • QuickStart • Result • 🌊 LakeBench is a large-scale benchmark designed to test the mettle of table discovery methods on a much larger scale,"
"NLCTables: A Dataset for Marrying Natural Language Conditions with Table
  Discovery",2504.15849v1,opendata,\cite{opendata},OpenData,,,True,False,,,,https://open.canada.ca/,,,OpenData,NYC Open Data,https://opendata.cityofnewyork.us/,"NYC Open Data logo # Open Data for All New Yorkers Open Data is free public data published by New York City agencies and other partners. Attend a training class or sign up for the NYC Open Data mailing list to get the latest news and find out about upcoming events. ### NYC Open Data Week Explore how other people use Open Data! NYC Open Data Week ### New to Open Data View details on Open Data APIs. Ask a question, leave a comment, or suggest a dataset to the NYC Open Data team. ## Discover NYC Data View recently published datasets on the data catalog. View some of the most popular datasets on the data catalog."
"NLCTables: A Dataset for Marrying Natural Language Conditions with Table
  Discovery",2504.15849v1,venetis_recovering_2011,\cite{venetis_recovering_2011},Recovering semantics of tables on the web,,,True,False,"Venetis, Petros and Halevy, Alon and Madhavan, Jayant and Paşca, Marius and Shen, Warren and Wu, Fei and Miao, Gengxin and Wu, Chung",2011.0,,,10.14778/2002938.2002939,Proc. VLDB Endow.,Recovering semantics of tables on the web,Recovering semantics of tables on the web | Proceedings of the VLDB ...,https://dl.acm.org/doi/10.14778/2002938.2002939,"The Web offers a corpus of over 100 million tables [6], but the meaning of each table is rarely explicit from the table itself. Header rows exist in few cases and even when they do, the attribute names are typically useless. We describe a system that attempts to recover the semantics of tables by enriching the table with additional annotations."
"NLCTables: A Dataset for Marrying Natural Language Conditions with Table
  Discovery",2504.15849v1,cafarella2009data,\cite{cafarella2009data},Data integration for the relational web,,,True,False,"Cafarella, Michael J and Halevy, Alon and Khoussainova, Nodira",2009.0,,,,Proc. VLDB Endow.,Data integration for the relational web,Data integration for the relational web | Proceedings of the VLDB Endowment,https://dl.acm.org/doi/10.14778/1687627.1687750,"The Web contains a vast amount of structured information such as HTML tables, HTML lists and deep-web databases; there is enormous potential in combining and re-purposing this data in creative ways. However, integrating data from this relational web raises several challenges that are not addressed by current data integration systems or mash-up"
"Understanding Accuracy-Fairness Trade-offs in Re-ranking through
  Elasticity in Economics",2504.14991v1,lipani2016fairness,\cite{lipani2016fairness},Fairness in Information Retrieval,,,True,False,"Lipani, Aldo",2016.0,,,,,Fairness in Information Retrieval,FAIR: Fairness-aware information retrieval evaluation,https://asistdl.onlinelibrary.wiley.com/doi/full/10.1002/asi.24648,"It moderates the exposure of information so that different information and resources get fair chances to receive users' attention. The optimization goal of diversity is to cover as many subtopics as possible for users. Fairness, in contrast, constraints how much exposure each subtopic should receive on the IR system side."
"Understanding Accuracy-Fairness Trade-offs in Re-ranking through
  Elasticity in Economics",2504.14991v1,deldjoo2022survey,\cite{deldjoo2022survey},A Survey of Research on Fair Recommender Systems,,,True,False,"Deldjoo, Yashar and Jannach, Dietmar and Bellogin, Alejandro and Difonzo, Alessandro and Zanzonelli, Dario",2022.0,,,,arXiv preprint arXiv:2205.11127,A Survey of Research on Fair Recommender Systems,A Survey of Research on Fair Recommender Systems,https://openreview.net/forum?id=K7emU6kWa9,"However, research on fairness in recommender systems is still a developing area. In this survey, we first review the fundamental concepts and notions of fairness that were put forward in the area in the recent past. Afterward, through a review of more than 160 scholarly publications, we present an overview of how research in this field is"
"Understanding Accuracy-Fairness Trade-offs in Re-ranking through
  Elasticity in Economics",2504.14991v1,Tang23FairBias,\cite{Tang23FairBias},When Fairness meets Bias: a Debiased Framework for Fairness aware Top-N Recommendation,,,True,False,"Tang, Jiakai and Shen, Shiqi and Wang, Zhipeng and Gong, Zhi and Zhang, Jingsen and Chen, Xu",2023.0,,,10.1145/3604915.3608770,,When Fairness meets Bias: a Debiased Framework for Fairness aware Top-N Recommendation,GitHub - jiawei-chen/RecDebiasing: This repository collects debiasing ...,https://github.com/jiawei-chen/RecDebiasing,When Fairness meets Bias a Debiased Framework for Fairness aware Top-N Recommendation. Recsys 2023. Two-sided Calibration for Quality-aware Responsible Recommendation. Recsys 2023. Rectifying Unfairness in Recommendation Feedback Loop. SIGIR 2023. Measuring Item Global Residual Value for Fair Recommendation. SIGIR 2023.
"Understanding Accuracy-Fairness Trade-offs in Re-ranking through
  Elasticity in Economics",2504.14991v1,xu2023p,\cite{xu2023p},P-MMF: Provider Max-min Fairness Re-ranking in Recommender System,,,True,False,"Xu, Chen and Chen, Sirui and Xu, Jun and Shen, Weiran and Zhang, Xiao and Wang, Gang and Dong, Zhenhua",2023.0,,,,,P-MMF: Provider Max-min Fairness Re-ranking in Recommender System,P-MMF: Provider Max-min Fairness Re-ranking in Recommender System,https://www.semanticscholar.org/paper/P-MMF:-Provider-Max-min-Fairness-Re-ranking-in-Xu-Chen/1e99d0818a46af3b245b45f22d24d9be7aaf3bf6/figure/2,"DOI: 10.1145/3543507.3583296 Corpus ID: 257496472; P-MMF: Provider Max-min Fairness Re-ranking in Recommender System @article{Xu2023PMMFPM, title={P-MMF: Provider Max-min Fairness Re-ranking in Recommender System}, author={Chen Xu and Sirui Chen and Jun‐rong Xu and Weiran Shen and Xiao Zhang and G. Wang and Zhen Dong}, journal={Proceedings of the ACM Web Conference 2023}, year={2023}, url"
"Understanding Accuracy-Fairness Trade-offs in Re-ranking through
  Elasticity in Economics",2504.14991v1,abdollahpouri2020multistakeholder,\cite{abdollahpouri2020multistakeholder},Multistakeholder Recommendation: Survey and Research Directions,,,True,False,"Abdollahpouri, Himan and Adomavicius, Gediminas and Burke, Robin and Guy, Ido and Jannach, Dietmar and Kamishima, Toshihiro and Krasnodebski, Jan and Pizzato, Luiz",2020.0,,,,User Modeling and User-Adapted Interaction,Multistakeholder Recommendation: Survey and Research Directions,Multistakeholder recommendation: Survey and research directions,https://link.springer.com/article/10.1007/s11257-019-09256-1,"Multistakeholder recommendation brings together research in a number of areas within the recommender systems community and beyond: (1) in economics, the areas of multisided platforms and fair division; (2) the growing interest in multiple objectives for recommender systems, including such concerns as fairness, diversity, and novelty; and (3) the application of personalization to matching problems."
"Understanding Accuracy-Fairness Trade-offs in Re-ranking through
  Elasticity in Economics",2504.14991v1,jaenich2024fairness,\cite{jaenich2024fairness},Fairness-Aware Exposure Allocation via Adaptive Reranking,,,True,False,"Jaenich, Thomas and McDonald, Graham and Ounis, Iadh",2024.0,,,,,Fairness-Aware Exposure Allocation via Adaptive Reranking,Fairness-Aware Exposure Allocation via Adaptive Reranking,https://dl.acm.org/doi/10.1145/3626772.3657794,"Our experiments on the TREC 2021 and 2022 Fair Ranking Track test collections show that our policies consistently improve the fairness of the exposure distribution in the final ranking, compared to standard adaptive re-ranking approaches, resulting in increases of up to ~13% in Attention Weighted Ranked Fairness (AWRF)."
"Understanding Accuracy-Fairness Trade-offs in Re-ranking through
  Elasticity in Economics",2504.14991v1,zafar2019fairness,\cite{zafar2019fairness},Fairness Constraints: A Flexible Approach for Fair Classification,,,True,False,"Zafar, Muhammad Bilal and Valera, Isabel and Gomez-Rodriguez, Manuel and Gummadi, Krishna P",2019.0,,,,The Journal of Machine Learning Research,Fairness Constraints: A Flexible Approach for Fair Classification,Fairness Constraints: A Flexible Approach for Fair Classification,https://jmlr.org/papers/v20/18-262.html,"Fairness Constraints: A Flexible Approach for Fair Classification . Muhammad Bilal Zafar, ... we take a step forward to fulfill that need and introduce a flexible constraint-based framework to enable the design of fair margin-based classifiers. The main technical innovation of our framework is a general and intuitive measure of decision"
"Understanding Accuracy-Fairness Trade-offs in Re-ranking through
  Elasticity in Economics",2504.14991v1,lambert1992distribution,\cite{lambert1992distribution},The Distribution and Redistribution of Income,,,True,False,"Lambert, Peter J.",1992.0,,,,,The Distribution and Redistribution of Income,Redistribution of income and wealth - Wikipedia,https://en.wikipedia.org/wiki/Redistribution_of_income_and_wealth,"Redistribution of income and wealth is the transfer of income and wealth (including physical property) from some individuals to others through a social mechanism such as taxation, welfare, public services, land reform, monetary policies, confiscation, divorce or tort law. [1] The term typically refers to redistribution on an economy-wide basis rather than between selected individuals."
"Understanding Accuracy-Fairness Trade-offs in Re-ranking through
  Elasticity in Economics",2504.14991v1,hanlon2010review,\cite{hanlon2010review},A Review of Tax Research,,,True,False,"Hanlon, Michelle and Heitzman, Shane",2010.0,,,,Journal of accounting and Economics,A Review of Tax Research,"A Review of Tax Research by Michelle Hanlon, Shane Heitzman - SSRN",https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1476561,"Abstract. In this paper, we present a review of tax research. We survey four main areas of the literature: 1) the informational role of income tax expense reported for financial accounting, 2) corporate tax avoidance, 3) corporate decision-making including investment, capital structure, and organizational form, and 4) taxes and asset pricing."
"Understanding Accuracy-Fairness Trade-offs in Re-ranking through
  Elasticity in Economics",2504.14991v1,nerre2001concept,\cite{nerre2001concept},The Concept of Tax Culture,,,True,False,"Nerr{\'e}, Birger",2001.0,,,,,The Concept of Tax Culture,The Culture of Taxation: Definition and Conceptual Approaches for Tax ...,https://so03.tci-thaijo.org/index.php/jpss/article/download/102374/79229/258812,"summarized the concept of tax culture as a part of national culture. Tax culture is influenced by internal and external surroundings and is reflected in tax principles and tools. Based on recent definitions, the culture of taxation is a relationship between taxpayers and tax collectors in a tax system."
"Unconstrained Monotonic Calibration of Predictions in Deep Ranking
  Systems",2504.14243v1,HB,\cite{HB},Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers,,,True,False,"Zadrozny, Bianca and Elkan, Charles",2001.0,,https://dl.acm.org/doi/10.5555/645530.655658,,,Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers,Obtaining Calibrated Probability Estimates from Decision Trees and ...,https://www.researchgate.net/publication/2368094_Obtaining_Calibrated_Probability_Estimates_from_Decision_Trees_and_Naive_Bayesian_Classifiers,This paper presents simple but successful methods for obtaining calibrated probability estimates from decision tree and naive Bayesian classifiers. Using the large and challenging KDD'98 contest
"Unconstrained Monotonic Calibration of Predictions in Deep Ranking
  Systems",2504.14243v1,IR,\cite{IR},Transforming classifier scores into accurate multiclass probability estimates,,,True,False,"Zadrozny, Bianca and Elkan, Charles",2002.0,,https://doi.org/10.1145/775047.775151,10.1145/775047.775151,,Transforming classifier scores into accurate multiclass probability estimates,PDF,https://www.researchgate.net/profile/Charles-Elkan/publication/2571315_Transforming_Classifier_Scores_into_Accurate_Multiclass_Probability_Estimates/links/0fcfd509ae852a8bb9000000/Transforming-Classifier-Scores-into-Accurate-Multiclass-Probability-Estimates.pdf,"Transforming Classiﬁer Scores into Accurate Multiclass Probability Estimates Bianca Zadrozny and Charles Elkan Department of Computer Science and Engineering 0114 University of California, San Diego"
"Unconstrained Monotonic Calibration of Predictions in Deep Ranking
  Systems",2504.14243v1,SIR,\cite{SIR},Calibrating User Response Predictions in Online Advertising,,,True,False,"Deng, Chao and Wang, Hao and Tan, Qing and Xu, Jian and Gai, Kun",2020.0,,https://doi.org/10.1007/978-3-030-67667-4_13,10.1007/978-3-030-67667-4_13,,Calibrating User Response Predictions in Online Advertising,PDF,https://link.springer.com/content/pdf/10.1007/978-3-030-67667-4_13.pdf,"This paper presents a comprehensive calibration solution for online advertising, which transforms predicted probabilities to posterior probabilities. It proposes a Smoothed Isotonic Regression algorithm to handle data sparsity and a Post-Click Conversion Estimation Model to deal with delayed responses."
"Unconstrained Monotonic Calibration of Predictions in Deep Ranking
  Systems",2504.14243v1,PlattScaling,\cite{PlattScaling},Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods,,,True,False,"Platt, John and others",1999.0,,https://home.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf,,Advances in large margin classifiers,Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods,PDF,https://users.cs.fiu.edu/~sjha/class2023/Lecture7/Slides/1999PlattScaling.pdf,"Learn how to create probabilistic outputs for support vector machines using a sigmoid function that maps the SVM outputs into posterior probabilities. Compare the Platt scaling approach with other methods that use logit link, Gaussian fit, or regularized likelihood."
"Unconstrained Monotonic Calibration of Predictions in Deep Ranking
  Systems",2504.14243v1,BetaCalib,\cite{BetaCalib},Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers,,,True,False,"Kull, Meelis and Silva Filho, Telmo and Flach, Peter",2017.0,,http://proceedings.mlr.press/v54/kull17a.html,,,Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers,betacal by REFRAME,https://reframe.github.io/betacal/,"Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers. Meelis Kull, Telmo de Menezes e Silva Filho and Peter Flach. For optimal decision making under variable class distributions and misclassification costs a classifier needs to produce well-calibrated estimates of the posterior"
"Unconstrained Monotonic Calibration of Predictions in Deep Ranking
  Systems",2504.14243v1,error,\cite{error},"On the error of linear interpolation and the orientation, aspect ratio, and internal angles of a triangle",,,True,False,"Cao, Weiming",2005.0,,https://dl.acm.org/doi/abs/10.1137/S0036142903433492,,SIAM journal on numerical analysis,"On the error of linear interpolation and the orientation, aspect ratio, and internal angles of a triangle","On the Error of Linear Interpolation and the Orientation, Aspect Ratio ...",https://epubs.siam.org/doi/10.1137/S0036142903433492,Get full access to this article. View all available purchase options and get full access to this article.
"Unconstrained Monotonic Calibration of Predictions in Deep Ranking
  Systems",2504.14243v1,ScaleCalib,\cite{ScaleCalib},Scale Calibration of Deep Ranking Models,,,True,False,"Yan, Le and Qin, Zhen and Wang, Xuanhui and Bendersky, Michael and Najork, Marc",2022.0,,https://doi.org/10.1145/3534678.3539072,10.1145/3534678.3539072,,Scale Calibration of Deep Ranking Models,PDF,https://marc.najork.org/papers/kdd2022-calrank.pdf,"Ranking scale calibration; Learning-to-rank; Sponsored search ACM Reference Format: Le Yan, Zhen Qin, Xuanhui Wang, Michael Bendersky, and Marc Najork. 2022. Scale Calibration of Deep Ranking Models. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD This work is licensed under a Creative Commons"
"Unconstrained Monotonic Calibration of Predictions in Deep Ranking
  Systems",2504.14243v1,BBP,\cite{BBP},Beyond Binary Preference: Leveraging Bayesian Approaches for Joint Optimization of Ranking and Calibration,,,True,False,"Liu, Chang and Wang, Qiwei and Lin, Wenqing and Ding, Yue and Lu, Hongtao",2024.0,,https://doi.org/10.1145/3637528.3671577,10.1145/3637528.3671577,,Beyond Binary Preference: Leveraging Bayesian Approaches for Joint Optimization of Ranking and Calibration,PDF,https://edwlin.github.io/pubs/kdd2024-bbp.pdf,"Beyond Binary Preference: Leveraging Bayesian Approaches for Joint Optimization of Ranking and Calibration ChangLiu∗† isonomialiu@sjtu.edu.cn ShanghaiJiaoTongUniversity Shanghai,China QiweiWang∗ shimmerwang@tencent.com Tencent,Shenzhen,China WenqingLin‡ edwlin@tencent.com Tencent,Shenzhen,China YueDing‡ dingyue@sjtu.edu.cn"
"FashionDPO:Fine-tune Fashion Outfit Generation Model using Direct
  Preference Optimization",2504.12900v1,PORAnchors,\cite{PORAnchors},Personalized Outfit Recommendation With Learnable Anchors,,,True,False,"Zhi Lu and
                  Yang Hu and
                  Yan Chen and
                  Bing Zeng",2021.0,,,,,Personalized Outfit Recommendation With Learnable Anchors,Personalized Outfit Recommendation With Learnable Anchors,https://openaccess.thecvf.com/content/CVPR2021/html/Lu_Personalized_Outfit_Recommendation_With_Learnable_Anchors_CVPR_2021_paper.html,"In this work, we propose a new solution for personalized outfit recommendation that is capable of handling this case. We use a stacked self-attention mechanism to model the high-order interactions among the items. ... To accommodate the variety of users' preferences, we characterize each user with a set of anchors, i.e. a group of learnable"
"FashionDPO:Fine-tune Fashion Outfit Generation Model using Direct
  Preference Optimization",2504.12900v1,A-FKG,\cite{A-FKG},"{\textdollar}A{\^{}}3{\textdollar}-FKG: Attentive Attribute-Aware
                  Fashion Knowledge Graph for Outfit Preference Prediction",,,True,False,"Huijing Zhan and
                  Jie Lin and
                  Kenan Emir Ak and
                  Boxin Shi and
                  Ling{-}Yu Duan and
                  Alex C. Kot",2022.0,,,,{IEEE} Trans. Multim.,"{\textdollar}A{\^{}}3{\textdollar}-FKG: Attentive Attribute-Aware
                  Fashion Knowledge Graph for Outfit Preference Prediction",yasdel/FashionXrecsys: Fashion Recommender Systems - GitHub,https://github.com/yasdel/FashionXrecsys,A3-FKG: Attentive Attribute-Aware Fashion Knowledge Graph for Outfit Preference Prediction: IEEE Transactions on Multimedia: U-I Interaction: 2020: Banerjee et al. BOXREC: Recommending a Box of Preferred Outfits in Online Shopping: ACM Transactions on Intelligent Systems and Technology: U-I features: Link: Link Link: 2020: Li et al.
"FashionDPO:Fine-tune Fashion Outfit Generation Model using Direct
  Preference Optimization",2504.12900v1,personalCom,\cite{personalCom},Personalized Capsule Wardrobe Creation with Garment and User Modeling,,,True,False,"Xue Dong and
                  Xuemeng Song and
                  Fuli Feng and
                  Peiguang Jing and
                  Xin{-}Shun Xu and
                  Liqiang Nie",2019.0,,,,,Personalized Capsule Wardrobe Creation with Garment and User Modeling,Personalized Capsule Wardrobe Creation with Garment and User Modeling ...,https://dl.acm.org/doi/10.1145/3343031.3350905,"To this end, we introduce a combinatorial optimization-based personalized capsule wardrobe creation framework, named PCW-DC, which jointly integrates both garment modeling (\textiti.e., wardrobe compatibility) and user modeling (\textiti.e., preferences, body shapes)."
"FashionDPO:Fine-tune Fashion Outfit Generation Model using Direct
  Preference Optimization",2504.12900v1,PFOG,\cite{PFOG},Personalized fashion outfit generation with user coordination preference learning,,,True,False,"Yujuan Ding and
                  P. Y. Mok and
                  Yunshan Ma and
                  Yi Bin",2023.0,,,,Inf. Process. Manag.,Personalized fashion outfit generation with user coordination preference learning,POG: Personalized Outfit Generation for Fashion Recommendation at ...,https://dl.acm.org/doi/10.1145/3292500.3330652,"This work represents a first step towards an industrial-scale fashion outfit generation and recommendation solution, which goes beyond generating outfits based on explicit queries, or merely recommending from existing outfit pools. ... Personalized fashion outfit generation with user coordination preference learning. Abstract . This paper"
"FashionDPO:Fine-tune Fashion Outfit Generation Model using Direct
  Preference Optimization",2504.12900v1,SD,\cite{SD},High-Resolution Image Synthesis with Latent Diffusion Models,,,True,False,"Robin Rombach and
                  Andreas Blattmann and
                  Dominik Lorenz and
                  Patrick Esser and
                  Bj{\""{o}}rn Ommer",2022.0,,,,,High-Resolution Image Synthesis with Latent Diffusion Models,High-Resolution Image Synthesis With Latent Diffusion Models,https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html,"High-Resolution Image Synthesis With Latent Diffusion Models  By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. @InProceedings{Rombach_2022_CVPR, author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\""orn}, title = {High-Resolution Image Synthesis With Latent Diffusion Models}, booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, month = {June}, year = {2022}, pages = {10684-10695} }"
"FashionDPO:Fine-tune Fashion Outfit Generation Model using Direct
  Preference Optimization",2504.12900v1,lora,\cite{lora},"QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language
                  Models",,,True,False,"Yuhui Xu and
                  Lingxi Xie and
                  Xiaotao Gu and
                  Xin Chen and
                  Heng Chang and
                  Hengheng Zhang and
                  Zhengsu Chen and
                  Xiaopeng Zhang and
                  Qi Tian",2024.0,,,,,"QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language
                  Models",QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models,https://arxiv.org/abs/2309.14717,"QA-LoRA is a method to reduce the computational burden of large language models (LLMs) by quantizing and adapting them. The paper introduces the algorithm, its implementation, and its applications to LLaMA and LLaMA2 models."
"FashionDPO:Fine-tune Fashion Outfit Generation Model using Direct
  Preference Optimization",2504.12900v1,yang2018recommendation,\cite{yang2018recommendation},From recommendation to generation: A novel fashion clothing advising framework,,,True,False,"Yang, Zilin and Su, Zhuo and Yang, Yang and Lin, Ge",2018.0,,,,,From recommendation to generation: A novel fashion clothing advising framework,‪Zhuo Su‬ - ‪Google Scholar‬,https://scholar.google.com/citations?user=TGd0JnQAAAAJ&hl=en,"From recommendation to generation: A novel fashion clothing advising framework. Z Yang, Z Su, Y Yang, G Lin ... 2018. 27: 2018: A data-driven editing framework for automatic 3D garment modeling. L Liu, Z Su, X Fu, L Liu, R Wang, X Luo. Multimedia Tools and Applications 76 ... Research and implementation of personalized clothing recommendation"
"FashionDPO:Fine-tune Fashion Outfit Generation Model using Direct
  Preference Optimization",2504.12900v1,GP-VTON,\cite{GP-VTON},"{GP-VTON:} Towards General Purpose Virtual Try-On via Collaborative
                  Local-Flow Global-Parsing Learning",,,True,False,"Zhenyu Xie and
                  Zaiyu Huang and
                  Xin Dong and
                  Fuwei Zhao and
                  Haoye Dong and
                  Xijin Zhang and
                  Feida Zhu and
                  Xiaodan Liang",2023.0,,,,,"{GP-VTON:} Towards General Purpose Virtual Try-On via Collaborative
                  Local-Flow Global-Parsing Learning",,,
"FashionDPO:Fine-tune Fashion Outfit Generation Model using Direct
  Preference Optimization",2504.12900v1,BDPO,\cite{BDPO},"Boost Your Own Human Image Generation Model via Direct Preference
                  Optimization with {AI} Feedback",,,True,False,"Sanghyeon Na and
                  Yonggyu Kim and
                  Hyunjoon Lee",2024.0,,,,CoRR,"Boost Your Own Human Image Generation Model via Direct Preference
                  Optimization with {AI} Feedback",Boost Your Own Human Image Generation Model via Direct Preference ...,https://huggingface.co/papers/2405.20216,"Boost Your Own Human Image Generation Model via Direct Preference Optimization with AI Feedback. Published on May 30. Upvote -Authors: Sanghyeon Na, ... for constructing a specialized DPO dataset for training human image generation models without the need for costly human feedback. We also propose a modified loss function that enhances the DPO"
"FashionDPO:Fine-tune Fashion Outfit Generation Model using Direct
  Preference Optimization",2504.12900v1,SPO,\cite{SPO},"Step-aware Preference Optimization: Aligning Preference with Denoising
                  Performance at Each Step",,,True,False,"Zhanhao Liang and
                  Yuhui Yuan and
                  Shuyang Gu and
                  Bohan Chen and
                  Tiankai Hang and
                  Ji Li and
                  Liang Zheng",2024.0,,,,CoRR,"Step-aware Preference Optimization: Aligning Preference with Denoising
                  Performance at Each Step",SPO-Diffusion-Models/Step-Aware_Preference_Models - Hugging Face,https://huggingface.co/SPO-Diffusion-Models/Step-Aware_Preference_Models,"Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step Abstract Recently, Direct Preference Optimization (DPO) has extended its success from aligning large language models (LLMs) to aligning text-to-image diffusion models with human preferences. Unlike most existing DPO methods that assume all diffusion"
Constrained Auto-Regressive Decoding Constrains Generative Retrieval,2504.09935v1,white2025surveyinformationaccess,\cite{white2025surveyinformationaccess},Information Access in the Era of Generative AI,,,True,False,Ryen W. White and Chirag Shah,,,https://doi.org/10.1007/978-3-031-73147-1,,,Information Access in the Era of Generative AI,Information Access in the Era of Generative AI (The Information ...,https://www.amazon.com/Information-Access-Era-Generative-Retrieval/dp/3031731468,"This book discusses GenAI and its role in information access - often referred to as Generative Information Retrieval (GenIR) - or more broadly, information interaction. The role of GenAI in information access is complex and dynamic, with many dimensions."
Constrained Auto-Regressive Decoding Constrains Generative Retrieval,2504.09935v1,Zhang2023TermSetsCB,\cite{Zhang2023TermSetsCB},Term-Sets Can Be Strong Document Identifiers For Auto-Regressive Search Engines,,,True,False,Peitian Zhang and Zheng Liu and Yujia Zhou and Zhicheng Dou and Zhao Cao,,,https://api.semanticscholar.org/CorpusID:258841428,,ArXiv,Term-Sets Can Be Strong Document Identifiers For Auto-Regressive Search Engines,Term-Sets Can Be Strong Document Identifiers For Auto-Regressive Search ...,https://arxiv.org/abs/2305.13859v1,"Auto-regressive search engines emerge as a promising paradigm for next-gen information retrieval systems. These methods work with Seq2Seq models, where each query can be directly mapped to the identifier of its relevant document. As such, they are praised for merits like being end-to-end differentiable. However, auto-regressive search engines also confront challenges in retrieval quality"
Constrained Auto-Regressive Decoding Constrains Generative Retrieval,2504.09935v1,dynamic-retriever2023,\cite{dynamic-retriever2023},DynamicRetriever: A Pre-trained Model-based IR System Without an Explicit Index,,,True,False,Yujia Zhou and Jing Yao and Zhicheng Dou and Ledell Wu and Ji-Rong Wen,,April,https://doi.org/10.1007/s11633-022-1373-9,,Mach. Intell. Res.,DynamicRetriever: A Pre-trained Model-based IR System Without an Explicit Index,DynamicRetriever: A Pre-trained Model-based IR System Without an ...,https://link.springer.com/article/10.1007/s11633-022-1373-9,"Web search provides a promising way for people to obtain information and has been extensively studied. With the surge of deep learning and large-scale pre-training techniques, various neural information retrieval models are proposed, and they have demonstrated the power for improving search (especially, the ranking) quality. All these existing search methods follow a common paradigm, i.e"
Constrained Auto-Regressive Decoding Constrains Generative Retrieval,2504.09935v1,liuRobustnessGenerativeRetrieval2023,\cite{liuRobustnessGenerativeRetrieval2023},On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective,,,True,False,Yu{-}An Liu and Ruqing Zhang and Jiafeng Guo and Wei Chen and Xueqi Cheng,,,https://doi.org/10.48550/arXiv.2306.12756,10.48550/ARXIV.2306.12756,CoRR,On the Robustness of Generative Retrieval Models: An Out-of-Distribution Perspective,On the Robustness of Generative Information Retrieval Models: An Out-of ...,https://link.springer.com/chapter/10.1007/978-3-031-88711-6_26,"In this paper, we have analyzed the out-of-distribution robustness of several representative generative and dense retrieval models on the KILT benchmark. Specifically, we have proposed four perspectives to define out-of-distribution robustness. Our results exposed significant vulnerabilities in OOD robustness of generative IR models."
Constrained Auto-Regressive Decoding Constrains Generative Retrieval,2504.09935v1,wangNOVOLearnableInterpretable2023,\cite{wangNOVOLearnableInterpretable2023},NOVO: Learnable and Interpretable Document Identifiers for Model-Based IR,,,True,False,"Wang, Zihan and Zhou, Yujia and Tu, Yiteng and Dou, Zhicheng",,,https://doi.org/10.1145/3583780.3614993,10.1145/3583780.3614993,,NOVO: Learnable and Interpretable Document Identifiers for Model-Based IR,ZihanWang314/NOVO - GitHub,https://github.com/ZihanWang314/NOVO,Contribute to ZihanWang314/NOVO development by creating an account on GitHub. Code for our CKIM 2023 Long Paper NOVO: Learnable and Interpretable Document Identifiers for Model-Based IR.
Constrained Auto-Regressive Decoding Constrains Generative Retrieval,2504.09935v1,mehtaDSIpp2023,\cite{mehtaDSIpp2023},{DSI}++: Updating Transformer Memory with New Documents,,,True,False,"Mehta, Sanket Vaibhav  and Gupta, Jai  and Tay, Yi  and Dehghani, Mostafa  and Tran, Vinh Q.  and Rao, Jinfeng  and Najork, Marc  and Strubell, Emma  and Metzler, Donald",,,https://aclanthology.org/2023.emnlp-main.510/,10.18653/v1/2023.emnlp-main.510,,{DSI}++: Updating Transformer Memory with New Documents,DSI++: Updating Transformer Memory with New Documents,https://aclanthology.org/2023.emnlp-main.510/,"DSI++: Updating Transformer Memory with New Documents - ACL Anthology Anthology ID:2023.emnlp-main.510 Volume:Proceedings of the 2023 Conference on Empirical Methods in Natural Language ProcessingMonth:December Year:2023 Address:Singapore Editors:Houda Bouamor, Juan Pino, Kalika BaliVenue:EMNLPSIG:Publisher:Association for Computational Linguistics Note:Pages:8198–8213 Language:URL:https://aclanthology.org/2023.emnlp-main.510/DOI:10.18653/v1/2023.emnlp-main.510Bibkey:mehta-etal-2023-dsi Cite (ACL):Sanket Vaibhav Mehta, Jai Gupta, Yi Tay, Mostafa Dehghani, Vinh Q. Association for Computational Linguistics.Cite (Informal):DSI++: Updating Transformer Memory with New Documents (Mehta et al., EMNLP 2023)Copy Citation:BibTeX Markdown MODS XML Endnote More options…PDF:https://aclanthology.org/2023.emnlp-main.510.pdfVideo:https://aclanthology.org/2023.emnlp-main.510.mp4 title = ""{DSI}++: Updating Transformer Memory with New Documents"", <title>DSI++: Updating Transformer Memory with New Documents</title> <namePart type=""family"">Mehta</namePart> <namePart type=""given"">Houda</namePart> <namePart type=""given"">Juan</namePart> <namePart type=""given"">Kalika</namePart> DSI++: Updating Transformer Memory with New Documents (Mehta et al., EMNLP 2023) *   DSI++: Updating Transformer Memory with New Documents (Mehta et al., EMNLP 2023)"
Constrained Auto-Regressive Decoding Constrains Generative Retrieval,2504.09935v1,guoContinualGenerative2024,\cite{guoContinualGenerative2024},CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks,,,True,False,Jiafeng Guo and Changjiang Zhou and Ruqing Zhang and Jiangui Chen and Maarten de Rijke and Yixing Fan and Xueqi Cheng,,,https://arxiv.org/abs/2402.16767,,,CorpusBrain++: A Continual Generative Pre-Training Framework for Knowledge-Intensive Language Tasks,CorpusBrain++: A Continual Generative Pre-Training Framework for ...,https://arxiv.org/html/2402.16767v1,"Knowledge-intensive language tasks (KILTs) refer to a series of language-related tasks that require access to external knowledge sources such as Wikipedia for accurate answer generation (Petroni et al., 2020).In current mainstream approaches, a two-step process is commonly employed (Chen et al., 2017; Kwiatkowski et al., 2019; Yang et al., 2018), consisting of a retriever and a reader."
Constrained Auto-Regressive Decoding Constrains Generative Retrieval,2504.09935v1,nishinoUnderstandingCV2025,\cite{nishinoUnderstandingCV2025},Understanding the impact of introducing constraints at inference time on generalization error,,,True,False,"Nishino, Masaaki and Nakamura, Kengo and Yasuda, Norihito",,,,,,Understanding the impact of introducing constraints at inference time on generalization error,Understanding the Impact of Introducing Constraints at Inference Time ...,https://proceedings.mlr.press/v235/nishino24a.html,"@InProceedings{pmlr-v235-nishino24a, title = {Understanding the Impact of Introducing Constraints at Inference Time on Generalization Error}, author = {Nishino, Masaaki and Nakamura, Kengo and Yasuda, Norihito}, booktitle = {Proceedings of the 41st International Conference on Machine Learning}, pages = {38253--38263}, year = {2024}, editor = {Salakhutdinov, Ruslan and Kolter, Zico and Heller"
